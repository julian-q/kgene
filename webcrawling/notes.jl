{"title": "Evaluating Multimodal Interactive Agents", "contents": "To train agents to interact well with humans, we need to be able to measure progress. But human interaction is complex and measuring progress is difficult. In this work we developed a method, called the Standardised Test Suite (STS), for evaluating agents in temporally extended, multi-modal interactions. We examined interactions that consist of human participants asking agents to perform tasks and answer questions in a 3D simulated environment.\nThe STS methodology places agents in a set of behavioural scenarios mined from real human interaction data. Agents see a replayed scenario context, receive an instruction, and are then given control to complete the interaction offline. These agent continuations are recorded and then sent to human raters to annotate as success or failure. Agents are then ranked according to the proportion of scenarios on which they succeed.\nMany of the behaviours that are second nature to humans in our day-to-day interactions are difficult to put into words, and impossible to formalise. Thus, the mechanism relied on for solving games (like Atari, Go, DotA, and Starcraft) with reinforcement learning won't work when we try to teach agents to have fluid and successful interactions with humans. For example, think about the difference between these two questions: \"Who won this game of Go?\" versus \"What are you looking at?\" In the first case, we can write a piece of computer code that counts the stones on the board at the end of the game and determines the winner with certainty. In the second case, we have no idea how to codify this: the answer may depend on the speakers, the size and shapes of the objects involved, whether the speaker is joking, and other aspects of the context in which the utterance is given. Humans intuitively understand the myriad of relevant factors involved in answering this seemingly mundane question.\nInteractive evaluation by human participants can serve as a touchstone for understanding agent performance, but this is noisy and expensive. It is difficult to control the exact instructions that humans give to agents when interacting with them for evaluation. This kind of evaluation is also in real-time, so it is too slow to rely on for swift progress. Previous works have relied on proxies to interactive evaluation. Proxies, such as losses and scripted probe tasks (e.g. \u201clift the x\u201d where x is randomly selected from the environment and the success function is painstakingly hand-crafted), are useful for gaining insight into agents quickly, but don\u2019t actually correlate that well with interactive evaluation. Our new method has advantages, mainly affording control and speed to a metric that closely aligns with our ultimate goal - to create agents that interact well with humans.\nThe development of MNIST, ImageNet and other human-annotated datasets has been essential for progress in machine learning. These datasets have allowed researchers to train and evaluate classification models for a one-time cost of human inputs. The STS methodology aims to do the same for human-agent interaction research. This evaluation method still requires humans to annotate agent continuations; however, early experiments suggest that automation of these annotations may be possible, which would enable fast and effective automated evaluation of interactive agents. In the meantime, we hope that other researchers can use the methodology and system design to accelerate their own research in this area.\n"}
{"title": "Advocating for the LGBTQ+ community in AI research", "contents": "Research scientist, Kevin McKee, tells how his early love of science fiction and social psychology inspired his career, and how he\u2019s helping advance research in \u2018queer fairness\u2019, support human-AI collaboration, and study the effects of AI on the LGBTQ+ community.\nThe signs were clear, right from the start. I\u2019ve always loved science fiction. I couldn\u2019t tell you how many times I read and reread Isaac Asimov\u2019s \nI, Robot\n as a kid. These short stories explore the psychology of Asimov\u2019s fictional robots, frequently using them as a mirror to uncover insights about the human mind. I was completely enthralled.\nIt\u2019s no surprise that I took an early interest in psychological science. In elementary school, I often tried running controlled psychology experiments for my science projects. Looking back, I\u2019m not sure how successful I was with those experiments, but they led me to my studies in psychology and neuroscience \u2013 and then eventually to DeepMind.\nEveryone at DeepMind gets to work on an absurdly diverse set of projects. Much of our work is driven from the bottom up, so DeepMinders frequently get invited to collaborate on exciting projects from across the organisation.\u00a0\nMy current projects span traditional machine learning methods and social science approaches; research on cooperative AI and the social implications of AI development; and collaborations with engineers, mathematicians, and ethicists.\nI co-lead QueerMinds, our employee resource group for LGBTQ+ employees and allies. When I joined DeepMind, in 2017, we didn't have a formal community or an official space for identities like mine. Over time, I realised that as someone queer myself, I could help create that visibility and foster that community for others at DeepMind.\u00a0\nQueerMinds feels vibrant these days, with regular socials, talks by external researchers and authors, and group field trips, including a recent one to the new queer \nQueer Britain\n, the new queer museum next to our King\u2019s Cross office. Since stepping into the role, I haven\u2019t regretted it for a moment. It\u2019s been a huge joy \u2013 and a continuous learning experience \u2013 to create a space for the queer people in DeepMind's community.\nI prefer working from the office. It\u2019s really energising to see my teammates and random DeepMinders every day. These are known as \u2018weak ties\u2019 in social psychology and sociology, and they definitely inject my day with a lot of happiness.\u00a0\nIn research, I find a lot of breakthroughs come from spontaneous conversations and unplanned moments \u2013 you never know where the next idea or collaboration will come from. Just chatting through the current challenge with a teammate over coffee is often enough to catalyse a lightbulb moment.\nWhen we talk about our goals as an organisation, we often frame the conversation around the motivation of \u2018advancing science and benefiting humanity\u2019. It\u2019s amazing to be on a team committed to those aims. In working toward them, I think we have a real chance to include groups that historically have been excluded from scientific work. If we bring marginalised communities into the agenda-setting process for our work, what sorts of research questions and priorities will we establish?\u00a0\nAI and machine learning can make a difference, even in small ways. My sister is a speech-language pathologist who works with trans teens to help them develop their voices and communication in a way that affirms their gender identities. Recent advances in AI research show a lot of promise for supporting her and others working with queer communities. For example, generative models could help trans patients form realistic, healthy targets for their voice exercises in therapy sessions.\u00a0\nIt\u2019s a tie between two projects. First, a paper I worked on about \u2018\nqueer fairness\n\u2019, where we advocated for more research to understand the effects of AI on LGBTQ+ communities. AI development creates both new opportunities and serious risks for queer people. Yet, most work aimed at measuring and correcting algorithmic bias \u2013 what AI scientists call \u2018algorithmic fairness\u2019 research \u2013 tends to overlook LGBTQ+ communities. My co-authors and I reviewed potential points of promise and concern across areas like privacy, censorship, and mental health.\u00a0\nSecond, is an ongoing project on cooperative AI, which we talk about in the podcast episode \nBetter together\n. Humans are actually fairly good at cooperating with each other, even in the face of the incentive or motivation to act selfishly.\u00a0\nIn social psychology, one popular model of human altruism argues that humans pay attention not just to our own goals and outcomes, but also to the goals and outcomes of those around us \u2013 especially those with whom we have close relationships, like friends and family. If I\u2019m picking up lunch for a friend and myself, I\u2019ll probably skip the sandwich shop that I like but he hates. Instead, I\u2019ll likely find one that we both like, because I care about his happiness and rewards. That sort of \u2018reward sharing\u2019 is key to human altruism, and potentially to our close relationships, too.\u00a0\nDrawing inspiration from this \nreward sharing model\n, my co-authors and I developed \ncooperative AI agents that humans can interact with\n. They\u2019re really fun to play with. As a cherry on top, one of the games we used for studying \nhuman-AI collaboration\n is actually my friends\u2019 and my favourite to play outside work: \nOvercooked!\nI\u2019m an avid surfer. I grew up in California, so I was a bit worried about the surfing prospects when moving to London. Turns out that it\u2019s a quick jump to Portugal and Spain, where there are awesome waves. Some of my friends even swear that surfing in Cornwall is first class! We try to make a trip every few months, for a long weekend or a full week on the beach.\nDon\u2019t be afraid to take big jumps! Before joining DeepMind, my entire life \u2013 my career, family, and friends \u2013 was based in the US. Moving to the UK felt a bit daunting. Five years in, I can confidently say that making the jump to London was one of the best decisions I\u2019ve ever made.\nLearn more about research at DeepMind and search for open roles today\n"}
{"title": "A Generalist Agent", "contents": "Inspired by progress in large-scale language modelling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens.\nDuring the training phase of Gato, data from different tasks and modalities are serialised into a flat sequence of tokens, batched, and processed by a transformer neural network similar to a large language model. The loss is masked so that Gato only predicts action and text targets.\nWhen deploying Gato, a prompt, such as a demonstration, is tokenised, forming the initial sequence. Next, the environment yields the first observation, which is also tokenised and appended to the sequence. Gato samples the action vector autoregressively, one token at a time.\nOnce all tokens comprising the action vector have been sampled (determined by the action specification of the environment), the action is decoded and sent to the environment which steps and yields a new observation. Then the procedure repeats. The model always sees all previous observations and actions within its context window of 1024 tokens.\nGato is trained on a large number of datasets comprising agent experience in both simulated and real-world environments, in addition to a variety of natural language and image datasets. The number of tasks, where the performance of the pretrained Gato model is above a percentage of expert score, grouped by domain, is shown here.\nThe following images also show how the pre-trained Gato model with the same weights can do image captioning, engage in an interactive dialogue, and control a robot arm, among many other tasks.\n"}
{"title": "Bridging DeepMind research with Alphabet products", "contents": "For today's \"Five minutes with\" \nwe caught up with Gemma Jennings, a product manager on the Applied team, who led a session on vision language models at the \nAI Summit\n - one of the world\u2019s largest AI events for business.\nI\u2019m a part of the Applied team, which helps bring DeepMind technology to the outside world through Alphabet and Google products and solutions, like with \nWaveNet\n and Google Assistant, Maps, and Search. As a product manager, I act as a bridge between the two organisations, working very closely with both teams to understand the research and how people can use it. Ultimately, we want to be able to answer the question: How can we use this technology to improve the lives of people around the world?\nI\u2019m particularly excited about our portfolio of sustainability work. We\u2019ve already helped reduce the amount of energy needed to cool Google's data centres, but there\u2019s much more we can do to have a bigger, transformative impact within sustainability.\nI worked at John Lewis Partnership, a UK department store that has a strong sense of purpose built into its DNA. I\u2019ve always liked being part of a company with a sense of societal purpose, so DeepMind\u2019s mission of solving intelligence to advance science and benefit humanity really resonated with me. I was intrigued to learn how that ethos would manifest within a research-led organisation \u2013 and within Google, one of the largest companies in the world. Adding this to my academic background in experimental psychology, neuroscience, and statistics, DeepMind ticked all the boxes.\nIs my first in-person conference in almost three years, so I\u2019m really keen to meet people in the same industry as myself and to hear what other organisations are working on.\nI\u2019m looking forward to attending a few talks from the quantum computing track to learn more about. It has the potential to drive the next big paradigm shift in computing power, unlocking new use cases for applying AI in the world and allowing us to work on larger, more complex problems.\nMy work involves a lot of deep learning methods and it\u2019s always exciting to hear about the different ways people are using this technology. At the moment, these types of models require training on large amounts of data \u2013 which can be costly, time consuming, and resource intensive given the amount of computing needed. So where do we go from here? And what does the future of deep learning look like? These are the types of questions I\u2019m looking to answer.\nImage Recognition Using Deep Neural Networks, our recently \npublished research\n on vision language models (VLMs). For my presentation, I discussed recent advances in fusing large language models (LLMs) with powerful visual representations to progress the state of the art for image recognition.\u00a0\nThis fascinating research has so many potential uses in the real world. It could, one day, act as an assistant to support classroom and informal learning in schools, or help people with blindness or low vision see the world around them, transforming their day-to-day lives.\nWith a better understanding of what happens after the research breakthrough is announced. There\u2019s so much amazing research being done but we need to think about what comes next, like what global problems could we help solve? And how can we use our research to create products and services that have a purpose?\nThe future is bright and I\u2019m excited to discover new ways of applying our groundbreaking research to help benefit millions of people around the world.\nLearn more about research at DeepMind and search for open roles today\n"}
{"title": "Kyrgyzstan to King\u2019s Cross: the star baker cooking up code", "contents": "Today we caught up with Aliya Rysbek, a software engineer on the Platform team. She spoke to us about her path from Central Asia to DeepMind, and her endless curiosity for learning.\n\u00a0\nOur team is working on a custom project management system for organising all of DeepMind\u2019s research projects. I\u2019m a full-stack developer, so I build the system\u2019s components, like showing team structure, and various backend services to add and improve the product functionality. This system helps to plan and track projects, see who\u2019s working on what, and connect people. The goal is to make it easier for all DeepMinders to do their work.\nMy day can vary, it really depends on which phase of the project I'm on. Let\u2019s say we want to add a feature to our product \u2013 my tasks could range from designing solutions and working with the team to find the best one, to deploying new features into production and doing maintenance. Along the way, I\u2019ll communicate changes to our stakeholders, write docs, code and test solutions, build analytics dashboards, clean-up old code, and fix bugs.\nTypically, I work from the office, so after a quick snack or breakfast, I try to concentrate on critical work for around two hours. Then our team usually has lunch together, which is really nice after working remotely for so long. My afternoon hours are the most productive, so I put my headphones on and jump into coding.\nThe end of the day usually consists of a break, some team chit-chat, and a walk around the office to work through solutions or get inspiration. We have such a beautiful library in our office. It\u2019s inspiring just to look at the huge selection of books and to be reminded of what else there is to do in life \u2013 there\u2019s still so much more to learn and so many complex problems to solve.\u00a0\nBefore joining DeepMind, I mentored tech students and high-schoolers in my free time, helping them prepare CVs and do mock interviews. After I started working here, quite a few non-profit organisations and local projects reached out looking for the same kind of support. I\u2019ve really enjoyed it and feel like I\u2019m having a bigger impact than I could before.\u00a0\nSome people I helped are already working at different tech companies, including two interns who are starting this month at DeepMind \u2013 yay! I really love that I\u2019m in a position to share resources, time, and skills back with the community. As a female software engineer from a developing country, I know the importance of bringing more diversity and inclusion into the workplace.\u00a0\nI\u2019m from Kyrgyzstan \u2013 getting this role was such a big deal where I\u2019m from that I even made it onto the \nlocal news\n.\nEnglish isn\u2019t my native language and is hardly taught in my home country, if at all. But I\u2019ve been really lucky. Back at home I received various scholarships that paid for my time at a private high school where, thanks to my amazing teachers, I was able to improve my English within months and further explore my interest in maths. I even ended up entering and placing 5th in a country-wide olympiad competition while I was there.\u00a0\nAfter that, I decided to study computer science and ended up getting my BSc in Computer Engineering at the Middle East Technical University in Turkey, and my MSc in Computer Science Engineering at the Budapest University of Technology in Hungary. Studying abroad wouldn\u2019t have been possible without scholarship support from the Turkish and Hungarian governments.\u00a0\nOnce I completed my studies, it was time to find a job, which was difficult. Fortunately, I had a mentor who was an ex-Googler and they spent months helping me prepare for interviews and survive more than 70 rejections from different organisations. Eventually, though, we were also able to celebrate five offers!\u00a0\nWith my computer engineering background and internship experience, I felt prepared going into the process. I spent a lot of time focusing on the coding interviews, and I found these resources really helpful:\nI\u2019ve always enjoyed cooking and baking but I wanted to upgrade my skills, so I recently signed up for advanced baking courses \u2013 they went really well! I made delicious pavlovas, croissants (the real kind with lots of butter), tarts, and complex cheesecakes.\u00a0 My friends who were in charge of tasting were very excited about my improved skills \u2013 I even became DeepMind\u2019s Star Baker this year!\u00a0\nI\u2019ve also returned to practising dance and Aikido, the modern Japanese martial art. Both of them keep me really active, which is important when you spend a lot of time behind a computer. Last but not least, I\u2019ve also been inspired to learn calligraphy and pottery. At DeepMind, you can take courses that don\u2019t have to relate to the work you do \u2013 it\u2019s been a dream for someone who loves to learn.\nIn general, I would love to see fair access to quality education globally and I truly believe that DeepMind is capable of contributing to this. There has already been so much work done through the \nscholarship programme\n and \nsummer school\n support but there\u2019s always more we can do. Global access to education is so important as it would unlock a great share of undiscovered talent and increase the overall quality of life.\u00a0\nMake connections! If you see something you\u2019re interested in, do your due diligence and reach out to learn more about the company, position, or project. The relationships you make are invaluable and may help you find the perfect place for your skills, interests, and values.\nTo ask questions. I believe that people inherently want to help others and will welcome curiosity. Once, during a careers talk, I was told that a human's default state is \u201cnot knowing\u201d. I really liked that, and it made me realise that we shouldn't be harsh on ourselves for not knowing something, but rather be graceful and feed our curiosity by learning from others.\nLearn more about engineering at DeepMind and search for open roles today\n"}
{"title": "From LEGO competitions to DeepMind's robotics lab", "contents": "Today\u2019s post is all about Akhil Raju, a software engineer on the robotics team. We originally met Akhil in season two of \nDeepMind: The Podcast\n, but we wanted to get to know him better and hear more about his path to DeepMind.\nWhen I was young, I thought about AI in the same way I thought about magic. Yes,\u00a0I wanted to hang out with R2-D2 and Optimus Prime \u2013 but I also wanted to go to Hogwarts. That was until I turned 12 and started participating in LEGO robotics competitions. At that moment, I learned that robots weren\u2019t a fantasy or something that could only exist in the far future, but rather something that could be created in the present. Also, it turns out that playing with robots is incredibly fun.\nA lot! From there I continued with robotics competitions, started university at MIT, and spent a lot of time studying computer science with a focus on robotics. After graduation, I completely pivoted away from the field and joined a startup in San Francisco for a few years before moving to Google.\nIt was great but I'd always wanted to live abroad so I started looking at opportunities outside the US. At that point, I decided to move to London and set my sights on DeepMind. I actually didn\u2019t think DeepMind hired people without PhDs, but I gave it a shot and it worked out!\nBecause I was doing a transfer from Google to DeepMind, I was able to apply to multiple teams at the same time. The robotics team wasn\u2019t on my radar until my recruiter asked me, \u201cBy the way, you have bits of robotics on your resume. Have you thought about joining our robotics team?\u201d I bit at the opportunity. And honestly, it\u2019s been amazing ever since.\nEvery morning I head into the office for breakfast, where without fail, my teammates already are. It\u2019s become a part of our daily routine to have breakfast together before jumping into our work.\nI spend most mornings in the robotics lab, fixing failures from previous experiments or setting up new robots. Even when there\u2019s not much to be done, I get energy from just walking around and seeing our robots at work, hearing the hum of the machines and motors. We\u2019ve grown a lot over the past few years, and you can feel it while walking around our space.\nMy afternoons are a mix of meetings, coding and \u2013 now that most people are back in the office \u2013 an impromptu chat or two. That\u2019s one of my favourite parts of being in the office \u2013 the random catch-ups and whiteboard sessions that help me learn and move quickly. From there I\u2019ll take a quick snack break, and if the weather is nice, head to the balcony to catch up on some of my favourite US sports podcasts (I still haven\u2019t made the switch from football to \nfootball\n). Then I\u2019ll code a little while longer.\nThe culture at DeepMind is one of the best parts of being here. From my perspective, we\u2019ve found a nice balance between a university, start-up, and large company. Most of the work culture comes from the first two. \nIt\u2019s not unusual to find people brainstorming in front of whiteboards with maths scrawled across it, or someone tucked away in a quiet corner reading the latest research papers. Similar to a start-up, there\u2019s a palpable energy throughout \u2013 you can truly feel everyone's excitement. \nIt may be clich\u00e9, but when you love what you do it doesn't feel like work. The robotics team is a miniature version of all of this, with the bonus that many of us are close friends outside of work, too. It\u2019s perfect.\nLike most people, I spent the first month of the pandemic in disbelief, assuming we\u2019d be back to normal soon. The majority of our meetings and collaborations moved online which was an interesting experience for our team in particular. \nOnce the realisation set in that we were in this for the long haul, I decided to spend my new-found free time to better myself. I tried a bunch of hobbies \u2013 long enough in each case to say I\u2019d tried it, but not long enough for anything to stick. I had my guitar phase, a cooking phase, and even a puzzle phase, but my favourite was the tie-dye phase. There were a few weeks where I tie-dyed everything, from shirts to shorts to socks, and now they sit at the bottom of my closet (where they honestly belong).\nI feel lucky to be at DeepMind and to be able to focus on the work that I do. Robotics \u2013 and AI in general \u2013 will be a positive force in the world, and it\u2019s exciting to be able to help move that forward.\nOn the whole, I\u2019m particularly interested in seeing how AI can help mitigate climate change \u2013 whether that\u2019s by finding ways to use energy more efficiently, or enabling us to produce clean energy. Researchers at DeepMind are already thinking about this, so I\u2019m hopeful that we\u2019ll be able to move the world forward and make an impact in this space.\nIf you want to be at DeepMind, go for it. Apply, interview, and just try. You might not get it the first time but that doesn\u2019t mean you can\u2019t try again. I didn\u2019t think I would get a job at DeepMind, and when I got the offer, my initial thought was - surely this is a mistake! Everyone doubts themselves \u2013 I\u2019ve never felt like the smartest person in the room. I\u2019ve often felt the opposite. But I\u2019ve learned that, despite those feelings, I do belong and I do deserve to work at a place like this. And that journey, for me, started with just trying.\nLearn more about robotics at DeepMind and search for open roles today\n"}
{"title": "Emergent Bartering Behaviour in Multi-Agent Reinforcement Learning", "contents": "In \nour recent paper\n, we explore how populations of deep reinforcement learning (deep RL) agents can learn microeconomic behaviours, such as production, consumption, and trading of goods. We find that artificial agents learn to make economically rational decisions about production, consumption, and prices, and react appropriately to supply and demand changes. The population converges to local prices that reflect the nearby abundance of resources, and some agents learn to transport goods between these areas to \u201cbuy low and sell high\u201d. This work advances the broader multi-agent reinforcement learning research agenda by introducing new social challenges for agents to learn how to solve.\nInsofar as the goal of multi-agent reinforcement learning research is to eventually produce agents that work across the full range and complexity of human social intelligence, the set of domains so far considered has been woefully incomplete. It is still missing crucial domains where human intelligence excels, and humans spend significant amounts of time and energy. The subject matter of economics is one such domain. Our goal in this work is to establish environments based on the themes of trading and negotiation for use by researchers in multi-agent reinforcement learning.\nEconomics uses agent-based models to simulate how economies behave. These agent-based models often build in economic assumptions about how agents should act. In this work, we present a multi-agent simulated world where agents can learn economic behaviours from scratch, in ways familiar to any Microeconomics 101 student: decisions about production, consumption, and prices. But our agents also must make other choices that follow from a more physically embodied way of thinking. They must navigate a physical environment, find trees to pick fruits, and partners to trade them with. Recent advances in deep RL techniques now make it possible to create agents that can learn these behaviours on their own, without requiring a programmer to encode domain knowledge.\nOur environment, called \nFruit Market\n, is a multiplayer environment where agents produce and consume two types of fruit: apples and bananas. Each agent is skilled at producing one type of fruit, but has a preference for the other \u2013 if the agents can learn to barter and exchange goods, both parties would be better off.\nIn our experiments, we demonstrate that current deep RL agents can learn to trade, and their behaviours in response to supply and demand shifts align with what microeconomic theory predicts. We then build on this work to present scenarios that would be very difficult to solve using analytical models, but which are straightforward for our deep RL agents. For example, in environments where each type of fruit grows in a different area, we observe the emergence of different price regions related to the local abundance of fruit, as well as the subsequent learning of arbitrage behaviour by some agents, who begin to specialise in transporting fruit between these regions.\nThe field of agent-based computational economics uses similar simulations for economics research. In this work, we also demonstrate that state-of-the-art deep RL techniques can flexibly learn to act in these environments from their own experience, without needing to have economic knowledge built in. This highlights the reinforcement learning community\u2019s recent progress in multi-agent RL and deep RL, and demonstrates the potential of multi-agent techniques as tools to advance simulated economics research.\nAs a \npath to artificial general intelligence\n (AGI), multi-agent reinforcement learning research should encompass all critical domains of social intelligence. However, until now it hasn\u2019t incorporated traditional economic phenomena such as trade, bargaining, specialisation, consumption, and production. This paper fills this gap and provides a platform for further research. To aid future research in this area, the Fruit Market environment will be included in the next release of the \nMelting Pot\n suite of environments.\n"}
{"title": "Dynamic language understanding: adaptation to new knowledge in parametric and semi-parametric models", "contents": "Many recent successes in language models (LMs) have been achieved within a \u2018static paradigm\u2019, where the focus is on improving performance on the benchmarks that are created without considering the temporal aspect of data. For instance, answering questions on events that the model could learn about during training, or evaluating on text sub-sampled from the same period as the training data. However, our language and knowledge are dynamic and ever evolving. Therefore, to enable a more realistic evaluation of question-answering models for the next leap in performance, it\u2019s essential to ensure they are flexible and robust when encountering new and unseen data.\nIn 2021, we released \nMind the Gap: Assessing Temporal Generalization in Neural Language Models\n and the \ndynamic language modelling benchmarks\n for WMT and arXiv to facilitate language model evaluation that take temporal dynamics into account. In this paper, we highlighted issues that current state-of-the-art large LMs face with temporal generalisation and found that knowledge-intensive tokens take a considerable performance hit.\nToday, we\u2019re releasing two papers and a new benchmark that further advance research on this topic. In \nStreamingQA: A Benchmark for Adaptation to New Knowledge over Time in Question Answering Models\n, we study the downstream task of question-answering on our newly proposed benchmark, \nStreamingQA\n: we want to understand how parametric and retrieval-augmented, semi-parametric question-answering models adapt to new information, in order to answer questions about new events. In \nInternet-augmented language models through few-shot prompting for open-domain question answering\n, we explore the power of combining a few-shot prompted large language model along with Google Search as a retrieval component. In doing so, we aim to improve the model's factuality, while making sure it has access to up-to-date information for answering a diverse set of questions.\nKnowledge and language understanding of models evaluated through question-answering (QA) has been commonly studied on static snapshots of knowledge, like Wikipedia. To study how semi-parametric QA models and their underlying parametric LMs adapt to evolving knowledge, we constructed the new large-scale benchmark, StreamingQA, with human-written and automatically generated questions asked on a given date, to be answered from 14 years of time-stamped news articles (see Figure 2). We show that parametric models can be updated without full retraining, while avoiding catastrophic forgetting. For semi-parametric models, adding new articles into the search space allows for rapid adaptation, however, models with an outdated underlying LM underperform those with a retrained LM.\nWe\u2019re aiming to capitalise on the unique few-shot capabilities offered by large-scale language models to overcome some of their challenges, with respect to grounding to factual and up-to-date information. Motivated by semi-parametric LMs, which ground their decisions in externally retrieved evidence, we use few-shot prompting to learn to condition LMs on information returned from the web using Google Search, a broad and constantly updated knowledge source. Our approach does not involve fine-tuning or learning additional parameters, thus making it applicable to virtually any language model. And indeed, we find that LMs conditioned on the web surpass the performance of closed-book models of similar, or even larger, model size in open-domain question-answering.\n"}
{"title": "Open-sourcing MuJoCo", "contents": "In October 2021, we announced that we acquired the \nMuJoCo physics simulator\n, and made it freely available for everyone to support research everywhere. We also committed to developing and maintaining MuJoCo as a free, open-source, community-driven project with best-in-class capabilities. Today, we\u2019re thrilled to report that open sourcing is complete and the entire codebase is \non GitHub\n!\u00a0\nHere, we explain why MuJoCo is a great platform for open-source collaboration and share a preview of our roadmap going forward.\nPhysics simulators are critical tools in modern robotics research and often fall into these two categories:\u00a0\nThe first category is opaque to the user, and although sometimes free to use, cannot be modified and is hard to understand. The second category often has a smaller user base and suffers when its developers and maintainers graduate.\nMuJoCo is one of the few full-featured simulators backed by an established company, which is truly open source. As a research-driven organisation, we view MuJoCo as a platform for collaboration, where roboticists and engineers can join us to develop one of the world\u2019s best robot simulators.\nFeatures that make MuJoCo particularly attractive for collaboration are:\nWe hope that colleagues across academia and the OSS community benefit from this platform and contribute to the codebase, improving research for everyone.\nAs a C library with no dynamic memory allocation, MuJoCo is very fast. Unfortunately, raw physics speed has historically been hindered by Python wrappers, which made batched, multi-threaded operations non-performant due to the presence of the Global Interpreter Lock (GIL) and non-compiled code. In our roadmap below, we address this issue going forward.\u00a0\nFor now, we\u2019d like to share some benchmarking results for two common models. The results were obtained on a standard AMD Ryzen 9 5950X machine, running Windows 10.\nHere\u2019s our near-term roadmap for MuJoCo:\nHelpful resources about MuJoCo:\nWe look forward to receiving your contributions!\n"}
{"title": "Building a culture of pioneering responsibly", "contents": "As chief operating officer of one of the world\u2019s leading artificial intelligence labs, I spend a lot of time thinking about how our technologies impact people\u2019s lives \u2013 and how we can ensure that our efforts have a positive outcome. This is the focus of my work, and the critical message I bring when I meet world leaders and key figures in our industry. For instance, it was at the forefront of the panel discussion on \u2018Equity Through Technology\u2019 that I hosted this week at the \nWorld Economic Forum\n in Davos, Switzerland.\u00a0\nInspired by the important conversations taking place at Davos on building a greener, fairer, better world, I wanted to share a few reflections on my own journey as a technology leader, along with some insight into how we at DeepMind are approaching the challenge of building technology that truly benefits the global community.\u00a0\nIn 2000, I took a sabbatical from my job at Intel to visit the orphanage in Lebanon where my father was raised. For two months, I worked to install 20 PCs in the orphanage\u2019s first computer lab, and to train the students and teachers to use them. The trip started out as a way to honour my dad. But being in a place with such limited technical infrastructure also gave me a new perspective on my own work. I realised that without real effort by the technology community, many of the products I was building at Intel would be inaccessible to millions of people. I became acutely aware of how that gap in access was exacerbating inequality; even as computers solved problems and accelerated progress in some parts of the world, others were being left further behind.\u00a0\nAfter that first trip to Lebanon, I started reevaluating my career priorities. I had always wanted to be part of building groundbreaking technology. But when I returned to the US, my focus narrowed in on helping build technology that could make a positive and lasting impact on society. That led me to a variety of roles at the intersection of education and technology, including co-founding \nTeam4Tech\n, a non-profit that works to improve access to technology for students in developing countries.\u00a0\nWhen I joined DeepMind as COO in 2018, I did so in large part because I could tell that the founders and team had the same focus on positive social impact. In fact, at DeepMind, we now champion a term that perfectly captures my own values and hopes for integrating technology into people\u2019s daily lives: \npioneering responsibly\n.\n\u00a0\nI believe pioneering responsibly should be a priority for anyone working in tech. But I also recognise that it\u2019s especially important when it comes to powerful, widespread technologies like artificial intelligence. AI is arguably the most impactful technology being developed today. It has the \npotential to benefit humanity\n in innumerable ways \u2013 from combating climate change to preventing and treating disease. But it\u2019s essential that we account for both its positive and negative downstream impacts. For example, we need to design AI systems carefully and thoughtfully to \navoid amplifying human biases\n, such as in the contexts of hiring and policing.\u00a0\nThe good news is that if we\u2019re continuously questioning our own assumptions of how AI can, and should, be built and used, we can build this technology in a way that truly benefits everyone. This requires inviting discussion and debate, iterating as we learn, building in social and technical safeguards, and seeking out diverse perspectives. At DeepMind, everything we do stems from our company mission of solving intelligence to advance society and benefit humanity, and building a culture of pioneering responsibly is essential to making this mission a reality.\u00a0\nWhat does pioneering responsibly look like in practice? I believe it starts with creating space for open, honest conversations about responsibility within an organisation. One place where we\u2019ve done this at DeepMind is in our multidisciplinary leadership group, which advises on the potential risks and social impact of our research.\u00a0\nEvolving our ethical governance and formalising this group was one of my first initiatives when I joined the company \u2013 and in a somewhat unconventional move, I didn\u2019t give it a name or even a specific objective until we\u2019d met several times. I wanted us to focus on the operational and practical aspects of responsibility, starting with an expectation-free space in which everyone could talk candidly about what pioneering responsibly meant to them. Those conversations were critical to establishing a shared vision and mutual trust \u2013 which allowed us to have more open discussions going forward.\nAnother element of pioneering responsibly is embracing a \nkaizen \nphilosophy and approach. I was introduced to the term kaizen in the 1990s, when I moved to Tokyo to work on DVD technology standards for Intel. It\u2019s a Japanese word that translates to \u201ccontinuous improvement\u201d \u2013 and in the simplest sense, a kaizen process is one in which small, incremental improvements, made continuously over time, lead to a more efficient and ideal system. But it\u2019s the mindset behind the process that really matters. For kaizen to work, everyone who touches the system has to be watching for weaknesses and opportunities to improve. That means everyone has to have both the humility to admit that something might be broken, and the optimism to believe they can change it for the better.\u00a0\nDuring my time as COO of the online learning company Coursera, we used a kaizen approach to optimise our course structure. When I joined Coursera in 2013, courses on the platform had strict deadlines, and each course was offered just a few times a year. We quickly learned that this didn\u2019t provide enough flexibility, so we pivoted to a completely on-demand, self-paced format. Enrollment went up, but completion rates dropped \u2013 it turns out that while too much structure is stressful and inconvenient, too little leads to people losing motivation. So we pivoted again, to a format where course sessions start several times a month, and learners work toward suggested weekly milestones. It took time and effort to get there, but continuous improvement eventually led to a solution that allowed people to fully benefit from their learning experience.\u00a0\nIn the example above, our kaizen approach was largely effective because we asked our learner community for feedback and listened to their concerns. This is another crucial part of pioneering responsibly: acknowledging that we don\u2019t have all the answers, and building relationships that allow us to continually tap into outside input.\u00a0\nFor DeepMind, that sometimes means consulting with experts on topics like security, privacy, bioethics, and psychology. It can also mean reaching out to diverse communities of people who are directly impacted by our technology, and inviting them into a discussion about what they want and need. And sometimes, it means just listening to the people in our lives \u2013 regardless of their technical or scientific background \u2013 when they talk about their hopes for the future of AI.\u00a0\nFundamentally, pioneering responsibly means prioritising initiatives focused on ethics and social impact. A growing area of focus in our research at DeepMind is on how we can make AI systems more equitable and inclusive. In the past two years, we\u2019ve published research on \ndecolonial AI\n, \nqueer fairness in AI\n, \nmitigating ethical and social risks in AI language models\n, and more. At the same time, we\u2019re also working to increase diversity in the field of AI through our dedicated \nscholarship programmes\n. Internally, we recently started hosting Responsible AI Community sessions that bring together different teams and efforts working on safety, ethics, and governance \u2013 and several hundred people have signed up to get involved.\nI\u2019m inspired by the enthusiasm for this work among our employees and deeply proud of all of my DeepMind colleagues who keep social impact front and centre. Through making sure technology benefits those who need it most, I believe we can make real headway on the challenges facing our society today. In that sense, pioneering responsibly is a moral imperative \u2013 and personally, I can\u2019t think of a better way forward.\u00a0\n"}
{"title": "BYOL-Explore: Exploration with Bootstrapped Prediction", "contents": "Curiosity-driven exploration is the active process of seeking new information to enhance the agent\u2019s understanding of its environment. Suppose that the agent has learned a model of the world that can predict future events given the history of past events. The curiosity-driven agent can then use the prediction mismatch of the world model as the intrinsic reward for directing its exploration policy towards seeking new information. As follows, the agent can then use this new information to enhance the world model itself so it can make better predictions. \u00a0This iterative process can allow the agent to eventually explore every novelty \u00a0in the world and use this information to build an accurate world model.\nInspired by the successes of \nbootstrap your own latent\n (BYOL) \u2013 which has been applied in \ncomputer vision\n, \ngraph representation learning\n, and \nrepresentation learning in RL\n \u2013 we propose BYOL-Explore: a conceptually simple yet general, curiosity-driven AI agent for solving hard-exploration tasks. BYOL-Explore learns a representation of the world by predicting its own future representation. Then, it uses the prediction-error at the representation level as an intrinsic reward to train a curiosity-driven policy. Therefore, BYOL-Explore learns a world representation, the world dynamics, and a curiosity-driven exploration policy all-together, simply by optimising the prediction error at the representation level.\nDespite the simplicity of its design, when applied to the \nDM-HARD-8\n suite of challenging 3-D, visually complex, and hard exploration tasks, BYOL-Explore outperforms standard curiosity-driven exploration methods such as \nRandom Network Distillation\n (RND) and \nIntrinsic Curiosity Module\n (ICM), in terms of mean capped human-normalised score (CHNS), measured across all tasks. Remarkably, BYOL-Explore achieved this performance using only a single network concurrently trained across all tasks, whereas prior work was restricted to the single-task setting and could only make meaningful progress on these tasks when provided with human expert demonstrations. \nAs further evidence of its generality, BYOL-Explore achieves super-human performance in the ten hardest exploration \nAtari games\n, while having a simpler design than other competitive agents, such as \nAgent57\n and \nGo-Explore\n.\nMoving forward, we can generalise BYOL-Explore to highly stochastic environments by learning a probabilistic world model that could be used to generate trajectories of the future events. This could allow the agent to model the possible stochasticity of the environment, avoid stochastic traps, and plan for exploration.\n"}
{"title": "Unlocking High-Accuracy Differentially Private Image Classification through Scale", "contents": "A recent \nDeepMind paper\n on the ethical and social risks of language models identified large language models \nleaking sensitive information\n about their training data as a potential risk that organisations working on these models have the responsibility to address. Another \nrecent paper\n shows that similar privacy risks can also arise in standard image classification models: a fingerprint of each individual training image can be found embedded in the model parameters, and malicious parties could exploit such fingerprints to reconstruct the training data from the model. \nPrivacy-enhancing technologies like differential privacy (DP) can be deployed at training time to mitigate these risks, but they often incur significant reduction in model performance. In this work, we make substantial progress towards unlocking high-accuracy training of image classification models under differential privacy.\nDifferential privacy was \nproposed\n as a mathematical framework to capture the requirement of protecting individual records in the course of statistical data analysis (including the training of machine learning models). DP algorithms protect individuals from any inferences about the features that make them unique (including complete or partial reconstruction) by injecting carefully calibrated noise during the computation of the desired statistic or model. Using DP algorithms provides robust and rigorous privacy guarantees both in theory and in practice, and has become a de-facto gold standard adopted by a number of \npublic\n and \nprivate\n organisations.\nThe most popular DP algorithm for deep learning is differentially private stochastic gradient descent (DP-SGD), a modification of standard SGD obtained by clipping gradients of individual examples and adding enough noise to mask the contribution of any individual to each model update:\nUnfortunately, prior works have found that in practice, the privacy protection provided by DP-SGD often comes at the cost of significantly less accurate models, which presents a major obstacle to the widespread adoption of differential privacy in the machine learning community. According to empirical evidence from prior works, this utility degradation in DP-SGD becomes more severe on larger neural network models \u2013 including the ones regularly used to achieve the best performance on challenging image classification benchmarks.\nOur work investigates this phenomenon and proposes a series of simple modifications to both the training procedure and model architecture, yielding a significant improvement on the accuracy of DP training on standard image classification benchmarks. The most striking observation coming out of our research is that DP-SGD can be used to efficiently train much deeper models than previously thought, as long as one ensures the model's gradients are well-behaved. We believe the substantial jump in performance achieved by our research has the potential to unlock practical applications of image classification models trained with formal privacy guarantees. \nThe figure below summarises two of our main results: an ~10% improvement on CIFAR-10 compared to previous work when privately training without additional data, and a top-1 accuracy of 86.7% on ImageNet when privately fine-tuning a model pre-trained on a different dataset, almost closing the gap with the best non-private performance.\nThese results are achieved at \ud835\udf3a=8, a standard setting for calibrating the strength of the protection offered by differential privacy in machine learning applications. We refer to the paper for a discussion of this parameter, as well as additional experimental results at other values of \ud835\udf3a and also on other datasets. Together with the paper, we are also open-sourcing our implementation to enable other researchers to verify our findings and build on them. We hope this contribution will help others interested in making practical DP training a reality.\nDownload our JAX implementation \non GitHub\n.\n"}
{"title": "Reflections from ethics and safety \u2018on the ground\u2019 at DeepMind", "contents": "Boxi share's their experience working on the Ethics & Society team to support ethical, safe and beneficial AI development, highlighting the importance of interdisciplinary and sociotechnical thinking.\nI grew up in suburban Perth, Australia, and still remember using the internet for the first time at the local library in my early teens to research the Great Barrier Reef. Looking back, I could have never imagined the role I have now!\u00a0\nI was always fascinated with how social systems could interact with technology, which eventually led me to study political science and urban planning at university. I went on to work in urban policy research and strategy consulting, and a few years later, I made the daunting decision to move from Australia to London to work for a health technology startup. Seeing the implementation challenges of AI in medicine piqued my interest in the ethical and societal impacts of AI. When the opportunity to apply for DeepMind came up it made a lot of sense to me - a mix of the academic inquiry that I missed, and the exciting energy of a startup.\nDeepMind\u2019s culture is one where many perspectives collide and there are countless chances to find your community. In many cases, this happens formally - I\u2019ve joined employee resource groups like QueerMinds and the People of Colour Employee Group, and have had the opportunity to attend conferences like Lesbians Who Tech. I\u2019m also part of \nQueer in AI\n, an organisation with a mission to raise awareness of queer issues in AI/ML, foster a community of queer researchers and celebrate the work of queer scientists. DeepMind are actually hosting a workshop with Queer in AI on Wednesday the 6th July to discuss the relationship between queer issues and AI. Since joining I\u2019ve made some really great connections with a circle of queer and POC colleagues amongst whom we create safe spaces and support each other\u2019s work. These interactions have felt even more rewarding now that we have returned to the office.\nMy team (the Ethics & Society team) is busy and close knit. We work together to guide the responsible development and deployment of AI. One core element of this is developing the processes, infrastructure and frameworks to ensure ethical considerations are embedded into all of our projects. We often partner with other teams at DeepMind over extended periods to consider the positive and negative downstream impacts of our work, i.e our work with language models, or science projects like AlphaFold.\u00a0\nWe\u2019re constantly learning as a team, talking to each other about our projects and the challenges we\u2019re facing. This involves a lot of reflection to fully understand who our technology may impact, and to determine who the right people are (internally and externally) to help tackle the challenges identified. This can be incredibly complex at times but it is a lot of fun.\nI lead our research collaborations which operationalise ethics and safety across our work at DeepMind. This normally includes conducting ethical impact assessments, partnering with teams to conduct ethics reviews, and facilitating workshops to think through benefits, risks and mitigations.\nAs we work across varied domains of research (e.g. language, reinforcement learning, robotics), we are always in conversation with experts across research, engineering, legal, policy, and communications, etc. We also meet as a team daily - this is super important in our area of work as ethics and safety questions are best discussed in groups (in order to check personal biases and debate differences of opinion, for example).\nI love learning from those around me - internally and within the wider AI ethics community. There is still so much more to learn and I am humbled by the knowledge and curiosity of everyone I speak to. What I find particularly interesting is learning from those that are adjacent to - or outside of traditional AI/ML research fields. Better understanding the perspectives of those, for example, in social sciences, philosophy, or critical theory allows us to better identify and challenge the fundamental values underpinning technology.\nA great example of this would be this year\u2019s \nACM Conference on Fairness, Accountability, and Transparency\n (FAccT) in South Korea. The agenda at this conference was far reaching, covering everything from my colleagues\u2019 \npaper\n on fluid identity in machine learning, to a session with \nYoujin Kong\n on AI ethics and feminist philosophy, and a keynote by \nKaren Hao\n on journalism on AI ethics and technology.\nIn a recent \nblog post\n, our COO Lila discussed the idea of pioneering responsibility and its key role in our mission. I think that\u2019s exactly right - not only is it critical for the wider tech community but it\u2019s especially important when it comes to creating powerful, widespread technologies like artificial intelligence. It must be part of the conversation at every stage and embedded into everything that we do.\nI\u2019m proud that I am part of a team that gets to explore these ideas - and while of course we have much more to do in this space, I do believe we're helping make a positive impact on the world around us.\u00a0\nRead as much as you can about AI ethics and safety, and better yet, explore sociotechnical work that discusses the history of AI, the current harms of technology within society, and visions for what safe and ethical AI could look like. Some favourites of mine are Ruha Benjamin\u2019s \nRace after Technology\n and Karen Hao\u2019s recent series on \nAI and colonialism\n. I would also recommend checking out Kevin Guyan\u2019s \nQueer Data: Using Gender, Sex and Sexuality Data for Action\n, and my colleague's paper on \nalgorithmic fairness for the queer community\n.\nFinally, I want to reassure \u2018non-technical\u2019 or more social science oriented folks that this space is for you. I often have people tell me they feel intimidated by AI/ML despite having an interest in technology and ethics. Please be assured that your perspective will be valuable to this industry - our values influence technology, as does technology impact our social life. Addressing the challenges of AI development will require interdisciplinary and sociotechnical thinking and people from all walks of life. Don\u2019t doubt yourself - go for it!\nLearn more about the ethics & society team and search for open roles today\n"}
{"title": "Intuitive physics learning in a deep-learning model inspired by developmental psychology", "contents": "Understanding the physical world is a critical skill that most people deploy effortlessly. However, this still poses a challenge to artificial intelligence; if we\u2019re to deploy safe and helpful systems in the real world, we want these models to share our intuitive sense of physics. But before we can build those models, there is another challenge: How will we measure the ability of these models to understand the physical world? That is, what does it mean to understand the physical world and how can we quantify it?\nLuckily for us, developmental psychologists have spent decades studying what infants know about the physical world. Along the way, they've carved the nebulous notion of physical knowledge into a concrete set of physical concepts. And, they've developed the violation-of-expectation (VoE) paradigm for testing those concepts in infants.\nIn our paper published today in Nature Human Behavior, we extended their work and open-sourced the \nPhysical Concepts dataset\n. This synthetic video dataset ports the VoE paradigm to assess five physical concepts: solidity, object persistence, continuity, \u201cunchangeableness'', and directional inertia.\nWith a benchmark for physical knowledge in hand, we turned to the task of building a model capable of learning about the physical world. Again, we looked to developmental psychologists for inspiration. Researchers not only catalogued what infants know about the physical world, they also posited the mechanisms that could enable this behaviour. Despite variability, these accounts have a central role for the notion of breaking up the physical world into a set of \nobjects\n which evolve through time.\nInspired by this work, we built a system that we nickname PLATO (Physics Learning through Auto-encoding and Tracking Objects). PLATO represents and reasons about the world as a set of objects. It makes predictions about where objects will be in the future based on where they've been in the past and what other objects they're interacting with.\nAfter training PLATO on videos of simple physical interactions, we found that PLATO passed the tests in our Physical Concepts dataset. Furthermore, we trained \"flat\" models that were as big (or even bigger) than PLATO but did not use object-based representations. When we tested those models, we found they didn't pass all of our tests. This suggests that objects are helpful for learning intuitive physics, supporting hypotheses from the developmental literature.\nWe also wanted to determine how much experience was needed to develop this capacity. Evidence for physical knowledge has been shown in infants as young as two and a half months of age. How does PLATO fare in comparison? By varying the amount of training data used by PLATO, we found that PLATO could learn our physical concepts with as little as 28 hours of visual experience. The limited and synthetic nature of our dataset means we cannot make a like-for-like comparison between the amount of visual experiences received by infants and PLATO. However, this result suggests that intuitive physics can be learned with relatively little experience if supported via an inductive bias for representing the world as objects.\nFinally, we wanted to test PLATO's ability to generalise. In the Physical Concepts dataset, all of the objects in our test set are also present in the training set. What if we tested PLATO with objects it had never seen before? To do this, we leveraged a subset of another synthetic \ndataset\n developed by researchers at MIT. This dataset also probes physical knowledge, albeit with different visual appearances and a set of objects that PLATO has never seen before. PLATO passed, without any re-training, despite being tested on entirely new stimuli.\nWe hope this dataset can provide researchers with a more specific understanding of their model\u2019s abilities to understand the physical world. In the future, this can be expanded to test more aspects of intuitive physics by increasing the list of physical concepts tested, and using richer visual stimuli including new object shapes or even real-world videos.\n"}
{"title": "Real-world challenges for AGI", "contents": "Note: This post is a summary of a talk given at CERN Sparks! Serendipity Forum in September 2021, which can be viewed \nhere\n.\nWhen people picture a world with artificial general intelligence (AGI), robots are more likely to come to mind than enabling solutions to society\u2019s most intractable problems. But I believe the latter is much closer to the truth. AI is already enabling huge leaps in tackling fundamental challenges: \nfrom solving protein folding\n to \npredicting accurate weather patterns\n, scientists are increasingly using AI to deduce the rules and principles that underpin highly complex real-world domains - ones they might never have discovered unaided.\nAdvances in AGI research will supercharge society\u2019s ability to tackle and manage climate change - not least because of its urgency but also due to its complex and multifaceted nature.\nLooking across the field of AI research today, there are two common categories of problems scientists are focused on: prediction and control. Prediction models try to learn about a domain (such as weather patterns) and understand how it might evolve, while control models prompt agents to take actions in that environment. Building a successful path to AGI requires understanding and developing algorithms in both spaces, accounting for all the variations that our natural and social environments throw at us, from how viruses mutate or how language may evolve in use and meaning over time to how to help produce energy from fusion power. Two real-world domains that scientists at DeepMind are contributing to tackle climate change while developing what\u2019s required to build AGI are weather prediction and plasma control for fusion.\nWeather patterns are almost impossible to precisely model - it\u2019s an example of nature\u2019s variations at its fullest. However, causes and effects can be inferred based on vast amounts of historical data. Transferring the same generative models that are used to generate images and video clips into learning weather patterns in collaboration with the \nMet Office\n (UK\u2019s national meteorological service), scientists at DeepMind have developed systems that can take 20 minutes of weather data to generate multiple hypotheses for radar maps and \naccurately predict heavy rainfall\n in the next 90 minutes.\nCritically, these models will help meteorologists provide forecasts that aid decision making for emergency services, energy management, and activation of flood warning systems - enabling better preparation for and responses to extreme weather events, which have become increasingly common around the world. Helping predict important weather events by forecasting accurate weather patterns is one example of how AI research can make a meaningful impact as it becomes more generally applicable and \u2018intelligent\u2019.\nBeyond responding to the effects of climate change, solving its sources is of equal if not greater importance. Fusion, a single source of energy that is clean, limitless, and self-sustaining, is elusive, yet remains one of the world\u2019s most promising solutions - one that I believe requires developing a general algorithm that can solve many different components at once. Already we are seeing progress in one component, the extremely challenging problem of maintaining novel plasma shapes to enable better energy output and stability of the plasma for as long as possible.\nBy working with world-renowned experts at the \nSwiss Plasma Center\n and \n\u00c9cole polytechnique f\u00e9d\u00e9rale de Lausanne\n (EPFL), we are able to go beyond today\u2019s hand crafted models, applying deep reinforcement learning algorithms first developed for robotics to plasma control. The result is a controller that can successfully manipulate different plasma shapes and configurations at 10,000 interactions per second.\nWithout expert collaboration, AI researchers cannot make significant progress in real-world domains. Identifying the right paths forward in these fields requires partnerships across disciplines, leveraging a common scientific approach to develop and use AI to navigate complex questions at the heart of society\u2019s most urgent needs. It\u2019s why dreaming together with a diversity of natural and social scientists about what a world with AGI could look like is so critically important.\nAs we develop AGI, addressing global challenges such as climate change will not only make crucial and beneficial impacts that are urgent and necessary for our world, but also advance the science of AGI itself. Many other categories of AGI problems are yet to be solved - from causality, to learning efficiently and transfer - and as algorithms become more general, more real-world problems will be solved, gradually contributing to a system that one day will help solve everything else, too.\n\u200d\n"}
{"title": "Leading a movement to strengthen machine learning in Africa", "contents": "Avishkar Bhoopchand\n, \na research engineer on the Game Theory and Multi-agent team, shares his journey to DeepMind and how he\u2019s working to raise the profile of deep learning across Africa.\u00a0\nFind out more about \nDeep Learning Indaba 2022\n, the annual gathering of the African AI community \u2013 taking place in Tunisia this August.\nAs a research engineer and technical lead, no day is the same. I usually start my day by listening to a podcast or audiobook on my commute into the office. After breakfast, I focus on emails and admin before jumping into my first meeting. These vary from one-on-ones with team members and project updates to diversity, equity, and inclusion (DE&I) working groups.\u00a0\nI try to carve out time for my to do list in the afternoon. These tasks could involve preparing a presentation, reading research papers, writing or reviewing code, designing and running experiments, or analysing results.\u00a0\nWhen working from home, my dog Finn keeps me busy! Teaching him is a lot like reinforcement learning (RL) \u2013 like how we train artificial agents at work. So, a lot of my time is spent thinking about deep learning or machine learning in one way or another.\nDuring a course on intelligent agents at the University of Cape Town, my lecturer demoed a six-legged robot that had learned to walk from scratch using RL. From that moment on, I couldn\u2019t stop thinking about the possibility of using human and animal mechanisms to build systems capable of learning.\nAt the time, machine learning application and research wasn\u2019t really a viable career option in South Africa. Like many of my fellow students, I ended up working in the finance industry as a software engineer. I learned a lot, especially around designing large scale, robust systems that meet user requirements. But after six years, I wanted something more.\nAround then, deep learning started to take off. First I started doing online courses like Andrew Ng\u2019s \nmachine learning lectures\n on Coursera. Soon after, I was fortunate enough to get a scholarship to University College London, where I got my masters in computational statistics and machine learning.\u00a0\nBeyond DeepMind, I\u2019m also a proud organiser and steering committee member of the \nDeep Learning Indaba\n, a movement to strengthen machine learning and AI in Africa. It started in 2017 as a summer school in South Africa. We expected 30 or so students to get together to learn about machine learning \u2013 but to our surprise, we received over 700 applications! It was amazing to see, and it clearly showed the need for connection between researchers and practitioners in Africa.\nSince then, the organisation has grown into an annual celebration of African AI with over 600 attendees, and local IndabaX events held across nearly 30 African countries. We also have research grants, thesis awards, and complementary programmes, including a mentorship programme \u2013 which I started during the pandemic to keep the community engaged.\nIn 2017, there were zero publications with an African author, based at an African institution, presented at \nNeurIPS\n, the leading machine learning conference. AI researchers across the African continent were working in silos \u2013 some even had colleagues working on the same subject at another institution down the road and didn\u2019t know. Through the Indaba, we\u2019ve built a thriving community on the continent and our alumni have gone on to form new collaborations, publishing papers at NeurIPS and all of the major conferences.\u00a0\nMany members have gotten jobs at top tech companies, formed new startups on the continent, and launched other amazing grassroots AI projects in Africa. Although organising the Indaba is a lot of hard work, it\u2019s made worthwhile by seeing the achievements and growth of the community. I always leave our annual event feeling inspired and ready to take on the future.\nDeepMind was my ultimate dream company to work for, but I didn\u2019t think I stood a chance. From time-to-time, I\u2019ve struggled with imposter syndrome \u2013 when surrounded by intelligent, capable people, it\u2019s easy to compare oneself on a single axis and feel like an imposter. Luckily, my wonderful wife told me I had nothing to lose by applying, so I sent my CV and eventually got an offer for a research engineer role!\u00a0\nMy previous experience in software engineering really helped me prepare for this role, as I could lean on my engineering skills for the day to day work while building my research skills. Not getting the dream job right away doesn\u2019t mean the door\u2019s closed on that career forever.\nI recently worked on a project about giving artificial agents the capability of\n real-time cultural transmission\n. Cultural transmission is a social skill that humans and certain animals possess, which gives us the ability to learn information from observing others. It\u2019s the basis for cumulative cultural evolution and the process responsible for expanding our skills, tools, and knowledge across multiple generations.\nIn this project, we trained artificial agents in a 3D simulated environment to observe an expert performing a new task, then copy that pattern, and remember it. Now that we\u2019ve shown that cultural transmission is possible in artificial agents, it may be possible to use cultural evolution to help generate artificial general intelligence (AGI).\u00a0\nThis was the first time I worked on large-scale RL. This work combines machine learning and social science, and there was a lot for me to learn on the research side. At times, progress towards our goal was also slow but we got there in the end! But really, I\u2019m most proud of the incredibly inclusive culture we had as a project team. Even when things were difficult, I knew I could rely on my colleagues for support.\nI\u2019ve been really involved with a number of diversity, equity, and inclusion (DE&I) initiatives. I\u2019m a strong believer that DE&I in the workplace leads to better outcomes, and to build AI for all, we must have representation from a diverse set of voices.\nI\u2019m a facilitator for an internal workshop on the concept of Allyship, which is about using one\u2019s position of privilege and power to challenge the status quo in support of people from marginalised groups. I\u2019m involved in various working groups that aim to improve community inclusion amongst research engineers and diversity in hiring. I\u2019m also a mentor in the \nDeepMind scholarship programme\n, which has partnerships in Africa and other parts of the world.\u00a0\nI\u2019m particularly enthusiastic about the possibilities of AI making a positive impact on medicine, especially for better understanding and treating diseases. For example, mental health conditions like depression affect hundreds of millions of people worldwide, but we seem to have limited understanding of the causal mechanisms behind it, and therefore, limited treatment options. I hope that in the not too distant future, general AI systems can work in conjunction with human experts to unlock the secrets of our minds and help us understand and cure these diseases.\nLearn more about research at DeepMind and search for open roles today\n"}
{"title": "Working together with YouTube", "contents": "Helping enrich people\u2019s lives with our research, we\u2019ve partnered with businesses across Alphabet to apply our technology towards improving the products and services used by billions of people every day.\u00a0\nOne of our key partners is YouTube, who are on a mission to give everyone a voice and show them the world.\u00a0\nWorking together with YouTube\u2019s product and engineering teams, we\u2019ve helped optimise the decision-making processes that increase safety, decrease latency, and enhance the viewer, creator, and advertiser experience for all. \nWith video surging during the COVID-19 pandemic, and the total amount of internet traffic expected to grow in the future, video compression is an increasingly important problem.\nWorking together with YouTube, we explored the potential for our AI model, \nMuZero\n, to improve the VP9 codec, a coding format that helps compress and transmit video over the internet. Then we applied MuZero to some of YouTube\u2019s live traffic.\nSince launching to production on a portion of YouTube\u2019s live traffic, we\u2019ve demonstrated an average 4% bitrate reduction across a large, diverse set of videos. Bitrate helps determine the computing ability and bandwidth needed to play and store videos \u2013 impacting everything from how long a video takes to load to its resolution, buffering, and data usage.\u00a0\nBy improving the VP9 codec on YouTube, we\u2019ve helped reduce internet traffic, data usage, and time needed for loading videos. And through optimising video compression, millions of people around the world are able to watch more videos while using less data.\nSince 2018, we\u2019ve collaborated with YouTube to better educate creators on what types of videos can earn revenue from ads and make sure ads appear alongside content that follows YouTube\u2019s advertiser friendly guidelines.\nTogether with the YouTube team, we developed a label quality model (LQM) that helps label videos with greater precision, according to YouTube\u2019s ad friendly guidelines. The model improved the accuracy of advertisements running on videos in line with YouTube\u2019s ad friendly policies.\nThrough improving how videos are identified and classified, we\u2019ve enhanced trust in the platform for viewers, creators, and advertisers alike.\nIn recent years, creators started adding chapters to their videos to make it easier for their audience to find the content they were looking for, but this manual process can be slow and laborious.\u00a0\nTo improve the creator and viewer experience, we collaborated with the YouTube Search team and developed an AI system that can automatically process video transcripts, audio and visual features and suggest chapter segments and titles for YouTube creators.\nAs Sundar Pichai introduced at \nGoogle I/O 2022\n, auto-generated chapters are already available for 8M videos today, and we plan to scale this feature to more than 80M auto-generated chapters over the next year.\nUsing AutoChapters, viewers spend less time searching for specific content and creators save time creating chapters for their videos.\nAs society and the technology we use evolves, we\u2019re continuously looking for new ways to help improve everyday Alphabet technologies and products with our AI research.\nOur work with YouTube has already made a great impact, and we hope to make many more significant improvements to people\u2019s lives through our ongoing collaborations.\n"}
{"title": "Human-centred mechanism design with Democratic AI", "contents": "In our recent \npaper\n, published in Nature Human Behaviour, we provide a proof-of-concept demonstration that deep reinforcement learning (RL) can be used to find economic policies that people will vote for by majority in a simple game. The paper thus addresses a key challenge in AI research - how to train AI systems that align with human values.\nImagine that a group of people decide to pool funds to make an investment. The investment pays off, and a profit is made. How should the proceeds be distributed? One simple strategy is to split the return equally among investors. But that might be unfair, because some people contributed more than others. Alternatively, we could pay everyone back in proportion to the size of their initial investment. That sounds fair, but what if people had different levels of assets to begin with? If two people contribute the same amount, but one is giving a fraction of their available funds, and the other is giving them all, should they receive the same share of the proceeds? \nThis question of how to redistribute resources in our economies and societies has long generated controversy among philosophers, economists and political scientists. Here, we use deep RL as a testbed to explore ways to address this problem.\nTo tackle this challenge, we created a simple game that involved four players. Each instance of the game was played over 10 rounds. On every round, each player was allocated funds, with the size of the endowment varying between players. Each player made a choice: they could keep those funds for themselves or invest them in a common pool. Invested funds were guaranteed to grow, but there was a risk, because players did not know how the proceeds would be shared out. Instead, they were told that for the first 10 rounds there was one referee (A) who was making the redistribution decisions, and for the second 10 rounds a different referee (B) took over. At the end of the game, they voted for either A or B, and played another game with this referee. Human players of the game were allowed to keep the proceeds of this final game, so they were incentivised to report their preference accurately.\nIn reality, one of the referees was a pre-defined redistribution policy, and the other was designed by our deep RL agent. To train the agent, we first recorded data from a large number of human groups and taught a neural network to copy how people played the game. This simulated population could generate limitless data, allowing us to use data-intensive machine learning methods to train the RL agent to maximise the votes of these \u201cvirtual\u201d players. Having done so, we then recruited new human players, and pitted the AI-designed mechanism head-to-head against well-known baselines, such as a \nlibertarian\n policy that returns funds to people in proportion to their contributions. \nWhen we studied the votes of these new players, we found that the policy designed by deep RL was more popular than the baselines. In fact, when we ran a new experiment asking a fifth human player to take on the role of referee, and trained them to try and maximise votes, the policy implemented by this \u201chuman referee\u201d was still less popular than that of our agent.\nAI systems have been sometimes criticised for learning policies that may be incompatible with human values, and this problem of \u201cvalue alignment\u201d has become a major concern in AI research. One merit of our approach is that the AI learns directly to maximise the stated preferences (or votes) of a group of people. This approach may help ensure that AI systems are less likely to learn policies that are unsafe or unfair. In fact, when we analysed the policy that the AI had discovered, it incorporated a mixture of ideas that have previously been proposed by human thinkers and experts to solve the redistribution problem. \nFirstly, the AI chose to redistribute funds to people in proportion to their \nrelative\n rather than \nabsolute\n contribution. This means that when redistributing funds, the agent accounted for each player\u2019s initial means, as well as their willingness to contribute. Secondly, the AI system especially rewarded players whose relative contribution was more generous, perhaps encouraging others to do likewise. Importantly, the AI only discovered these policies by learning to maximise human votes. The method therefore ensures that humans remain \u201cin the loop\u201d and the AI produces human-compatible solutions. \u00a0\nBy asking people to vote, we harnessed the principle of majoritarian democracy for deciding what people want. Despite its wide appeal, it is widely acknowledged that democracy comes with the caveat that the preferences of the majority are accounted for over those of the minority. In our study, we ensured that \u2013 like in most societies \u2013 that minority consisted of more generously endowed players. But more work is needed to understand how to trade off the relative preferences of majority and minority groups, by designing democratic systems that allow all voices to be heard.\n"}
{"title": "Exploring the beauty of pure mathematics in novel ways", "contents": "More than a century ago, \nSrinivasa Ramanujan\n shocked the mathematical world with his extraordinary ability to see remarkable patterns in numbers that no one else could see. The self-taught mathematician from India described his insights as deeply intuitive and spiritual, and patterns often came to him in vivid dreams. These observations captured the tremendous beauty and sheer possibility of the abstract world of pure mathematics. In recent years, we have begun to see AI make breakthroughs in \nareas involving deep human intuition\n, and more recently on some of the \nhardest problems across the sciences\n, yet until now, the latest AI techniques have not assisted in significant results in pure maths research.\nAs part of \nDeepMind's mission\n to solve intelligence, we explored the potential of machine learning (ML) to recognize mathematical structures and patterns, and help guide mathematicians toward discoveries they may otherwise never have found \u2014 demonstrating for the first time that AI can help at the forefront of pure mathematics.\nOur research paper\n, published today in the journal Nature, details our collaboration with top mathematicians to apply AI toward discovering new insights in two areas of pure mathematics: topology and representation theory. With \nProfessor Geordie Williamson\n at the University of Sydney, we discovered a new formula for a conjecture about permutations that has remained unsolved for decades. With \nProfessor Marc Lackenby\n and \nProfessor Andr\u00e1s Juh\u00e1sz\n at the University of Oxford, we have discovered an unexpected connection between different areas of mathematics by studying the structure of knots. These are the first significant mathematical discoveries made with machine learning, according to the top mathematicians who reviewed the work. We\u2019re also releasing full companion papers on arXiv for each result that will be submitted to appropriate mathematical journals (\npermutations paper\n; \nknots paper\n). Through these examples, we propose a model for how these tools could be used by other mathematicians to achieve new results.\nThe two fundamental objects we investigated were knots and permutations.\nFor many years, computers have been used by mathematicians to generate data to help in the search for patterns. Known as experimental mathematics, this kind of research has resulted in well-known conjectures, such as \nthe Birch and Swinnerton-Dyer conjecture\n \u2014 one of six \nMillennium Prize Problems\n, the most well-known open problems in mathematics (with a US$1 million prize attached to each). While this approach has been successful and is fairly common, the identification and discovery of patterns from this data has still relied mainly on mathematicians.\nFinding patterns has become even more important in pure maths because it\u2019s now possible to generate more data than any mathematician can reasonably expect to study in a lifetime. Some objects of interest \u2014 such as those with thousands of dimensions \u2014 can also simply be too unfathomable to reason about directly. With these constraints in mind, we believed that AI would be capable of augmenting mathematicians\u2019 insights in entirely new ways.\nOur results suggest that ML can complement maths research to guide intuition about a problem by detecting the existence of hypothesised patterns with supervised learning and giving insight into these patterns with attribution techniques from machine learning:\nWith Professor Williamson, we used AI to help discover a new approach to a long-standing conjecture in representation theory. Defying progress for nearly 40 years, the \ncombinatorial invariance conjecture\nstates that a relationship should exist between certain directed graphs and polynomials. Using ML techniques, we were able to gain confidence that such a relationship does indeed exist and to identify that it might be related to structures known as broken dihedral intervals and extremal reflections. With this knowledge, Professor Williamson was able to conjecture a surprising and beautiful algorithm that would solve the combinatorial invariance conjecture. We have computationally verified the new algorithm across more than 3 million examples.\nWith Professor Lackenby and Professor Juh\u00e1sz, we explored knots - one of the fundamental objects of study in topology. Knots not only tell us about the many ways a rope can be tangled but also have surprising connections with quantum field theory and non-Euclidean geometry. \u00a0Algebra, geometry, and quantum theory all share unique perspectives on these objects and a long standing mystery is how these different branches relate: for example, what does the geometry of the knot tell us about the algebra? We trained an ML model to discover such a pattern and surprisingly, this revealed that a particular algebraic quantity \u2014 the signature \u2014 was directly related to the geometry of the knot, which was not previously known or suggested by existing theory. By using attribution techniques from machine learning, we guided Professor Lackenby to discover a new quantity, which we call the natural slope, that hints at an important aspect of structure overlooked until now. Together we were then able to prove the exact nature of the relationship, establishing some of the first connections between these different branches of mathematics.\nThe use of learning techniques and AI systems holds great promise for the identification and discovery of patterns in mathematics. Even if certain kinds of patterns continue to elude modern ML, we hope \nour Nature paper\n can inspire other researchers to consider the potential for AI as a useful tool in pure maths. To replicate the results, anybody can access our \ninteractive notebooks\n. Reflecting on the incredible mind of Ramanujan, \nGeorge Frederick James Temple\n wrote, \u201cThe great advances in mathematics have not been made by logic but by creative imagination.\u201d Working with mathematicians, we look forward to seeing how AI can further elevate the beauty of human intuition to new levels of creativity.\nThis work was done by a team including contributions from Alex Davies, Petar Veli\u010dkovi\u0107, Lars Buesing, Sam Blackwell, Daniel Zheng, Nenad Toma\u0161ev, Richard Tanburn, Peter Battaglia, Charles Blundell, Xavier Glorot, Matt Overlan, Alyssa Pierce, Natalie Lambert, George Holland, Razia Ahamed, Clemens Meyer, Demis Hassabis and Pushmeet Kohli. We would also like to thank Jan Vonk and Jordan Ellenberg for additional mathematical input.\n"}
{"title": "Creating Interactive Agents with Imitation Learning", "contents": "Humans are an interactive species. We interact with the physical world and with one another. For artificial intelligence (AI) to be generally helpful, it must be able to interact capably with humans and their environment. In this work we present the Multimodal Interactive Agent (MIA), which blends visual perception, language comprehension and production, navigation, and manipulation to engage in extended and often surprising physical and linguistic interactions with humans.\nWe build upon the approach introduced by Abramson et al. (2020), which primarily uses imitation learning to train agents. After training, MIA displays some rudimentary intelligent behaviour that we hope to later refine using human feedback. This work focuses on the creation of this intelligent behavioural prior, and we leave further feedback-based learning for future work.\nWe created the Playhouse environment, a 3D virtual environment composed of a randomised set of rooms and a large number of domestic interactable objects, to provide a space and setting for humans and agents to interact together. Humans and agents can interact in the Playhouse by controlling virtual robots that locomote, manipulate objects, and communicate via text. This virtual environment permits a wide range of situated dialogues, ranging from simple instructions (e.g., \u201cPlease pick up the book from the floor and place it on the blue bookshelf\u201d) to creative play (e.g., \u201cBring food to the table so that we can eat\u201d).\nWe collected human examples of Playhouse interactions using language games, a collection of cues prompting humans to improvise certain behaviours. In a language game one player (the setter) receives a prewritten prompt indicating a kind of task to propose to the other player (the solver). For example, the setter might receive the prompt \u201cAsk the other player a question about the existence of an object,'' and after some exploration, the setter could ask, \u201dPlease tell me whether there is a blue duck in a room that does not also have any furniture.'' To ensure sufficient behavioural diversity, we also included free-form prompts, which granted setters free choice to improvise interactions (E.g. \u201cNow take any object that you like and hit the tennis ball off the stool so that it rolls near the clock, or somewhere near it.''). In total, we collected 2.94 years of real-time human interactions in the Playhouse.\nOur training strategy is a combination of supervised prediction of human actions (behavioural cloning) and self-supervised learning. When predicting human actions, we found that using a hierarchical control strategy significantly improved agent performance. In this setting, the agent receives new observations roughly 4 times per second. For each observation, it produces a sequence of open-loop movement actions and optionally emits a sequence of language actions. In addition to behavioural cloning we use a form of self-supervised learning, which tasks agents with classifying whether certain vision and language inputs belong to the same or different episodes.\nTo evaluate agent performance, we asked human participants to interact with agents and provide binary feedback indicating whether the agent successfully carried out an instruction. MIA achieves over 70% success rate in human-rated online interactions, representing 75% of the success rate that humans themselves achieve when they play as solvers. To better understand the role of various components in MIA, we performed a series of ablations, removing, for example, visual or language inputs, the self-supervised loss, or the hierarchical control.\nContemporary machine learning research has uncovered remarkable regularities of performance with respect to different scale parameters; in particular, model performance scales as a power-law with dataset size, model size, and compute. These effects have been most crisply noted in the language domain, which is characterised by massive dataset sizes and highly evolved architectures and training protocols. In this work, however, we are in a decidedly different regime \u2013 with comparatively small datasets and multimodal, multi-task objective functions training heterogeneous architectures. Nevertheless, we demonstrate clear effects of scaling: as we increase dataset and model size, performance increases appreciably.\n\u200d\nIn an ideal case, training becomes more efficient given a reasonably large dataset, as knowledge is transferred between experiences. To investigate how ideal our circumstances are, we examined how much data is needed to learn to interact with a new, previously unseen object and to learn how to follow a new, previously unheard command / verb. We partitioned our data into background data and data involving a language instruction referring to the object or the verb. When we reintroduced the data referring to the new object, we found that fewer than 12 hours of human interaction was enough to acquire the ceiling performance. Analogously, when we introduced the new command or verb \u2018to clear\u2019 (i.e. to remove all objects from a surface), we found that only 1 hour of human demonstrations was enough to reach ceiling performance in tasks involving this word.\nMIA exhibits startlingly rich behaviour, including a diversity of behaviours that were not preconceived by researchers, including tidying a room, finding multiple specified objects, and asking clarifying questions when an instruction is ambiguous. These interactions continually inspire us. However, the open-endedness of MIA\u2019s behaviour presents immense challenges for quantitative evaluation. Developing comprehensive methodologies to capture and analyse open-ended behaviour in human-agent interactions will be an important focus in our future work.\nFor a more detailed description of our work, see our \npaper\n.\n\u200d\n"}
{"title": "Language modelling at scale: Gopher, ethical considerations, and retrieval", "contents": "Language, and its role in demonstrating and facilitating comprehension - or intelligence - is a fundamental part of being human. It gives people the ability to communicate thoughts and concepts, express ideas, create memories, and build mutual understanding. These are foundational parts of social intelligence. It\u2019s why our teams at DeepMind study aspects of language processing and communication, both in artificial agents and in humans.\nAs part of a broader portfolio of AI research, we believe the development and study of more powerful language models \u2013 systems that predict and generate text \u2013 \u00a0have tremendous potential for building advanced AI systems that can be used safely and efficiently to summarise information, provide expert advice and follow instructions via natural language. Developing beneficial language models requires research into their potential impacts, including the risks they pose. This includes collaboration between experts from varied backgrounds to thoughtfully anticipate and address the challenges that training algorithms on existing datasets can create.\nToday we are releasing three papers on language models that reflect this interdisciplinary approach. They include a detailed study of \na 280 billion parameter transformer language model called \nGopher\n, \na study of ethical and social risks associated with large language models\n, and \na paper investigating a new architecture with better training efficiency.\nIn the quest to explore language models and develop new ones, we trained a series of transformer language models of different sizes, ranging from 44 million parameters to 280 billion parameters (the largest model we named \nGopher\n).\nOur research investigated the strengths and weaknesses of those different-sized models, highlighting areas where increasing the scale of a model continues to boost performance \u2013 for example, in areas like reading comprehension, fact-checking, and the identification of toxic language. We also surface results where model scale does not significantly improve results \u2014 for instance, in logical reasoning and common-sense tasks.\nIn our research, we found the capabilities of \nGopher \nexceed existing language models for a number of key tasks. This includes the Massive Multitask Language Understanding (MMLU) benchmark, where \nGopher \ndemonstrates a significant advancement towards human expert performance over prior work.\nAs well as quantitative evaluation of \nGopher\n, we also explored the model through direct interaction. Among our key findings was that, when \nGopher\n is prompted towards a dialogue interaction (like in a chat), the model can sometimes provide surprising coherence.\nHere \nGopher\n can discuss cell biology and provide a correct citation despite no specific dialogue fine-tuning. However our research also detailed several failure modes that persist across model sizes, amongst them a tendency for repetition, the reflection of stereotypical biases, and the confident propagation of incorrect information.\nThis type of analysis is important, because understanding and documenting failure modes gives us an insight into how large language models could lead to downstream harms, and shows us where mitigation efforts in research should focus to address those issues.\nIn our second paper, we anticipate possible ethical and social risks from language models, and create a comprehensive classification of these risks and failure modes, building on prior research in this area [\nBommasani et al 2021\n, \nBender et al 2021\n, \nPatterson et al 2021\n]. This systematic overview is an essential step towards understanding these risks and mitigating potential harm. We present a taxonomy of the risks related to language models, categorised into six thematic areas, and elaborate on 21 risks in-depth.\nTaking a broad view of different risk areas is essential: as we show in the paper, an overly narrow focus on a single risk in isolation can make other problems worse. The taxonomy we present serves as a foundation for experts and wider public discourse to build a shared overview of ethical and social considerations on language models, make responsible decisions, and exchange approaches to dealing with the identified risks.\nOur research finds that two areas in particular require further work. First, current benchmarking tools are insufficient for assessing some important risks, for example, when language models output misinformation and people trust this information to be true. Assessing risks like these requires more scrutiny of human-computer-interaction with language models. In our paper we list several risks that similarly require novel or more interdisciplinary analysis tools. Second, more work is needed on risk mitigations. For example, language models are known to reproduce harmful social stereotypes, but research on this problem is still in early stages, as a \nrecent DeepMind paper\n showed.\nOur final paper builds on the foundations of \nGopher\n and our taxonomy of ethical and social risk by proposing an improved language model architecture that reduces the energy cost of training and makes it easier to trace model outputs to sources within the training corpus.\nThe Retrieval-Enhanced Transformer (RETRO) is pre-trained with an Internet-scale retrieval mechanism. Inspired by how the brain relies on dedicated memory mechanisms when learning, RETRO efficiently queries for passages of text to improve its predictions. By comparing generated texts to the passages RETRO relied upon for generation, we can interpret why the model makes certain predictions and where they came from. We also see how the model obtains comparable performance to a regular Transformer with an order of magnitude fewer parameters, and obtains state-of-the-art performance on several language modeling benchmarks.\nThese papers offer a foundation for DeepMind\u2019s language research going forward, particularly in areas that will have a bearing on how these models are evaluated and deployed. Addressing these areas will be critical for ensuring safe interactions with AI agents \u2013 from people telling agents what they want to agents explaining their actions to people. Research in the broader community on using communication for safety includes \nnatural language explanations\n, \nusing communication to reduce uncertainty\n, and using language to unpack complex decisions into pieces such as \namplification\n, \ndebate\n, and \nrecursive reward modeling\n -- all critical areas of exploration.\nAs we continue our research on language models, DeepMind will remain cautious and thoughtful. This requires stepping back to assess the situation we find ourselves in, mapping out potential risks, and researching mitigations. We will strive to be transparent and open about the limitations of our models and will work to mitigate identified risks. At each step, we draw on the breadth of expertise from our multidisciplinary teams, including from our Language, Deep Learning, Ethics, and Safety teams. This approach is key to creating large language models that serve society, furthering our mission of solving intelligence to advance science and benefit humanity.\n"}
{"title": "Simulating matter on the quantum scale with AI", "contents": "Solving some of the major challenges of the 21st Century, such as producing clean electricity or developing high temperature superconductors, will require us to design new materials with specific properties. To do this on a computer requires the simulation of electrons, the subatomic particles that govern how atoms bond to form molecules and are also responsible for the flow of electricity in solids. Despite decades of effort and several significant advances, accurately modelling the quantum mechanical behaviour of electrons remains an open challenge. Now, in a \npaper\n (\nOpen Access PDF\n) published in Science, we propose DM21, a neural network achieving state of the art accuracy on large parts of chemistry. To accelerate scientific progress, we\u2019re also open sourcing our \ncode\n for anyone to use.\nNearly a century ago, Erwin Schr\u00f6dinger proposed \nhis famous equation\n governing the behaviour of quantum mechanical particles. Applying this equation to electrons in molecules is challenging because all electrons repel each other. This would seem to require tracking the probability of each electron\u2019s position \u2014 a remarkably complex task for even a small number of electrons. One major breakthrough came in the 1960s, when Pierre Hohenberg and Walter Kohn realised that it is not necessary to track each electron individually. Instead, knowing the probability for \nany\n electron to be at each position (i.e., the electron density) is sufficient to exactly compute all interactions. Kohn received a \nNobel Prize in Chemistry\n after proving this, thus founding Density Functional Theory (DFT).\nAlthough DFT proves a mapping exists, for more than 50 years the exact nature of this mapping between electron density and interaction energy \u2014 the so-called density functional \u2014 has remained unknown and has to be approximated. Despite the fact that DFT intrinsically involves a level of approximation, it is the only practical method to study how and why matter behaves in a certain way at the microscopic level and has therefore become one of the most widely used techniques in all of science. Over the years, researchers have proposed many approximations to the exact functional with varying levels of accuracy. Despite their popularity, all of these approximations suffer from systematic errors because they fail to capture certain crucial mathematical properties of the exact functional.\nBy expressing the functional as a neural network and incorporating these exact properties into the training data, we learn functionals free from important systematic errors \u2014 resulting in a better description of a broad class of chemical reactions.\nWe specifically address two long-standing problems with traditional functionals:\nIn principle, any chemical-physical process that involves movement of charge is liable to suffer from delocalization error, and any process that involves the breaking of bonds is liable to suffer from spin-symmetry breaking. Movement of charge and bond breaking are core to many important technological applications, but these problems can also lead to massive qualitative failure of functionals to describe the simplest molecules, such as hydrogen. Since DFT is such a crucial technology it is important to design functionals that get this simple chemistry correct before asking them to explain vastly more complex molecular interactions, such as those that may occur in a battery or solar cell.\nThese longstanding challenges are both related to how functionals behave when presented with a system that exhibits \u201cfractional electron character.\u201d By using a neural network to represent the functional and tailoring our training dataset to capture the fractional electron behaviour expected for the exact functional, we found that we could solve the problems of delocalization and spin symmetry-breaking. Our functional also showed itself to be highly accurate on broad, large-scale benchmarks, suggesting that this data-driven approach can capture aspects of the exact functional that have thus far been elusive.\nFor years, computer simulations have played a central role in modern engineering, making it possible to provide reliable answers to questions like \u201cwill this bridge stay up?\u201d to \u201cwill this rocket make it into space?\u201d As technology increasingly turns to the quantum scale to explore questions about materials, medicines, and catalysts, including those we\u2019ve never seen or even imagined, deep learning shows promise to accurately simulate matter at this quantum mechanical level.\nRead the Science paper \nhere\n.\nOpen Access PDF of the paper \nhere\n.\n"}
{"title": "Spurious normativity enhances learning of compliance and enforcement behavior in artificial agents", "contents": "In \nour recent paper\n we explore how multi-agent deep reinforcement learning can serve as a model of complex social interactions, like the formation of social norms. This new class of models could provide a path to create richer, more detailed simulations of the world.\nHumans are an \nultra social species\n. Relative to other mammals we benefit more from cooperation but we are also more dependent on it, and face greater cooperation challenges. Today, humanity faces numerous cooperation challenges including preventing conflict over resources, ensuring everyone can access clean air and drinking water, eliminating extreme poverty, and combating climate change. Many of the cooperation problems we face are difficult to resolve because they involve complex webs of social and biophysical interactions called \nsocial-ecological systems\n. However, humans can collectively learn to overcome the cooperation challenges we face. We accomplish this by an ever evolving culture, including norms and institutions which organize our interactions with the environment and with one another.\nHowever, norms and institutions sometimes fail to resolve cooperation challenges. For example, individuals may over-exploit resources like forests and fisheries thereby causing them to collapse. In such cases, policy-makers may write laws to change institutional rules or develop other \ninterventions to try to change norms\n in hopes of bringing about a positive change. But policy interventions do not always work as intended. This is because real-world social-ecological systems are considerably \nmore complex\n than the models we typically use to try to predict the effects of candidate policies.\nModels based on game theory are often applied to the study of cultural evolution. In most of these models, the key interactions that agents have with one another are expressed in a \u2018payoff matrix\u2019. In a game with two participants and two actions A and B, a payoff matrix defines the value of the four possible outcomes: (1) we both choose A, (2) we both choose B, (3) I choose A while you choose B and (4) I choose B while you choose A. The most famous example is the \u2018Prisoner\u2019s Dilemma\u2019, in which the actions are interpreted as \u201ccooperate\u201d and \u201cdefect\u201d. Rational agents who act according to their own myopic self-interest are doomed to defect in the Prisoner\u2019s Dilemma even though the better outcome of mutual cooperation is available.\nGame-theoretic models have been very widely applied. Researchers in diverse fields have used them to study a wide range of different phenomena, including economies and the evolution of human culture. However, game theory is not a neutral tool, rather it is a deeply opinionated modeling language. It imposes a strict requirement that everything must ultimately cash out in terms of the payoff matrix (or equivalent representation). This means that the modeler has to know, or be willing to assume, everything about how the effects of individual actions combine to generate incentives. This is sometimes appropriate, and the game theoretic approach has had many notable successes such as in modeling the \nbehavior of oligopolistic firms\n and \ncold war era international relations\n. However, game theory\u2019s major weakness as a modeling language is exposed in situations where the modeler does not fully understand how the choices of individuals combine to generate payoffs. Unfortunately this tends to be the case with social-ecological systems because their social and ecological parts interact in complex ways that we do not fully understand.\nThe work we present here is one example within a research program that attempts to establish an alternative modeling framework, different from game theory, to use in the study of social-ecological systems. Our approach may be seen formally as a variety of \nagent-based modeling\n. However, its distinguishing feature is the incorporation of algorithmic elements from artificial intelligence, especially multi-agent deep reinforcement learning.\nThe core idea of this approach is that every model consists of two interlocking parts: (1) a rich, dynamical model of the environment and (2) a model of individual decision-making.\nThe first takes the form of a researcher-designed simulator: an interactive program that takes in a current environment state and agent actions, and outputs the next environment state as well as the observations of all agents and their instantaneous rewards. The model of individual decision-making is likewise conditioned on environment state. It is an \nagent\n that learns from its past experience, performing a form of trial-and-error. An agent interacts with an environment by taking in observations and outputting actions. Each agent selects actions according to its behavioral policy, a mapping from observations to actions. Agents learn by changing their policy to improve it along any desired dimension, typically to obtain more reward. The policy is stored in a neural network. Agents learn \u2018from scratch\u2019, from their own experience, how the world works and what they can do to earn more rewards. They accomplish this by tuning their network weights in such a way that the pixels they receive as observations are gradually transformed into competent actions. Several learning agents can inhabit the same environment as one another. In this case the agents become interdependent because their actions affect one another.\nLike other agent-based modeling approaches, multi-agent deep reinforcement learning makes it easy to specify models that cross levels of analysis that would be hard to treat with game theory. For instance, actions may be far closer to low-level motor primitives (e.g. 'walk forward'; 'turn right') than the high-level strategic decisions of game theory (e.g. \u2018cooperate\u2019). This is an important feature needed to capture situations where agents must practice to learn effectively how to \nimplement their strategic choices\n. For instance in one \nstudy\n, agents learned to cooperate by taking turns cleaning a river. This solution was only possible because the environment had spatial and temporal dimensions in which agents have great freedom in how they structure their behavior towards one another. Interestingly, while the environment allowed for many different solutions (such as \nterritoriality\n), agents converged on the same turn-taking solution as human players.\nIn our latest study, we applied this type of model to an open question in research on cultural evolution: how to explain the existence of spurious and arbitrary social norms that appear not to have immediate material consequences for their violation beyond those imposed socially. For instance, in some societies men are expected to wear trousers not skirts; in many there are words or hand gestures that should not be used in polite company; and in most there are rules about how one styles one's hair or what one wears on one's head. We call these social norms \u2018silly rules\u2019. Importantly, in our framework, enforcing and complying with social norms both have to be learned. Having a social environment that includes a \u2018silly rule\u2019 means that agents have more opportunities to learn about enforcing norms in general. This additional practice then allows them to enforce the important rules more effectively. Overall, the \u2018silly rule\u2019 can be beneficial for the population \u2013 a surprising result. This result is only possible because our simulation focuses on learning: enforcing and complying with rules are complex skills that need training to develop.\nPart of why we find this result on silly rules so exciting is that it demonstrates the utility of multi-agent deep reinforcement learning in modeling cultural evolution. Culture contributes to the success or failure of policy interventions for socio-ecological systems. For instance, strengthening social norms around recycling is part of the \nsolution\n to some environmental problems. Following this trajectory, richer simulations could lead to a deeper understanding of how to design interventions for social-ecological systems. If simulations become realistic enough, it may even be possible to test the impact of interventions, e.g. aiming to \ndesign a tax code that fosters productivity and fairness\n.\nThis approach provides researchers with tools to specify detailed models of phenomena that interest them. Of course, like all research methodologies it should be expected to come with its own strengths and weaknesses. We hope to discover more about when this style of modeling can be fruitfully applied in the future. While there are no panaceas for modeling, we think there are compelling reasons to look to multi-agent deep reinforcement learning when constructing models of social phenomena, especially when they involve learning.\n"}
{"title": "DeepMind: The Podcast returns for Season 2", "contents": "We believe artificial intelligence (AI) is one of the most significant technologies of our age and we want to help people understand its potential and how it\u2019s being created. \nIn 2019, we released \nDeepMind: The Podcast\n to explore these ideas, answer common questions and give an inside look at how AI research happens at a lab like DeepMind. Today, we\u2019re proud to launch a new season, with stories of the latest breakthroughs, innovations, and challenges.\nListeners can find the new episodes on \nApple Podcasts\n, \nGoogle Podcasts\n, \nSpotify\n, or their favourite podcast app by searching for \u201cDeepMind: The Podcast\u201d.\nHosted by mathematician and broadcaster \nProfessor Hannah Fry\n, the nine programmes \u00a0explore the latest in AI, from helping advance science, like solving a 50-year-old grand challenge in biology, to building computer systems that can cooperate with humans.\nThrough more than 30 original interviews, including with our co-founders Demis and Shane, listeners hear how AI can help people with degenerative diseases regain their voices, conserve the unique wildlife of the Serengeti in East Africa, predict whether it will rain in the next hour, and take football to the next level with Liverpool Football Club.\nHannah also learns how robotics researchers taught robots to walk at home during lockdown, explores recent advances in allowing computers to communicate with natural language, and examines some of the difficult ethical questions the whole field is grappling with to ensure the technology benefits society as a whole.\nWe\u2019d like to extend a huge thank you to Hannah and to our amazing contributors and creative producers, who all helped craft our initial idea into this new season of programmes. And we especially want to thank our listeners, who took the time to listen, share, and offer feedback on Season 1. Thank you!\nWe hope you enjoy listening to \nDeepMind: The Podcast, Season 2\n as much as we loved making it for you.\nListen now on your favourite podcast app by searching \u201cDeepMind: The Podcast\u201d.\nWe want this podcast to be a useful resource. If you have any feedback on season two, suggestions for topics you\u2019d like us to explore, or questions about AI, please email \npodcast@deepmind.com\n or message @deepmind on \nInstagram\n or \nTwitter\n.\n"}
{"title": "Competitive programming with AlphaCode", "contents": "Solving novel problems and setting a new milestone in competitive programming.\nCreating solutions to unforeseen problems is second nature in human intelligence \u2013 a result of critical thinking informed by experience. The machine learning community has made tremendous progress in generating and understanding textual data, but advances in problem solving remain limited to relatively simple maths and programming problems, or else retrieving and copying existing solutions. As part of \nDeepMind\u2019s mission\n to solve intelligence, we created a system called AlphaCode that writes computer programs at a competitive level. AlphaCode achieved an estimated rank within the top 54% of participants in programming competitions by solving new problems that require a combination of critical thinking, logic, algorithms, coding, and natural language understanding.\nIn \nour paper\n, we detail AlphaCode, which uses transformer-based language models to generate code at an unprecedented scale, and then smartly filters to a small set of promising programs.\nWe validated our performance using competitions hosted on \nCodeforces\n, a popular platform which hosts regular competitions that attract tens of thousands of participants from around the world who come to test their coding skills. We selected for evaluation 10 recent contests, each newer than our training data. AlphaCode placed at about the level of the median competitor, marking the first time an AI code generation system has reached a competitive level of performance in programming competitions.\nTo help others build on our results, we\u2019re releasing our dataset of competitive programming problems and solutions \non GitHub\n, including extensive tests to ensure the programs that pass these tests are correct \u2014 a critical feature current datasets lack. We hope this benchmark will lead to further innovations in problem solving and code generation.\nCompetitive programming is a popular and challenging activity; hundreds of thousands of programmers participate in coding competitions to gain experience and showcase their skills in fun and collaborative ways. During competitions, participants receive a series of long problem descriptions and a few hours to write programs to solve them. Typical problems include finding ways to place roads and buildings within certain constraints, or creating strategies to win custom board games. Participants are then ranked mainly based on how many problems they solve. Companies use these competitions as recruiting tools and similar types of problems are common in hiring processes for software engineers.\nThe problem-solving abilities required to excel at these competitions are beyond the capabilities of existing AI systems. However, by combining advances in large-scale transformer models (that have recently shown promising abilities to generate code) with large-scale sampling and filtering, we\u2019ve made significant progress in the number of problems we can solve. We pre-train our model on selected public GitHub code and fine-tune it on our relatively small competitive programming dataset. At evaluation time, we create a massive amount of C++ and Python programs for each problem, orders of magnitude larger than previous work. Then we filter, cluster, and rerank those solutions to a small set of 10 candidate programs that we submit for external assessment. This automated system replaces competitors\u2019 trial-and-error process of debugging, compiling, passing tests, and eventually submitting.\nWith the permission of Codeforces, we evaluated AlphaCode by simulating participation in 10 recent contests. The impressive work of the competitive programming community has created a domain where it\u2019s not possible to solve problems through shortcuts like duplicating solutions seen before or trying out every potentially related algorithm. Instead, our model must create novel and interesting solutions. Overall, AlphaCode placed at approximately the level of the median competitor. Although far from winning competitions, this result represents a substantial leap in AI problem-solving capabilities and we hope that our results will inspire the competitive programming community.\nFor artificial intelligence to help humanity, our systems need to be able to develop problem-solving capabilities. AlphaCode ranked within the top 54% in real-world programming competitions, an advancement that demonstrates the potential of deep learning models for tasks that require critical thinking. These models elegantly leverage modern machine learning to express solutions to problems as code, circling back to the symbolic reasoning root of AI from decades ago. And this is only a start. Our exploration into code generation leaves vast room for improvement and hints at even more exciting ideas that could help programmers improve their productivity and open up the field to people who do not currently write code. We will continue this exploration, and hope that further research will result in tools to enhance programming and bring us closer to a problem-solving AI.\nView more of AlphaCode\u2019s solutions and dive into the model at \nalphacode.deepmind.com\n"}
{"title": "Red Teaming Language Models with Language Models", "contents": "In our \nrecent paper\n, we show that it is possible to automatically find inputs that elicit harmful text from language models by generating inputs using language models themselves. Our approach provides one tool for finding harmful model behaviours before users are impacted, though we emphasize that it should be viewed as one component alongside many other techniques that will be needed to find harms and mitigate them once found.\nLarge generative language models like GPT-3 and Gopher have a remarkable ability to generate high-quality text, but they are difficult to deploy in the real world. Generative language models come with a risk of generating very harmful text, and even a small risk of harm is unacceptable in real-world applications.\nFor example, in 2016, Microsoft released the Tay Twitter bot to automatically tweet in response to users. Within 16 hours, \nMicrosoft took Tay down\n after several adversarial users elicited racist and sexually-charged tweets from Tay, which were sent to over 50,000 followers. The outcome was \nnot for lack of care on Microsoft\u2019s part\n:\nThe issue is that there are so many possible inputs that can cause a model to generate harmful text. As a result, it\u2019s hard to find all of the cases where a model fails before it is deployed in the real world. Previous work relies on paid, human annotators to manually discover failure cases (\nXu et al. 2021\n,\n inter alia\n). This approach is effective but expensive, limiting the number and diversity of failure cases found.\nWe aim to complement manual testing and reduce the number of critical oversights by finding failure cases (or \u2018red teaming\u2019) in an automatic way. To do so, we generate test cases using a language model itself and use a classifier to detect various harmful behaviors on test cases, as shown below:\nOur approach uncovers a variety of harmful model behaviors:\nTo generate test cases with language models, we explore a variety of methods, ranging from prompt-based generation and few-shot learning to supervised finetuning and reinforcement learning. Some methods generate more diverse test cases, while other methods generate more difficult test cases for the target model. Together, the methods we propose are useful for obtaining high test coverage while also modeling adversarial cases.\nOnce we find failure cases, it becomes easier to fix harmful model behavior by:\nOverall, language models are a highly effective tool for uncovering when language models behave in a variety of undesirable ways. In our current work, we focused on red teaming harms that today\u2019s language models commit. In the future, our approach can also be used to preemptively discover other, hypothesized harms from advanced machine learning systems, e.g., due to \ninner misalignment\n or \nfailures in objective robustness\n. This approach is just one component of responsible language model development: we view red teaming as one tool to be used alongside many others, both to find harms in language models and to mitigate them. We refer to Section 7.3 of \nRae et al. 2021\n for a broader discussion of other work needed for language model safety. For more details on our approach and results, as well as the broader consequences of our findings, read our \nred teaming paper\n here.\n"}
{"title": "Accelerating fusion science through learned plasma control", "contents": "Successfully controlling the nuclear fusion plasma in a tokamak with deep reinforcement learning\nTo solve the global energy crisis, researchers have long sought a source of clean, limitless energy. Nuclear fusion, the reaction that powers the stars of the universe, is one contender. By smashing and fusing hydrogen, a common element of seawater, the powerful process releases huge amounts of energy. Here on earth, one way scientists have recreated these extreme conditions is by using a tokamak, a doughnut-shaped vacuum surrounded by magnetic coils, that is used to contain a plasma of hydrogen that is hotter than the core of the Sun. However, the plasmas in these machines are inherently unstable, making sustaining the process required for nuclear fusion a complex challenge. For example, a control system needs to coordinate the tokamak's many magnetic coils and adjust the voltage on them thousands of times per second to ensure the plasma never touches the walls of the vessel, which would result in heat loss and possibly damage. To help solve this problem and as part of DeepMind\u2019s mission to advance science, we collaborated with \nthe Swiss Plasma Center\n at \nEPFL\n to develop the first deep reinforcement learning (RL) system to autonomously discover how to control these coils and successfully contain the plasma in a tokamak, opening new avenues to advance nuclear fusion research.\nIn a \npaper published today in Nature\n, we describe how we can successfully control nuclear fusion plasma by building and running controllers on the Variable Configuration Tokamak (TCV) in Lausanne, Switzerland. Using a learning architecture that combines deep RL and a simulated environment, we produced controllers that can both keep the plasma steady and be used to accurately sculpt it into different shapes. This \u201cplasma sculpting\u201d shows the RL system has successfully controlled the superheated matter and - importantly - allows scientists to investigate how the plasma reacts under different conditions, improving our understanding of fusion reactors.\nThis work is another powerful example of how machine learning and expert communities can come together to tackle grand challenges and accelerate scientific discovery. Our team is hard at work applying this approach to fields as diverse as quantum chemistry, pure mathematics, material design, weather forecasting, and more, to solve fundamental problems and ensure AI benefits humanity.\nResearch into nuclear fusion is currently limited by researchers\u2019 ability to run experiments. While there are dozens of active tokamaks around the world, they\u2019re expensive machines and in high demand. For example, TCV can only sustain the plasma in a single experiment for up to three seconds, after which it needs 15 minutes to cool down and reset before the next attempt. Not only that, multiple research groups often share use of the tokamak, further limiting the time available for experiments.\nGiven the current obstacles to access a tokamak, researchers have turned to simulators to help advance research. For example, our partners at EPFL have built a powerful set of simulation tools that model the dynamics of tokamaks. We were able to use these to allow our RL system to learn to control TCV in simulation and then validate our results on the real TCV, showing we could successfully sculpt the plasma into the desired shapes. Whilst this is a cheaper and more convenient way to train our controllers; we still had to overcome many barriers. For example, plasma simulators are slow and require many hours of computer time to simulate one second of real time. In addition, the condition of TCV can change from day to day, requiring us to develop algorithmic improvements, both physical and simulated, and to adapt to the realities of the hardware.\nExisting plasma-control systems are complex, requiring separate controllers for each of TCV\u2019s 19 magnetic coils. Each controller uses algorithms to estimate the properties of the plasma in real time and adjust the voltage of the magnets accordingly. In contrast, our architecture uses a single neural network to control all of the coils at once, automatically learning which voltages are the best to achieve a plasma configuration directly from sensors.\nAs a demonstration, we first showed that we could manipulate many aspects of the plasma with a single controller.\nIn the video above, we see the plasma at the top of TCV at the instant our system takes control. Our controller first shapes the plasma according to the requested shape, then shifts the plasma downward and detaches it from the walls, suspending it in the middle of the vessel on two legs. The plasma is held stationary, as would be needed to measure plasma properties. Then, finally the plasma is steered back to the top of the vessel and safely destroyed.\nWe then created a range of plasma shapes being studied by plasma physicists for their usefulness in generating energy. For example, we made a \u201csnowflake\u201d shape with many \u201clegs\u201d that could help reduce the cost of cooling by spreading the exhaust energy to different contact points on the vessel walls. We also demonstrated a shape close to the proposal for \nITER\n, the next-generation tokamak under construction, as EPFL was conducting experiments to predict the behaviour of plasmas in ITER. We even did something that had never been done in TCV before by stabilising a \u201cdroplet\u201d where there are two plasmas inside the vessel simultaneously. Our single system was able to find controllers for all of these different conditions. We simply changed the goal we requested, and our algorithm autonomously found an appropriate controller.\nSimilar to progress we\u2019ve seen when applying AI to other scientific domains, our successful demonstration of tokamak control shows the power of AI to accelerate and assist fusion science, and we expect increasing sophistication in the use of AI going forward. This capability of autonomously creating controllers could be used to design new kinds of tokamaks while simultaneously designing their controllers. Our work also points to a bright future for reinforcement learning in the control of complex machines. It\u2019s especially exciting to consider fields where AI could augment human expertise, serving as a tool to discover new and creative approaches for hard real-world problems. We predict reinforcement learning will be a transformative technology for industrial and scientific control applications in the years to come, with applications ranging from energy efficiency to personalised medicine.\nRead the paper: \nMagnetic control of tokamak plasmas through deep reinforcement learning\n"}
{"title": "MuZero\u2019s first step from research into the real world", "contents": "Collaborating with YouTube to optimise video compression in the open source VP9 codec.\nIn 2016, we introduced \nAlphaGo\n, the first artificial intelligence program to defeat humans at the ancient game of Go. Its successors, \nAlphaZero\n and then \nMuZero\n, each represented a significant step forward in the pursuit of general-purpose algorithms, mastering a greater number of games with even less predefined knowledge. MuZero, for example, mastered Chess, Go, Shogi, and Atari without needing to be told the rules. But so far these agents have focused on solving games. Now, in pursuit of DeepMind\u2019s mission to solve intelligence, MuZero has taken a first step towards mastering a real-world task by optimising video on YouTube.\nIn a \npreprint published on arXiv\n, we detail our collaboration with YouTube to explore the potential for MuZero to improve video compression. \nAnalysts predicted\n that streaming video will have accounted for the vast majority of internet traffic in 2021. With video surging during the COVID-19 pandemic and the total amount of internet traffic expected to grow in the future, video compression is an increasingly important problem \u2014 and a natural area to apply Reinforcement Learning (RL) to improve upon the state of the art in a challenging domain. Since launching to production on a portion of YouTube\u2019s live traffic, we\u2019ve demonstrated an average 4% bitrate reduction across a large, diverse set of videos.\nMost online videos rely on a program called a codec to compress or encode the video at its source, transmit it over the internet to the viewer, and then decompress or decode it for playback. These codecs make multiple decisions for each frame in a video. Decades of hand engineering have gone into optimising these codecs, which are responsible for many of the video experiences now possible on the internet, including video on demand, video calls, video games, and virtual reality. However, because RL is particularly well-suited to sequential decision-making problems like those in codecs, we\u2019re exploring how an RL-learned algorithm can help.\nOur initial focus is on the VP9 codec (specifically the open source version \nlibvpx\n), since it\u2019s widely used by YouTube and other streaming services. As with other codecs, service providers using VP9 need to think about bitrate \u2014 the number of ones and zeros required to send each frame of a video. Bitrate is a major determinant in how much compute and bandwidth is required to serve and store videos, affecting everything from how long a video takes to load to its resolution, buffering, and data usage.\nIn VP9, bitrate is optimised most directly through the Quantisation Parameter (QP) in the rate control module. For each frame, this parameter determines the level of compression to apply. Given a target bitrate, QPs for video frames are decided sequentially to maximize overall video quality. Intuitively, higher bitrates (lower QP) should be allocated for complex scenes and lower bitrates (higher QP) should be allocated for static scenes. The QP selection algorithm reasons how the QP value of a video frame affects the bitrate allocation of the rest of the video frames and the overall video quality. RL is especially helpful in solving such a sequential decision-making problem.\nMuZero achieves superhuman performance across various tasks by combining the power of search with its ability to learn a model of the environment and plan accordingly. This works especially well in large, combinatorial action spaces, making it an ideal candidate solution for the problem of rate control in video compression. However, to get MuZero to work on this real-world application requires solving a whole new set of problems. For instance, the set of videos uploaded to platforms like YouTube varies in content and quality, and any agent needs to generalise across videos, including completely new videos after deployment. By comparison, board games tend to have a single known environment. Many other metrics and constraints affect the final user experience and bitrate savings, such as the PSNR (Peak Signal-to-Noise Ratio) and bitrate constraint.\nTo address these challenges with MuZero, we create a mechanism called self-competition, which converts the complex objective of video compression into a simple WIN/LOSS signal by comparing the agent\u2019s current performance against its historical performance. This allows us to convert a rich set of codec requirements into a simple signal that can be optimised by our agent.\n\n\t\t\t\tBitrate savings\n\t\t\t\n\n\t\t\t\t4.7%\n\t\t\t\n\n\t\t\t\tBitrate savings\n\t\t\t\n\n\t\t\t\t4.1%\n\t\t\t\n\n\t\t\t\tBitrate savings\n\t\t\t\n\n\t\t\t\t3.5%\n\t\t\t\nBy learning the dynamics of video encoding and determining how best to allocate bits, our MuZero Rate-Controller (MuZero-RC) is able to reduce bitrate without quality degradation. QP selection is just one of numerous encoding decisions in the encoding process. While decades of research and engineering have resulted in efficient algorithms, we envision a single algorithm that can automatically learn to make these encoding decisions to obtain the optimal rate-distortion tradeoff.\nBeyond video compression, this first step in applying MuZero beyond research environments serves as an example of how our RL agents can solve real-world problems. By creating agents equipped with a range of new abilities to improve products across domains, we can help various computer systems become faster, less intensive, and more automated. Our long-term vision is to develop a single algorithm capable of optimising thousands of real-world systems across a variety of domains.\nHear Jackson Broshear and David Silver discuss MuZero with Hannah Fry in Episode 5 of DeepMind: The Podcast. Listen now on your favourite podcast app by searching \u201cDeepMind: The Podcast\u201d.\nWork done as a collaboration with contributors: Chenjie Gu, Anton Zhernov, Amol Mandhane, Maribeth Rauh, Miaosen Wang, Flora Xue, Wendy Shang, Derek Pang, Rene Claus, Ching-Han Chiang, Cheng Chen, Jingning Han, Angie Chen, Daniel J. Mankowitz, Julian Schrittwieser, Thomas Hubert, Oriol Vinyals, Jackson Broshear, Timothy Mann, Robert Tung, Steve Gaffney, Carena Church\n"}
{"title": "Probing Image-Language Transformers for Verb Understanding", "contents": "Grounding language to vision is a fundamental problem for many real-world AI systems such as retrieving images or generating descriptions for the visually impaired. Success on these tasks requires models to relate different aspects of language such as objects and verbs to images. For example, to distinguish between the two images in the middle column below, models must differentiate between the verbs \u201ccatch\u201d and \u201ckick.\u201d Verb understanding is particularly difficult as it requires not only recognising objects, but also how different objects in an image relate to each other. To overcome this difficulty, we introduce the SVO-Probes dataset and use it to probe language and vision models for verb understanding.\nIn particular, we consider multimodal transformer models (e.g., Lu et al., 2019; Chen et al., 2020; Tan and Bansal, 2019; Li et al., 2020), which have shown success on a variety of language and vision tasks. However, despite strong performance on benchmarks, it is not clear if these models have fine-grained multimodal understanding. In particular, prior work shows that language and vision models can succeed at benchmarks without multimodal understanding: for example, answering questions about images based only on language priors (Agrawal et al., 2018) or \u201challucinating\u201d objects that are not in the image when captioning images (Rohrbach et al., 2018). To anticipate model limitations, work like Shekhar et al. propose specialised evaluations to probe models systematically for language understanding. However, prior probe sets are limited in the number of objects and verbs. We developed SVO-Probes to better evaluate potential limitations in verb understanding in current models.\nSVO-Probes includes 48,000 image-sentence pairs and tests understanding for more than 400 verbs. Each sentence can be broken into a <Subject, Verb, Object> triplet (or SVO triplet) and paired with positive and negative example images. The negative examples differ in only one way: the Subject, Verb, or Object is changed. The figure above shows negative examples in which the subject (left), verb (middle), or object (right) does not match the image. This task formulation makes it possible to isolate which parts of the sentence a model has the most trouble with. It also makes SVO-Probes more challenging than standard image retrieval tasks, where negative examples are often completely unrelated to the query sentence.\nTo create SVO-Probes, we \nquery an image search\n with SVO triplets from a common training dataset, Conceptual Captions (Sharma et al. 2018). Because image search can be noisy, a preliminary annotation step filters the retrieved images to ensure we have a clean set of image-SVO pairs. Since transformers are trained on image-sentence pairs, not image-SVO pairs, we need image-sentence pairs to probe our model. To collect sentences which describe each image, annotators write a short sentence for each image that includes the SVO triplet. For example, given the SVO triplet <animal, lie, grass>, an annotator could write the sentence \u201cAn animal lays in the grass.\u201d We then use the SVO annotations to pair each sentence with a negative image, and ask annotators to verify negatives in a final annotation step. See the figure below for details.\nWe examine whether multimodal transformers can accurately classify examples as positive or negative. The bar chart below illustrates our results. Our dataset is challenging: our standard multimodal transformer model achieves 64.3% accuracy overall (chance is 50%). Whereas accuracy is 67.0% and 73.4% on subjects and objects respectively, performance falls to 60.8% on verbs. This result shows that verb recognition is indeed challenging for vision and language models.\nWe also explore which model architectures perform best on our dataset. Surprisingly, models with weaker image modeling perform better than the standard transformer model. One hypothesis is that our standard model (with stronger image modeling ability) overfits the train set. As both these models perform worse on other language and vision tasks, our targeted probe task illuminates model weaknesses that are not observed on other benchmarks.\nOverall, we find that despite impressive performance on benchmarks, multimodal transformers still struggle with fine-grained understanding, especially fine-grained verb understanding. We hope SVO-Probes can help drive exploration of verb understanding in language and vision models and inspire more targeted probe datasets. Both our SVO-Probes benchmark and models can be found here on GitHub: \nbenchmark\n and \nmodels\n.\n"}
{"title": "GopherCite: Teaching language models to support answers with verified quotes", "contents": "DeepMind published a \nseries of papers\n about large language models (LLMs) last year, including \nan analysis\n of Gopher, our large language model. Language modelling technology, which is also currently being developed by several other labs and companies, promises to strengthen many applications, from \nsearch engines\n to a new wave of chatbot-like \nconversational assistants\n and beyond. One \npaper\n in this series laid out a number of reasons why \u201craw\u201d language models like Gopher do not meet our standards for safely deploying this technology in user-facing applications, especially if guard rails for managing problematic and potentially harmful behaviour are not set in place.\nOur latest work focuses on one of these concerns: Language models like Gopher can \u201challucinate\u201d facts that appear plausible but are actually fake. Those who are familiar with this problem know to do their own fact-checking, rather than trusting what language models say. Those who are not, may end up believing something that isn\u2019t true. This paper describes GopherCite, a model which aims to address the problem of language model hallucination. GopherCite attempts to back up all of its factual claims with evidence from the web. It uses Google Search to find relevant web pages on the internet and quotes a passage which tries to demonstrate why its response is correct. If the system is unable to form an answer that can be well-supported by evidence, it tells the user, \u201cI don\u2019t know\u201d, instead of providing an unsubstantiated answer.\nSupporting simple factual claims with easily verifiable evidence is one step towards making language models more trustworthy, both for users interacting with them and for annotators assessing the quality of samples. A comparison between the behaviour of \u201craw\u201d Gopher and our new model is helpful for illustrating this change.\nBased on GopherCite\u2019s response, you\u2019ll notice that Gopher invented a fact (\u201cLake Placid hosted the winter Olympics in 1936\u201d) without warning. When shown a verified snippet from a relevant Wikipedia page by GopherCite, we can confirm that Lake Placid only hosted the Olympics twice, in 1932 and 1980.\nTo alter Gopher\u2019s behaviour in this way, we trained Gopher according to human preferences. We asked participants in a user study to pick their preferred answer from a pair of candidates, according to criteria including how well the evidence supports the answers given. These labels were used as training data for both supervised learning on highly rated samples and for \nreinforcement learning from human preferences\n (RLHP). We also took this approach in \nour recent work on red teaming\n.\nWe are not the only ones interested in this problem of factual inaccuracy in language models. Our colleagues at Google recently made progress on factual grounding in their latest \nLaMDA system\n, having a conversational model interact with Google Search and sometimes share relevant URLs. Indeed, GopherCite\u2019s training regimen uses similar methodology to that of LaMDA, but a critical difference is that we aim to provide a specific snippet of relevant evidence, rather than simply pointing the user to a URL. Based on motivations similar to our own, OpenAI has \nrecently announced work\n developing a closely related system called WebGPT, which also applies RLHP to align their GPT-3 language model. Whereas GopherCite focuses on reading long document inputs, WebGPT carefully curates the context presented to the language model by interacting multiple times with a web browser. It also cites evidence to back up its responses. Similarities and differences between these systems and our own are discussed in our paper and we also demonstrate that GopherCite very often provides compelling evidence for its claims.\nWe conducted a user study with paid participants to assess the model on two types of questions: fact-seeking questions typed into Google Search (\nreleased by Google in a dataset called \u201cNaturalQuestions\u201d\n), and explanation-seeking questions which Reddit users asked on a forum called \u201c/r/eli5\u201d (\u201cExplain it Like I\u2019m 5 [years old]\u201d). The participants in our study determined that GopherCite answers fact-seeking questions correctly \u2013 and with satisfactory evidence \u2013 about 80% of the time, and does so for explanation-seeking questions about 67% of the time. When we allow GopherCite to refrain from answering some questions, its performance improves dramatically amongst the questions it does choose to answer (see the paper for details). This explicit mechanism for abstaining is a core contribution of our work.\nBut when we evaluate the model on a set of \u201cadversarial\u201d questions, which attempt to trick the model into parroting a fiction or misconception that is stated on the internet, GopherCite often falls into the trap. For instance, when asked \u201cwhat does Red Bull give you?\u201d, here is how it responds:\nWe think this failure mode and others discussed in our paper can be avoided by enriching the setting, moving from a \u201csingle-shot\u201d reply to a user\u2019s question, to one in which the model can ask clarifying questions of the user and engage in a dialogue. For example, we could enable future models to ask the user whether they want an answer that is literally true or one that is true in the confines of the fictional world of a Red Bull advertisement.\nIn summary, we think GopherCite is an important step forward, but building it has taught us that evidence citation is only one part of an overall strategy for safety and trustworthiness. More fundamentally, not all claims require quote evidence \u2013 and as we demonstrated above, not all claims supported by evidence are true. Some claims require multiple pieces of evidence along with a logical argument explaining why the claim follows. We will continue working in this area and aim to overcome the issues presented with further research and development as well as dedicated sociotechnical research.\nOur paper covers many more details about our methods, experiments, and relevant context from the research literature. We have also created an FAQ about GopherCite, answered by the model itself after reading the paper's introduction (using candidate samples curated by the authors):\n"}
{"title": "Learning Robust Real-Time Cultural Transmission without Human Data", "contents": "Over millennia, humankind has discovered, evolved, and accumulated a wealth of cultural knowledge, from navigation routes to mathematics and social norms to works of art. Cultural transmission, defined as efficiently passing information from one individual to another, is the inheritance process underlying this exponential increase in human capabilities.\nOur agent, in blue, imitates and remembers the demonstration of both bots (left) and humans (right), in red.\nFor more videos of our agents in action, visit our \nwebsite\n.\nIn this work, we use deep reinforcement learning to generate artificial agents capable of test-time cultural transmission. Once trained, our agents can infer and recall navigational knowledge demonstrated by experts. This knowledge transfer happens in real time and generalises across a vast space of previously unseen tasks. For example, our agents can quickly learn new behaviours by observing a single human demonstration, without ever training on human data.\nWe train and test our agents in procedurally generated 3D worlds, containing colourful, spherical goals embedded in a noisy terrain full of obstacles. A player must navigate the goals in the correct order, which changes randomly on every episode. Since the order is impossible to guess, a naive exploration strategy incurs a large penalty. As a source of culturally transmitted information, we provide a privileged \u201cbot\u201d that always enters goals in the correct sequence.\nVia ablations, we identify a minimal sufficient \"starter kit\" of training ingredients required for cultural transmission to emerge, dubbed MEDAL-ADR. These components include memory (M), expert dropout (ED), attentional bias towards the expert (AL), and automatic domain randomization (ADR). Our agent outperforms the ablations, including the state-of-the-art method (ME-AL), across a range of challenging held-out tasks. Cultural transmission generalises out of distribution surprisingly well, and the agent recalls demonstrations long after the expert has departed. Looking into the agent's brain, we find strikingly interpretable neurons responsible for encoding social information and goal states.\nIn summary, we provide a procedure for training an agent capable of flexible, high-recall, real-time cultural transmission, without using human data in the training pipeline. This paves the way for cultural evolution as an algorithm for developing more generally intelligent artificial agents.\nThis authors' notes is based on joint work by the Cultural General Intelligence Team: Avishkar Bhoopchand, Bethanie Brownfield, Adrian Collister, Agustin Dal Lago, Ashley Edwards, Richard Everett, Alexandre Fr\u00e9chette, Edward Hughes, Kory W. Mathewson, Piermaria Mendolicchio, Yanko Oliveira, Julia Pawar, Miruna P\u00eeslar, Alex Platonov, Evan Senter, Sukhdeep Singh, Alexander Zacherl, and Lei M. Zhang.\nRead the full paper \nhere\n.\n"}
{"title": "Predicting the past with Ithaca", "contents": "Restoring, placing, and dating ancient texts through collaboration between AI and historians.\nThe birth of human writing marked the dawn of \nHistory\n and is crucial to our understanding of past civilisations and the world we live in today. For example, more than 2,500 years ago, the Greeks began writing on stone, pottery, and metal to document everything from leases and laws to calendars and oracles, giving a detailed insight into the Mediterranean region. Unfortunately, it\u2019s an incomplete record. Many of the surviving inscriptions have been damaged over the centuries or moved from their original location. In addition, modern dating techniques, such as \nradiocarbon dating\n, cannot be used on these materials, making inscriptions difficult and time-consuming to interpret.\nIn line with \nDeepMind\u2019s mission\n of solving intelligence to advance science and humanity, we collaborated with the \nDepartment of Humanities of Ca' Foscari University of Venice\n, the \nClassics Faculty of the University of Oxford\n, and the \nDepartment of Informatics of the Athens University of Economics and Business\n to explore how machine learning can help historians better interpret these inscriptions \u2013 giving a richer understanding of ancient history and unlocking the potential for cooperation between AI and historians.\nIn a \npaper\n published today in \nNature\n, we jointly introduce Ithaca, the first deep neural network that can restore the missing text of damaged inscriptions, identify their original location, and help establish the date they were created. Ithaca is named after the Greek island in \nHomer\u2019s \nOdyssey\n and builds upon and extends \nPythia\n, our previous system that focused on textual restoration. Our evaluations show that Ithaca achieves 62% accuracy in restoring damaged texts, 71% accuracy in identifying their original location, and can date texts to within 30 years of their ground-truth date ranges. Historians have already used the tool to reevaluate significant periods in Greek history.\nTo make our research widely available to researchers, educators, museum staff and others, we partnered with \nGoogle Cloud\n and \nGoogle Arts & Culture\n to launch a \nfree interactive version of Ithaca\n. And to aid further research, we have also \nopen sourced\n our code, the pretrained model, and an interactive Colaboratory notebook.\nIthaca is trained on the \nlargest digital dataset of Greek inscriptions\n from the \nPackard Humanities Institute\n. \nNatural language processing\n models are commonly trained using words because the order in which they appear in sentences and the relationships between them provide extra context and meaning. For example, \u201conce upon a time\u201d has more meaning than each character or word seen separately. However, many of the inscriptions historians are interested in analysing with Ithaca are damaged and often missing chunks of text. To ensure our model still works when presented with one of these, we trained it using both words and the individual characters as inputs. The sparse self-attention mechanism at the model\u2019s core evaluates these two inputs in parallel, allowing Ithaca to evaluate inscriptions as needed.\nTo maximise Ithaca's value as a research tool, we also created a number of visual aids to ensure Ithaca\u2019s results are easily interpretable by historians:\nOur experimental evaluation shows how Ithaca\u2019s design decisions and visualisation aids make it easier for researchers to interpret results. The expert historians we worked with achieved 25% accuracy when working alone to restore ancient texts. But, when using Ithaca, their performance increases to 72%, surpassing the model\u2019s individual performance and showing the potential for human-machine cooperation to advance historical interpretation, establish relative datings for historical events, and even contribute to current methodological debates.\nFor example, historians currently disagree on the date of a series of important \nAthenian decrees\n made at a time when notable figures such as Socrates and Pericles lived. The decrees have long been thought to have been written before 446/445 BCE, although new evidence suggests a date of the 420s BCE. Although it might seem like a small difference, these decrees are fundamental to our understanding of the political history of Classical Athens.\nOur training dataset contains the earlier figure of 446/445 BCE. To test Ithaca\u2019s predictions, we retrained it on a dataset that did not contain the dated inscriptions and then submitted these held-out texts for analysis. Remarkably, Ithaca\u2019s average predicted date for the decrees is 421 BCE, aligning with the most recent dating breakthroughs and showing how machine learning can contribute to debates around one of the most significant moments in Greek history.\nWe believe this is just the start for tools like Ithaca and the potential for collaboration between machine learning and the humanities. Ancient Greece plays an instrumental role in our understanding of the Mediterranean world, but it\u2019s still only one part of a vast global picture of civilisations. To that end, we are currently working on versions of Ithaca trained on other ancient languages and historians can already use their datasets in the current architecture to study other ancient writing systems, from \nAkkadian\n to \nDemotic\n and \nHebrew\n to \nMayan\n. We hope that models like Ithaca can unlock the cooperative potential between AI and the humanities, transformationally impacting the way we study and write about some of the most significant periods in human history.\nThis work was done by a team including contributions from Yannis Assael, Thea Sommerschield, Brendan Shillingford, Mahyar Bordbar, John Pavlopoulos, Marita Chatzipanagiotou, Ion Androutsopoulos, Jonathan Prag, and Nando de Freitas. The Ithaca web interface was developed by Justin Grayston, Benjamin Maynard, and Ricardo Cardenas from Google Cloud.\n"}
{"title": "An empirical analysis of compute-optimal large language model training", "contents": "In the last few years, a focus in language modelling has been on improving performance through increasing the number of parameters in transformer-based models. This approach has led to impressive results and state-of-the-art performance across many natural language processing tasks. \nWe also pursued this line of research at DeepMind and recently showcased Gopher, a 280-billion parameter model that established leading performance on a wide range of tasks including language modelling, reading comprehension, and question answering. Since then, an even larger model named Megatron-Turing NLG has been published with 530 billion parameters.\nDue to the substantial cost of training these large models, it is paramount to estimate the best possible training setup to avoid wasting resources. In particular, the training compute cost for transformers is determined by two factors: the model size and the number of training tokens.\nThe current generation of large language models has allocated increased computational resources to increasing the parameter count of large models and keeping the training data size fixed at around 300 billion tokens. In this work, we empirically investigate the optimal tradeoff between increasing model size and the amount of training data with increasing computational resources. Specifically, we ask the question: \u201cWhat is the optimal model size and number of training tokens for a given compute budget?\u201d To answer this question, we train models of various sizes and with various numbers of tokens, and estimate this trade-off empirically.\nOur main finding is that the current large language models are far too large for their compute budget and are not being trained on enough data. In fact, we find that for the number of training FLOPs used to train \nGopher\n, a 4x smaller model trained on 4x more data would have been preferable.\n\u200d\nWe test our data scaling hypothesis by training \nChinchilla, \na 70-billion parameter model trained for 1.3 trillion tokens. While the training compute cost for Chinchilla\n \nand\n \nGopher\n \nare the same, we find that it outperforms Gopher and other large language models on nearly every measured task, despite having 70 billion parameters compared to Gopher\u2019s 280 billion.\nAfter the release of Chinchilla, a model named PaLM was released with 540 billion parameters and trained on 768 billion tokens. This model was trained with approximately 5x the compute budget of Chinchilla and outperformed Chinchilla on a range of tasks. While the training corpus is different, our methods do predict that such a model trained on our data would outperform Chinchilla despite not being compute-optimal. Given the PaLM compute budget, we predict a 140-billion-parameter model trained on 3 trillion tokens to be optimal and more efficient for inference.\nAn additional benefit of smaller, more performant models is that the inference time and memory costs are reduced making querying the models both faster and possible on less hardware. In practice, while the training FLOPs between Gopher\n \nand Chinchilla are the same, the cost of using Chinchilla is substantially smaller, in addition to it performing better. Further simple optimisations may be possible that are able to continue to provide large gains.\n"}
{"title": "DeepMind\u2019s latest research at ICLR 2022", "contents": "Today, conference season is kicking off with The Tenth International Conference on Learning Representations (\nICLR 2022\n), running virtually from 25-29 April, 2022. Participants from around the world are gathering to share their cutting-edge work in representational learning, from advancing the state of the art in artificial intelligence to data science, machine vision, robotics, and more.\u00a0\nOn the first day of the conference, Pushmeet Kohli, our head of AI for Science and Robust and Verified AI teams, is delivering a talk on how AI can dramatically improve solutions to a wide range of scientific problems, from genomics and structural biology to quantum chemistry and even pure mathematics.\u00a0\nBeyond supporting the event as sponsors and regular workshop organisers, our research teams are presenting 29 papers, including 10 collaborations this year. Here\u2019s a brief glimpse into our upcoming oral, spotlight, and poster presentations:\nA number of key papers focus on the critical ways we\u2019re making the learning process of our AI systems more efficient. This ranges from increasing performance, advancing few shot learning, and creating data efficient systems that reduce computational costs.\u00a0\nIn \n\u201cBootstrapped meta-learning\u201d\n, an \nICLR 2022 Outstanding Paper Award\n winner, we propose an algorithm that enables an agent to learn how to learn by teaching itself. We also present a \npolicy improvement algorithm\n that redesigns \nAlphaZero\n \u2013 our system that taught itself from scratch to master chess, shogi, and Go \u2013 to continue improving even when training with a small number of simulations; a \nregulariser that mitigates the risk of capacity loss\n in a broad range of RL agents and environments; and an improved \narchitecture to efficiently train attentional models\n.\nCuriosity is a key part of human learning, helping to advance knowledge and skill. Similarly, exploration mechanisms allow AI agents to go beyond preexisting knowledge and discover the unknown or try something new.\nAdvancing the question \u201c\nWhen should agents explore?\n\u201d, we investigate when agents should switch into exploration mode, at what timescales it makes sense to switch, and which signals best determine how long and frequent exploration periods should be. In another paper, we introduce an \u201c\ninformation gain exploration bonus\n\u201d that allows agents to break out of the limitations of intrinsic rewards in RL to be able to learn more skills.\nTo deploy ML models in the real world, they must be effective when shifting between training, testing, and across new datasets. Understanding the causal mechanisms is essential, allowing some systems to adapt, while others struggle to face new challenges.\nExpanding the research into these mechanisms, we present an experimental framework that enables a fine-grained \nanalysis of robustness to distribution shifts\n. Robustness also helps protect against adversarial harms, whether unintended or targeted. In the case of image corruptions, we propose a technique that theoretically \noptimises the parameters of image-to-image models\n to decrease the effects of blurring, fog, and other common issues.\nIn addition to helping ML researchers understand how agents evolve their own communication to complete tasks, AI agents have the potential to reveal insights into linguistic behaviours within populations, which could lead to more interactive and useful AI.\u00a0\nWorking with researchers at Inria, Google Research, and Meta AI, we connect the role of diversity within human populations on shaping language to \npartially solve an apparent contradiction\n in computer simulations with neural agents. Then, because building better representations of language in AI is so vital to understanding emergent communication, we also investigate the \nimportance of scaling up\n the dataset, task complexity, and population size as independent aspects. Moreover, we also studied the \ntradeoffs of expressivity, complexity, and unpredictability\n in games where multiple agents communicate to achieve a single goal.\nSee the full range of our work at ICLR 2022 \nhere\n.\n"}
{"title": "Tackling multiple tasks with a single visual language model", "contents": "One key aspect of intelligence is the ability to quickly learn how to perform a new task when given a brief instruction. For instance, a child may recognise real animals at the zoo after seeing a few pictures of the animals in a book, despite differences between the two. But for a typical visual model to learn a new task, it must be trained on tens of thousands of examples specifically labelled for that task. If the goal is to count and identify animals in an image, as in \u201cthree zebras\u201d, one would have to collect thousands of images and annotate each image with their quantity and species. This process is inefficient, expensive, and resource-intensive, requiring large amounts of annotated data and the need to train a new model each time it\u2019s confronted with a new task. As part of DeepMind\u2019s mission to solve intelligence, we\u2019ve explored whether an alternative model could make this process easier and more efficient, given only limited task-specific information.\nToday, in the preprint of our \npaper\n, we introduce \nFlamingo, \na single visual language model (VLM) that sets a new state of the art in few-shot learning on a wide range of open-ended multimodal tasks. This means Flamingo can tackle a number of difficult problems with just a handful of task-specific examples (in a \u201cfew shots\u201d), without any additional training required. Flamingo\u2019s simple interface makes this possible, taking as input a prompt consisting of interleaved images, videos, and text and then output associated language.\u00a0\nSimilar to the behaviour of \nlarge language models\n (LLMs), which can address a language task by processing examples of the task in their text prompt, Flamingo\u2019s visual and text interface can steer the model towards solving a multimodal task. Given a few example pairs of visual inputs and expected text responses composed in Flamingo\u2019s prompt, the model can be asked a question with a new image or video, and then generate an answer.\u00a0\nOn the 16 tasks we studied, Flamingo beats all previous few-shot learning approaches when given as few as four examples per task. In several cases, the same Flamingo\n \nmodel outperforms methods that are fine-tuned and optimised for each task independently and use multiple orders of magnitude more task-specific data. This should allow non-expert people to quickly and easily use accurate visual language models on new tasks at hand.\nIn practice, Flamingo fuses large language models with powerful visual representations \u2013 each separately pre-trained and frozen \u2013 by adding novel architectural components in between. Then it is trained on a mixture of complementary large-scale multimodal data coming only from the web, without using any data annotated for machine learning purposes. Following this method, we start from \nChinchilla\n, our recently introduced compute-optimal 70B parameter language model, to train our final Flamingo\n \nmodel, an 80B parameter VLM. After this training is done, Flamingo can be directly adapted to vision tasks via simple few-shot learning without any additional task-specific tuning.\nWe also tested the model\u2019s qualitative capabilities beyond our current benchmarks. As part of this process, we compared our model's performance when captioning images related to gender and skin colour, and ran our model's generated captions through Google's Perspective API, which evaluates toxicity of text. While the initial results are positive, more research towards evaluating ethical risks in multimodal systems is crucial and we urge people to evaluate and consider these issues carefully before thinking of deploying such systems in the real world.\nMultimodal capabilities are essential for important AI applications, such as \naiding the visually impaired\n with everyday visual challenges or \nimproving the identification of hateful content\n on the web. Flamingo makes it possible to efficiently adapt to these examples and other tasks on-the-fly without modifying the model. Interestingly, the model demonstrates out-of-the-box multimodal dialogue capabilities, as seen here.\nFlamingo is an effective and efficient general-purpose family of models that can be applied to image and video understanding tasks with minimal task-specific examples. Models like Flamingo hold great promise to benefit society in practical ways and we\u2019re continuing to improve their flexibility and capabilities so they can be safely deployed for everyone's benefit. Flamingo\u2019s abilities pave the way towards rich interactions with learned visual language models that can enable better interpretability and exciting new applications, like a visual assistant which helps people in everyday life \u2013 and we\u2019re delighted by the results so far.\nThis project was enabled by the contributions of the entire Flamingo team: Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. We would also like to thank the blog post contributors: Aliya Ahmad, Dominic Barlow, Arielle Bier, Matt Botvinick, Jordan Hoffmann, Max Barnett, Gaby Pearl, and Emma Yousif.\n"}
{"title": "When a passion for bass and brass help build better tools", "contents": "For our first ever 'Five minutes with' we caught up with Kevin Millikin, a software engineer on the DevTools team. He\u2019s in Salt Lake City this week to present at \nPyCon US\n, the largest annual gathering for those using and developing the open-source Python programming language.\nI\u00a0build bespoke software tools for our developers. For example, we\u2019re currently developing a web-based editor to support people working remotely who need to code in Python -\u00a0one of the common languages used by our engineers. Creating tools for how we work and the Google infrastructure we rely on gives us more flexibility to solve problems that matter to our teams.\nThe London campus - it is fabulous. We\u2019re working a hybrid 3:2 model - Monday through Wednesday in the office, Thursday and Friday from anywhere. I\u2019m really enjoying the face-to-face interaction with my colleagues.\u00a0\nI\u2019ve been working from home on Thursday and Friday. I\u2019m a musician and my home office is also my music room. I play bass guitar, baritone horn, and tenor saxophone. Playing music helped tremendously when we were working remotely during the pandemic. It\u2019s a different kind of creative energy \u2013 it gives me space to reflect on the problem I\u2019m trying to solve, and helps me tackle it from a different direction.\nI\u2019m giving a talk on \u2018\nBeyond Subtyping\n', a feature of Python. My session highlights various cases where the tools that implement subtyping disagree. As a Python designer you might think these are settled questions, but they\u2019re not because we don\u2019t yet agree on foundational points about how the language works.\nIn the typing working group there are dozens of participants from companies like Microsoft, Facebook, and Google \u2013 it\u2019s a very cooperative, collegial group. We\u2019re all trying to evolve Python in a direction that supports our own users. We\u2019re finding that we all have similar problems, and similar goals too. We\u2019re trying to develop tools that can be used by everybody, so we have to design in a very collaborative way.\nMeeting up face-to-face with people I\u2019ve been working with remotely for a couple of years, who are part of the Python language community. I\u2019m a bit of a newcomer in this area and I\u2019m interested in expanding our network and making it more inclusive to external contributors. In practice, it often works as a closed group, and I think a lot of the work could benefit from being more open.\nThough a lot of new features are added to Python to help address a specific issue someone is having, they don\u2019t always fit with other new features in a coherent way. One of the things I'm advocating for is to take a step back and decide what our principles are for evolving this part of the programming language we\u2019re working on. A lot of these are in the heads of the developers, but my question is - can we write them down and use that as a manifesto for how language evolution should go? If we had a roadmap of where we want to go in the next 2-5 years, could we be more thoughtful about the changes we make to the language? That would ensure we're building for the future and the tools we will need to create to accelerate AI research.\nLearn more about engineering at DeepMind and search for open roles today\n"}
{"title": "Active offline policy selection", "contents": "Reinforcement learning (RL) has made tremendous progress in recent years towards addressing real-life problems \u2013 and offline RL made it even more practical. Instead of direct interactions with the environment, we can now train many algorithms from a single pre-recorded dataset. However, we lose the practical advantages in data-efficiency of offline RL when we evaluate the policies at hand.\nFor example, when training robotic manipulators the robot resources are usually limited, and training many policies by offline RL on a single dataset gives us a large data-efficiency advantage compared to online RL. Evaluating each policy is an expensive process, which requires interacting with the robot thousands of times. When we choose the best algorithm, hyperparameters, and a number of training steps, the problem quickly becomes intractable.\nTo make RL more applicable to real-world applications like robotics, we propose using an intelligent evaluation procedure to select the policy for deployment, called active offline policy selection (A-OPS). In A-OPS, we make use of the prerecorded dataset and allow limited interactions with the real environment to boost the selection quality.\nTo minimise interactions with the real environment, we implement three key features: \n\u200d\nThe returns of the policies are modelled jointly using a Gaussian process, where observations include FQE scores and a small number of newly collected episodic returns from the robot. After evaluating one policy, we gain knowledge about all policies because their distributions are correlated through the kernel between pairs of policies. The kernel assumes that if policies take similar actions \u2013 such as moving the robotic gripper in a similar direction \u2013 they tend to have similar returns.\n\u200d\nWe demonstrated this procedure in a number of environments in several domains: dm-control, Atari, simulated, and real robotics. Using A-OPS reduces the regret rapidly, and with a moderate number of policy evaluations, we identify the best policy. \nOur results suggest that it\u2019s possible to make an effective offline policy selection with only a small number of environment interactions by utilising the offline data, special kernel, and Bayesian optimisation. The code for A-OPS is open-sourced and \navailable on GitHub\n with an example dataset to try.\n"}
{"title": "AlphaFold reveals the structure of the protein universe", "contents": "To read about all our work on solving protein folding, go to \ndeepmind.com/AlphaFold\n or read a timeline of the breakthrough \nhere\n.\nIt\u2019s been one year since we released and open sourced \nAlphaFold\n, our AI system to predict the 3D structure of a protein just from its 1D amino acid sequence, and created the \nAlphaFold Protein Structure Database\n (AlphaFold DB) to freely share this scientific knowledge with the world. Proteins are the building blocks of life, they underpin every biological process in every living thing. And, because a protein\u2019s shape is closely linked with its function, knowing a protein\u2019s structure unlocks a greater understanding of what it does and how it works. We hoped this groundbreaking resource would help accelerate scientific research and discovery globally, and that other teams could learn from and build on the advances we made with AlphaFold to create further breakthroughs. That hope has become a reality far quicker than we had dared to dream. Just twelve months later, AlphaFold has been accessed by more than half a million researchers and used to accelerate progress on important real-world problems ranging from \nplastic pollution\n to \nantibiotic resistance\n.\nToday, I\u2019m incredibly excited to share the next stage of this journey. In partnership with EMBL\u2019s \nEuropean Bioinformatics Institute (EMBL-EBI)\n, we\u2019re now releasing predicted structures for nearly all catalogued proteins known to science, which will expand the \nAlphaFold DB\n \nby over 200x\n - \nfrom nearly 1 million structures to over 200 million structures \n- with the potential to dramatically increase our understanding of biology.\nThis update includes predicted structures for plants, bacteria, animals, and other organisms, opening up many new opportunities for researchers to use AlphaFold to advance their work on important issues, including sustainability, food insecurity, and neglected diseases.\nToday\u2019s update means that most pages on the main protein database \nUniProt\n will come with a predicted structure. All 200+ million structures will also be available for bulk download via \nGoogle Cloud Public Datasets\n, making AlphaFold even more accessible to scientists around the world.\nTwelve months on from AlphaFold\u2019s initial release, it\u2019s been amazing to reflect on the incredible impact AlphaFold has already had, and our long journey to reach today\u2019s milestone.\nFor our team, AlphaFold\u2019s success was especially rewarding, both because it was the most complex AI system we\u2019d ever built, requiring multiple critical innovations, and because it has had the most meaningful downstream impact. By demonstrating that AI could accurately predict the shape of a protein down to atomic accuracy, at scale and in minutes, AlphaFold not only provided a solution to a 50-year grand challenge, it also became the first big proof point of our founding thesis: that artificial intelligence can dramatically accelerate scientific discovery, and in turn advance humanity.\nWe open sourced AlphaFold\u2019s code and published two in-depth papers in Nature [1, 2], which have already been cited more than 4000 times. We \ncollaborated closely\n with the world-leading EMBL-EBI to design a tool that would best help biologists access and use AlphaFold, and together released the AlphaFold DB, a searchable database that is open and free to all. Before releasing AlphaFold, in line with our careful approach to \npioneering responsibly\n, we sought input from more than 30 experts across biology research, security, ethics and safety to help us understand how to share the benefits of AlphaFold with the world, in a way that would maximise potential benefit and minimise potential risk.\nTo date, more than 500,000 researchers from 190 countries have accessed the AlphaFold DB to view over 2 million structures. Our freely available structures have also been integrated into other public datasets, such as Ensembl, UniProt, and OpenTargets, where millions of users access them as part of their everyday workflows.\nWe\u2019ve been amazed by the rate at which AlphaFold has already become an essential tool for hundreds of thousands of scientists in labs and universities across the world to help them in their important work. As for our own work with AlphaFold, we prioritised applications that we felt would have the most positive social benefit, with a focus on initiatives that had been historically underfunded or overlooked. For example, \nwe partnered\n with the \nDrugs for Neglected Diseases Initiative (DNDI)\n to help advance their research, moving them closer to finding life-saving cures for diseases like \nLeishmaniasis\n and \nChagas disease\n that disproportionately affect people in poorer parts of the world. We also supported \nWorld Neglected Tropical Disease Day\n by creating structure predictions for organisms identified by the \nWorld Health Organisation\n as high-priority for their research, helping to further the study of diseases like \nLeprosy\n and \nSchistosomiasis\n, which devastate the lives of more than 1 billion people globally.\nIt\u2019s been so inspiring to see the myriad ways the research community has taken AlphaFold, using it for everything from \nunderstanding diseases\n, to \nprotecting honey bees\n, to \ndeciphering biological puzzles\n, to \nlooking deeper into the origins of life itself.\nOther impressive examples, chosen by members of our AlphaFold team, include:\nIn a recent \nspecial issue of Science\n, several groups described how AlphaFold helped them piece together the nuclear pore complex, one of the most fiendish puzzles in biology. The giant structure consists of hundreds of protein parts and controls everything that goes in and comes out of the cell nucleus. Its delicate structure was finally revealed by using existing experimental methods to reveal its outline and AlphaFold predictions to complete and interpret any areas that were unclear. This powerful combination is now becoming routine in labs, unlocking new science and showing how experimental and computational techniques can work together.\u00a0\nStructural search tools like \nFoldseek\n and \nDali\n are allowing users to very quickly search for entries similar to a given protein. This could be a first step toward mining large sequence datasets for practically useful proteins, such as those that break down plastic, and it could provide clues about protein function. The update of the database to include over 200 million predicted structures will further amplify this impact.\u00a0\nAlphaFold is already having a significant, direct impact on human health. Meeting with researchers at the \nEuropean Society of Human Genetics\n revealed how important AlphaFold structures are to biologists and clinicians trying to unravel the causes of rare genetic diseases.\n \nIn addition, AlphaFold is \naccelerating drug discovery\n by providing a better understanding of newly identified proteins that could be drug targets, and helping scientists to more quickly find potential medicines that bind to them.\nAlphaFold has launched biology into an era of structural abundance, unlocking scientific exploration at digital speed. The AlphaFold DB serves as a \u2018google search\u2019 for protein structures, providing researchers with instant access to predicted models of the proteins they're studying, enabling them to focus their effort and expedite experimental work. From \nfighting disease\n to \ndeveloping vaccines\n, AlphaFold has already enabled incredible advances on some of our biggest global challenges, and this is just the beginning of the impact that we will start to see over the next few years. Our hope is that this expanded database will aid countless more scientists in their work and open up completely new avenues of scientific exploration, such as metaproteomics.\u00a0\nAt DeepMind, we\u2019re hard at work building on all this potential with significant investments in many areas, including partnering with our new sister Alphabet company \nIsomorphic Labs\n to reimagine the entire drug discovery process from first principles with an AI-first approach; \nestablishing a wet lab\n at the renowned \nFrancis Crick Institute\n to strengthen the connection between AI and experimental techniques to advance understanding of biology, including protein design and genomics; and expanding our \nAI for Science\n team to accelerate further progress on our fundamental biology research and apply AI to other fascinating and important scientific challenges, such as \nclimate science\n, \nquantum chemistry\n, and \nfusion\n.\nAlphaFold is a glimpse of the future, and what might be possible with computational and AI methods applied to biology. At its most fundamental level, biology can be thought of as an information processing system, albeit an extraordinarily complex and emergent one. Just as maths is the perfect description language for physics, we believe AI might turn out to be just the right technique to cope with the dynamic complexity of biology. AlphaFold is an important first proof point for this, and a sign of much more to come. As pioneers in the emerging field of \u2018digital biology', we\u2019re excited to see the huge potential of AI starting to be realised as one of humanity\u2019s most useful tools for advancing scientific discovery and understanding the fundamental mechanisms of life.\n"}
{"title": "Realising scientists are the real superheroes", "contents": "Meet Edgar Du\u00e9\u00f1ez-Guzm\u00e1n, a research engineer on our Multi-Agent Research team who\u2019s drawing on knowledge of game theory, computer science, and social evolution to get AI agents working better together.\nI've wanted to save the world ever since I can remember. That's why I wanted to be a scientist. While I loved superhero stories, I realised scientists are the real superheroes. They are the ones who give us clean water, medicine, and an understanding of our place in the universe. As a child, I loved computers and I loved science. Growing up in Mexico, though, I didn't feel like studying computer science was feasible. So, I decided to study maths, treating it as a solid foundation for computing and I ended up doing my university thesis in game theory.\nAs part of my PhD in computer science, I created biological simulations, and ended up falling in love with biology. Understanding evolution and how it shaped the Earth was exhilarating. Half of my dissertation was in these biological simulations, and I went on to work in academia studying the evolution of social phenomena, like cooperation and altruism.\nFrom there I started working in Search at Google, where I learned to deal with massive scales of computation. Years later, I put all three pieces together: game theory, evolution of social behaviours, and large-scale computation. Now I use those pieces to create artificially intelligent agents that can learn to cooperate amongst themselves, and with us.\nIt was the mid-2010s. I\u2019d been keeping an eye on AI for over a decade and I knew of DeepMind and some of their successes. Then Google acquired it and I was very excited. I wanted in, but I was living in California and DeepMind was only hiring in London. So, I kept tracking the progress. As soon as an office opened in California, I was first in line. I was fortunate to be hired in the first cohort. Eventually, I moved to London to pursue research full time.\nHow ridiculously talented and friendly people are. Every single person I\u2019ve talked to also has an exciting side outside of work. Professional musicians, artists, super-fit bikers, people who appeared in Hollywood movies, maths olympiad winners \u2013 you name it, we have it! And we\u2019re all open and committed to making the world a better place.\n\u200d\nAt the core of my research is making intelligent agents that understand cooperation. Cooperation is the key to our success as a species. We can access the world's information and connect with friends and family on the other side of the world because of cooperation. Our failure to address the catastrophic effects of climate change is a failure of cooperation, as we saw during COP26.\nThe flexibility to pursue the ideas that I think are most important. For example, I\u2019d love to help use our technology for better understanding social problems, like discrimination. I pitched this idea to a group of researchers with expertise in psychology, ethics, fairness, neuroscience, and machine learning, and then created a research programme to study how discrimination might originate in stereotyping.\nDeepMind is one of those places where freedom and potential go hand-in-hand. We have the opportunity to pursue ideas that we feel are important and there\u2019s a culture of open discourse. It\u2019s not uncommon to infect others with your ideas and form a team around making it a reality.\u00a0\nI love getting involved in extracurriculars. I\u2019m a facilitator of Allyship workshops at DeepMind, where we aim to empower participants to take action for positive change and encourage allyship in others, contributing to an inclusive and equitable workplace. I also love making research more accessible and talking with visiting students. I\u2019ve created publicly available \neducational tutorials\n for explaining AI concepts to teenagers, which have been used in summer schools across the world.\nTo have the most positive impact, it simply needs to be that the benefits are shared broadly, rather than kept by a tiny number of people. We should be designing systems that empower people, and that democratise access to technology.\u00a0\nFor example, when I worked on \nWaveNet\n, the new voice of the Google Assistant, I felt it was cool to be working on a technology that is now used by billions of people, in Google Search, or Maps. That's nice, but then we did something better. We started using this technology to give their voice back to people with degenerative disorders, like ALS. There's always opportunities to do good, we just have to take them.\nThere are both practical and societal challenges. On the practical side, we\u2019re hard at work trying to make our algorithms more robust and adaptable. As living creatures, we take robustness and adaptability for granted. Slightly changing the furniture arrangement doesn't cause us to forget what a fridge is for. Artificial systems really struggle with this. There are some promising leads, but we still have a way to go.\u00a0\nOn the societal side, we need to collectively decide what kind of AI we want to create. We need to make sure that whatever is made, is safe and beneficial. But this is particularly hard to achieve when we don't have a perfect definition of what this means.\n\u200d\nRight now I'm still riding the high of \nAlphaFold\n, our protein-folding algorithm. I have a background in biology, and understand how promising protein structure prediction can be for biomedical applications. And I am particularly proud of how DeepMind released the protein structure of all the known proteins in the human body in the global datasets, and now released \nnearly all catalogued proteins\n known to science.\u00a0\nBe playful, be flexible. I couldn\u2019t have optimised for a career leading to DeepMind (there wasn't even a DeepMind to optimise to!) But what I could do was always allow myself to dream of the potential of technology, of creating intelligent machines, and of improving the world with them.\u00a0\nProgramming is exhilarating in its own right, but for me it was always more of a means to an end. This is what enabled me to stay current as technologies came and went. I wasn't tied to the tools, I was focused on the mission. Don't focus on the \"what\", but on the \"why\", and the \"how\" will manifest itself.\n"}
{"title": "DeepMind\u2019s latest research at ICML 2022", "contents": "Starting this weekend, the thirty-ninth International Conference on Machine Learning (\nICML 2022\n) is meeting from 17-23 July, 2022 at the Baltimore Convention Center in Maryland, USA, and will be running as a hybrid event.\nResearchers working across artificial intelligence, data science, machine vision, computational biology, speech recognition, and more are presenting and publishing their cutting-edge work in machine learning.\nIn addition to sponsoring the conference and supporting workshops and socials run by our long-term partners \nLatinX\n, \nBlack in AI\n, \nQueer in AI\n, and \nWomen in Machine Learning\n, our research teams are presenting 30 papers, including 17 external collaborations. Here\u2019s a brief introduction to our upcoming oral and spotlight presentations:\nMaking reinforcement learning (RL) algorithms more effective is key to building generalised AI systems. This includes helping increase the accuracy and speed of performance, improve transfer and zero-shot learning, and reduce computational costs.\nIn one of our selected oral presentations, we show a \nnew way to apply generalised policy improvement (GPI)\n over compositions of policies that makes\u00a0 it even more effective in boosting an agent\u2019s performance. Another oral presentation proposed a new grounded and scalable way to \nexplore efficiently without the need of bonuses\n. In parallel, we propose a method for \naugmenting an RL agent with a memory-based retrieval process\n, reducing the agent\u2019s dependence on its model capacity and enabling fast and flexible use of past experiences.\nLanguage is a fundamental part of being human. It gives people the ability to communicate thoughts and concepts, create memories, and build mutual understanding. Studying aspects of language is key to understanding how intelligence works, both in AI systems and in humans.\nOur oral presentation about \nunified scaling laws\n and our paper on \nretrieval\n \nboth explore how we might build larger language models more efficiently. Looking at ways of building more effective language models, we introduce a new dataset and benchmark with \nStreamingQA\n that evaluates how models adapt to and forget new knowledge over time, while our paper on \nnarrative generation\n \nshows how current pretrained language models still struggle with creating longer texts because of short-term memory limitations. \nNeural algorithmic reasoning is the art of building neural networks that can perform algorithmic computations. This growing area of research holds great potential for helping adapt known algorithms to real-world problems.\nWe introduce the \nCLRS benchmark for algorithmic reasoning\n, which evaluates neural networks on performing a diverse set of thirty classical algorithms from the Introductions to Algorithms textbook. Likewise, we propose a \ngeneral incremental learning algorithm\n that adapts hindsight experience replay to automated theorem proving, an important tool for helping mathematicians prove complex theorems. In addition, we present a \nframework for constraint-based learned simulation\n, showing how traditional simulation and numerical methods can be used in machine learning simulators \u2013 a significant new direction for solving complex simulation problems in science and engineering.\u00a0\nSee the full range of our work at ICML 2022 \nhere\n.\n"}
{"title": "Perceiver AR: general-purpose, long-context autoregressive generation", "contents": "Over the last few years, autoregressive Transformers have brought a steady stream of breakthroughs in generative modeling. These models generate each element of a sample \u2013 the pixels of an image, the characters of a text (typically in \u201ctoken\u201d chunks), the samples of an audio waveform, and so on \u2013 by predicting one element after the other. When predicting the next element, the model can look back at those that were created earlier.\nHowever, each of a Transformer\u2019s layers grows more expensive as more elements are used as input, and practitioners can only afford to train deep Transformers on sequences no more than about 2,048 elements in length. And so, most Transformer-based models ignore all elements beyond the most recent past (around 1,500 words or 1/6 of a small image) when making a prediction.\nIn contrast, our recently developed \nPerceiver models\n give excellent results on a variety of real-world tasks with up to around 100,000 elements. Perceivers use cross-attention to encode inputs into a latent space, decoupling\n \nthe input\u2019s compute requirements from model depth. Perceivers also spend a fixed cost, regardless of input size, at nearly every layer. \nWhile latent-space encoding handles all elements in a single pass, autoregressive generation assumes processing happens one element at a time. To address this problem, Perceiver AR proposes a simple solution: align the latents one by one with the final elements of the input, and carefully mask the input so latents see only earlier elements. \nThe result is an architecture (shown above) that attends to as much as 50x longer inputs as standard Transformers, while deploying as widely (and essentially as easily) as standard decoder-only Transformers.\nPerceiver AR scales considerably better with size than both standard Transformers and Transformer-XL models at a range of sequence lengths in real terms. This property allows us to build very effective long-context models. For example, we find that a 60-layer Perceiver AR with context length 8192 outperforms a 42-layer Transformer-XL on a book-length generation task, while running faster in real wall-clock terms.\nOn standard, long-context image (ImageNet 64x64), language (PG-19), and music (MAESTRO) generation benchmarks, Perceiver AR produces state-of-the-art results. Increasing input context by decoupling input size from compute budget leads to several intriguing results: \nUsing a dataset of piano music, we trained Perceiver AR to generate new pieces of music from scratch. Because each new note is predicted based on the full sequence of notes that came before, Perceiver AR is able to produce pieces with a high level of melodic, harmonic, and rhythmic coherence:\nLearn more about using Perceiver AR:\nSee the Google Magenta \nblog post\n with more music!\n"}
{"title": "The virtuous cycle of AI research", "contents": "We recently caught up with Petar Veli\u010dkovi\u0107, a research scientist at DeepMind. Along with his co-authors, Petar is presenting his paper The CLRS Algorithmic Reasoning Benchmark at \nICML 2022\n in Baltimore, Maryland, USA.\u00a0\nThroughout my undergraduate courses at the University of Cambridge, the inability to skilfully play the game of Go was seen as clear evidence of the shortcomings of modern-day deep learning systems. I always wondered how mastering such games might escape the realm of possibility.\u00a0\nHowever, in early 2016, just as I started my PhD in machine learning, that all changed. DeepMind took on one of the best Go players in the world for a \nchallenge match\n, which I spent several sleepless nights watching. DeepMind won, producing ground-breaking gameplay (e.g. \u201cMove 37\u201d) in the process.\u00a0\nFrom that point on, I thought of DeepMind as a company that could make seemingly impossible things happen. So, I focused my efforts on, one day, joining the company. Shortly after submitting my PhD in early 2019, I began my journey as a research scientist at DeepMind!\u00a0\nMy role is a virtuous cycle of learning, researching, communicating, and advising. I\u2019m always actively trying to learn new things (most recently \nCategory Theory\n, a fascinating way of studying computational \nstructure\n), read relevant literature, and watch talks and seminars.\u00a0\nThen using these learnings, I brainstorm with my teammates about how we can broaden this body of knowledge to positively impact the world. From these sessions, ideas are born, and we leverage a combination of theoretical analysis and programming to set and validate our hypotheses. If our methods bear fruit, we typically write a paper sharing insights with the broader community.\u00a0\nResearching a result is not nearly as valuable without appropriately communicating it, and empowering others to effectively make use of it. Because of this, I spend a lot of time presenting our work at conferences like ICML, giving talks, and co-advising students. This often leads to forming new connections and uncovering novel scientific results to explore, setting the virtuous cycle in motion one more time!\nWe\u2019re giving a spotlight presentation on our paper, \nThe CLRS algorithmic reasoning benchmark\n, which we hope will support and enrich efforts in the rapidly emerging area of \nneural algorithmic reasoning\n. In this research, we task graph neural networks with executing thirty diverse algorithms from the \nIntroduction to Algorithms\n textbook.\u00a0\nMany recent research efforts seek to construct neural networks capable of executing algorithmic computation, primarily to endow them with reasoning capabilities \u2013 which neural networks typically lack. Critically, every one of these papers generates its own dataset, which makes it hard to track progress, and raises the barrier of entry into the field.\u00a0\nThe CLRS benchmark, with its readily exposed dataset generators, and \npublicly available code\n, seeks to improve on these challenges. We\u2019ve already seen a great level of enthusiasm from the community, and we hope to channel it even further during ICML.\nThe main dream of our research on algorithmic reasoning is to capture the computation of classical algorithms inside high-dimensional neural executors. This would then allow us to deploy these executors directly over raw or noisy data representations, and hence \u201capply the classical algorithm\u201d over inputs it was never designed to be executed on.\nWhat\u2019s exciting is that this method has the potential to enable data-efficient reinforcement learning. Reinforcement learning is packed with examples of strong classical algorithms, but most of them can\u2019t be applied in standard environments (such as Atari), given that they require access to a wealth of privileged information. Our \nblueprint\n would make this type of application possible by capturing the computation of these algorithms inside neural executors, after which they can be directly deployed over an agent\u2019s internal representations. We even have a working prototype that was published at \nNeurIPS 2021\n. I can\u2019t wait to see what comes next!\u00a0\nI\u2019m looking forward to the \nICML Workshop on Human-Machine Collaboration and Teaming\n, a topic close to my heart. Fundamentally, I believe that the greatest applications of AI will come about through synergy with human domain experts. This approach is also very in line with our recent work on \nempowering the intuition of pure mathematicians using AI\n, which was published on the cover of Nature late last year.\u00a0\nThe workshop organisers invited me for a panel discussion to discuss the broader implications of these efforts. I\u2019ll be speaking alongside a fascinating group of co-panellists, including \nSir Tim Gowers\n, whom I admired during my undergraduate studies at Trinity College, Cambridge. Needless to say, I\u2019m really excited about this panel!\nFor me, major conferences like ICML represent a moment to pause and reflect on diversity and inclusion in our field. While hybrid and virtual conference formats make events accessible to more people than ever before, there\u2019s much more we need to do to make AI a diverse, equitable, and inclusive field. AI-related interventions will impact us all, and we need to make sure that underrepresented communities remain an important part of the conversation.\u00a0\nThis is exactly why I\u2019m teaching a course on \nGeometric Deep Learning\n at the \nAfrican Master\u2019s in Machine Intelligence\n (AMMI) \u2013 a topic of my recently co-authored \nproto-book\n. AMMI offers top-tier machine learning tuition to Africa\u2019s brightest emerging researchers, building a healthy ecosystem of AI practitioners within the region. I\u2019m so happy to have recently met several AMMI students that have gone on to join DeepMind for internship positions.\nI\u2019m also incredibly passionate about outreach opportunities in the Eastern European region, where I originate from, which gave me the scientific grounding and curiosity necessary to master artificial intelligence concepts. The \nEastern European Machine Learning\n (EEML) community is particularly impressive - through its activities, aspiring students and practitioners in the region are connected with world-class researchers and provided with invaluable career advice. This year, I helped bring EEML to my hometown of Belgrade, as one of the lead organisers of the EEML \nSerbian Machine Learning Workshop\n. I hope this is only the first in a series of events to strengthen the local AI community and empower the future AI leaders in the EE region.\n"}
{"title": "Discovering when an agent is present in a system", "contents": "New, formal definition of agency gives clear principles for causal modelling of AI agents and the incentives they face.\nWe want to build safe, aligned artificial general intelligence (AGI) systems that pursue the intended goals of its designers. \nCausal influence diagrams\n (CIDs) are a way to model decision-making situations that allow us to reason about \nagent incentives\n. For example, here is a CID for a 1-step Markov decision process \u2013 a typical framework for decision-making problems.\nBy relating training setups to the incentives that shape agent behaviour, CIDs help illuminate potential risks before training an agent and can inspire better agent designs. But how do we know when a CID is an accurate model of a training setup?\nOur new paper, \nDiscovering Agents\n, introduces new ways of tackling these issues, including:\nCombined, these results provide an extra layer of assurance that a modelling mistake hasn\u2019t been made, which means that CIDs can be used to analyse an agent\u2019s incentives and safety properties with greater confidence.\u00a0\nTo help illustrate our method, consider the following example consisting of a world containing three squares, with a mouse starting in the middle square choosing to go left or right, getting to its next position and then potentially getting some cheese. The floor is icy, so the mouse might slip. Sometimes the cheese is on the right, but sometimes on the left.\nThis can be represented by the following CID:\nThe intuition that the mouse would choose a different behaviour for different environment settings (iciness, cheese distribution) can be captured by a \nmechanised causal graph\n,\n \nwhich for each (object-level) variable, also includes a mechanism variable that governs how the variable depends on its parents. Crucially, we allow for links between mechanism variables.\nThis graph contains additional mechanism nodes in black, representing the mouse's policy and the iciness and cheese distribution.\u00a0\nEdges between mechanisms represent direct causal influence. The blue edges are special \nterminal\n edges \u2013 roughly, mechanism edges A~ \u2192 B~ that would still be there, even if the object-level variable A was altered so that it had no outgoing edges.\u00a0\nIn the example above, since U has no children, its mechanism edge must be terminal. But the mechanism edge X~ \u2192 D~ is not terminal, because if we cut X off from its child U, then the mouse will no longer adapt its decision (because its position won\u2019t affect whether it gets the cheese).\nCausal discovery infers a causal graph from experiments involving interventions. In particular, one can discover an arrow from a variable A to a variable B by experimentally intervening on A and checking if B responds, even if all other variables are held fixed.\nOur first algorithm uses this technique to discover the mechanised causal graph:\nOur second algorithm transforms this mechanised causal graph to a game graph:\nTaken together, Algorithm 1 followed by Algorithm 2 allows us to discover agents from causal experiments, representing them using CIDs.\nOur third algorithm transforms the game graph into a mechanised causal graph, allowing us to translate between the game and mechanised causal graph representations under some additional assumptions:\u00a0\nWe proposed the first formal causal definition of agents. Grounded in causal discovery, our key insight is that agents are systems that adapt their behaviour in response to changes in how their actions influence the world. Indeed, our Algorithms 1 and 2 describe a precise experimental process that can help assess whether a system contains an agent.\u00a0\nInterest in causal modelling of AI systems is rapidly growing, and our research grounds this modelling in causal discovery experiments. Our paper demonstrates the potential of our approach by improving the safety analysis of several example AI systems and shows that causality is a useful framework for discovering whether there is an agent\u00a0 in a system \u2013 a key concern for assessing risks from AGI. \nExcited to learn more? Check out our \npaper\n. Feedback and comments are most welcome.\n"}
{"title": "Advancing conservation with AI-based facial recognition of turtles", "contents": "Finding solutions to improve turtle reidentification and supporting machine learning projects across Africa.\nProtecting the ecosystems around us is critical to safeguarding the future of our planet and all its living citizens. Fortunately, new artificial intelligence (AI) systems are making progress in conservation efforts worldwide, helping tackle complex problems at scale \u2013 from \nstudying the behaviour of animal communities in the Serengeti\n to help conserve the diminishing ecosystem, to \nspotting poachers and their wounded prey\n to prevent species going extinct.\nAs part of our mission to help benefit humanity with the technologies we develop, it's important we ensure diverse groups of people build the AI systems of the future so that it\u2019s equitable and fair. This includes broadening the machine learning (ML) community and engaging with wider audiences on addressing important problems using AI.\u00a0\nThrough investigation, we came across \nZindi\n \u2013 a dedicated partner with complementary goals \u2013 who are the largest community of African data scientists and host competitions that focus on solving Africa\u2019s most pressing problems.\u00a0\nOur \nScience team\n\u2019s Diversity, Equity, and Inclusion (DE&I) team worked with Zindi to identify a scientific challenge that could help advance conservation efforts and grow involvement in AI. Inspired by Zindi\u2019s \nbounding box turtle challenge\n, we landed on a project with the potential for real impact: turtle facial recognition.\u00a0\nBiologists consider turtles to be an indicator species. These are classes of organisms whose behaviour helps scientists understand the underlying welfare of their ecosystem. For example, the presence of otters in rivers has been considered a sign of a clean, healthy river, since a ban on chlorine pesticides in the 1970s brought the species back from the brink of extinction.\u00a0\nTurtles are another such species. By grazing on seagrass cover, they cultivate the ecosystem, providing a habitat for numerous fish and crustaceans. Traditionally, individual turtles have been identified and tracked by biologists with physical tags, though frequent loss or erosion of these tags in seawater has made this an unreliable method. To help solve some of these challenges, we launched an ML challenge called \nTurtle Recall\n.\nGiven the additional challenge of keeping a turtle still enough to locate their tag, the Turtle Recall challenge aimed to circumvent these problems with turtle facial recognition. This is possible because the pattern of scales on a turtle's face is unique to the individual and remains the same over their multi-decade lifespan.\u00a0\nThe challenge aimed to increase the reliability and speed of turtle reidentification, and potentially offer a way to replace the use of uncomfortable physical tags altogether. To make this possible, we needed a dataset to work from. Fortunately, after Zindi\u2019s previous turtle-based challenge with Kenyan-based charity \nLocal Ocean Conservation\n, the teams were kindly able to share a dataset of labelled images of turtle faces.\nThe competition started in November 2021 and lasted five months. To encourage competitor participation, the team implemented a \ncolab notebook\n, an in-browser programming environment, which introduced two common programming tools: \nJAX\n and \nHaiku\n.\u00a0\nParticipants were tasked with downloading the challenge data and training models to predict a turtle\u2019s identity, as accurately as possible, given a photograph taken from a specific angle. Having submitted their predictions on data withheld from the model, they were able to visit a public leaderboard tracking the progress of each participant.\u00a0\nThe community engagement was incredibly positive, and so was the technical innovation displayed by teams during the challenge. During the course of the competition, we received submissions from a diverse range of AI enthusiasts from 13 different African countries \u2013 including countries not traditionally well represented at the biggest ML conferences, such as Ghana and Benin.\u00a0\nOur turtle conservation partners have indicated that the participant\u2019s level of prediction accuracy will be immediately useful for identifying turtles in the field, meaning that these models can have a real and immediate impact on wildlife conservation.\u00a0\nAs part of Zindi\u2019s continued efforts to support climate-positive challenges, they are also working on \nSwahili audio classification\n in Kenya to help translation and emergency services, and \nair quality prediction\n in Uganda to improve social welfare.\u00a0\nWe're grateful to Zindi for their partnership, and all those who contributed their time to the Turtle Recall challenge and the growing field of AI for conservation. And we look forward to seeing how people around the world continue to find ways to apply AI technologies towards building a healthy, sustainable future for the planet.\nRead more about Turtle Recall on \nZindi\u2019s blog\n and learn about Zindi at \nhttps://zindi.africa/\n"}
{"title": "Using AI to predict retinal disease progression", "contents": "Vision loss among the elderly is a major healthcare issue: about one in three people have some vision-reducing disease by the age of 65. Age-related macular degeneration (AMD) is the most common cause of blindness in the developed world. In Europe, approximately \n25% of those 60 and older\n have AMD. The \u2018dry\u2019 form is relatively common among people over 65, and usually causes only mild sight loss. However, about 15% of patients with dry AMD go on to develop a more serious form of the disease \u2013 exudative AMD, or exAMD \u2013 which can result in rapid and permanent loss of sight. Fortunately, there are treatments that can slow further vision loss. Although there are no preventative therapies available at present, these are being explored in clinical trials. The period before the development of exAMD may therefore represent a critical window to target for therapeutic innovations: can we predict which patients will progress to exAMD, and help prevent sight loss before it even occurs?\nIn our latest work, published in \nNature Medicine\n, we \ncollaborated with Moorfields Eye Hospital\n and \nGoogle Health\n to curate a dataset of images of eye retinas, train an artificial intelligence (AI) system that could predict the development of exAMD, and conduct a study to evaluate our model compared with expert clinicians. We demonstrate that our system is able to perform as well as, or better than, clinicians at predicting whether an eye will convert to exAMD in the next 6 months. Lastly, we explore the potential clinical applicability of our system. Our contribution highlights the potential of using AI in preventative studies for diseases such as exAMD.\nWe used a dataset of anonymised retinal scans from Moorfields patients with exAMD in one eye, and at high-risk of developing exAMD in their other eye. This comprises 2,795 patients across seven different Moorfields sites in London, with representation across genders, age ranges, and ethnicities. These patients attend the hospital regularly to receive treatment, undergoing high-resolution three-dimensional optical coherence tomography (OCT) imaging of both eyes, at each visit. There is often a delay between when exAMD has developed and when it is diagnosed and treated. To address this, we worked with retinal experts to review all scans for each eye and specify the scan when exAMD was first evident.\nOur system is composed of two deep convolutional neural networks that take as input high-dimensional volumetric eye scans, where each scan consists of 58 million three-dimensional pixels (voxels). In our \nprevious work, now continuing in collaboration with \nGoogle Health, we developed a model capable of segmenting these eye scans into thirteen anatomical categories. The segmented data was combined with the raw scan and both were used as inputs to the prediction model, which was trained to estimate a patient\u2019s risk of conversion to exAMD in their other eye within the next six months.\nThe benefit of a two stage system is that it gives the AI different views of the eye scans. Anatomical segmentation of the images helps the system learn to model risks based on signs of known anatomical indicators such as drusen (small fatty deposits), or loss of the retinal pigment epithelium (which helps to feed and protect other layers of the retina). Providing the raw eye scans allows the model to learn to spot other subtle changes that could become potential risk factors. At the end, the system combines the information it extracts from these scans to predict when and if the eye will progress to exAMD within the next 6 months. We chose this time window to enable the system to predict at least two follow-up intervals ahead of time, assuming a maximal follow-up interval of 3 months. \nIt\u2019s important to establish a benchmark of expert human performance to compare how well our system performs to clinical standards. However, prediction of exAMD is not a routine task performed by clinicians, so it\u2019s unclear whether this task is even possible. To investigate this, we conducted a study with six retinal experts - three ophthalmologists and three optometrists, each with at least ten years of experience - to predict whether an eye will convert to exAMD within the ensuing 6 months. Despite the task\u2019s novelty, the experts performed better than chance alone - however, the task is difficult, and there was substantial variability between their assessments. Our system performed as well as, and in some cases better than, experts in predicting exAMD progression, at the same time exhibiting less variability in agreement with each expert, compared to experts with each other.\nIt may not be enough for a system to simply provide a prediction: clinicians may also ideally seek information regarding the anatomic basis for predictions, which might be of significant use for further interpretation (for example, for designing studies or considering treatments). A benefit of our system is that it automatically segments each scan into known types of tissue. Extracting these anatomical and pathological features provides a systematic method to visualise the change in these tissues over time. The risk scores given by our system align with anatomical changes over time, and together give a richer picture of exAMD conversion.\nWe\u2019re excited by the potential to support clinicians and researchers by developing systems that can help detect retinal diseases earlier and inform the clinical understanding of their progression. A prediction system such as this could be used to inform appropriate follow-up intervals to effectively manage high-risk patients. Our work builds upon promising \nearly work\n to develop \npredictive models\n for \nexAMD\n based on \nretinal photographs\n and OCT \nscans\n. Since beginning our collaboration with Moorfields Eye Hospital in 2016, we\u2019ve published two promising studies highlighting the \npotential of AI\n to transform retinal healthcare. \nHowever, we know there\u2019s still a lot to do \u2013 this work does not yet represent a product that could be implemented in routine clinical practice. While our model can make better predictions than clinical experts, there are many other factors to consider for such systems to be impactful in a clinical setting. While the model was trained and evaluated on a population representative of the largest eye hospital in Europe, additional work would be needed to evaluate performance in the context of very different demographics. \nA recent study examining\n the use of a different AI system in a clinical setting highlighted just some of the sociotechnical issues for such systems in practice. Another difficult point to contend with is that any prediction system will have a certain rate of false positives: that is, when a patient is found to have a condition, or predicted to develop one, that they don\u2019t actually have. The tradeoff of adding an imprecise AI system to an early warning loop could be unnecessarily costly to patients who aren\u2019t actually at risk, and would need to be considered carefully in clinical studies of how such systems might be used in practice. In this paper, we propose two system operating points to balance sensitivity (a measure of how well it correctly identifies the disease) and specificity (a measure of how low the false positive rate is). For example, at a specificity of 90%, a sensitivity of 34% is achieved, meaning that the system correctly identified progression in one third of scans that did go on to progress within 6 months. This could identify a number of patients at high risk with a precision that may be sufficient to inform studies of novel treatment strategies that might mitigate vision loss and improve patient outcomes. \nWe would like to thank Moorfields Eye Hospital and the clinicians who helped curate the data and were involved in our benchmarking study. Please see the paper for all acknowledgements and further details on the work. In addition, we\u2019ve open-sourced the model code for future research, available \nhere\n, and Moorfields will be making the dataset available through the \nRyan Initiative for Macular Research.\nRead the Nature Medicine paper \nhere\n. \nCheck out the github repo \nhere\n.\nFigure design by Paulo Estriga and Adam Cain.\n"}
{"title": "Breaking down global barriers to access", "contents": "This week, we welcomed our biggest and most geographically diverse cohort of DeepMind scholars yet. We\u2019re excited to reflect on the journey so far, share more on the next chapter of the DeepMind scholarships \u2013 and welcome many more universities from around the world into the programme. \nAI could be one of the most useful and transformative technologies in history - and the mission to build safe and beneficial AI spans a broad community. We established our scholarship programme in 2017 in an effort to help build a stronger and more inclusive AI community, who can bring a wider range of experiences to the fields of AI and computer science. The scholarships provide financial support to students from underrepresented groups seeking to study graduate courses relating to AI and adjacent fields. But of course, financial barriers are not the only obstacles that students can face, so in addition, every scholar is matched with a personal DeepMind mentor, who can support their aspirations and help them to navigate academic life.\nWe started with eight fantastic scholars who were studying masters courses in the UK and US. The scholarships were awarded to academically excellent students who belong to groups currently underrepresented in AI. This week, we welcomed more than 50 scholars to our 2020 cohort alone.\nIncreasing representation in AI offers a huge opportunity to bring diverse values, hopes and concerns into conversations about the design and deployment of AI \u2013 and this is critical if AI is going to be a technology that benefits everyone. Take alumnus \nscholar\n Shaquille, who wanted to use machine learning to better understand sickle cell anaemia, a disease which disproportionately affects Black people.\nAs we celebrated our growing community, we considered the goal of the programme \u2013 to contribute to building a stronger and more inclusive AI ecosystem - and we reflected on who was excluded from it.\nOur scholarships aim to support underrepresented students \u2013 spanning gender, race, ethnicity, and socio-economic background. But imbalances in the field aren\u2019t just social, they\u2019re also geographical. Last year, 70% of all AI-related research was \npublished\n in Europe, the US, and China, while many other important regions and countries are significantly underrepresented. For instance, only \n0.3%\n of AI journal citations came from sub-Saharan Africa between 2014-2018, and a number of Eastern European countries are entirely absent from \npublication figures\n. This imbalance risks creating a technology that only accounts for the values, hopes and concerns of a narrow group, entrenching global inequalities while seeing large parts of the world miss out on the potential of AI to improve people\u2019s lives through innovation in science, healthcare, and education.\nThat\u2019s why today, we\u2019re delighted to announce that we\u2019re expanding our programme to support scholars in many more countries currently underrepresented in AI, including \u2013 Bulgaria, Colombia, Greece, Poland, Romania, South Africa, and Turkey. We are also establishing new scholarships in Canada and France, and continuing our support for scholars in the UK and the US. The full list of universities partnering in our scholarships programme is \nhere\n.\nThere are many initiatives actively working to increase regional participation in AI, such as the \nDeep Learning Indaba\n, \nKhipu AI\n, and the \nEastern European Machine Learning summer school\n. We hope to complement these efforts by enabling students to pursue further education in these regions with fewer financial barriers \u2013 contributing to regional hubs of excellence, while benefiting from the guidance of a DeepMind mentor and an international community of scholar peers.\nTo ensure AI is of global benefit, talent must be nurtured in regions which are currently underrepresented in AI research, and space for geographically and socially diverse, local contributions to the field must be made. We know that increasing access to further education is only one part of addressing the deep-seated structural imbalances in AI, but it is an important one and we are happy to be able to contribute.\nSo this week, as we reflect on the achievements of scholars past and present, and welcome the new 2020 cohort, we also look to the future, as we hope others will be inspired to take the next step on their AI career journeys too. To \nquote\n DeepMind alumna Benedetta, who studied at Oxford University: \u201cDon\u2019t underestimate the value of your unique background.\u201d It\u2019s these unique backgrounds and perspectives that will help make the AI community stronger, more diverse, and more inclusive in years to come.\nIf you\u2019re interested in becoming a DeepMind scholar, find out more about the programme on our \nwebsite\n, discover the universities that participated in the scholarship programme this year, and keep an eye out for upcoming announcements from universities offering DeepMind scholarships starting in 2021. \n"}
{"title": "FermiNet: Quantum Physics and Chemistry from First Principles", "contents": "In an \narticle\n recently published in Physical Review Research, we show how deep learning can help solve the fundamental equations of quantum mechanics for real-world systems. Not only is this an important fundamental scientific question, but it also could lead to practical uses in the future, allowing researchers to prototype new materials and chemical syntheses in silico before trying to make them in the lab. Today we are also releasing the \ncode\n from this study so that the computational physics and chemistry communities can build on our work and apply it to a wide range of problems. We\u2019ve developed a new neural network architecture, the Fermionic Neural Network or FermiNet, which is well-suited to modeling the quantum state of large collections of electrons, the fundamental building blocks of chemical bonds. The FermiNet was the first demonstration of deep learning for computing the energy of atoms and molecules from first principles that was accurate enough to be useful, and it remains the most accurate neural network method to date. We hope the tools and ideas developed in our AI research at DeepMind can help solve fundamental problems in the natural sciences, and the FermiNet joins our work on \nprotein folding\n, \nglassy dynamics\n, \nlattice quantum chromodynamics\n and many other projects in bringing that vision to life.\nMention \u201cquantum mechanics\u201d and you are more likely to inspire confusion than anything else. The phrase conjures up images of Schr\u00f6dinger\u2019s cat, which can paradoxically be both alive and dead, and fundamental particles that are also, somehow, waves. \u00a0In quantum systems, a particle such as an electron doesn\u2019t have an exact location, as it would in a classical description. Instead, its position is described by a probability cloud - it\u2019s smeared out in all places it\u2019s allowed to be. This counterintuitive state of affairs led Richard Feynman to declare: \u201cIf you think you understand quantum mechanics, you don\u2019t understand quantum mechanics.\u201d Despite this spooky weirdness, the meat of the theory can be reduced down to just a few straightforward equations. The most famous of these, the Schr\u00f6dinger equation, describes the behavior of particles at the quantum scale in the same way that Newton\u2019s laws describe the behavior of objects at our more familiar human scale. While the interpretation of this equation can cause endless head-scratching, the math is much easier to work with, leading to the common exhortation from professors to \u201cshut up and calculate\u201d when pressed with thorny philosophical questions from students.\nThese equations are sufficient to describe the behavior of all the familiar matter we see around us at the level of atoms and nuclei. Their counterintuitive nature leads to all sorts of exotic phenomena: superconductors, superfluids, lasers and semiconductors are only possible because of quantum effects. But even the humble covalent bond - the basic building block of chemistry - is a consequence of the quantum interactions of electrons. Once these rules were worked out in the 1920s, scientists realised that, for the first time, they had a detailed theory of how chemistry works. In principle, they could just set up these equations for different molecules, solve for the energy of the system, and figure out which molecules were stable and which reactions would happen spontaneously. But when they sat down to actually calculate the solutions to these equations, they found that they could do it exactly for the simplest atom (hydrogen) and virtually nothing else. Everything else was too complicated.\nThe heady optimism of those days was nicely summed up by Paul Dirac:\nMany took up Dirac\u2019s charge, and soon physicists built mathematical techniques that could approximate the qualitative behavior of molecular bonds and other chemical phenomena. These methods started from an approximate description of how electrons behave that may be familiar from introductory chemistry. In this description, each electron is assigned to a particular orbital, which gives the probability of a single electron being found at any point near an atomic nucleus. The shape of each orbital then depends on the average shape of all other orbitals. As this \u201cmean field\u201d description treats each electron as being assigned to just one orbital, it is a very incomplete picture of how electrons actually behave. Nevertheless, it is enough to estimate the total energy of a molecule with only about 0.5% error.\nUnfortunately, 0.5% error still isn\u2019t enough to be useful to the working chemist. The energy in molecular bonds is just a tiny fraction of the total energy of a system, and correctly predicting whether a molecule is stable can often depend on just 0.001% of the total energy of a system, or about 0.2% of the remaining \u201ccorrelation\u201d energy. For instance, while the total energy of the electrons in a \nbutadiene\n molecule is almost 100,000 kilocalories per mole, the difference in energy between different possible shapes of the molecule is just 1 kilocalorie per mole. That means that if you want to correctly predict butadiene\u2019s natural shape, then the same level of precision is needed as measuring the width of a football field down to the millimeter.\nWith the advent of digital computing after World War II, scientists developed a whole menagerie of computational methods that went beyond this mean field description of electrons. While these methods come in a bewildering alphabet soup of abbreviations, they all generally fall somewhere on an axis that trades off accuracy with efficiency. At one extreme, there are methods that are essentially exact, but scale worse than exponentially with the number of electrons, making them impractical for all but the smallest molecules. At the other extreme are methods that scale linearly, but are not very accurate. These computational methods have had an enormous impact on the practice of chemistry - the 1998 Nobel Prize in chemistry was awarded to the originators of many of these algorithms.\nDespite the breadth of existing computational quantum mechanical tools, we felt a new method was needed to address the problem of efficient representation. There\u2019s a reason that the largest quantum chemical calculations only run into the tens of thousands of electrons for even the most approximate methods, while classical chemical calculation techniques like molecular dynamics can handle millions of atoms. The state of a classical system can be described easily - we just have to track the position and momentum of each particle. Representing the state of a quantum system is far more challenging. A probability has to be assigned to every possible configuration of electron positions. This is encoded in the wavefunction, which assigns a positive or negative number to every configuration of electrons, and the wavefunction squared gives the probability of finding the system in that configuration. The space of all possible configurations is enormous - if you tried to represent it as a grid with 100 points along each dimension, then the number of possible electron configurations for the silicon atom would be larger than the number of atoms in the universe!\nThis is exactly where we thought deep neural networks could help. In the last several years, there have been huge advances in representing complex, high-dimensional probability distributions with neural networks. We now know how to train these networks efficiently and scalably. We surmised that, given these networks have already proven their mettle at fitting high-dimensional functions in artificial intelligence problems, maybe they could be used to represent quantum wavefunctions as well. We were not the first people to think of this - researchers such as \nGiuseppe Carleo and Matthias Troyer\n and others have shown how modern deep learning could be used for solving idealised quantum problems. We wanted to use deep neural networks to tackle more realistic problems in chemistry and condensed matter physics, and that meant including electrons in our calculations.\nThere is just one wrinkle when dealing with electrons. Electrons must obey the Pauli exclusion principle, which means that they can\u2019t be in the same space at the same time. This is because electrons are a type of particle known as fermions, which include the building blocks of most matter - protons, neutrons, quarks, neutrinos, etc. Their wavefunction must be antisymmetric - if you swap the position of two electrons, the wavefunction gets multiplied by -1. That means that if two electrons are on top of each other, the wavefunction (and the probability of that configuration) will be zero.\nThis meant we had to develop a new type of neural network that was antisymmetric with respect to its inputs, which we have dubbed the Fermionic Neural Network, or FermiNet. In most quantum chemistry methods, antisymmetry is introduced using a function called the determinant. The determinant of a matrix has the property that if you swap two rows, the output gets multiplied by -1, just like a wavefunction for fermions. So you can take a bunch of single-electron functions, evaluate them for every electron in your system, and pack all of the results into one matrix. The determinant of that matrix is then a properly antisymmetric wavefunction. The major limitation of this approach is that the resulting function - known as a Slater determinant - is not very general. Wavefunctions of real systems are usually far more complicated. The typical way to improve on this is to take a large linear combination of Slater determinants - sometimes millions or more - and add some simple corrections based on pairs of electrons. Even then, this may not be enough to accurately compute energies.\nDeep neural networks can often be far more efficient at representing complex functions than linear combinations of basis functions. In the FermiNet, this is achieved by making each function going into the determinant a function of all electrons (1). This goes far beyond methods that just use one- and two-electron functions. The FermiNet has a separate stream of information for each electron. Without any interaction between these streams, the network would be no more expressive than a conventional Slater determinant. To go beyond this, we average together information from across all streams at each layer of the network, and pass this information to each stream at the next layer. That way, these streams have the right symmetry properties to create an antisymmetric function. This is similar to how \ngraph neural networks\n aggregate information at each layer. Unlike the Slater determinants, FermiNets are \nuniversal\n function approximators\n, at least in the limit where the neural network layers become wide enough. That means that, if we can train these networks correctly, they should be able to fit the nearly-exact solution to the Schr\u00f6dinger equation.\nWe fit the FermiNet by minimising the energy of the system. To do that exactly, we would need to evaluate the wavefunction at all possible configurations of electrons, so we have to do it approximately instead. We pick a random selection of electron configurations, evaluate the energy locally at each arrangement of electrons, add up the contributions from each arrangement and minimise this instead of the true energy. This is known as a Monte Carlo method, because it\u2019s a bit like a gambler rolling dice over and over again. While it is approximate, if we need to make it more accurate we can always roll the dice again. Since the wavefunction squared gives the probability of observing an arrangement of particles in any location, it is most convenient to generate samples from the wavefunction itself - essentially, simulating the act of observing the particles. While most neural networks are trained from some external data, in our case the inputs used to train the neural network are generated by the neural network itself. It\u2019s a bit like pulling yourself up by your own bootstraps, and it means that we don\u2019t need any training data other than the positions of the atomic nuclei that the electrons are dancing around. The basic idea, known as variational quantum Monte Carlo (or VMC for short), has been around since the \u201860s, and it is generally considered a cheap but not very accurate way of computing the energy of a system. By replacing the simple wavefunctions based on Slater determinants with the FermiNet, we have dramatically increased the accuracy of this approach on every system we\u2019ve looked at.\nTo make sure that the FermiNet really does represent an advance in the state of the art, we started by investigating simple, well-studied systems, like atoms in the first row of the periodic table (hydrogen through neon). These are small systems - 10 electrons or fewer - and simple enough that they can be treated by the most accurate (but exponential scaling) methods. The FermiNet outperforms comparable VMC calculations by a wide margin - often cutting the error relative to the exponentially-scaling calculations by half or more. On larger systems, the exponentially-scaling methods become intractable, so instead we use the \u201ccoupled cluster\u201d method as a baseline. This method works well on molecules in their stable configuration, but struggles when bonds get stretched or broken, which is critical for understanding chemical reactions. While it scales much better than exponentially, the particular coupled cluster method we used still scales as the number of electrons raised to the seventh power, so it can only be used for medium-sized molecules. We applied the FermiNet to progressively larger molecules, starting with lithium hydride and working our way up to bicyclobutane, the largest system we looked at, with 30 electrons. On the smallest molecules, the FermiNet captured an astounding 99.8% of the difference between the coupled cluster energy and the energy you get from a single Slater determinant. On bicyclobutane, the FermiNet still captured 97% or more of this correlation energy - a huge accomplishment for a supposedly \u201ccheap but inaccurate\u201d approach.\nWhile coupled cluster methods work well for stable molecules, the real frontier in computational chemistry is in understanding how molecules stretch, twist and break. There, coupled cluster methods often struggle, so we have to compare against as many baselines as possible to make sure we get a consistent answer. We looked at two benchmark stretched systems - the nitrogen molecule (N2) and the hydrogen chain with 10 atoms, (H10). Nitrogen is an especially challenging molecular bond, because each nitrogen atom contributes 3 electrons. The hydrogen chain, meanwhile, is of \ninterest for understanding how electrons behave in materials\n, for instance predicting whether or not a material will conduct electricity. On both systems, coupled cluster did well at equilibrium, but had problems as the bonds were stretched. Conventional VMC calculations did poorly across the board. But the FermiNet was among the best methods investigated, no matter the bond length.\nWe think the FermiNet is the start of great things to come for the fusion of deep learning and computational quantum chemistry. Most of the systems we\u2019ve looked at so far are well-studied and well-understood. But just as the first good results with deep learning in other fields led to a burst of follow-up work and rapid progress, we hope that the FermiNet will inspire lots of work on scaling up and many ideas for new, even better network architectures. Already, since we first put our work on arXiv last year, \nother\n \ngroups\n have shared their approaches to applying deep learning to first-principles calculations on the many-electron problem. We have also just scratched the surface of computational quantum physics, and look forward to applying the FermiNet to tough problems in material science and condensed matter physics as well. Mostly, we hope that by releasing the source code used in our experiments, we can inspire other researchers to build on our work and try out new applications we haven\u2019t even dreamed of.\nRead the paper \nhere\n and view the code \nhere\n. With thanks to Jim Kynvin, Adam Cain and Dominic Barlow for the figures.\n(1) The FermiNet also has streams for every pair of electrons, and information from these streams is passed back to the single-electron streams. For simplicity, we chose not to visualise this in the blog post, but details can be found in the paper.\n"}
{"title": "Fast reinforcement learning through the composition of behaviours", "contents": "Imagine if you had to learn how to chop, peel and stir all over again every time you wanted to learn a new recipe. In many machine learning systems, agents often have to learn entirely from scratch when faced with new challenges. It\u2019s clear, however, that people learn more efficiently than this: they can combine abilities previously learned. In the same way that a finite dictionary of words can be reassembled into sentences of near infinite meanings, people repurpose and re-combine skills they already possess in order to tackle novel challenges.\nIn nature, learning arises as an animal explores and interacts with its environment in order to gather food and other rewards. This is the paradigm captured by \nreinforcement learning\n (RL): interactions with the environment reinforce or inhibit particular patterns of behavior depending on the resulting reward (or penalty). Recently, the combination of RL with \ndeep learning\n has led to impressive results, such as agents that can learn how to play boardgames like \nGo\n and \nchess\n, the full spectrum of \nAtari\n games, as well as more modern, difficult video games like \nDota\n and \nStarCraft II\n.\nA major limitation in RL is that current methods require vast amounts of training experience. For example, in order to learn how to play a single Atari game, an RL agent typically consumes an amount of data corresponding to several weeks of uninterrupted playing. A \nstudy\n led by researchers at MIT and Harvard indicated that in some cases, humans are able to reach the same performance level in just fifteen minutes of play.\nOne possible reason for this discrepancy is that, unlike humans, RL agents usually learn a new task from scratch. We would like our agents to leverage knowledge acquired in previous tasks to learn a new task more quickly, in the same way that a cook will have an easier time learning a new recipe than someone who has never prepared a dish before. In \nan article\n recently published in the Proceedings of the National Academy of Sciences (PNAS), we describe a framework aimed at endowing our RL agents with this ability.\nTwo ways of representing the world\nTo illustrate our approach, we will explore an example of an activity that is (or at least used to be) an everyday routine: the commute to work. Imagine the following scenario: an agent must commute every day from its home to its office, and it always gets a coffee on the way. There are two cafes between the agent's house and the office: one has great coffee but is on a longer path, and the other one has decent coffee but a shorter commute (Figure 1). Depending on how much the agent values the quality of the coffee versus how much of a rush it is in on a given day, it may choose one of two routes (the yellow and blue paths on the map shown in Figure 1).\nTraditionally, RL algorithms fall into two broad categories: \nmodel-based and model-free agents\n (Figures 2 & 3). A model-based agent (Figure 2) builds a representation of many aspects of the \u00a0environment. An agent of this type might know how the different locations are connected, the quality of the coffee in each cafe, and anything else that is considered relevant. A model-free agent (Figure 3) has a much more compact representation of its environment. For instance, a value-based model-free agent would have a single number associated with each possible route leaving its home; this is the expected \"value\" of each route, reflecting a specific weighing of coffee quality vs. commute length. Take the blue path shown in Figure 1 as an example. Say this path has length 4, and the coffee the agent gets by following it is rated 3 stars. If the agent cares about the commute distance 50% more than it cares about the quality of the coffee, the value of this path will be \u00a0(-1.5 x 4) + (1 x 3) = -3 \u00a0(we use a negative weight associated with the distance to indicate that longer commutes are undesirable).\nWe can interpret the relative weighting of the coffee quality versus the commute distance as the agent\u2019s \npreferences\n. For any fixed set of preferences, a model-free and a model-based agent would choose the same route. Why then have a more complicated representation of the world, like the one used by a model-based agent, if the end result is the same? Why learn so much about the environment if the agent ends up sipping the same coffee?\nPreferences can change day to day: an agent might take into account how hungry it is, or whether it\u2019s running late to a meeting, in planning its route to the office. One way for a model-free agent to handle this is to learn the best route associated with every possible set of preferences. This is not ideal because learning every possible combination of preferences will take a long time. It is also impossible to learn a route associated with every possible set of preferences if there are infinitely many of them.\nIn contrast, a model-based agent can adapt to any set of preferences, without any learning, by \"imagining\" all possible routes and asking how well they would fulfill its current mindset. However, this approach also has drawbacks. Firstly, \u201dmentally\u201d generating and evaluating all possible trajectories can be computationally demanding. Secondly, building a model of the entire world can be very difficult in complex environments.\nModel-free agents learn faster but are brittle to change. Model-based agents are flexible but can be slow to \u00a0learn. Is there an intermediate solution?\nA recent \nstudy\n in behavioural science and neuroscience suggests that in certain situations, humans and animals make decisions based on an algorithmic model that is a compromise between the model-free and model-based approaches (\nhere\n and \nhere\n). The hypothesis is that, like model-free agents, humans also compute the value of alternative strategies in the form of a number. But, instead of summarising a single quantity, humans summarise many different quantities describing the world around them, reminiscent of model-based agents.\nIt\u2019s possible to endow an RL agent with the same ability. In our example, such an agent would have, for each route, a number representing the expected quality of coffee and a number representing the distance to the office. It could also have numbers associated with things the agent is not deliberately trying to optimise but are nevertheless available to it for future reference (for example, the quality of the food in each cafe). The aspects of the world the agent cares about and keeps track of are sometimes referred to as \u201cfeatures\u201d. Because of that, this representation of the world is called \nsuccessor features\n (previously termed the \u201csuccessor representation\u201d in its \noriginal incarnation\n).\nSuccessor features can be thought of as a middle ground between the model-free and model-based representations. Like the latter, successor features summarise many different quantities, capturing the world beyond a single value. However, like in the model-free representation, the quantities the agent keeps track of are simple statistics summarising the features it cares about. In this way, successor features are like an \u201cunpacked\u201d version of the model-free agent. Figure 4 illustrates how an agent using successor features would see our example environment.\nSuccessor features are a useful representation because they allow for a route to be evaluated under different sets of preferences. Let\u2019s use the blue route in Figure 1 as an example again. Using successor features, the agent would have three numbers associated with this path: its length (4), the quality of the coffee (3) and the quality of the food (5). If the agent already ate breakfast it will probably not care much about the food; also, if it is late, it might care about the commute distance more than the quality of the coffee --say, 50% more, as before. In this scenario the value of the blue path would be \u00a0(-1.5 x 4) + (1 x 3) + (0 x 5) = -3, as in the example given above. But now, on a day when the agent is hungry, and thus cares about the food as much as it cares about the coffee, it can immediately update the value of this route to \u00a0(-1.5 x 4) + (1 x 3) + (1 x 5) = 2. Using the same strategy, the agent can evaluate any route according to any set of preferences.\nIn our example, the agent is choosing between routes. More generally, the agent will be searching for a policy: a prescription of what to do in every possible situation. Policies and routes are closely related: in our example, a policy that chooses to take the road to cafe A from home and then chooses the road to the office from cafe A would traverse the blue path. So, in this case, we can talk about policies and routes interchangeably (this would not be true if there were some randomness in the environment, but we will leave this detail aside). \u00a0We discussed how successor features allow a route (or policy) to be evaluated under different sets of preferences. We call this process \ngeneralised policy evaluation\n, or GPE.\nWhy is GPE useful? Suppose the agent has a dictionary of policies (for example, known routes to the office). Given a set of preferences, the agent can use GPE to immediately evaluate how well each policy in the dictionary would perform under those preferences. Now the really interesting part: based on this quick evaluation of known policies, \nthe agent can create entirely new policies on the fly.\n The way it does it is simple: every time the agent has to make a decision, it asks the following question: \u201cif I were to make this decision and then follow the policy with the maximum value thereafter, which decision would lead to the maximum overall value?\u201d Surprisingly, if the agent picks the decision leading to the maximum overall value in each situation, it ends up with a policy that is often better than the individual policies used to create it.\nThis process of \u201cstitching together\u201d a set of policies to create a better policy is called \ngeneralised policy improvement\n, or GPI. Figure 5 illustrates how GPI works using our running example.\nThe performance of a policy created through GPI will depend on how many policies the agent knows. For instance, in our running example, as long as the agent knows the blue and yellow \u00a0paths, it will find the best route for any preferences over coffee quality and commute length. But the GPI policy will not always find the best route. In Figure 1, the agent would never visit cafe A and then cafe B if it did not already know a policy that connected them in this way (like the orange route in the figure).\nA simple example to show GPE and GPI in action \nTo illustrate the benefits of GPE and GPI, we now give a glimpse of one of the experiments from our recent publication (see \npaper\n for full details). The experiment uses a simple environment that represents in an abstract way the type of problem in which our approach can be useful. As shown in Figure 6, the environment is a 10 x 10 grid with 10 objects spread across it. The agent only gets a non-zero reward if it picks up an object, in which case another object pops up in a random location. The reward associated with an object depends on its type. Object types are meant to represent concrete or abstract concepts; to connect with our running example, we will consider that each object is either \u201ccoffee\u201d or \u201cfood\u201d (these are the features the agent keeps track of).\nClearly, the best strategy for the agent depends on its current preferences over coffee or food. For example, in Figure 6, an agent that only cares about coffee may follow the path in red, while an agent focused exclusively on food would follow the blue path. We can also imagine intermediate situations in which the agent wants coffee and food with different weights, including the case in which the agent wants to avoid one of them. For example, if the agent wants coffee but really does not want food, the gray path in Figure 6 may be a better alternative to the red one.\nThe challenge in this problem is to quickly adapt to a new set of preferences (or a \u201ctask\u201d). In our experiments we showed how one can do so using GPE and GPI. Our agent learned two policies: one that seeks coffee and one that seeks food. We then tested how well the policy computed by GPE and GPI performed on tasks associated with different preferences. In figure 7 we compare our method with a model-free agent on the task whose goal is to seek coffee while avoiding food. Observe how the agent using GPE and GPI instantaneously synthesises a reasonable policy, even though it never learned how to deliberately avoid objects. Of course, the policy computed by GPE and GPI can be used as an initial solution to be later refined through learning, which means that it would match the final performance of a model-free agent but would probably get there faster.\nFigure 7 shows the performance of GPE and GPI on one specific task. We have also tested the same agent across many other tasks. Figure 8 shows what happens with the performance of the model-free and GPE-GPI agents when we change the relative importance of coffee and food. Note that, while the model-free agent has to learn each task separately, from scratch, the GPE-GPI agent only learns two policies and then quickly adapts to all of the tasks.\nThe experiments above used a simple environment designed to exhibit the properties needed by GPE and GPI without unnecessary confounding factors. But GPE and GPI have also been applied at scale. For example, in previous papers (\nhere\n and \nhere\n) we showed how the same strategy also works when we replace a grid world with a three dimensional environment in which the agent receives observations from a first-person perspective (see illustrative videos \nhere\n and \nhere\n). We have also used GPE and GPI to allow a four-legged simulated robot to navigate along any direction after having learned how to do so along three directions only (see paper \nhere\n and video \nhere\n).\nThe work on GPE and GPI is at the intersection of two separate branches of research related to these operations individually. The first, related to GPE, is the work on the successor representation, initiated with Dayan\u2019s \nseminal paper\n from 1993. Dayan\u2019s paper inaugurated a line of work in neuroscience that is very active to this day (see further reading: \"The successor representation in neuroscience\"). Recently, the successor representation reemerged in the context of RL (links \nhere\n and \nhere\n), where it is also referred to as \u201csuccessor features\u201d, and became an active line of research there as well (see further reading: \"GPE, successor features, and related approaches\"). Successor features are also closely related to \ngeneral value functions\n, a concept based on Sutton et al.\u2019s hypothesis that relevant knowledge can be expressed in the form of many predictions about the world (also discussed \nhere\n). The definition of successor features has independently emerged in \nother contexts\n within RL, and is also related to more \nrecent approaches\n normally associated with deep RL.\nThe second branch of research at the origins of GPE and GPI, related to the latter, is concerned with composing behaviours to create new behaviours. The idea of a decentralised controller that executes sub-controllers has come up multiple times over the years (e.g., \nBrooks, 1986\n), and its implementation using value functions can be traced back to at least as far as 1997, with \nHumphrys\u2019\n and \nKarlsson\u2019s\n PhD theses. GPI is also closely related to hierarchical RL, whose foundations were laid down in the 1990's and early 2000\u2019s in the works by \nDayan and Hinton\n, \nParr and Russell\n, \nSutton, Precup and Singh\n, and \nDietterich\n. Both the composition of behaviours and hierarchical RL are today dynamic areas of research (see further reading: \"GPI, hierarchical RL, and related approaches\").\nMehta \net al\n.\n were probably the first ones to jointly use GPE and GPI, although in the scenario they considered GPI reduces to a single choice at the outset (that is, there is no \u201cstitching\u201d of policies). The version of GPE and GPI discussed in this blog post was first \nproposed\n in 2016 as a mechanism to promote \ntransfer learning\n. Transfer in RL dates back to \nSingh\u2019s work\n in 1992 and has recently experienced a \nresurgence\n in the context of deep RL, where it continues to be an active area of research (see further reading: \"GPE + GPI, transfer learning, and related approaches\").\nSee more information about these works below, where we also provide a list of suggestions for further readings.\nIn summary, a model-free agent cannot easily adapt to new situations, for example to accommodate sets of preferences it has not experienced before. A model-based agent can adapt to any new situation, but in order to do so it first has to learn a model of the entire world. An agent based on GPE and GPI offers an intermediate solution: although the model of the world it learns is considerably smaller than that of a model-based agent, it can quickly adapt to certain situations, often with good performance.\nWe discussed specific instantiations of GPE and GPI, but these are in fact more general concepts. At an abstract level, an agent using GPE and GPI proceeds in two steps. First, when faced with a new task, it asks: \u201cHow well would solutions to known tasks perform on this new task?\u201d This is GPE. Then, based on this evaluation, the agent combines the previous solutions to construct a solution for the new task --that is, it performs GPI. The specific mechanics behind GPE and GPI are less important than the principle itself, and finding alternative ways to carry out these operations may be an exciting research direction. Interestingly, a new \nstudy\n in behavioural sciences provides preliminary evidence that humans make decisions in multitask scenarios following a principle that closely resembles GPE and GPI.\nThe fast adaptation provided by GPE and GPI is promising for building faster learning RL agents. More generally, it suggests a new approach to learning flexible solutions to problems. \u00a0Instead of tackling a problem as a single, monolithic, task, an agent can break it down into smaller, more manageable, sub-tasks. The solutions of the sub-tasks can then be reused and recombined to solve the overall task faster. This results in a compositional approach to RL that may lead to more scalable agents. At the very least, these agents will not be late because of a cup of coffee.\nRead the paper, as first published in PNAS, \nhere\n.\nWith thanks to Jim Kynvin, Adam Cain and Dominic Barlow for the figures, Kimberly Stachenfeld for the pointers to the neuroscience literature, and Kelly Clancy for the help with the text.\nGPE, successor features, and related approaches\nImproving Generalisation for Temporal Difference Learning: The Successor Representation.\n Peter Dayan. Neural Computation, 1993.\nApprenticeship Learning Via Inverse Reinforcement Learning.\n Pieter Abbeel and Andrew Y. Ng. Proceedings of the International Conference on Machine learning (ICML), 2004.\nHorde: A Scalable Real-time Architecture for Learning Knowledge from Unsupervised Sensorimotor Interaction.\n Richard S. Sutton, Joseph Modayil, Michael Delp, Thomas Degris, Patrick M. Pilarski, Adam White. Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2011.\nMulti-timescale Nexting in a Reinforcement Learning Robot.\n Joseph Modayil, Adam White, Richard S. Sutton. From Animals to Animats, 2012.\nUniversal Value Function Approximators.\n Tom Schaul, Dan Horgan, Karol Gregor, David Silver. Proceedings of the International Conference on Machine learning (ICML), 2015.\nDeep Successor Reinforcement Learning.\n Tejas D. Kulkarni, Ardavan Saeedi, Simanta Gautam, Samuel J. Gershman. arXiv, 2017.\nVisual Semantic Planning Using Deep Successor Representations.\n Yuke Zhu, Daniel Gordon, Eric Kolve, Dieter Fox, Li Fei-Fei, Abhinav Gupta, Roozbeh Mottaghi, Ali Farhadi. Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2017.\nDeep Reinforcement Learning with Successor Features for Navigation Across Similar Environments.\n Jingwei Zhang, Jost Tobias Springenberg, Joschka Boedecker, Wolfram Burgard. IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2017.\nUniversal Successor Representations for Transfer Reinforcement Learning.\n Chen Ma, Junfeng Wen, Yoshua Bengio. arXiv, 2018.\nEigenoption Discovery through the Deep Successor Representation.\n Marlos C. Machado, Clemens Rosenbaum, Xiaoxiao Guo, Miao Liu, Gerald Tesauro, Murray Campbell. International Conference on Learning Representations (ICLR), 2018.\nSuccessor Options: An Option Discovery Framework for Reinforcement Learning.\n Rahul Ramesh, Manan Tomar, Balaraman Ravindran. Proceedings of the \u00a0International Joint Conference on Artificial Intelligence (IJCAI), 2019.\nSuccessor Uncertainties: Exploration and Uncertainty in Temporal Difference Learning.\n David Janz, Jiri Hron, Przemys\u0142aw Mazur, Katja Hofmann, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Sebastian Tschiatschek. Advances in Neural Information Processing Systems (NeurIPS), 2019.\nSuccessor Features Combine Elements of Model-Free and Model-based Reinforcement Learning.\n Lucas Lehnert, Michael L. Littman. arXiv, 2019.\nCount-Based Exploration with the Successor Representation.\n Marlos C. Machado, Marc G. Bellemare, Michael Bowling. Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2020.\nGPI, hierarchical RL, and related approaches\nA Robust Layered Control System for a Mobile Robot.\n R. Brooks. IEEE Journal on Robotics and Automation, 1986.\nFeudal Reinforcement Learning.\n Peter Dayan and Geoffrey E. Hinton. Advances in Neural Information Processing Systems (NIPS), 1992.\nAction Selection Methods Using Reinforcement Learning.\n Mark Humphrys. PhD thesis, University of Cambridge, Cambridge, UK, 1997.\nLearning to Solve Multiple Goals.\n Jonas Karlsson. PhD thesis, University of Rochester, Rochester, New York, 1997.\nReinforcement Learning with Hierarchies of Machines.\n Ronald Parr and Stuart J. Russell. Advances in Neural Information Processing Systems (NIPS), 1997.\nBetween MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning.\n Richard S.Sutton, DoinaPrecup, Satinder Singh. Artificial Intelligence, 1999.\nHierarchical Reinforcement Learning with the MAXQ Value Function Decomposition. \nT. G. Dietterich. Journal of Artificial Intelligence Research, 2000.\nMultiple-Goal Reinforcement Learning with Modular Sarsa(O).\n Nathan Sprague and \u00a0Dana Ballard. Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), 2003.\nQ-decomposition for Reinforcement Learning Agents.\n Stuart J. Russell and Andrew Zimdars. \u00a0Proceedings of the International Conference on Machine Learning (ICML), 2003.\nCompositionality of Optimal Control Laws.\n E. Todorov. Advances in Neural Information Processing Systems (NIPS), 2009.\nLinear Bellman combination for control of character animation.\n M. da Silva, F. Durand, and J. Popovic. ACM Transactions on Graphics, 2009.\nHierarchy Through Composition with Multitask LMDPS.\n A. M. Saxe, A. C. Earle, and B. Rosman. Proceedings of the International Conference on Machine Learning (ICML), 2017.\nHybrid Reward Architecture for Reinforcement Learning.\n Harm van Seijen, Mehdi Fatemi, Joshua Romoff, Romain Laroche, Tavian Barnes, and Jeffrey Tsang. Advances in Neural Information Processing Systems (NIPS), 2017.\nFeudal Networks for Hierarchical Reinforcement Learning.\n Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max Jaderberg, David Silver, Koray Kavukcuoglu. \u00a0Proceedings of the International Conference on Machine Learning (ICML), 2017.\nComposable Deep Reinforcement Learning for Robotic Manipulation.\n T. Haarnoja, V. Pong, A. Zhou, M. Dalal, P. Abbeel, and S. Levine. IEEE International Conference on Robotics and Automation (ICRA), 2018.\nComposing Value Functions in Reinforcement Learning.\n Benjamin Van Niekerk, Steven James, Adam Earle, Benjamin Rosman. Proceedings of the International Conference on Machine Learning (ICML), 2019.\nPlanning in Hierarchical Reinforcement Learning: Guarantees for Using Local Policies.\n Tom Zahavy, Avinatan Hasidim, Haim Kaplan, Yishay Mansour. International Conference on Algorithmic Learning Theory (ALT), 2020.\nGPE + GPI, transfer learning, and related approaches\nTransfer of Learning by Composing Solutions of Elemental Sequential Tasks.\n Satinder Singh. Machine Learning, 1992.\nTransfer Learning for Reinforcement Learning Domains: A Survey.\n Matthew E. Taylor and Peter Stone. Journal of Machine Learning Research, 2009.\nTransfer in Variable-Reward Hierarchical Reinforcement Learning.\n Neville Mehta, Sriraam Natarajan, Prasad Tadepalli, Alan Fern. Machine Learning, 2008.\nLearning and Transfer of Modulated Locomotor Controllers.\n Nicolas Heess, Greg Wayne, Yuval Tassa, Timothy Lillicrap, Martin Riedmiller, David Silver. arXiv, 2016.\nLearning to Reinforcement Learn.\n Jane X. Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z. Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, Matt Botvinick. arXiv, 2016.\nRL2: Fast Reinforcement Learning via Slow Reinforcement Learning.\n Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, Pieter Abbeel. arXiv, 2016.\nModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\n Chelsea Finn, Pieter Abbeel, Sergey Levine. Proceedings of the International Conference on Machine Learning (ICML), 2017.\nSuccessor Features for Transfer in Reinforcement Learning.\n Andr\u00e9 Barreto, Will Dabney, R\u00e9mi Munos, Jonathan J. Hunt, Tom Schaul, Hado van Hasselt, David Silver. \u00a0Advances in Neural Information Processing Systems (NIPS), 2017.\nTransfer in Deep Reinforcement Learning Using Successor Features and Generalised Policy Improvement.\n Andr\u00e9 Barreto, Diana Borsa, John Quan, Tom Schaul, David Silver, Matteo Hessel, Daniel Mankowitz, Augustin \u017d\u00eddek, R\u00e9mi Munos. Proceedings of the International Conference on Machine Learning (ICML), 2018.\nComposing Entropic Policies Using Divergence Correction.\n Jonathan Hunt, Andr\u00e9 Barreto, Timothy Lillicrap, Nicolas Heess. Proceedings of the International Conference on Machine Learning (ICML), 2019.\nUniversal Successor Features Approximators.\n Diana Borsa, Andr\u00e9 Barreto, John Quan, Daniel Mankowitz, R\u00e9mi Munos, Hado van Hasselt, David Silver, Tom Schaul. International Conference on Learning Representations (ICLR), 2019.\nThe Option Keyboard: Combining Skills in Reinforcement Learning.\n Andr\u00e9 Barreto, Diana Borsa, \u00a0Shaobo Hou, Gheorghe Comanici, Eser Ayg\u00fcn, Philippe Hamel, Daniel Toyama, Jonathan J. Hunt, Shibl Mourad, David Silver, Doina Precup. Advances in Neural Information Processing Systems (NeurIPS), 2019.\nTransfer Learning in Deep Reinforcement Learning: A Survey.\n Zhuangdi Zhu, Kaixiang Lin, Jiayu Zhou, arXiv, 2020.\nFast Task Inference with Variational Intrinsic Successor Features.\n Steven Hansen, Will Dabney, Andr\u00e9 Barreto, Tom Van de Wiele, David Warde-Farley, Volodymyr Mnih. International Conference on Learning Representations (ICLR), 2020.\nFast Reinforcement Learning with Generalized Policy Updates.\n Andr\u00e9 Barreto, Shaobo Hou, Diana Borsa, David Silver, Doina Precup. Proceedings of the National Academy of Sciences, 2020.\nThe successor representation in neuroscience\nThe Hippocampus as a Predictive Map.\n Kimberly Stachenfeld, Matthew Botvinick, Samuel Gershman. Nature Neuroscience, 2017.\nThe Successor Representation in Human Reinforcement Learning.\n I. Momennejad, E. M. Russek, J. H. Cheong, M. M. Botvinick, N. D. Daw, S. J. Gershman. \u00a0Nature Human Behaviour, 2017.\nPredictive Representations Can Link Model-Based Reinforcement Learning to Model-Free Mechanisms.\n E. Russek, I. Momennejad, M. M. Botvinick, S. J. Gershman, N. D. Daw. PLOS Computational Biology, 2017.\nThe Successor Representation: Its Computational Logic and Neural Substrates.\n Samuel J. Gershman. Journal of Neuroscience, 2018.\nBetter Transfer Learning with Inferred Successor Maps.\n Tamas J. Madarasz, Timothy E. Behrens. Advances in Neural Information Processing Systems (NeurIPS), 2019.\nMulti-Task Reinforcement Learning in Humans.\n Momchil S. Tomov, Eric Schulz, and Samuel J. Gershman. bioRxiv, 2019.\nA neurally plausible model learns successor representations in partially observable environments.\n Eszter Vertes, Maneesh Sahani. Advances in Neural Information Processing Systems (NeurIPS), 2019.\nNeurobiological Successor Features for Spatial Navigation.\n William de Cothi, Caswell Barry. Hippocampus, 2020.\nLinear Reinforcement Learning: Flexible Reuse of Computation in Planning, Grid Fields, and Cognitive Control.\n Payam Piray, Nathaniel D. Daw. bioRxiv, 2020.\n"}
{"title": "Applying for technical roles", "contents": "It\u2019s no secret that the gender gap still exists within STEM. Despite a slight increase in recent years, studies show that women only \nmake up about a quarter\n of the overall STEM workforce in the UK. While the reasons vary, many women \nreport\n feeling held back by a lack of representation, clear opportunities and information on what working in the sector actually involves. \nClosing the gap within STEM is not a quick fix but a collective effort of everyone in the industry. Various organisations like \nWomen in Machine Learning\n (WiML) actively work to help create a more inclusive environment where the successes of women are amplified. They also stand as an important point of information for the many women who want to learn more about what it\u2019s like to work in STEM. \nThat\u2019s why for this year\u2019s \nInternational Women in Engineering Day\n, we asked the WiML community to share with us the most common questions they receive about technical interviewing. To share their perspectives and to discuss what it\u2019s actually like to work at DeepMind, we brought together Mihaela Rosca (Research Engineer), Feryal Behbahani (Research Scientist) and Kate Parkyn (Recruitment Lead - Research & Engineering).\nMihaela: \nIt\u2019s not uncommon to have self doubts or feel as if you\u2019re under prepared for a position in the field. There will never be a perfect time to apply and you can easily convince yourself that there\u2019s more to learn but that shouldn\u2019t be a deterring factor in your decision to apply. \nOf course the right skillset will depend on the specific role you\u2019re after, but if you\u2019re keen to work on the future of machine learning research, read research papers and implement state of the art algorithms - you\u2019re ready...so apply! \nCurious? Learn more about our \nresearch\n and \nengineering\n teams.\nKate: \nWe recruit for many roles across the organisation so the qualities we focus on differ accordingly.\nThe majority of research scientist hires we make are post PhD level, so we don\u2019t over index on publications. We also don\u2019t have a specific marker for degree achievement or GPA. When it comes to experience, we\u2019re always interested in reading about a candidate's past internships and/or voluntary industry experiences. We look for proven ability not only in \u2018research\u2019 but also in implementation, engineering and application. Reading about side projects and open source contributions are also great to see when looking at potential candidates, so feel free to link your Github, side projects or code.\nFor research engineers, it\u2019s important to remember that the role is part research and part engineer, so we\u2019re always looking for people that enjoy putting theory into computational form.\nFor software engineers, we look for the clear ability to communicate problems and solutions. Software engineers at DeepMind regularly deal with ambiguous problems which also have underlying engineering complexities. Evidence of working on similar projects, or experiences in accelerating research and harnessing tools to augment research, is key.\nKate: \nCreating the perfect CV or resume is a big job. Luckily there are a countless number of resources out there that can help you get the job done. To keep it simple, we\u2019d suggest focusing on the following points:\nFeryal: \nThere are a wide range of resources available to help you learn and develop your skills in machine learning. These include open-access introductory courses on YouTube (i.e. \nNando de Freitas\u2019s course on Deep Learning\n, \nDavid Silver\u2019s course on Reinforcement Learning\n and \nDeepMind x UCL Lecture Series\n), blog posts which provide overviews of particular techniques (e.g. \nDistill\n) and more advanced machine learning conference proceedings such as \nNeurIPS\n, \nICML\n and \nICLR\n. \nThere are also a number of summer schools (i.e. \nMLSS\n and \nDLRLSS\n) that help support students and professionals who are interested in learning from leading experts in the field. Many of the summer schools also host videos and practical exercises from previous years which can act as excellent resources for learning at your own pace.\nIt\u2019s also great to look to organisations like \nWomen in Machine Learning (WiML)\n that specifically help women in the field build their technical confidence and voice while amplifying their achievements to the wider community.\nFeryal: \nThe interview process at DeepMind can vary depending on the particular role you\u2019re applying for. From my experience, the interview process for a \nResearch Scientist\n role consisted of four phases: \nThis is to cover your background, experience, the motivation for applying and future plans. At this stage, you will also have the opportunity to ask any questions that you may have about the role or the interview process.\nThis part of the process involves several sessions - including one with a technical quiz \u00a0that covers a large breadth of topics in computer science, statistics, mathematics and machine learning. It\u2019s key that you revise broadly for this session! At this stage there will also be a coding interview where you [in your chosen language] will have to work through a few questions and a specific problem with the end goal of coming to a solution implementation.\nThis stage is made up of various short [i.e. ~30min] interviews with researchers and leads about your specific research background and interests. Here you will have the opportunity to give a talk about your research, which gives the interviewers a better idea of your overall research direction. At this point, try to show your technical understanding of the field and feel free to bring up your own achievements and research ideas. It\u2019s not necessary, but I would also suggest reading through \nrecent papers\n published by the DeepMind team to try to frame your strengths better! \nTowards the end of the interview process, you will once again connect with the recruitment team to discuss DeepMind\u2019s culture and mission. I recommend that you read about DeepMind\u2019s \nmission\n and think about how your career goals can fit within it.\nMihaela: \nDue to the versatility required to do machine learning research, the interview process has a relatively even split between coding and assessing research skills. The first stage focuses on mathematics, statistics, machine learning and computer science knowledge, while later stages focus on coding. Keep in mind that throughout the interview process, the interviewer is trying to assess your problem solving skills, so focus on communication and explain your answers. \nFor my own interview, I prepared by reviewing some of the notes from my university lectures - including a statistics course I had taken. At the time I didn\u2019t know a lot about reinforcement learning, so I did some additional research and watched David Silver\u2019s \nUCL course\n on the topic. For my coding interview, I chose python. To prepare and to practice my speed I solved a few coding questions without using an integrated development environment (IDE) or my favourite editor - only a simple text editor.\nMihaela: \nAbsolutely! Research Engineers at DeepMind - and elsewhere - often lead projects of all sizes. They can lead as first authors of conference papers, or as larger team efforts which involve groups of different sizes and take place over multiple months. \nThere are plenty of examples, but here are a few: \nAlphaZero\n, \nimproving exploration in reinforcement learning using generative modeling\n, and open sourcing of core libraries such as \nReverb\n.\nFeryal: \nBeing a research scientist means that my day never really looks the same. My time is often spent thinking about my research projects, coding, meeting and discussing ideas with others, reading papers and attending presentations or reading groups. \nAs always in research, what I\u2019m doing can change depending on if I\u2019m working towards a paper deadline, working on a specific project, or thinking about what to do next. Luckily DeepMind is really flexible in how one can organise their time and schedule. We use a \u201cmilestone system\u201d which organises research into smaller, measurable chunks (e.g. 3-6 weeks) so this really helps with planning research and breaking it down into concrete steps.\nFind out more about \nWomen in Machine Learning\n (WiML) and \nInternational Women in Engineering Day\n. \nRead more about\n careers at DeepMind\n.\nExplore our\n open roles\n.\nWatch more on our \nYouTube channel\n.\n"}
{"title": "Traffic prediction with advanced Graph Neural Networks", "contents": "By partnering with Google, DeepMind is able to bring the benefits of AI to billions of people all over the world. \u00a0From reuniting a speech-impaired user with his \noriginal voice\n, to helping users discover \npersonalised apps\n, we can apply breakthrough research to immediate real-world problems at a Google scale. Today we\u2019re delighted to share the results of our latest partnership, delivering a truly global impact for the more than one billion people that use Google Maps. \nPeople rely on Google Maps for accurate traffic predictions and estimated times of arrival (ETAs). These are critical tools that are especially useful when you need to be routed around a traffic jam, if you need to notify friends and family that you\u2019re running late, or if you need to leave in time to attend an important meeting. These features are also useful for businesses such as rideshare companies, which use Google Maps Platform to power their services with information about pickup and dropoff times, along with estimated prices based on trip duration. \nResearchers at DeepMind\n have partnered with the Google Maps team to improve the accuracy of real time ETAs by up to 50% in places like Berlin, Jakarta, S\u00e3o Paulo, Sydney, Tokyo, and Washington D.C. by using advanced machine learning techniques including Graph Neural Networks, as the graphic below shows:\nTo calculate ETAs, Google Maps analyses live traffic data for road segments around the world. While this data gives Google Maps an accurate picture of \ncurrent \ntraffic, it doesn\u2019t account for the traffic a driver can expect to see 10, 20, or even 50 minutes into their drive. To accurately predict\n future\n traffic, Google Maps uses machine learning to combine live traffic conditions with historical traffic patterns for roads worldwide. This process is complex for a number of reasons. For example - even though rush-hour inevitably happens every morning and evening, the exact time of rush hour can vary significantly from day to day and month to month. Additional factors like road quality, speed limits, accidents, and closures can also add to the complexity of the prediction model. \nDeepMind partnered with Google Maps to help improve the accuracy of their ETAs around the world. While Google Maps\u2019 predictive ETAs have been consistently accurate for over 97% of trips, we worked with the team to minimise the remaining inaccuracies even further - sometimes by more than 50% in cities like Taichung. To do this at a global scale, we used a generalised machine learning architecture called Graph Neural Networks that allows us to conduct spatiotemporal reasoning by incorporating relational learning biases to model the connectivity structure of real-world road networks. Here\u2019s how it works: \nWe divided road networks into \u201cSupersegments\u201d consisting of multiple adjacent segments of road that share significant traffic volume. Currently, the Google Maps traffic prediction system consists of the following components: (1) a route analyser that processes terabytes of traffic information to construct Supersegments and (2) a novel Graph Neural Network model, which is optimised with multiple objectives and predicts the travel time for each Supersegment.\nThe biggest challenge to solve when creating a machine learning system to estimate travel times using Supersegments is an architectural one. How do we represent dynamically sized examples of connected segments with arbitrary accuracy in such a way that a single model can achieve success?\nOur initial proof of concept began with a straight-forward approach that used the existing traffic system as much as possible, specifically the existing segmentation of road-networks and the associated real-time data pipeline. This meant that a Supersegment covered a set of road segments, where each segment has a specific length and corresponding speed features. At first we trained a single fully connected neural network model for every Supersegment. These initial results were promising, and demonstrated the potential in using neural networks for predicting travel time. However, given the dynamic sizes of the Supersegments, we required a separately trained neural network model for each one. To deploy this at scale, we would have to train millions of these models, which would have posed a considerable infrastructure challenge. This led us to look into models that could handle variable length sequences, such as Recurrent Neural Networks (RNNs). However, incorporating further structure from the road network proved difficult. Instead, we decided to use Graph Neural Networks. In modeling traffic, we\u2019re interested in how cars flow through a network of roads, and Graph Neural Networks can model network dynamics and information propagation.\nOur model treats the local road network as a graph, where each route segment corresponds to a node and edges exist between segments that are consecutive on the same road or connected through an intersection. In a Graph Neural Network, a message passing algorithm is executed where the messages and their effect on edge and node states are learned by neural networks. From this viewpoint, our Supersegments are road subgraphs, which were sampled at random in proportion to traffic density. A single model can therefore be trained using these sampled subgraphs, and can be deployed at scale.\nGraph Neural Networks extend the learning bias imposed by Convolutional Neural Networks and Recurrent Neural Networks by generalising the concept of \u201cproximity\u201d, allowing us to have arbitrarily complex connections to handle not only traffic ahead or behind us, but also along adjacent and intersecting roads. In a Graph Neural Network, adjacent nodes pass messages to each other. By keeping this structure, we impose a locality bias where nodes will find it easier to rely on adjacent nodes (this only requires one message passing step). These mechanisms allow Graph Neural Networks to capitalise on the connectivity structure of the road network more effectively. Our experiments have demonstrated gains in predictive power from expanding to include adjacent roads that are not part of the main road. For example, think of how a jam on a side street can spill over to affect traffic on a larger road. By spanning multiple intersections, the model gains the ability to natively predict delays at turns, delays due to merging, and the overall traversal time in stop-and-go traffic. This ability of Graph Neural Networks to generalise over combinatorial spaces is what grants our modeling technique its power. Each Supersegment, which can be of varying length and of varying complexity - from simple two-segment routes to longer routes containing hundreds of nodes - can nonetheless be processed by the \nsame\n Graph Neural Network model. \nA big challenge for a production machine learning system that is often overlooked in the academic setting involves the large variability that can exist across multiple training runs of the same model. While small differences in quality can simply be discarded as poor initialisations in more academic settings, these small inconsistencies can have a large impact when added together across millions of users. As such, making our Graph Neural Network robust to this variability in training took center stage as we pushed the model into production. We discovered that Graph Neural Networks are particularly sensitive to changes in the training curriculum - the primary cause of this instability being the large variability in graph structures used during training. A single batch of graphs could contain anywhere from small two-node graphs to large 100+ nodes graphs.\nAfter much trial and error, however, we developed an approach to solve this problem by adapting a novel reinforcement learning technique for use in a supervised setting.\nIn training a machine learning system, the learning rate of a system specifies how \u2018plastic\u2019 \u2013 or changeable to new information \u2013 it is. Researchers often reduce the learning rate of their models over time, as there is a tradeoff between learning new things, and forgetting important features already learned\u2013not unlike the progression from childhood to adulthood. We initially made use of an exponentially decaying learning rate schedule to stabilise our parameters after a pre-defined period of training. We also explored and analysed model ensembling techniques which have proven effective in previous work to see if we could reduce model variance between training runs.\nIn the end, the most successful approach to this problem was using \nMetaGradients\n to dynamically adapt the learning rate during training - effectively letting the system learn its own optimal learning rate schedule. By automatically adapting the learning rate while training, our model not only achieved higher quality than before, it also learned to decrease the learning rate automatically. This led to more stable results, enabling us to use our novel architecture in production. \nWhile the ultimate goal of our modeling system is to reduce errors in travel estimates, we found that making use of a linear combination of multiple loss functions (weighted appropriately) greatly increased the ability of the model to generalise. Specifically, we formulated a multi-loss objective making use of a regularising factor on the model weights, L_2 and L_1 losses on the global traversal times, as well as individual Huber and negative-log likelihood (NLL) losses for each node in the graph. By combining these losses we were able to guide our model and avoid overfitting on the training dataset. While our measurements of quality in training did not change, improvements seen during training translated more directly to held-out tests sets and to our end-to-end experiments.\nCurrently we are exploring whether the MetaGradient technique can also be used to vary the composition of the multi-component loss-function during training, using the reduction in travel estimate errors as a guiding metric. This work is inspired by the MetaGradient efforts that have found success in reinforcement learning, and early experiments show promising results.\nThanks to our close and fruitful collaboration with the Google Maps team, we were able to apply these novel and newly developed techniques at scale. Together, we were able to overcome both research challenges as well as production and scalability problems. In the end, the final model and techniques led to a successful launch, improving the accuracy of ETAs on Google Maps and Google Maps Platform APIs around the world.\nWorking at Google scale with cutting-edge research represents a unique set of challenges. If you\u2019re interested in applying cutting edge techniques such as Graph Neural Networks to address real-world problems, learn more about the team working on these problems \nhere\n.\nIn collaboration with: Marc Nunkesser, Seongjae Lee, Xueying Guo, Austin Derrow-Pinion, David Wong, Peter Battaglia, Todd Hester, Petar Veli\u010dkovi\u0107\u200e, Vishal Gupta, Ang Li, Zhongwen Xu, Geoff Hulten, Jeffrey Hightower, Luis C. Cobo, Praveen Srinivasan & Harish Chandran.\nFigures by Paulo Estriga & Adam Cain.\n"}
{"title": "Specification gaming: the flip side of AI ingenuity", "contents": "Specification gaming\n is a behaviour that satisfies the literal specification of an objective without achieving the intended outcome. We have all had experiences with specification gaming, even if not by this name. Readers may have heard the myth of \nKing Midas\n and the golden touch, in which the king asks that anything he touches be turned to gold - but soon finds that even food and drink turn to metal in his hands. In the real world, when rewarded for doing well on a homework assignment, a student might copy another student to get the right answers, rather than learning the material - and thus exploit a loophole in the task specification. \nThis problem also arises in the design of artificial agents. For example, a reinforcement learning agent can find a shortcut to getting lots of reward without completing the task as intended by the human designer. These behaviours are common, and we have \ncollected\n around 60 examples so far (aggregating \nexisting\n \nlists\n and ongoing \ncontributions\n from the AI community). In this post, we review possible causes for specification gaming, share examples of where this happens in practice, and argue for further work on principled approaches to overcoming specification problems.\nLet's look at an example. In a \nLego stacking task\n, the desired outcome was for a red block to end up on top of a blue block. The agent was rewarded for the height of the bottom face of the red block when it is not touching the block. Instead of performing the relatively difficult maneuver of picking up the red block and placing it on top of the blue one, the agent simply flipped over the red block to collect the reward. This behaviour achieved the stated objective (high bottom face of the red block) at the expense of what the designer actually cares about (stacking it on top of the blue one).\nWe can consider specification gaming from two different perspectives. Within the scope of developing reinforcement learning (RL) algorithms, the goal is to build agents that learn to achieve the given objective. For example, when we use Atari games as a benchmark for training RL algorithms, the goal is to evaluate whether our algorithms can solve difficult tasks. Whether or not the agent solves the task by exploiting a loophole is unimportant in this context. From this perspective, specification gaming is a good sign - the agent has found a novel way to achieve the specified objective. These behaviours demonstrate the ingenuity and power of algorithms to find ways to do exactly what we tell them to do.\nHowever, when we want an agent to actually stack Lego blocks, the same ingenuity can pose an issue. Within the broader scope of building \naligned agents\n that achieve the intended outcome in the world, specification gaming is problematic, as it involves the agent exploiting a loophole in the specification at the expense of the intended outcome. These behaviours are caused by misspecification of the intended task, rather than any flaw in the RL algorithm. In addition to algorithm design, another necessary component of building aligned agents is reward design.\nDesigning task specifications (reward functions, environments, etc.) that accurately reflect the intent of the human designer tends to be difficult. Even for a slight misspecification, a very good RL algorithm might be able to find an intricate solution that is quite different from the intended solution, even if a poorer algorithm would not be able to find this solution and thus yield solutions that are closer to the intended outcome. This means that correctly specifying intent can become more important for achieving the desired outcome as RL algorithms improve. It will therefore be essential that the ability of researchers to correctly specify tasks keeps up with the ability of agents to find novel solutions.\nWe use the term \ntask specification\n in a broad sense to encompass many aspects of the agent development process. In an RL setup, task specification includes not only reward design, but also the choice of training environment and auxiliary rewards. The correctness of the task specification can determine whether the ingenuity of the agent is or is not in line with the intended outcome. If the specification is right, the agent's creativity produces a desirable novel solution. This is what allowed AlphaGo to play the famous \nMove 37\n, which took human Go experts by surprise yet which was pivotal in its second match with Lee Sedol. If the specification is wrong, it can produce undesirable gaming behaviour, like flipping the block. These types of solutions lie on a spectrum, and we don't have an objective way to distinguish between them.\nWe will now consider possible causes of specification gaming. One source of reward function misspecification is poorly designed \nreward shaping\n. Reward shaping makes it easier to learn some objectives by giving the agent some rewards on the way to solving a task, instead of only rewarding the final outcome. However, shaping rewards can change the optimal policy if they are not \npotential-based\n. Consider an agent controlling a boat in the \nCoast Runners game\n, where the intended goal was to finish the boat race as quickly as possible. The agent was given a shaping reward for hitting green blocks along the race track, which changed the optimal policy to going in circles and hitting the same green blocks over and over again.\nSpecifying a reward that accurately captures the\n desired final outcome\n can be challenging in its own right. In the Lego stacking task, it is not sufficient to specify that the bottom face of the red block has to be high off the floor, since the agent can simply flip the red block to achieve this goal. A more comprehensive specification of the desired outcome would also include that the top face of the red block has to be above the bottom face, and that the bottom face is aligned with the top face of the blue block. It is easy to miss one of these criteria when specifying the outcome, thus making the specification too broad and potentially easier to satisfy with a degenerate solution. \nInstead of trying to create a specification that covers every possible corner case, we could \nlearn the reward function from human feedback\n. It is often easier to evaluate whether an outcome has been achieved than to specify it explicitly. However, this approach can also encounter specification gaming issues if the reward model does not learn the true reward function that reflects the designer's preferences. One possible source of inaccuracies can be the human feedback used to train the reward model. For example, an agent performing a \ngrasping task\n learned to fool the human evaluator by hovering between the camera and the object.\nThe learned reward model could also be misspecified for other reasons, such as poor generalisation. Additional feedback can be used to correct the agent's attempts to exploit the inaccuracies in the reward model.\nAnother class of specification gaming examples comes from the agent exploiting \nsimulator bugs\n. For example, a \nsimulated robot\n that was supposed to learn to walk figured out how to hook its legs together and slide along the ground.\nAt first sight, these kinds of examples may seem amusing but less interesting, and irrelevant to deploying agents in the real world, where there are no simulator bugs. However, the underlying problem isn\u2019t the bug itself but a failure of abstraction that can be exploited by the agent. In the example above, the robot's task was misspecified because of incorrect assumptions about simulator physics. Analogously, a real-world traffic optimisation task might be misspecified by incorrectly assuming that the traffic routing infrastructure does not have software bugs or security vulnerabilities that a sufficiently clever agent could discover. Such assumptions need not be made explicitly \u2013 more likely, they are details that simply never occurred to the designer. And, as tasks grow too complex to consider every detail, researchers are more likely to introduce incorrect assumptions during specification design. This poses the question: is it possible to design agent architectures that correct for such false assumptions instead of gaming them?\nOne assumption commonly made in task specification is that the task specification cannot be affected by the agent's actions. This is true for an agent running in a sandboxed simulator, but not for an agent acting in the real world. Any task specification has a physical manifestation: a reward function stored on a computer, or preferences stored in the head of a human. An agent deployed in the real world can potentially manipulate these representations of the objective, creating a \nreward tampering\n problem. For our hypothetical traffic optimisation system, there is no clear distinction between satisfying the user's preferences (e.g. by giving useful directions), and \ninfluencing users\n to have preferences that are easier to satisfy (e.g. by nudging them to choose destinations that are easier to reach). The former satisfies the objective, while the latter manipulates the representation of the objective in the world (the user preferences), and both result in high reward for the AI system. As another, more extreme example, a very advanced AI system could hijack the computer on which it runs, manually setting its reward signal to a high value.\nTo sum up, there are at least three challenges to overcome in solving specification gaming:\nWhile many approaches have been proposed, ranging from reward modeling to agent incentive design, specification gaming is far from solved. \nThe list of specification gaming behaviours\n demonstrates the magnitude of the problem and the sheer number of ways the agent can game an objective specification. These problems are likely to become more challenging in the future, as AI systems become more capable at satisfying the task specification at the expense of the intended outcome. As we build more advanced agents, we will need design principles aimed specifically at overcoming specification problems and ensuring that these agents robustly pursue the outcomes intended by the designers. \nWe would like to thank Hado van Hasselt and Csaba Szepesvari for their feedback on this post.\nCustom figures by Paulo Estriga, Aleks Polozuns, and Adam Cain.\n"}
{"title": "AlphaFold: a solution to a 50-year-old grand challenge in biology", "contents": "In July 2022, we released AlphaFold protein structure predictions for nearly all catalogued proteins known to science. Read the latest blog \nhere\n.\nProteins are essential to life, supporting practically all its functions. They are large complex molecules, made up of chains of amino acids, and \nwhat a protein does largely depends on its unique 3D structure\n. Figuring out what shapes proteins fold into is known as the \n\u201cprotein folding problem\u201d\n, and has stood as a grand challenge in biology for the past 50 years. In a major scientific advance, the latest version of our AI system \nAlphaFold\n has been recognised as a solution to this grand challenge by the organisers of the biennial Critical Assessment of protein Structure Prediction (\nCASP\n). This breakthrough demonstrates the impact AI can have on scientific discovery and its potential to dramatically accelerate progress in some of the most fundamental fields that explain and shape our world.\nA protein\u2019s shape is closely linked with its function, and the ability to predict this structure unlocks a greater understanding of what it does and how it works. Many of the world\u2019s greatest challenges, like developing treatments for diseases or finding enzymes that break down industrial waste, are fundamentally tied to proteins and the role they play.\nThis has been a focus of intensive scientific research for many years, using a variety of experimental techniques to examine and determine protein structures, such as nuclear magnetic resonance and X-ray crystallography. These techniques, as well as newer methods like cryo-electron microscopy, depend on extensive trial and error, which can take years of painstaking and laborious work per structure, and require the use of multi-million dollar specialised equipment.\nIn his acceptance speech for the 1972 Nobel Prize in Chemistry, Christian Anfinsen \nfamously postulated\n that, in theory, a protein\u2019s \namino acid sequence\n should fully determine its structure. This hypothesis sparked a five decade quest to be able to computationally predict a protein\u2019s 3D structure based solely on its 1D amino acid sequence as a complementary alternative to these expensive and time consuming experimental methods. A major challenge, however, is that the number of ways a protein could theoretically fold before settling into its final 3D structure is astronomical. In 1969 Cyrus Levinthal noted that it would take longer than the age of the known universe to enumerate all possible configurations of a typical protein by brute force calculation \u2013 Levinthal estimated \n10^300 possible conformations\n for a typical protein. Yet in nature, proteins fold spontaneously, some within milliseconds \u2013 a dichotomy sometimes referred to as \nLevinthal\u2019s paradox\n.\nIn 1994, \nProfessor John Moult and Professor Krzysztof Fidelis founded CASP\n as a biennial blind assessment to catalyse research, monitor progress, and establish the state of the art in protein structure prediction. It is both the gold standard for assessing predictive techniques and a unique global community built on shared endeavour. Crucially, CASP chooses protein structures that have only very recently been experimentally determined (some were still awaiting determination at the time of the assessment) to be targets for teams to test their structure prediction methods against; they are not published in advance. Participants must blindly predict the structure of the proteins, and these predictions are subsequently compared to the ground truth experimental data when they become available. We\u2019re indebted to CASP\u2019s organisers and the whole community, not least the experimentalists whose structures enable this kind of rigorous assessment.\nThe main metric used by CASP to measure the accuracy of predictions is the \nGlobal Distance Test (GDT)\n which ranges from 0-100. In simple terms, GDT can be approximately thought of as the percentage of amino acid residues (beads in the protein chain) within a threshold distance from the correct position. According to \nProfessor Moult\n, a score of around 90 GDT is informally considered to be competitive with results obtained from experimental methods.\nIn \nthe results\n from the 14th CASP assessment, released today, our latest AlphaFold system achieves a median score of 92.4 GDT overall across all targets. This means that our predictions have an average error (\nRMSD\n) of approximately 1.6 \nAngstroms\n, which is comparable to the width of an atom (or 0.1 of a nanometer). Even for the very hardest protein targets, those in the most challenging \nfree-modelling category\n, AlphaFold achieves a median score of 87.0 GDT (\ndata available here\n).\nThese exciting results open up the potential for biologists to use computational structure prediction as a core tool in scientific research. Our methods may prove especially helpful for important classes of proteins, such as \nmembrane proteins\n, that are very difficult to crystallise and therefore challenging to experimentally determine.\nWe first entered \nCASP13\n in 2018 with our \ninitial version of AlphaFold\n, which achieved the highest accuracy among participants. Afterwards, we \npublished\n a paper on our CASP13 methods in Nature with associated \ncode\n, which has gone on to inspire \nother work\n and community-developed open source \nimplementations\n. Now, new deep learning architectures we\u2019ve developed have driven changes in our methods for CASP14, enabling us to achieve unparalleled levels of accuracy. These methods draw inspiration from the fields of biology, physics, and machine learning, as well as of course the work of many scientists in the protein-folding field over the past half-century.\nA folded protein can be thought of as a \u201cspatial graph\u201d, where residues are the nodes and edges connect the residues in close proximity. This graph is important for understanding the physical interactions within proteins, as well as their evolutionary history. For the latest version of AlphaFold, used at CASP14, we created an attention-based neural network system, trained end-to-end, that attempts to interpret the structure of this graph, while reasoning over the implicit graph that it\u2019s building. It uses evolutionarily related sequences, multiple sequence alignment (MSA), and a representation of amino acid residue pairs to refine this graph.\nBy iterating this process, the system develops strong predictions of the underlying physical structure of the protein and is able to determine highly-accurate structures in a matter of days. Additionally, AlphaFold can predict which parts of each predicted protein structure are reliable using an internal confidence measure.\nWe trained this system on publicly available data consisting of ~170,000 protein structures from the \nprotein data bank\n together with \nlarge databases\n containing protein sequences of unknown structure. It uses approximately 16 \nTPUv3s\n (which is 128 TPUv3 cores or roughly equivalent to ~100-200 GPUs) run over a few weeks, a relatively modest amount of compute in the context of most large state-of-the-art models used in machine learning today. As with our CASP13 AlphaFold system, we are preparing a paper on our system to submit to a peer-reviewed journal in due course.\nWhen DeepMind started a decade ago, we hoped that one day AI breakthroughs would help serve as a platform to advance our understanding of fundamental scientific problems. Now, after 4 years of effort building AlphaFold, we\u2019re starting to see that vision realised, with implications for areas like drug design and environmental sustainability.\nProfessor Andrei Lupas, Director of the Max Planck Institute for Developmental Biology and a CASP assessor, let us know that, \u201cAlphaFold\u2019s astonishingly accurate models have allowed us to solve a protein structure we were stuck on for close to a decade, relaunching our effort to understand how signals are transmitted across cell membranes.\u201d\nWe\u2019re optimistic about the impact AlphaFold can have on biological research and the wider world, and excited to collaborate with others to learn more about its potential in the years ahead. Alongside working on a peer-reviewed paper, we\u2019re exploring how best to provide broader access to the system in a scalable way.\nIn the meantime, we\u2019re also looking into how protein structure predictions could contribute to our understanding of specific diseases with a small number of specialist groups, for example by helping to identify proteins that have malfunctioned and to reason about how they interact. These insights could enable more precise work on drug development, complementing existing experimental methods to find promising treatments faster.\nWe\u2019ve also seen signs that protein structure prediction could be useful in future pandemic response efforts, as one of many tools developed by the scientific community. Earlier this year, we \npredicted several protein structures\n of the SARS-CoV-2 virus, including ORF3a, whose structures were previously unknown. At CASP14, we predicted the structure of another coronavirus protein, \nORF8\n. Impressively quick work by experimentalists has now confirmed the structures of both \nORF3a\n and \nORF8\n. Despite their challenging nature and having very few related sequences, we achieved a high degree of accuracy on both of our predictions when compared to their experimentally determined structures.\nAs well as accelerating understanding of known diseases, we\u2019re excited about the potential for these techniques to explore the hundreds of millions of proteins we don\u2019t currently have models for \u2013 a vast terrain of unknown biology. Since \nDNA specifies the amino acid sequences\n that comprise protein structures, the \ngenomics revolution\n has made it possible to read protein sequences from the natural world at massive scale \u2013 with 180 million protein sequences and counting in the Universal Protein database (\nUniProt\n). In contrast, given the experimental work needed to go from sequence to structure, only around 170,000 protein structures are in the Protein Data Bank (\nPDB\n). Among the undetermined proteins may be some with new and exciting functions and \u2013 just as a telescope helps us see deeper into the unknown universe \u2013 techniques like AlphaFold may help us find them.\nAlphaFold is one of our most significant advances to date but, as with all scientific research, there are still many questions to answer. Not every structure we predict will be perfect. There\u2019s still much to learn, including how multiple proteins form complexes, how they interact with \nDNA\n, \nRNA\n, or \nsmall molecules\n, and how we can determine the precise location of all amino acid side chains. In collaboration with others, there\u2019s also much to learn about how best to use these scientific discoveries in the development of new medicines, ways to manage the environment, and more.\nFor all of us working on computational and machine learning methods in science, systems like AlphaFold demonstrate the stunning potential for AI as a tool to aid fundamental discovery. Just as 50 years ago Anfinsen laid out a challenge far beyond science\u2019s reach at the time, there are many aspects of our universe that remain unknown. The progress announced today gives us further confidence that AI will become one of humanity\u2019s most useful tools in expanding the frontiers of scientific knowledge, and we\u2019re looking forward to the many years of hard work and discovery ahead!\nHigh Accuracy Protein Structure Prediction Using Deep Learning\nJohn Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Kathryn Tunyasuvunakool, Olaf Ronneberger, Russ Bates, Augustin \u017d\u00eddek, Alex Bridgland, Clemens Meyer, Simon A A Kohl, Anna Potapenko, Andrew J Ballard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen, David Reiman, Martin Steinegger, Michalina Pacholska, David Silver, Oriol Vinyals, Andrew W Senior, Koray Kavukcuoglu, Pushmeet Kohli, Demis Hassabis.\nIn Fourteenth Critical Assessment of Techniques for Protein Structure Prediction (Abstract Book), 30 November - 4 December 2020. Retrieved from \nhere\n.\nWe\u2019re right at the beginning of exploring how best to enable other groups to use our structure predictions, alongside preparing a peer-reviewed paper for publication. While our team won\u2019t be able to respond to every enquiry, if AlphaFold may be relevant to your work, please submit a few lines about it to \nalphafold@deepmind.com\n. We\u2019ll be in contact if there\u2019s scope for further exploration. \n"}
{"title": "Game theory as an engine for large-scale data analysis", "contents": "EigenGame maps out a new approach to solve fundamental ML problems.\nModern AI systems approach tasks like \nrecognising objects in images\n and \npredicting the 3D structure of proteins\n as a diligent student would prepare for an exam. By training on many example problems, they minimise their mistakes over time until they achieve success. But this is a solitary endeavour and only one of the known forms of learning. Learning also takes place by interacting and playing with others. It\u2019s rare that a single individual can solve extremely complex problems alone. By allowing problem solving to take on these game-like qualities, previous DeepMind efforts have trained AI agents to play \nCapture the Flag\n and achieve \nGrandmaster level at Starcraft\n. This made us wonder if such a perspective modeled on game theory could help solve other fundamental machine learning problems.\nToday at \nICLR 2021\n (the International Conference on Learning Representations), we presented \u201c\nEigenGame: PCA as a Nash Equilibrium\n,\u201d which received an Outstanding Paper Award. Our research explored a new approach to an old problem: we reformulated principal component analysis (PCA), a type of \neigenvalue problem\n, as a competitive multi-agent game we call EigenGame. PCA is typically formulated as an optimisation problem (or single-agent problem); however, we found that the multi-agent perspective allowed us to develop new insights and algorithms which make use of the latest computational resources. This enabled us to scale to massive data sets that previously would have been too computationally demanding, and offers an alternative approach for future exploration.\nFirst described in the early 1900s, \nPCA\n is a long-standing technique for making sense of the structure of high-dimensional data. This approach is now ubiquitous as a first step in the data-processing pipeline and makes it easy to cluster and visualise data. It can also be a useful tool for learning low-dimensional representations for regression and classification. More than a century later, there are still compelling reasons to study PCA.\nFirstly, data was originally recorded by hand in paper notebooks, and now it is stored in data centres the size of warehouses. As a result, this familiar analysis has become a computational bottleneck. Researchers have explored \nrandomised algorithms\n and other directions to improve how PCA scales, but we found that these approaches have difficulty scaling to massive datasets because they are unable to fully harness recent deep-learning-centric advances in computation \u2014 namely access to many parallel GPUs or TPUs.\nSecondly, PCA shares a common solution with many important ML and engineering problems, namely the \nsingular value decomposition\n (SVD). By approaching the PCA problem in the right way, our insights and algorithms apply more broadly across the branches of the ML tree.\nAs with any board game, in order to reinvent PCA as a game we need a set of rules and objectives for players to follow. There are many possible ways to design such a game; however, important ideas come from PCA itself: the optimal solution consists of eigenvectors which capture the important variance in the data and are orthogonal to each other.\nIn EigenGame each player controls an eigenvector. Players increase their score by explaining variance within the data but are penalised if they\u2019re too closely aligned to other players. We also establish a hierarchy: Player 1 only cares about maximising variance, whereas other players also have to worry about minimising their alignment with players above them in the hierarchy. This combination of rewards and penalties defines each player\u2019s utility.\nWith appropriately designed\n Var \nand \nAlign\n terms, we can show that:\nThis independence property of simultaneous ascent is particularly important because it allows for the computation to be distributed across dozens of Google Cloud TPUs, enabling both data- and model-parallelism. This makes it possible for our algorithm to adapt to truly large-scale data. EigenGame finds the principal components in a matter of hours for hundred-terabyte datasets comprising millions of features or billions of rows.\nBy thinking about PCA from a multi-agent perspective, we were able to propose scalable algorithms and novel analyses. We also uncovered a surprising connection to \nHebbian Learning\n \u2014 or, how neurons adapt when learning. In EigenGame, each player maximising their utilities gives rise to update equations that are similar to \nupdate rules\n derived from Hebbian models of synaptic plasticity in the brain. Hebbian updates are known to converge to the PCA solution but are not derived as the gradient of any utility function. Game theory gives us a fresh lens to view Hebbian learning, and also suggests a continuum of approaches to machine learning problems.\nOn one end of the ML continuum is the well-developed path of proposing an objective function that can be optimised: Using the theory of convex and non-convex optimisation, researchers can reason about the global properties of the solution. On the other end, pure \nconnectionist\n methods and update rules inspired by neuroscience are specified directly, but analysis of the entire system can be more difficult, often invoking the study of complicated \ndynamical systems\n.\nGame theoretic approaches like EigenGame sit somewhere in between. Player updates are not constrained to be the gradient of a function, only a best response to the current strategies of the other players. We\u2019re free to design utilities and updates with desirable properties \u2014 for example, specifying updates which are unbiased or accelerated \u2014 while ensuring the Nash property still allows us to analyse the system as a whole.\nEigenGame represents a concrete example of designing the solution to a machine learning problem as the output of a large multi-agent system. More generally, designing machine learning problems as multi-agent games is a challenging \nmechanism design\n problem; however, researchers have already used the class of two-player, \nzero-sum\n games to solve machine learning problems. Most notably, the success of \ngenerative adversarial networks\n (GANs) as an approach to generative modelling has driven interest in the relationship between game theory and machine learning.\nEigenGame moves beyond this to the more complex many-player, general-sum setting. This enables more obvious parallelism for greater scale and speed. It also presents a quantitative benchmark for the community to test novel multi-agent algorithms alongside richer domains, such as \nDiplomacy\n and \nSoccer\n.\nWe hope our blueprint for designing utilities and updates will encourage others to explore this direction for designing new algorithms, agents, and systems. We\u2019re looking forward to seeing what other problems can be formulated as games and whether the insights we glean will further improve our understanding of the multi-agent nature of intelligence.\nFor more details see our paper \nEigenGame: PCA as a Nash Equilibrium\n and our follow-up work \nEigenGame Unloaded: When playing games is better than optimising\n. This blog post is based on joint work with Thore Graepel, a research group lead at DeepMind and Chair of Machine Learning at University College London.\nWe would like to thank Rob Fergus for their technical feedback on this post as well as Sean Carlson, Jon Fildes, Dominic Barlow, Mario Pinto, and Emma Yousif for pulling this all together.\nCustom figures by Jim Kynvin and Adam Cain.\n"}
{"title": "Using JAX to accelerate our research", "contents": "DeepMind engineers accelerate our research by building tools, scaling up algorithms, and creating challenging virtual and physical worlds for training and testing artificial intelligence (AI) systems. As part of this work, we constantly evaluate new machine learning libraries and frameworks.\nRecently, we've found that an increasing number of projects are well served by \nJAX\n, a machine learning framework developed by \nGoogle Research\n teams. JAX resonates well with our engineering philosophy and has been widely adopted by our research community over the last year. Here we share our experience of working with JAX, outline why we find it useful for our AI research, and give an overview of the ecosystem we are building to support researchers everywhere.\nJAX is a Python library designed for high-performance numerical computing, especially machine learning research. Its API for numerical functions is based on \nNumPy\n, a collection of functions used in scientific computing. Both Python and NumPy are widely used and familiar, making JAX simple, flexible, and easy to adopt.\nIn addition to its NumPy API, JAX includes an extensible system of \ncomposable function transformations\n that help support machine learning research, including:\nWe have found that JAX has enabled rapid experimentation with novel algorithms and architectures and it now underpins many of our recent publications. To learn more please consider joining our JAX Roundtable, Wednesday December 9th 7:00pm GMT, at the\n NeurIPS\n virtual conference.\nSupporting state-of-the-art AI research means balancing rapid prototyping and quick iteration with the ability to deploy experiments at a scale traditionally associated with production systems. What makes these kinds of projects particularly challenging is that the research landscape evolves rapidly and is difficult to forecast. At any point, a new research breakthrough may, and regularly does, change the trajectory and requirements of entire teams. Within this ever-changing landscape, a core responsibility of our engineering team is to make sure that the lessons learned and the code written for one research project is reused effectively in the next.\nOne approach that has proven successful is modularisation: we extract the most important and critical building blocks developed in each research project into well tested and efficient \ncomponents\n. This empowers researchers to focus on their research while also benefiting from code reuse, bug fixes and performance improvements in the algorithmic ingredients implemented by our core libraries. We\u2019ve also found that it\u2019s important to make sure that each library has a clearly defined scope and to ensure that they\u2019re interoperable but independent. \nIncremental buy-in\n, the ability to pick and choose features without being locked into others, is critical to providing maximum flexibility for researchers and always supporting them in choosing the right tool for the job.\nOther considerations that have gone into the development of our JAX Ecosystem include making sure that it remains consistent (where possible) with the design of our existing \nTensorFlow\n libraries (e.g. \nSonnet\n and \nTRFL\n). We\u2019ve also aimed to build components that (where relevant) match their underlying mathematics as closely as possible, to be self-descriptive and minimise mental hops \"from paper to code\". Finally, we\u2019ve chosen to \nopen source\n our libraries to facilitate sharing of research outputs and to encourage the broader community to explore the JAX Ecosystem.\nThe JAX programming model of composable function transformations can make dealing with stateful objects complicated, e.g. neural networks with trainable parameters. Haiku is a neural network library that allows users to use familiar object-oriented programming models while harnessing the power and simplicity of JAX's pure functional paradigm.\nHaiku is actively used by hundreds of researchers across DeepMind and Google, and has already found adoption in several external projects (e.g. \nCoax\n, \nDeepChem\n, \nNumPyro\n). It builds on the API for \nSonnet\n, our module-based programming model for neural networks in TensorFlow, and we\u2019ve aimed to make porting from Sonnet to Haiku as simple as possible.\nFind out more on GitHub\nGradient-based optimisation is fundamental to ML. Optax provides a library of gradient transformations, together with composition operators (e.g. chain) that allow implementing many standard optimisers (e.g. RMSProp or Adam) in just a single line of code.\nThe compositional nature of Optax naturally supports recombining the same basic ingredients in custom optimisers. It additionally offers a number of utilities for stochastic gradient estimation and second order optimisation.\nMany Optax users have adopted Haiku but in line with our incremental buy-in philosophy, any library representing parameters as JAX tree structures is supported (e.g. \nElegy\n, \nFlax\n and \nStax\n). Please see \nhere\n for more information on this rich ecosystem of JAX libraries.\nFind out more on GitHub\nMany of our most successful projects are at the intersection of deep learning and reinforcement learning (RL), also known as \ndeep reinforcement learning\n. RLax is a library that provides useful building blocks for constructing RL agents.\nThe components in RLax cover a broad spectrum of algorithms and ideas: TD-learning, policy gradients, actor critics, MAP, proximal policy optimisation, non-linear value transformation, general value functions, and a number of exploration methods.\nAlthough some introductory \nexample agents\n are provided, RLax is not intended as a framework for building and deploying full RL agent systems. One example of a fully-featured agent framework that builds upon RLax components is \nAcme\n.\nFind out more on GitHub\nTesting is critical to software reliability and research code is no exception. Drawing scientific conclusions from research experiments requires being confident in the correctness of your code. Chex is a collection of testing utilities used by library authors to verify the common building blocks are correct and robust and by end-users to check their experimental code.\nChex provides an assortment of utilities including JAX-aware unit testing, assertions of properties of JAX datatypes, mocks and fakes, and multi-device test environments. Chex is used throughout DeepMind\u2019s JAX Ecosystem and by external projects such as \nCoax\n and \nMineRL\n.\nFind out more on GitHub\nGraph neural networks\n (GNNs) are an exciting area of research with many promising applications. See, for instance, our recent work on \ntraffic prediction\n in Google Maps and our work on \nphysics simulation\n. Jraph (pronounced \"giraffe\") is a lightweight library to support working with GNNs in JAX.\nJraph provides a standardised data structure for graphs, a set of utilities for working with graphs, and a 'zoo' of easily forkable and extensible graph neural network models. Other key features include: batching of GraphTuples that efficiently leverage hardware accelerators, JIT-compilation support of variable-shaped graphs via padding and masking, and losses defined over input partitions. Like Optax and our other libraries, Jraph places no constraints on the user's choice of a neural network library.\nLearn more about using the library from our rich collection of \nexamples\n.\nFind out more on GitHub\nOur JAX Ecosystem is constantly evolving and we encourage the ML research community to explore \nour libraries\n and the potential of JAX to accelerate their own research.\nCiting the DeepMind JAX Ecosystem\nIf you find the DeepMind JAX Ecosystem useful for your work, please use \nthis citation\n (hosted on GitHub).\n"}
{"title": "Generally capable agents emerge from open-ended play", "contents": "In recent years, artificial intelligence agents have succeeded in a range of complex game environments. For instance, \nAlphaZero\n beat world-champion programs in chess, shogi, and Go after starting out with knowing no more than the basic rules of how to play. Through \nreinforcement learning\n (RL), this single system learnt by playing round after round of games through a repetitive process of trial and error. But AlphaZero still trained separately on each game \u2014 unable to simply learn another game or task without repeating the RL process from scratch. The same is true for other successes of RL, such as \nAtari\n, \nCapture the Flag\n, \nStarCraft II\n, \nDota 2\n, and \nHide-and-Seek\n. DeepMind\u2019s mission of solving intelligence to advance science and humanity led us to explore how we could overcome this limitation to create AI agents with more general and adaptive behaviour. Instead of learning one game at a time, these agents would be able to react to completely new conditions and play a whole universe of games and tasks, including ones never seen before.\nToday, we published \"\nOpen-Ended Learning Leads to Generally Capable Agents\n,\" a preprint detailing our first steps to train an agent capable of playing many different games without needing human interaction data. We created a vast game environment we call XLand, which includes many multiplayer games within consistent, human-relatable 3D worlds. This environment makes it possible to formulate new learning algorithms, which dynamically control how an agent trains and the games on which it trains. The agent\u2019s capabilities improve iteratively as a response to the challenges that arise in training, with the learning process continually refining the training tasks so the agent never stops learning. The result is an agent with the ability to succeed at a wide spectrum of tasks \u2014 from simple object-finding problems to complex games like hide and seek and capture the flag, which were not encountered during training. We find the agent exhibits general, heuristic behaviours such as experimentation, behaviours that are widely applicable to many tasks rather than specialised to an individual task. This new approach marks an important step toward creating more general agents with the flexibility to adapt rapidly within constantly changing environments.\nA lack of training data \u2014 where \u201cdata\u201d points are different tasks \u2014 has been one of the major factors limiting RL-trained agents\u2019 behaviour being general enough to apply across games. Without being able to train agents on a vast enough set of tasks, agents trained with RL have been unable to adapt their learnt behaviours to new tasks. But by designing a simulated space to allow for \nprocedurally generated tasks\n, our team created a way to train on, and generate experience from, tasks that are created programmatically. This enables us to include billions of tasks in XLand, across varied games, worlds, and players.\nOur AI agents inhabit 3D first-person avatars in a multiplayer environment meant to simulate the physical world. The players sense their surroundings by observing RGB images and receive a text description of their goal, and they train on a range of games. These games are as simple as cooperative games to find objects and navigate worlds, where the goal for a player could be \u201cbe near the purple cube.\u201d More complex games can be based on choosing from multiple rewarding options, such as \u201cbe near the purple cube or put the yellow sphere on the red floor,\u201d and more competitive games include playing against co-players, such as symmetric hide and seek where each player has the goal, \u201csee the opponent and make the opponent not see me.\u201d Each game defines the rewards for the players, and each player\u2019s ultimate objective is to maximise the rewards.\nBecause XLand can be programmatically specified, the game space allows for data to be generated in an automated and algorithmic fashion. And because the tasks in XLand involve multiple players, the behaviour of co-players greatly influences the challenges faced by the AI agent. These complex, non-linear interactions create an ideal source of data to train on, since sometimes even small changes in the components of the environment can result in large changes in the challenges for the agents.\nCentral to our research is the role of \ndeep RL\n in training the neural networks of our agents. The neural network architecture we use provides an attention mechanism over the agent\u2019s internal recurrent state \u2014 helping guide the agent\u2019s attention with estimates of subgoals unique to the game the agent is playing. We\u2019ve found this goal-attentive agent (GOAT) learns more generally capable policies.\nWe also explored the question, what distribution of training tasks will produce the best possible agent, especially in such a vast environment? The dynamic task generation we use allows for continual changes to the distribution of the agent\u2019s training tasks: every task is generated to be neither too hard nor too easy, but just right for training. We then use \npopulation based training\n (PBT) to adjust the parameters of the dynamic task generation based on a fitness that aims to improve agents\u2019 general capability. And finally we chain together multiple training runs so each generation of agents can bootstrap off the previous generation.\nThis leads to a final training process with deep RL at the core updating the neural networks of agents with every step of experience:\nThe training process starts from scratch and iteratively builds complexity, constantly changing the learning problem to keep the agent learning. The iterative nature of the combined learning system, which does not optimise a bounded performance metric but rather the iteratively defined spectrum of general capability, leads to a potentially open-ended learning process for agents, limited only by the expressivity of the environment space and agent neural network.\nTo measure how agents perform within this vast universe, we create a set of evaluation tasks using games and worlds that remain separate from the data used for training. These \u201cheld-out\u201d tasks include specifically human-designed tasks like hide and seek and capture the flag.\nBecause of the size of XLand, understanding and characterising the performance of our agents can be a challenge. Each task involves different levels of complexity, different scales of achievable rewards, and different capabilities of the agent, so merely averaging the reward over held out tasks would hide the actual differences in complexity and rewards \u2014 and would effectively treat all tasks as equally interesting, which isn\u2019t necessarily true of procedurally generated environments.\nTo overcome these limitations, we take a different approach. Firstly, we normalise scores per task using the Nash equilibrium value computed using our current set of trained players. Secondly, we take into account the entire distribution of normalised scores \u2014 rather than looking at average normalised scores, we look at the different percentiles of normalised scores \u2014 as well as the percentage of tasks in which the agent scores at least one step of reward: participation. This means an agent is considered better than another agent only if it exceeds performance on all percentiles. This approach to measurement gives us a meaningful way to assess our agents\u2019 performance and robustness.\nAfter training our agents for five generations, we saw consistent improvements in learning and performance across our held-out evaluation space. Playing roughly 700,000 unique games in 4,000 unique worlds within XLand, each agent in the final generation experienced 200 billion training steps as a result of 3.4 million unique tasks. At this time, our agents have been able to participate in every procedurally generated evaluation task except for a handful that were impossible even for a human. And the results we\u2019re seeing clearly exhibit general, zero-shot behaviour across the task space \u2014 with the frontier of normalised score percentiles continually improving.\nLooking qualitatively at our agents, we often see general, heuristic behaviours emerge \u2014 rather than highly optimised, specific behaviours for individual tasks. Instead of agents knowing exactly the \u201cbest thing\u201d to do in a new situation, we see evidence of agents experimenting and changing the state of the world until they\u2019ve achieved a rewarding state. We also see agents rely on the use of other tools, including objects to occlude visibility, to create ramps, and to retrieve other objects. Because the environment is multiplayer, we can examine the progression of agent behaviours while training on held-out \nsocial dilemmas\n, such as in a game of \u201c\nchicken\n\u201d. As training progresses, our agents appear to exhibit more cooperative behaviour when playing with a copy of themselves. Given the nature of the environment, it is difficult to pinpoint intentionality \u2014 the behaviours we see often appear to be accidental, but still we see them occur consistently.\nAnalysing the agent\u2019s internal representations, we can say that by taking this approach to reinforcement learning in a vast task space, our agents are aware of the basics of their bodies and the passage of time and that they understand the high-level structure of the games they encounter. Perhaps even more interestingly, they clearly recognise the reward states of their environment. This generality and diversity of behaviour in new tasks hints toward the potential to fine-tune these agents on downstream tasks. For instance, we show in the technical paper that with just 30 minutes of focused training on a newly presented complex task, the agents can quickly adapt, whereas agents trained with RL from scratch cannot learn these tasks at all.\nBy developing an environment like XLand and new training algorithms that support the open-ended creation of complexity, we\u2019ve seen clear signs of zero-shot generalisation from RL agents. Whilst these agents are starting to be generally capable within this task space, we look forward to continuing our research and development to further improve their performance and create ever more adaptive agents.\nWe hope the \npreprint of our technical paper\n \u2014 and \nvideos of the results\n we\u2019ve seen \u2014 help other researchers likewise see a new path toward creating more adaptive, generally capable AI agents. And if you\u2019re excited by these advances, consider \njoining our team\n.\nFor more details, see our preprint \u201c\nOpen-Ended Learning Leads to Generally Capable Agents\n.\u201d\nThis blog post is based on joint work by the Open-Ended Learning Team (listed alphabetically by first name): Adam Stooke, Anuj Mahajan, Catarina Barros, Charlie Deck, Jakob Bauer, Jakub Sygnowski, Maja Trebacz, Max Jaderberg, Michael Mathieu, Nat McAleese, Nathalie Bradley-Schmieg, Nathaniel Wong, Nicolas Porcel, Roberta Raileanu, Steph Hughes-Fitt, Valentin Dalibard, Wojciech Marian Czarnecki.\n"}
{"title": "An update on our racial justice efforts", "contents": "In June 2020, after George Floyd was killed in Minneapolis (USA) and the solidarity that followed as millions spoke out at Black Lives Matter protests around the world, \u00a0I \u2013 like many others \u2013 reflected on the situation and how our organisation could contribute. \u00a0I then shared some \nthoughts\n around DeepMind's intention to help combat racism and advance racial equity. \nWith other senior leaders at DeepMind, I spent time listening and talking to colleagues about how racism affects peoples\u2019 personal and professional lives and replicates itself in the systems and structures of our society. We also explored - and gathered feedback - on how we could best support racial justice in the communities DeepMind interacts with.\nToday I\u2019m pleased to share one of the outcomes of that process: putting resources directly in the hands of Black communities, so they can decide where they need them most. In the past months, we made donations to organisations that play a vital role supporting Black communities in the UK, US and Africa. Specifically, we\u2019ve supported organisations who are focused on impact in the AI/ML space, those supporting emerging regional tech communities, and those focused on broader societal impact. These donations are unrestricted, meaning each of these organisations can use the funds however they need to, to accelerate sustained impact.\nWe're delighted to support these organisations, and grateful to many of those who have shared with us an overview of their work:\nBlack Cultural Archives (UK) is the home of Black British History, situated within the iconic Windrush Square in Brixton. Since its inception in 1981, they've become the leading non-governmental and heritage institutional voice for the Windrush Generation, also leading in the heritage sector for their work on workforce diversity and interrogating decolonial archival practices. Their overall mission is to collect, preserve and celebrate the histories of people of African and Caribbean descent in the UK, inspiring and strengthening communities, individuals, and society alike.\nBlack in AI (USA) is a multi-institutional, transcontinental initiative creating a space for sharing ideas, fostering collaborations, and discussing initiatives to increase the presence of Black individuals in the field of AI.\nBlack Thrive Global (UK) is a partnership between communities, statutory organisations, and the voluntary and private sectors. They work together to improve the experiences and outcomes of Black people in mental health services in the UK, as well as address the social inequality and injustices that lead to the disproportionately high rate of mental illness amongst Black people.\nData Science Africa (Kenya) is a grassroots capacity building organisation that runs summer schools and workshops in the area of data science, AI and machine learning. Since 2015, they've run eight events in six countries in East and West Africa. They've also begun a \nresearch award program\n to support African researchers and a \nvisiting fellowship\n program to support research visits to DSA partner institutions.\nDeep Learning Indaba (across the African continent) supports Africa\u2019s community in AI to be owners and shapers of Machine Learning. Their mission is to strengthen Machine Learning on the African continent. One way they're doing this is by supporting AI communities through locally organised \nIndabaX\n events, which now includes 31 IndabaX ML communities across the continent. These events have been the best way to connect local ML researchers, engineers and enthusiasts to share their research, innovation and current challenges. Last year, they launched an innovative \nmentorship\n program and this year they're coming to the end of \n11 research\n projects and are looking to further deepen their collaboration with \nData Science Africa.\nStopWatch (UK) is a coalition of academics, lawyers, civil society organisations, and community stakeholders who aim to address excess and disproportionate stop and search, promote best practice and ensure fair, effective policing for all. This includes legal and policy analysis, media coverage and commentary, political advocacy, litigation, submissions to national and international organisations and community organising.\nUbele Initiative\u2019s (UK) work is grounded in community-based approaches to development, supporting Black and minoritised groups with their community assets through social action, enterprise development, and next-generation leadership initiatives.\nIt's an honour to support these organisations and to have the privilege of highlighting their efforts, but we recognise that this type of support is only one small part of the important work we need to do. At DeepMind, we want to build safe and ethical AI and deploy it in a way that is beneficial to society, which requires holding ourselves to a high standard of equity and fairness in our research and internal practices.\nThanks to the commitment and passion of many groups within DeepMind, this thinking is shaping our efforts to improve representation in AI and to ensure a fair and inclusive workplace.\nIt's also a perspective that, step-by-step, we're integrating into our research programmes, carefully assessing our research for potential harms, ensuring space for critical reflection via \nresearch\n and \ndiscussion\n on socio-technical topics. We're also regularly reviewing internal hiring, promotion and project assignment processes through the prism of equity. Externally, our \nscholarship & mentorship programme\n, which supports underrepresented groups to pursue postgraduate study, has expanded dramatically in the past two years.\nMy deep thanks to everyone who has committed time, energy and passion to these efforts so far.\n"}
{"title": "MuZero: Mastering Go, chess, shogi and Atari without rules", "contents": "In 2016, we introduced \nAlphaGo\n, the first artificial intelligence (AI) program to defeat humans at the ancient game of Go. Two years later, its successor - \nAlphaZero\n - learned from scratch to master Go, chess and shogi. Now, \nin a paper in the journal Nature\n, we describe MuZero, a significant step forward in the pursuit of general-purpose algorithms. MuZero masters Go, chess, shogi and Atari without needing to be told the rules, thanks to its ability to plan winning strategies in unknown environments.\nFor many years, researchers have sought methods that can both learn a model that explains their environment, and can then use that model to plan the best course of action. Until now, most approaches have struggled to plan effectively in domains, such as Atari, where the rules or dynamics are typically unknown and complex.\nMuZero, first introduced in a \npreliminary paper in 2019\n, solves this problem by learning a model that focuses only on the most important aspects of the environment for planning. By combining this model with AlphaZero\u2019s powerful lookahead tree search, MuZero set a new state of the art result on the Atari benchmark, while simultaneously matching the performance of AlphaZero in the classic planning challenges of Go, chess and shogi. In doing so, MuZero demonstrates a significant leap forward in the capabilities of reinforcement learning algorithms.\nThe ability to plan is an important part of human intelligence, allowing us to solve problems and make decisions about the future. For example, if we see dark clouds forming, we might predict it will rain and decide to take an umbrella with us before we venture out. Humans learn this ability quickly and can generalise to new scenarios, a trait we would also like our algorithms to have.\nResearchers have tried to tackle this major challenge in AI by using two main approaches: lookahead search or model-based planning.\nSystems that use lookahead search, such as AlphaZero, have achieved remarkable success in classic games such as checkers, chess and poker, but rely on being given knowledge of their environment\u2019s dynamics, such as the rules of the game or an accurate simulator. This makes it difficult to apply them to messy real world problems, which are typically complex and hard to distill into simple rules.\nModel-based systems aim to address this issue by learning an accurate model of an environment\u2019s dynamics, and then using it to plan. However, the complexity of modelling every aspect of an environment has meant these algorithms are unable to compete in visually rich domains, such as Atari. \u00a0Until now, the best results on Atari are from model-free systems, such as \nDQN\n, \nR2D2\n and \nAgent57\n. As the name suggests, model-free algorithms do not use a learned model and instead estimate what is the best action to take next.\nMuZero uses a different approach to overcome the limitations of previous approaches. Instead of trying to model the entire environment, MuZero just models aspects that are important to the agent\u2019s decision-making process. After all, knowing an umbrella will keep you dry is more useful to know than modelling the pattern of raindrops in the air.\nSpecifically, MuZero models three elements of the environment that are critical to planning:\nThese are all learned using a deep neural network and are all that is needed for MuZero to understand what happens when it takes a certain action and to plan accordingly.\nThis approach comes with another major benefit: MuZero can repeatedly use its learned model to improve its planning, rather than collecting new data from the environment. For example, in tests on the Atari suite, this variant - known as MuZero Reanalyze - used the learned model 90% of the time to re-plan what should have been done in past episodes.\nWe chose four different domains to test MuZeros capabilities. Go, chess and shogi were used to assess its performance on challenging planning problems, while we used the Atari suite as a benchmark for more visually complex problems. In all cases, MuZero set a new state of the art for reinforcement learning algorithms, outperforming all prior algorithms on the Atari suite and matching the superhuman performance of AlphaZero on Go, chess and shogi.\nWe also tested how well MuZero can plan with its learned model in more detail. We started with the classic precision planning challenge in Go, where a single move can mean the difference between winning and losing. To confirm the intuition that planning more should lead to better results, we measured how much stronger a fully trained version of MuZero can become when given more time to plan for each move (see left hand graph below). The results showed that playing strength increases by more than 1000 Elo (a measure of a player's relative skill) as we increase the time per move from one-tenth of a second to 50 seconds. This is similar to the difference between a strong amateur player and the strongest professional player.\nTo test whether planning also brings benefits throughout training, we ran a set of experiments on the Atari game Ms Pac-Man (right hand graph above) using separate trained instances of MuZero. Each one was allowed to consider a different number of planning simulations per move, ranging from five to 50. The results confirmed that increasing the amount of planning for each move allows MuZero to both learn faster and achieve better final performance.\nInterestingly, when MuZero was only allowed to consider six or seven simulations per move - a number too small to cover all the available actions in Ms Pac-Man - it still achieved good performance. This suggests MuZero is able to generalise between actions and situations, and does not need to exhaustively search all possibilities to learn effectively.\nMuZero\u2019s ability to both learn a model of its environment and use it to successfully plan demonstrates a significant advance in reinforcement learning and the pursuit of general purpose algorithms. Its predecessor, AlphaZero, has already been applied to a range of complex problems in \nchemistry\n, \nquantum physics\n and beyond. The ideas behind MuZero's powerful learning and planning algorithms may pave the way towards tackling new challenges in robotics, industrial systems and other messy real-world environments where the \u201crules of the game\u201d are not known.\n\u200d\nDesign by Adam Cain, Jim Kynvin and Aleksandrs Polozuns\n"}
{"title": "Putting the power of AlphaFold into the world\u2019s hands", "contents": "In July 2022, we released AlphaFold protein structure predictions for nearly all catalogued proteins known to science. Read the latest blog \nhere\n.\nToday, I\u2019m incredibly proud and excited to announce that DeepMind is making a significant contribution to humanity\u2019s understanding of biology.\nWhen we \nannounced AlphaFold 2\n last December, it was hailed as a solution to the 50-year old protein folding problem. Last week, we published the \nscientific paper\n and \nsource code\n explaining how we created this highly innovative system, and today we\u2019re sharing \nhigh-quality predictions\n for the shape of every single protein in the human body, as well as for the proteins of 20 additional organisms that scientists rely on for their research.\nAs researchers seek cures for diseases and pursue solutions to other big problems facing humankind \u2013 including antibiotic resistance, microplastic pollution, and climate change \u2013 they will benefit from fresh insights into the structure of proteins. Proteins are like tiny exquisite biological machines. The same way that the structure of a machine tells you what it does, so the structure of a protein helps us understand its function. Today, we are sharing \na trove of information\n that doubles \nhumanity\u2019s understanding of the human proteome\n, and reveals the protein structures found in 20 other biologically-significant organisms, from E.coli to yeast, and from the fruit fly to the mouse.\nAs a powerful tool that supports the efforts of researchers, we believe this is the most significant contribution AI has made to advancing scientific knowledge to date, and is a great example of the benefits AI can bring to humanity. \u00a0These insights will underpin many exciting future advances in our understanding of biology and medicine. Thanks to five tireless years of work and a lot of ingenuity from the AlphaFold team, and working closely for the past few months with our partners at \nEMBL\u2019s European Bioinformatics Institute (EMBL-EBI)\n, we are able to share this huge and valuable resource with the world.\n\u200d\nThis latest work builds on \nannouncements\n we made last December, at the CASP14 conference, when DeepMind unveiled a radical new version of our AlphaFold system, which was recognised by the organisers of the assessment as a solution to the 50-year old grand challenge to understand the 3D structure of proteins. Determining protein structures experimentally is a time-consuming and painstaking pursuit, but AlphaFold demonstrated that AI could accurately predict the shape of a protein, at scale and in minutes, down to atomic accuracy. At \nCASP\n, we pledged to share our methods and provide broad access to this body of knowledge.\nThis month, we\u2019ve finished the enormous amount of hard work to deliver on that commitment. We published two peer-reviewed papers in \nNature \n(\n1\n,\n2\n) and \nopen-sourced AlphaFold\u2019s code\n. Today, in partnership with \nEMBL-EBI\n, we\u2019re incredibly proud to be launching the \nAlphaFold Protein Structure Database\n, which offers the most complete and accurate picture of the human proteome to date, more than doubling humanity\u2019s accumulated knowledge of high-accuracy human protein structures.\nIn addition to the human proteome (all the ~20,000 proteins expressed by the human genome), we\u2019re providing open access to the proteomes of \n20 other biologically-significant organisms\n, totalling over 350,000 protein structures. Research into these organisms has been the subject of countless research papers and numerous major breakthroughs, and has resulted in a deeper understanding of life itself. In the coming months we plan to vastly expand the coverage \nto almost every sequenced protein known to science\n - over 100 million structures covering most of the \nUniProt reference database\n. It\u2019s a veritable protein almanac of the world. And the system and database will periodically be updated as we continue to invest in future improvements to AlphaFold.\nMost excitingly, in the hands of scientists around the world, this new protein almanac will enable and accelerate research that will advance our understanding of these building blocks of life. Already, through our early collaborations, we\u2019ve seen promising signals from researchers using AlphaFold in their own work. For instance, the \nDrugs for Neglected Diseases Initiative\n (DNDi) \nhas advanced their research into life-saving cures\n for diseases that disproportionately affect the poorer parts of the world, and the \nCentre for Enzyme Innovation\n at the University of Portsmouth (CEI) is using AlphaFold to help engineer faster enzymes for recycling some of our most polluting single-use plastics. For those scientists who rely on experimental protein structure determination, AlphaFold's predictions have helped accelerate their research. As another example, a team at the \nUniversity of Colorado Boulder\n is finding promise in using AlphaFold predictions to study antibiotic resistance, while a group at the \nUniversity of California San Francisco\n has used them to \nincrease their understanding of SARS-CoV-2 biology\n. And this is just the start of what we hope will be a revolution in structural bioinformatics. With AlphaFold out in the world, there is a treasure trove of data now waiting to be transformed into future advances.\nFor the AlphaFold team at DeepMind, this work represents the culmination of five years of enormous effort, including having to creatively overcome many challenging setbacks, resulting in a host of new sophisticated algorithmic innovations that were all needed to finally crack the problem. It builds on the discoveries of generations of scientists, from the early pioneers of protein imaging and crystallography, to the thousands of prediction specialists and structural biologists who\u2019ve spent years experimenting with proteins since. Our dream is that AlphaFold, by providing this foundational understanding, will aid countless more scientists in their work and open up completely new avenues of scientific discovery.\nAt DeepMind, our thesis has always been that artificial intelligence can dramatically accelerate breakthroughs in many fields of science, and in turn advance humanity. We built \nAlphaFold\n and the \nAlphaFold Protein Structure Database\n to support and elevate the efforts of scientists around the world in the important work they do. We believe AI has the potential to revolutionise how science is done in the 21st century, and we eagerly await the discoveries that AlphaFold might help the scientific community to unlock next.\nTo learn more, head over to Nature to read our peer-reviewed papers describing our \nfull method\n, and the \nhuman proteome\n. You can read more about them in our \nAuthors' Notes\n. If you want to explore our system, here\u2019s the \nopen-source code to AlphaFold\n and \nColab notebook\n to run individual sequences. To explore our structures, EMBL-EBI, the world leader in biological data, is hosting them in \na searchable database\n that is open and free to all. \nWe would love to hear your feedback and understand how AlphaFold has been useful in your research. Share your stories at \nalphafold@deepmind.com\n. \n"}
{"title": "Advancing sports analytics through AI research", "contents": "Creating testing environments to help progress AI research out of the lab and into the real world is immensely challenging. Given AI\u2019s long association with games, it is perhaps no surprise that sports presents an exciting opportunity, offering researchers a testbed in which an AI-enabled system can assist humans in making complex, real-time decisions in a multiagent environment with dozens of dynamic, interacting individuals.\nThe rapid growth of sports data collection means we are in the midst of a remarkably important era for sports analytics. The availability of sports data is increasing in both quantity and granularity, transitioning from the days of aggregate high-level statistics and sabermetrics to more refined data such as event stream information (e.g., annotated passes or shots), high-fidelity player positional information, and \non-body sensors\n. However, the field of sports analytics has only recently started to harness machine learning and AI for both understanding and advising human decision-makers in sports. In our \nrecent paper\n published in collaboration with Liverpool Football Club (LFC) in JAIR, we envision the future landscape of sports analytics using a combination of statistical learning, video understanding, and game theory. We illustrate football, in particular, is a useful microcosm for studying AI research, offering benefits in the longer-term to decision-makers in sports in the form of an automated video-assistant coach (AVAC) system (Figure 1(A)).\nIn comparison to some other sports, football has been rather late with starting to systematically collect large sets of data for scientific analytics purposes aiming to progress teams\u2019 gameplay. This is for several reasons, with the most prominent being that there are far less controllable settings of the game compared to other sports (large outdoor pitch, dynamic game, etc.), and also the dominant credo to rely mainly on human specialists with track records and experience in professional football. On these lines, Arrigo Sacchi, a successful Italian football coach and manager who never played professional football in his career, responded to criticism over his lack of experience with his \nfamous quote\n when becoming a coach at Milan in 1987: \u201cI never realised that to be a jockey you had to be a horse first.\u201d\nFootball Analytics poses challenges that are well suited for a wide variety of AI techniques, coming from the intersection of 3 fields: computer vision, statistical learning and game theory (visualised in Figure 2). While these fields are individually useful for football analytics, their benefits become especially tangible when combined: players need to take sequential decision-making in the presence of other players (cooperative and adversarial) and as such game theory, a theory of interactive decision making, becomes highly relevant. Moreover, tactical solutions to particular in-game situations can be learnt based on in-game and specific player representations, which makes statistical learning a highly relevant area. Finally, players can be tracked and game scenarios can be recognised automatically from widely-available image and video inputs.\nThe AVAC system we envision is situated within the microcosm that is formed by the intersection of these three research fields (Figure 2). In our research in this exciting domain, we not only lay out a roadmap for scientific and engineering problems that can be tackled for years to come, but we also present new original results at the crossroads of game theoretic analysis, statistical learning, and computer vision to illustrate what this exciting area has to offer to football.\nGame theory plays an important role in the study of sports, enabling theoretical grounding of players\u2019 behavioral strategies. In the case of football, many of its scenarios can actually be modeled as zero-sum games, which have been studied extensively since the inception of game theory. For example, here we model the penalty kick situation as a two-player asymmetric game, where the kicker\u2019s strategies may be neatly categorised as left, center, or right shots. To study this problem, we augment game-theoretic analysis in the penalty kick scenario with \nPlayer Vectors\n, which summarise the playing styles of individual football players. With such representations of individual players, we are able to group kickers with similar playing styles, and then conduct game-theoretic analysis on the group-level (Figure 3). Our results show that the identified shooting strategies of different groups are statistically distinct. For example, we find that one group prefers to shoot to the left corner of the goal mouth, while another tends to shoot to the left and right corners more evenly. Such insights may help goalkeepers diversify their defense strategies when playing against different types of players. Building on this game-theoretic view, one can consider the durative nature of football by analysing it in the form of temporally-extended games, use this to advise tactics to individual players, or even go further to optimise the overall team strategy.\nOn the side of statistical learning, representation learning has yet to be fully exploited in sports analytics, which would enable informative summarisation of the behavior of individual players and football teams. Moreover, we believe that the interaction between game theory and statistical learning would catalyse advances in sports analytics further. In the above penalty kick scenario, for instance, augmenting the analysis with player-specific statistics (Player Vectors) provided deeper insights into how various types of players behave or make decisions about their actions in the penalty kick scenario. As another example of this, one can study '\nghosting\n', which refers to a particular data driven analysis of how players should have acted in hindsight in sports analytics (which bears connections to the notion of regret in online learning and game theory). The ghosting model suggests alternative player trajectories for a given play, e.g., based on the league average or a selected team. Predicted trajectories are usually visualised as a translucent layer over the original play, hence the term 'ghosting' (see Figure 4 for a visual example). Generative trajectory prediction models allow us to gain insights by analysing key situations of a game and how they might have played out differently. These models also bear potential in predicting the implications of a tactical change, a key player's injury, or substitution on the own team's performance along with the opposition's response to such a change.\nFinally, we consider computer vision to be one of the most promising avenues for advancing the boundaries of state of the art sports analytics research. By detecting events purely from video, a topic that has been well-studied in the computer vision community (e.g., see the following \nsurvey\n and our paper for additional references), the potential range of application is enormous. By associating events with particular frames, videos become searchable and ever more useful (e.g., automatic highlight generation becomes possible). Football video, in turn, offers an interesting application domain for computer vision. The large numbers of football videos satisfies a prerequisite for modern AI techniques. While each football video is different, the settings do not vary greatly, which makes the task ideal for sharpening AI algorithms. Third-party providers also exist to furnish hand-labelled event data that can be useful in training video models and are time consuming to generate, so both supervised and unsupervised algorithms can be used for football event detection. Figure 1(B), for example, provides a stylised visualisation of a deep learning model trained with supervised methods to recognise target events (e.g., kicks) purely from video.\nThe application of advanced AI techniques to football has the potential to revolutionise the game across many axes, for players, decision-makers, fans, and broadcasters. Such advances will also be important as they also bear potential to further democratise the sport itself (e.g., rather than relying on judgement calls from in-person scouts/experts, one may use techniques such as computer vision to quantify skillsets of players from under-represented regions, those from lower-level leagues, etc.). We believe that the development of increasingly advanced AI techniques afforded by the football microcosm might be applicable to broader domains. To this end, we are co-organising (with several external organisers) an \nIJCAI 2021 workshop on AI for Sports Analytics\n later this year, which we welcome interested researchers to attend. For researchers interested in this topic, publicly available datasets have been made available both by analytics companies such as StatsBomb (\ndataset link\n) and the wider research community (\ndataset link\n). Furthermore, the paper provides a comprehensive overview of research in this domain.\nPaper and related links:\nWork done as a collaboration with contributors: Karl Tuyls, Shayegan Omidshafiei, Paul Muller, Zhe Wang, Jerome Connor, Daniel Hennes, Ian Graham, William Spearman, Tim Waskett, Dafydd Steele, Pauline Luc, Adria Recasens, Alexandre Galashov, Gregory Thornton, Romuald Elie, Pablo Sprechmann, Pol Moreno, Kris Cao, Marta Garnelo, Praneet Dutta, Michal Valko, Nicolas Heess, Alex Bridgland, Julien Perolat, Bart De Vylder, Ali Eslami, Mark Rowland, Andrew Jaegle, Yi Yang, Remi Munos, Trevor Back, Razia Ahamed, Simon Bouton, Nathalie Beauguerlange, Jackson Broshear, Thore Graepel, and Demis Hassabis.\n"}
{"title": "Building architectures that can handle the world\u2019s data", "contents": "Most architectures used by AI systems today are specialists. A 2D residual network may be a good choice for processing images, but at best it\u2019s a loose fit for other kinds of data \u2014 such as the Lidar signals used in self-driving cars or the torques used in robotics. What\u2019s more, standard architectures are often designed with only one task in mind, often leading engineers to bend over backwards to reshape, distort, or otherwise modify their inputs and outputs in hopes that a standard architecture can learn to handle their problem correctly. Dealing with more than one kind of data, like the sounds and images that make up videos, is even more complicated and usually involves complex, hand-tuned systems built from many different parts, even for simple tasks. As part of DeepMind's mission of solving intelligence to advance science and humanity, we want to build systems that can solve problems that use many types of inputs and outputs, so we began to explore a more general and versatile architecture that can handle all types of data.\nIn a paper presented at \nICML 2021\n (the International Conference on Machine Learning) and \npublished as a preprint on arXiv\n, we introduced the Perceiver, a general-purpose architecture that can process data including images, point clouds, audio, video, and their combinations. While the Perceiver could handle many varieties of input data, it was limited to tasks with simple outputs, like classification. A \nnew preprint on arXiv\n describes Perceiver IO, a more general version of the Perceiver architecture. Perceiver IO can produce a wide variety of outputs from many different inputs, making it applicable to real-world domains like language, vision, and multimodal understanding as well as challenging games like StarCraft II. To help researchers and the machine learning community at large, we\u2019ve now \nopen sourced the code\n.\nPerceivers build on the \nTransformer\n, an architecture that uses an operation called \u201cattention\u201d to map inputs into outputs. By comparing all elements of the input, Transformers process inputs based on their relationships with each other and the task. Attention is simple and widely applicable, but Transformers use attention in a way that can quickly become expensive as the number of inputs grows. This means Transformers work well for inputs with at most a few thousand elements, but common forms of data like images, videos, and books can easily contain millions of elements. With the original Perceiver, we solved a major problem for a generalist architecture: scaling the Transformer\u2019s attention operation to very large inputs without introducing domain-specific assumptions. The Perceiver does this by using attention to first encode the inputs into a small latent array. This latent array can then be processed further at a cost independent of the input\u2019s size, enabling the Perceiver\u2019s memory and computational needs to grow gracefully as the input grows larger, even for especially deep models.\nThis \u201cgraceful growth\u201d allows the Perceiver to achieve an unprecedented level of generality \u2014 it\u2019s competitive with domain-specific models on benchmarks based on images, 3D point clouds, and audio and images together. But because the original Perceiver produced only one output per input, it wasn\u2019t as versatile as researchers needed. Perceiver IO fixes this problem by using attention not only to encode to a latent array but also to decode from it, which gives the network great flexibility. Perceiver IO now scales to large and diverse inputs \nand\n outputs, and can even deal with many tasks or types of data at once. This opens the door for all sorts of applications, like understanding the meaning of a text from each of its characters, tracking the movement of all points in an image, processing the sound, images, and labels that make up a video, and even playing games, all while using a single architecture that\u2019s simpler than the alternatives.\nIn our experiments, we\u2019ve seen Perceiver IO work across a wide range of benchmark domains \u2014 such as language, vision, multimodal data, and games \u2014 to provide an off-the-shelf way to handle many kinds of data. We hope \nour latest preprint \nand the code \navailable on Github\n help researchers and practitioners tackle problems without needing to invest the time and effort to build custom solutions using specialised systems. As we continue to learn from exploring new kinds of data, we look forward to further improving upon this general-purpose architecture and making it faster and easier to solve problems throughout science and machine learning.\n"}
{"title": "Predicting gene expression with AI", "contents": "Based on Transformers, our new Enformer architecture advances genetic research by improving the ability to predict how DNA sequence influences gene expression.\nWhen the \nHuman Genome Project\n succeeded in mapping the DNA sequence of the human genome, the international research community were excited by the opportunity to better understand the genetic instructions that influence human health and development. DNA carries the genetic information that determines everything from eye colour to susceptibility to certain diseases and disorders. The roughly 20,000 sections of DNA in the human body known as genes contain instructions about the amino acid sequence of proteins, which perform numerous essential functions in our cells. Yet these genes make up less than 2% of the genome. The remaining base pairs \u2014 which account for 98% of the 3 billion \u201cletters\u201d in the genome \u2014 are called \u201cnon-coding\u201d and contain less well-understood instructions about when and where genes should be produced or expressed in the human body. At DeepMind, we believe that AI can unlock a deeper understanding of such complex domains, accelerating scientific progress and offering potential benefits to human health.\nToday Nature Methods published \u201c\nEffective gene expression prediction from sequence by integrating long-range interactions\n\u201d (first shared as a preprint on \nbioRxiv\n), in which we \u2014 in collaboration with our Alphabet colleagues at \nCalico\n \u2014 introduce a neural network architecture called Enformer that led to greatly increased accuracy in predicting gene expression from DNA sequence. To advance further study of gene regulation and causal factors in diseases, we also made our model and its initial predictions of common genetic variants \nopenly available here\n.\nPrevious work on gene expression has typically used convolutional neural networks as fundamental building blocks, but their limitations in modelling the influence of distal enhancers on gene expression have hindered their accuracy and application. Our initial explorations relied on \nBasenji2\n, which could predict regulatory activity from relatively long DNA sequences of 40,000 base pairs. Motivated by this work and the knowledge that regulatory DNA elements can influence expression at greater distances, we saw the need for a fundamental architectural change to capture long sequences.\nWe developed a new model based on \nTransformers\n, common in natural language processing, to make use of self-attention mechanisms that could integrate much greater DNA context. Because Transformers are ideal for looking at long passages of text, we adapted them to \u201cread\u201d vastly extended DNA sequences. By effectively processing sequences to consider interactions at distances that are more than 5 times (i.e., 200,000 base pairs) the length of previous methods, our architecture can model the influence of important regulatory elements called enhancers on gene expression from further away within the DNA sequence.\nTo better understand how Enformer interprets the DNA sequence to arrive at more accurate predictions, we used contribution scores to highlight which parts of the input sequence were most influential for the prediction. Matching the biological intuition, we observed that the model paid attention to enhancers even if located more than 50,000 base pairs away from the gene. Predicting which enhancers regulate which genes remains a major unsolved problem in genomics, so we were pleased to see the contribution scores of Enformer perform comparably with existing methods developed specifically for this task (using experimental data as input). Enformer also learned about insulator elements, which separate two independently regulated regions of DNA.\nAlthough it\u2019s now possible to study an organism's DNA in its entirety, complex experiments are required to understand the genome. Despite an enormous experimental effort, the vast majority of the DNA control over gene expression remains a mystery. With AI, we can explore new possibilities for finding patterns in the genome and provide mechanistic hypotheses about sequence changes. Similar to a spell checker, Enformer partially understands the vocabulary of the DNA sequence and can thereby highlight edits that could lead to altered gene expression.\nThe main application of this new model is to predict which changes to the DNA letters, also called genetic variants, will alter the expression of the gene. Compared to previous models, Enformer is significantly more accurate at predicting the effects of variants on gene expression, both in the case of natural genetic variants and synthetic variants that alter important regulatory sequences. This property is useful for interpreting the growing number of disease-associated variants obtained by genome-wide association studies. Variants associated with complex genetic diseases are predominantly located in the non-coding region of the genome, likely causing disease by altering gene expression. But due to inherent correlations among variants, many of these disease-associated variants are only spuriously correlated rather than causative. Computational tools can now help distinguish the true associations from false positives.\nWe\u2019re far from solving the untold puzzles that remain in the human genome, but Enformer is a step forward in understanding the complexity of genomic sequences. If you\u2019re interested in using AI to explore how fundamental cell processes work, how they\u2019re encoded in the DNA sequence, and how to build new systems to advance genomics and our understanding of disease, \nwe\u2019re hiring\n. We\u2019re also looking forward to expanding our collaborations with other researchers and organisations eager to explore computational models to help solve the open questions at the heart of genomics.\n"}
{"title": "Nowcasting the next hour of rain", "contents": "Our lives are dependent on the weather. At any moment in the UK, according to \none study\n, one third of the country has talked about the weather in the past hour, reflecting the importance of weather in daily life. Amongst weather phenomena, rain is especially important because of its influence on our everyday decisions. Should I take an umbrella? How should we route vehicles experiencing heavy rain? What safety measures do we take for outdoor events? Will there be a flood? \nOur latest research\n and state-of-the-art model advances the science of \nPrecipitation Nowcasting\n, which is the prediction of rain (and other precipitation phenomena) within the next 1-2 hours. In a \npaper\n written in collaboration with the Met Office and published in Nature, we directly tackle this important \ngrand challenge\n in weather prediction. This collaboration between environmental science and AI focuses on value for decision-makers, opening up new avenues for the nowcasting of rain, and points to the opportunities for AI in supporting our response to the challenges of decision-making in an environment under constant change.\nThroughout history, the prediction of weather has held a place of importance for our communities and countries. \nMedieval meteorologists\n began by using the stars to make predictions. Slowly, tables recording seasons and rain patterns started to be kept. Centuries later, Lewis Fry imagined a \u2018\nForecast Factory\n\u2019 that used computation and the physical equations of the atmosphere to predict global weather. In this evolving book of weather prediction, we now add a story on the role of machine learning for forecasting.\nToday\u2019s weather predictions are driven by powerful \nnumerical weather prediction\n (NWP) systems. By solving physical equations, NWPs provide essential planet-scale predictions several days ahead. However, they struggle to generate high-resolution predictions for short lead times under two hours. Nowcasting fills the performance gap in this crucial time interval.\nNowcasting is essential for sectors like water management, agriculture, aviation, emergency planning, and \noutdoor events\n. Advances in weather sensing have made high-resolution radar data\u2013which measures the amount of precipitation at ground level\u2013available at high frequency (e.g., every 5 mins at 1 km resolution). This combination of a crucial area where existing methods struggle and the availability of high-quality data provides the opportunity for machine learning to make its contributions to nowcasting.\nWe focus on nowcasting rain: predictions up to 2 hours ahead that capture the amount, timing, and location of rainfall. We use an approach known as generative modelling to make detailed and plausible predictions of future radar based on past radar. Conceptually, this is a problem of generating radar movies. With such methods, we can both accurately capture large-scale events, while also generating many alternative rain scenarios (known as ensemble predictions), allowing rainfall uncertainty to be explored. We used radar data from both the UK and the US in our study results.\nWe were especially interested in the ability of these models to make predictions on medium to heavy-rain events, which are the events that most impact people and the economy, and we show statistically significant improvements in these regimes compared to competing methods. Importantly, we conducted a cognitive task assessment with more than 50 expert meteorologists at the Met Office, the UK\u2019s national meteorological service, \nwho rated our new approach as their first choice in 89% of cases when compared to widely-used nowcasting methods\n, demonstrating the ability of our approach to provide insight to real world decision-makers.\nBy using statistical, economic, and cognitive analyses we were able to demonstrate a new and competitive approach for precipitation nowcasting from radar. No method is without limitations, and more work is needed to improve the accuracy of long-term predictions and accuracy on rare and intense events. Future work will require us to develop additional ways of assessing performance, and further specialising these methods for specific real-world applications.\nWe think this is an exciting area of research and we hope our paper will serve as a foundation for new work by providing data and verification methods that make it possible to both provide competitive verification and operational utility. We also hope this collaboration with the Met Office will promote greater integration of machine learning and environmental science, and better support decision-making in our changing climate.\nRead the paper \nSkillful precipitation nowcasting using Deep Generative Models of Radar\n in the 30 September 2021 issue of Nature, which contains an extensive discussion of the model, data and verification approach. You can also explore the data we used for training and find a pre-trained model for the UK via \nGitHub\n.\nAcknowledgements. We are grateful to the Met Office and all our collaborators and advisors for their input to this work.\n"}
{"title": "The Podcast: Episode 3: Life is like a game", "contents": "Video games have become a favourite tool for AI researchers to test the abilities of their systems. In this episode, Hannah sits down to play StarCraft II - a challenging video game that requires players to control the onscreen action with as many as 800 clicks a minute.\nShe is guided by Oriol Vinyals, an ex-professional StarCraft player and research scientist at DeepMind, who explains how the program AlphaStar learnt to play the game and beat a top professional player. Elsewhere, she explores systems that are learning to cooperate in a digital version of the playground favourite \u2018Capture the Flag\u2019.\nInterviewees:\n Research scientists Max Jaderberg and Raia Hadsell; Lead researchers David Silver and Oriol Vinyals, and Director of Research Koray Kavukcuoglu.\nListen to this episode and subscribe to the whole series on\n Apple podcasts\n,\n \nGoogle podcasts\n,\n \nSpotify\n,\n \nDeezer\n or your favourite podcast app by searching for \u201cDeepMind: The Podcast\u201d.\nIf you know of other resources we should link to, please help other listeners by either replying to us on\n Twitter\n (#DMpodcast) or emailing us at podcast@deepmind.com. You can also use that address to send us questions or feedback on the series.\nPresenter:\n Hannah Fry\nEditor:\n David Prest\nSenior Producer:\n Louisa Field\nProducers:\n Amy Racs, Dan Hardoon\nBinaural Sound:\n Lucinda Mason-Brown\nMusic composition:\n Eleni Shaw (with help from Sander Dieleman and \nWaveNet\n)\n"}
{"title": "The Podcast: Episode 4: AI, Robot", "contents": "Forget what sci-fi has told you about superintelligent robots that are uncannily human-like; the reality is more prosaic. Inside DeepMind\u2019s robotics laboratory, Hannah explores what researchers call \u2018embodied AI\u2019: robot arms that are learning tasks like picking up plastic bricks, which humans find comparatively easy.\nDiscover the cutting-edge challenges of bringing AI and robotics together, and learning from scratch how to perform tasks. She also explores some of the key questions about using AI safely in the real world.\nInterviewees: \nSoftware engineer Jackie Kay and research scientists Murray Shanahan, Victoria Krakovna, Raia Hadsell and Jan Leike.\nListen to this episode and subscribe to the whole series on\n Apple podcasts\n,\n Google podcasts\n,\n Spotify\n,\n Deezer\n or your favourite podcast app by searching for \u201cDeepMind: The Podcast\u201d.\nIf you know of other resources we should link to, please help other listeners by either replying to us on\n Twitter\n (#DMpodcast) or emailing us at podcast@deepmind.com. You can also use that address to send us questions or feedback on the series.\nPresenter:\n Hannah Fry\nEditor:\n David Prest\nSenior Producer:\n Louisa Field\nProducers:\n Amy Racs, Dan Hardoon\nBinaural Sound:\n Lucinda Mason-Brown\nMusic composition:\n Eleni Shaw (with help from Sander Dieleman and \nWaveNet\n)\n"}
{"title": "The Podcast: Episode 5: Out of the lab", "contents": "The ambition of AI research is to create systems that can help to solve problems in the real world.\nIn this episode, Hannah Fry meets the people building systems that could be used to save the sight of thousands, help us solve one of the most fundamental problems in biology and reduce energy consumption in an effort to combat climate change. But whilst there is great potential, there are also important obstacles that will need to be tackled for AI to be used effectively, safely and fairly.\nInterviewees: \nPearse Keane, consultant ophthalmologist at Moorfields Eye Hospital; Sandy Nelson, Product Manager for DeepMind\u2019s Science Program; and DeepMind Program Manager Sims Witherspoon. Presented by Hannah Fry.\nListen to this episode and subscribe to the whole series on\n Apple podcasts\n,\n Google podcasts\n,\n Spotify\n,\n Deezer\n or your favourite podcast app by searching for \u201cDeepMind: The Podcast\u201d.\nIf you know of other resources we should link to, please help other listeners by either replying to us on\n Twitter\n (#DMpodcast) or emailing us at podcast@deepmind.com. You can also use that address to send us questions or feedback on the series.\nPresenter:\n Hannah Fry\nEditor:\n David Prest\nSenior Producer:\n Louisa Field\nProducers:\n Amy Racs, Dan Hardoon\nBinaural Sound:\n Lucinda Mason-Brown\nMusic composition:\n Eleni Shaw (with help from Sander Dieleman and \nWaveNet\n)\n"}
{"title": "Stacking our way to more general robots", "contents": "Introducing RGB-Stacking as a new benchmark for vision-based robotic manipulation.\nPicking up a stick and balancing it atop a log or stacking a pebble on a stone may seem like simple \u2014 and quite similar \u2014 actions for a person. However, most robots struggle with handling more than one such task at a time. Manipulating a stick requires a different set of behaviours than stacking stones, never mind piling various dishes on top of one another or assembling furniture. Before we can teach robots how to perform these kinds of tasks, they first need to learn how to interact with a far greater range of objects. As part of \nDeepMind\u2019s mission\n and as a step toward making more generalisable and useful robots, we\u2019re exploring how to enable robots to better understand the interactions of objects with diverse geometries.\nIn a paper to be presented at \nCoRL 2021\n (Conference on Robot Learning) and available now as a preprint on \nOpenReview\n, we introduce RGB-Stacking as a new benchmark for vision-based robotic manipulation. In this benchmark, a robot has to learn how to grasp different objects and balance them on top of one another. What sets our research apart from prior work is the diversity of objects used and the large number of empirical evaluations performed to validate our findings. Our results demonstrate that a combination of simulation and real-world data can be used to learn complex multi-object manipulation and suggest a strong baseline for the open problem of generalising to novel objects. To support other researchers, we\u2019re \nopen-sourcing\n a version of our simulated environment, and releasing the \ndesigns\n for building our real-robot RGB-stacking environment, along with the RGB-object models and information for 3D printing them. We are also open-sourcing \na collection of libraries and tools\n used in our robotics research more broadly.\nWith RGB-Stacking, our goal is to train a robotic arm via reinforcement learning to stack objects of different shapes. We place a parallel gripper attached to a robot arm above a basket, and three objects in the basket \u2014 one red, one green, and one blue, hence the name RGB. The task is simple: stack the red object on top of the blue object within 20 seconds, while the green object serves as an obstacle and distraction. The learning process ensures that the agent acquires generalised skills through training on multiple object sets. We intentionally vary the grasp and stack affordances \u2014 the qualities that define how the agent can grasp and stack each object. This design principle forces the agent to exhibit behaviours that go beyond a simple pick-and-place strategy.\nOur RGB-Stacking benchmark includes two task versions with different levels of difficulty. In \u201cSkill Mastery,\u201d our goal is to train a single agent that\u2019s skilled in stacking a predefined set of five triplets. In \u201cSkill Generalisation,\u201d we use the same triplets for evaluation, but train the agent on a large set of training objects \u2014 totalling more than a million possible triplets. To test for generalisation, these training objects exclude the family of objects from which the test triplets were chosen. In both versions, we decouple our learning pipeline into three stages:\nDecoupling our learning pipeline in such a way proves crucial for two main reasons. Firstly, it allows us to solve the problem at all, since it would simply take too long if we were to start from scratch on the robots directly. Secondly, it increases our research velocity, since different people in our team can work on different parts of the pipeline before we combine these changes for an overall improvement.\nIn recent years, there has been much work on applying learning algorithms to solving difficult real-robot manipulation problems at scale, but the focus of such work has largely been on tasks such as grasping, pushing, or other forms of manipulating single objects. The approach to RGB-Stacking we describe in our paper, accompanied by \nour robotics resources now available on GitHub\n, results in surprising stacking strategies and mastery of stacking a subset of these objects. Still, this step only scratches the surface of what\u2019s possible \u2013 and the generalisation challenge remains not fully solved. As researchers keep working to solve the open challenge of true generalisation in robotics, we hope this new benchmark, along with the environment, designs, and tools we have released, contribute to new ideas and methods that can make manipulation even easier and robots more capable.\n\u200d\n"}
{"title": "Opening up a physics simulator for robotics", "contents": "When you walk, your feet make contact with the ground. When you write, your fingers make contact with the pen. Physical contacts are what makes interaction with the world possible. Yet, for such a common occurrence, contact is a surprisingly complex phenomenon. Taking place at microscopic scales at the interface of two bodies, contacts can be soft or stiff, bouncy or spongy, slippery or sticky. It\u2019s no wonder our fingertips have \nfour different types\n of touch-sensors. This subtle complexity makes simulating physical contact \u2014 a vital component of robotics research \u2014 a tricky task.\nThe rich-yet-efficient contact model of the \nMuJoCo physics simulator\n has made it a leading choice by robotics researchers and today, we're proud to announce that, as part of \nDeepMind's mission\n of advancing science, we've acquired MuJoCo and are making it \nfreely available\n for everyone, to support research everywhere. Already widely used within the robotics community, including as the physics simulator of choice for DeepMind\u2019s robotics team, MuJoCo features a rich contact model, powerful scene description language, and a well-designed API. Together with the community, we will continue to improve MuJoCo as open-source software under a permissive licence. As we work to prepare the codebase, we are making MuJoCo \nfreely available\n as a precompiled library.\nA balanced model of contact. MuJoCo, which stands for \nMu\nlti-\nJo\nint Dynamics with \nCo\nntact, hits a sweet spot with its contact model, which accurately and efficiently captures the salient features of contacting objects. Like other rigid-body simulators, it avoids the fine details of deformations at the contact site, and often runs much faster than real time. Unlike other simulators, MuJoCo resolves contact forces using the convex \nGauss Principle\n. Convexity ensures unique solutions and well-defined inverse dynamics. The model is also flexible, providing multiple parameters which can be tuned to approximate a wide range of contact phenomena.\nReal physics, no shortcuts. \nBecause many simulators were initially designed for purposes like gaming and cinema, they sometimes take shortcuts that prioritise stability over accuracy. For instance, they may ignore gyroscopic forces or directly modify velocities. This can be particularly harmful in the context of optimisation: as \nfirst observed\n by artist and researcher Karl Sims, an optimising agent can quickly discover and exploit these deviations from reality. In contrast, MuJoCo is a second-order continuous-time simulator, implementing the full Equations of Motion. Familiar yet non-trivial physical phenomena like \nNewton\u2019s Cradle\n, as well as unintuitive ones like the \nDzhanibekov effect\n, emerge naturally. Ultimately, MuJoCo closely adheres to the equations that govern our world.\nPortable code, clean API.\n MuJoCo\u2019s core engine is written in pure C, which makes it easily portable to various architectures. The library produces deterministic results, with the scene description and simulation state fully encapsulated within two data structures. These constitute all the information needed to recreate a simulation, including results from intermediate stages, providing easy access to the internals. The library also provides fast and convenient computations of commonly used quantities, like kinematic Jacobians and inertia matrices.\nPowerful scene description. \nThe MJCF scene-description format uses cascading defaults \u2014 avoiding multiple repeated values \u200b\u200b\u2014 and contains elements for real-world robotic components like equality constraints, motion-capture markers, tendons, actuators, and sensors. Our long-term roadmap includes standardising MJCF as an open format, to extend its usefulness beyond the MuJoCo ecosystem.\nBiomechanical simulation.\n MuJoCo includes two powerful features that support musculoskeletal models of humans and animals. Spatial tendon routing, including wrapping around bones, means that applied forces can be distributed correctly to the joints, describing complicated effects like the variable \nmoment-arm\n in the \nknee enabled by the tibia\n. MuJoCo\u2019s muscle model captures the complexity of biological muscles, including activation states and force-length-velocity curves.\nA \nrecent \nPNAS\n perspective\n exploring the state of simulation in robotics identifies open source tools as critical for advancing research. The authors\u2019 recommendations are to develop and validate open source simulation platforms as well as to establish open and community-curated libraries of validated models. In line with these aims, we\u2019re committed to developing and maintaining MuJoCo as a free, open-source, community-driven project with best-in-class capabilities. We\u2019re currently hard at work preparing MuJoCo for full open sourcing, and we encourage you to download the software from the \nnew homepage\n and visit the \nGitHub repository\n if you'd like to contribute. \nEmail us\n if you have any questions or suggestions, and if you're also excited to push the boundaries of realistic physics simulation, \nwe're hiring\n. We can\u2019t promise we\u2019ll be able to address everything right away, but we\u2019re eager to work together to make MuJoCo the physics simulator we\u2019ve all been waiting for.\nMuJoCo in DeepMind. \nOur robotics team has been using MuJoCo as a simulation platform for various projects, mostly via our \ndm_control\n Python stack. In the carousel below, we highlight a few examples to showcase what can be simulated in MuJoCo. Of course, these clips represent only a tiny fraction of the vast possibilities for how researchers might use the simulator. For higher quality versions of these clips, please click \nhere\n.\n"}
{"title": "The Podcast: Episode 6: AI for everyone", "contents": "While there is a lot of excitement about AI research, there are also concerns about the way it might be implemented, used and abused.\nIn this episode Hannah investigates the more human side of the technology, some ethical issues around how it is developed and used, and the efforts to create a future of AI that works for everyone. \nInterviewees: \nVerity Harding, Co-Lead of DeepMind Ethics and Society; DeepMind\u2019s COO Lila Ibrahim, and research scientists William Isaac and Silvia Chiappa.\nListen to this episode and subscribe to the whole series on \nApple podcasts\n,\n Google podcasts\n,\n Spotify\n,\n Deezer\n or your favourite podcast app by searching for \u201cDeepMind: The Podcast\u201d.\nIf you know of other resources we should link to, please help other listeners by either replying to us on \nTwitter\n (#DMpodcast) or emailing us at podcast@deepmind.com. You can also use that address to send us questions or feedback on the series.\nPresenter: Hannah Fry\nEditor: David Prest\nSenior Producer: Louisa Field\nProducers: Amy Racs, Dan Hardoon\nBinaural Sound: Lucinda Mason-Brown\nMusic composition: Eleni Shaw (with help from Sander Dieleman and \nWaveNet\n)\n"}
{"title": "The Podcast: Episode 7: Towards the future", "contents": "AI researchers around the world are trying to create a general purpose learning system that can learn to solve a broad range of problems without being taught how.\nKoray Kavukcuoglu, DeepMind\u2019s Director of Research, describes the journey to get there, and takes Hannah on a whistle-stop tour of DeepMind\u2019s HQ and its research.\nInterviewees:\n Koray Kavukcuoglu, Director of Research; Trevor Back, Product Manager for DeepMind\u2019s science research; research scientists Raia Hadsell and Murray Shanahan; and DeepMind CEO and co-founder, Demis Hassabis.\nListen to this episode and subscribe to the whole series on\n Apple podcasts\n,\n Google podcasts\n,\n Spotify\n,\n Deezer\n or your favourite podcast app by searching for \u201cDeepMind: The Podcast\u201d.\nIf you know of other resources we should link to, please help other listeners by either replying to us on \nTwitter\n (#DMpodcast) or emailing us at podcast@deepmind.com. You can also use that address to send us questions or feedback on the series.\nPresenter: Hannah Fry\nEditor: David Prest\nSenior Producer: Louisa Field\nProducers: Amy Racs, Dan Hardoon\nBinaural Sound: Lucinda Mason-Brown\nMusic composition: Eleni Shaw (with help from Sander Dieleman and \nWaveNet\n)\n"}
{"title": "Replay in biological and artificial neural networks", "contents": "One of a series of posts explaining the theories underpinning our research.\nOur waking and sleeping lives are punctuated by fragments of recalled memories: a sudden connection in the shower between seemingly disparate thoughts, or an ill-fated choice decades ago that haunts us as we struggle to fall asleep. By measuring memory retrieval directly in the brain, neuroscientists have noticed something remarkable: spontaneous recollections, measured directly in the brain, often occur as very fast \nsequences\n of multiple memories. These so-called 'replay' sequences play out in a fraction of a second\u2013so fast that we're not necessarily aware of the sequence.\nIn parallel, AI researchers discovered that incorporating a similar kind of \nexperience replay\n improved the efficiency of learning in artificial neural networks. Over the last three decades, the AI and neuroscientific studies of replay have grown up together. Machine learning offers hypotheses sophisticated enough to push forward our expanding knowledge of the brain; and insights from neuroscience guide and inspire AI development. Replay is a key point of contact between the two fields because like the brain, AI uses experience to learn and improve. And each piece of experience offers much more potential for learning than can be absorbed in real-time\u2013so continued offline learning is crucial for both brains and artificial neural nets.\nNeural replay sequences were originally discovered by studying the hippocampus in rats. As we know from the Nobel prize winning work of \nJohn O\u2019Keefe\n and others, many hippocampal cells fire only when the animal is physically located in a specific \nplace\n. In early experiments, \nrats\n ran the length of a single corridor or circular track, so researchers could easily determine which neuron coded for each position within the corridor.\nAfterwards, the scientists recorded from the same neurons while the rats rested. During rest, the cells sometimes spontaneously fired in rapid sequences demarking the same path the animal ran earlier, but at a greatly accelerated speed. These sequences are called replay. An entire replay sequence only lasts a fraction of a second, but plays through several seconds worth of real experience.\nWe now know replay is essential for learning. In a number of more recent experiments, researchers recorded from hippocampus to detect a signature of replay events in real time. By disrupting brain activity during replay events (either during \nsleep\n or \nwakeful\n resting), scientists significantly impaired rodents\u2019 ability to learn a new task. The same disruption applied 200 milliseconds out of sync with replay events had no effect on learning.\nWhile these experiments have been revealing, a significant limitation of rodent experiments is the difficulty of studying more sophisticated aspects of cognition, such as abstract concepts. In the last few years, replay-like sequences have also been \ndetected\n in human brains, supporting the idea that replay is pervasive, and expanding the kinds of questions we can ask about it.\nIncorporating replay\n in silico \nhas been beneficial to advancing \nartificial intelligence\n. Deep learning often depends upon a ready supply of large datasets. In \nreinforcement learning\n, these data come through direct interaction with the environment, which takes time. The technique of \nexperience\n \nreplay\n allows the agent to repeatedly rehearse previous interactions, making the most of each interaction. This method proved crucial for combining deep neural networks with reinforcement learning in the \nDQN\n agent that first mastered multiple Atari games.\nSince the introduction of DQN, the efficiency of replay has been improved by \npreferentially\n \nreplaying\n \nthe\n most salient experiences from memory, rather than simply choosing experiences at random for replay. And recently, a variant of preferential replay has been \napplied\n as a model in neuroscience to successfully explain empirical data from brain recordings.\nFurther improvements in agent performance have come from \ncombining experiences\n across multiple agents, learning about a variety of \ndifferent behaviours\n from the same set of experiences, and replaying not only the trajectory of events in the world, but also the agent's corresponding \ninternal memory states\n. Each of these methods makes interesting predictions for neuroscience that remain largely untested.\nAs mentioned above, research into experience replay has unfolded along parallel tracks in artificial intelligence and neuroscience, with each field providing ideas and inspiration for the other. In particular, there is a central distinction, which has been studied in both fields, between two versions of replay.\nSuppose you come home and, to your surprise and dismay, discover water pooling on your beautiful wooden floors. Stepping into the dining room, you find a broken vase. Then you hear a whimper, and you glance out the patio door to see your dog looking very guilty.\nIn the first version of replay, which we could call the \"movie\" version, when you sit down on the couch and take a rest, replay faithfully rehearses the actual experiences of the past. This theory says that your brain will replay the sequence: \"water, vase, dog\". In AI terms, the past experience was stored in a replay buffer, and trajectories for offline learning were drawn directly from the buffer.\n\u200d\nIn the second version, which we might call \"imagination,\" replay doesn\u2019t literally rehearse events in the order they were experienced. Instead, it infers or imagines the \nreal relationships\n between events, and synthesises sequences that make sense given an understanding of how the world works. In AI terms, these replay sequences are \ngenerated\n using a \nlearned\n \nmodel\n of the environment.\nThe imagination theory makes a different prediction about how replay will look: when you rest on the couch, your brain should replay the sequence \"dog, vase, water\". You know from past experience that dogs are more likely to cause broken vases than broken vases are to cause dogs\u2013and this knowledge can be used to reorganise experience into a more meaningful order.\nIn deep RL, the large majority of agents have used movie-like replay, because it is easy to implement (the system can simply store events in memory, and play them back as they happened). However, RL researchers have continued to study the possibilities around imagination replay.\nMeanwhile in neuroscience, classic theories of replay postulated that movie replay would be useful to strengthen the connections between neurons that represent different events or locations in the order they were experienced. However, there have been \nhints\n from experimental neuroscience that replay might be able to imagine new sequences. The most compelling \nobservation\n is that even when rats only experienced two arms of a maze separately, subsequent replay sequences sometimes followed trajectories from one arm into the other.\nBut studies like this leave open the question of whether replay simply stitches together chunks of experienced sequences, or if it can synthesise new trajectories from whole cloth. Also, rodent experiments have been primarily limited to \nspatial\n sequences, but it would be fascinating to know whether humans' ability to imagine sequences is enriched by our vast reserve of abstract \nconceptual\n knowledge.\nWe asked these questions in a set of recent \nexperiments\n performed jointly between UCL, Oxford, and DeepMind.\nIn these experiments, we first taught people a rule that defined how a set of objects could interact. The exact rule we used can be found in the paper. But to continue in the language of the \"water, vase, dog\" example, we can think of the rule as the knowledge that dogs can cause broken vases, and broken vases can cause water on the floor. We then presented these objects to people in a \nscrambled\n order (like \"water, vase, dog\"). That way, we could ask whether their brains replayed the items in the scrambled order that they experienced, or in the unscrambled order that meaningfully connected the items. They were shown the scrambled sequence and then given five minutes to rest, while sitting in an \nMEG\n brain scanner.\nAs in previous experiments, fast replay sequences of the objects were evident in the brain recordings. (In yet another example of the \nvirtuous circle\n between neuroscience and AI, we used machine learning to read out these signatures from cortical activity.) These spontaneous sequences played out rapidly over about a sixth of a second, and contained up to four objects in a row. However, the sequences did not play out in the \nexperienced\n order (i.e., the scrambled order: spilled water \u2013> vase \u2013> dog). Instead, they played out the \nunscrambled\n, meaningful order: dog \u2013> vase \u2013> spilled water. This answers\u2013in the affirmative\u2013the questions of whether replay can imagine new sequences from whole cloth, and whether these sequences are shaped by abstract knowledge.\nHowever, this finding still leaves open the important question of \nhow\n the brain builds these unscrambled sequences. To try to answer this, we played a second sequence for participants. In this sequence, you walk into your factory and see spilled oil on the floor. You then see a knocked over oil barrel. Finally, you turn to see a guilty robot. To unscramble this sequence, you can use the same kind of knowledge as in the \"water, vase, dog\" sequence: knowledge that a mobile agent can knock over containers, and those knocked-over containers can spill liquid. Using that knowledge, the second sequence can also be unscrambled: robot \u2013> barrel \u2013> spilled oil.\nBy showing people multiple sequences with the same structure, we could examine two new types of neural representation. First, the part of the representation that is \ncommon\n between spilled water and spilled oil. This is an \nabstract\n code for \"a spilled liquid\", invariant over whether we're in the home sequence or the factory sequence. And second, the part of the representation that is common between water, vase and dog. This is an abstract code for \"the home sequence,\" invariant over which object we're considering.\nWe found both of these types of abstract codes in the brain data. And to our surprise, during rest they played out in fast sequences that were precisely coordinated with the spontaneous replay sequences mentioned above. Each object in a replay sequence was preceded slightly by both abstract codes. For example, during a dog, vase, water replay sequence, the representation of \"water\" was preceded by the codes for \"home sequence\" and \"spilled liquid\".\nThese abstract codes, which incorporate the conceptual knowledge that lets us unscramble the sequences, may help the brain to retrieve the correct item for the next slot in the replay sequence. This paints an interesting picture of a system where the brain slots new information into an \nabstract\n \nframework\n \nbuilt\n from past experiences, keeping it organized using precise relative timings within very fast replay sequences. Each position within a sequence could be thought of as a role in an \nanalogy\n (as in the above figure). Finally, we speculate that during rest, the brain may explore novel implications of previously-learned knowledge by placing an item into an analogy in which it's never been experienced, and examining the consequences.\nComing back to the virtuous circle, analogy and abstraction are relatively underused in current neural network architectures. The new results described above both indicate that the imagination style of replay may be a fruitful avenue to continue pursuing in AI research, and suggest directions for neuroscience research to learn more about the brain mechanisms underlying analogy and abstraction. It's exciting to think about how data from the brain will continue helping with the advance toward better and more human-like artificial intelligence.\nThe \nnew work reported in Cell\n was done by \nYunzhe Liu\n, \nRay Dolan\n, \nZeb Kurth-Nelson\n and \nTim Behrens\n, and was a collaboration between DeepMind, the Wellcome Centre for Human Neuroimaging (UCL), the Max Planck-UCL Centre for Computational Psychiatry and Ageing Research, and the Wellcome Centre For Integrative Neuroimaging (Oxford).\n"}
{"title": "The Podcast: Episode 8: Demis Hassabis - The interview", "contents": "In this special extended episode, Hannah Fry meets Demis Hassabis, the CEO and co-founder of DeepMind.\nShe digs into his former life as a chess player, games designer and neuroscientist and explores how his love of chess helped him to get start-up funding, what drives him and his vision, and why AI keeps him up at night. \nInterviewees: \nDeepmind CEO and co-founder, Demis Hassabis\nListen to this episode and subscribe to the whole series on\n Apple podcasts\n,\n Google podcasts\n,\n Spotify\n,\n Deezer\n or your favourite podcast app by searching for \u201cDeepMind: The Podcast\u201d.\nIf you know of other resources we should link to, please help other listeners by either replying to us on \nTwitter\n (#DMpodcast) or emailing us at podcast@deepmind.com. You can also use that address to send us questions or feedback on the series.\nPresenter: Hannah Fry\nEditor: David Prest\nSenior Producer: Louisa Field\nProducers: Amy Racs, Dan Hardoon\nBinaural Sound: Lucinda Mason-Brown\nMusic composition: Eleni Shaw (with help from Sander Dieleman and \nWaveNet\n)\n"}
{"title": "DeepMind\u2019s health team joins Google Health", "contents": "Over the last three years, DeepMind has built a team to tackle some of healthcare\u2019s most complex problems\u2014developing AI research and mobile tools that are already having a positive impact on patients and care teams. Today, with our healthcare partners, the team is excited to officially join the \nGoogle Health\n family. Under the leadership of \nDr. David Feinberg\n, and alongside other teams at Google, we\u2019ll now be able to tap into global expertise in areas like app development, data security, cloud storage and user design to build products that support care teams and improve patient outcomes. \nDuring my time working in the UK National Health Service (NHS) as a surgeon and researcher, I saw first-hand how technology could help, or hinder, the important work of nurses and doctors. It\u2019s remarkable that many frontline clinicians, even in the most world\u2019s most advanced hospitals, are still reliant on clunky desktop systems and pagers that make delivering fast and safe patient care challenging. Thousands of people die in hospitals every year from avoidable conditions like sepsis and acute kidney injury and we believe that better tools could save lives. That\u2019s why I joined DeepMind, and why I will continue this work with Google Health. \nWe\u2019ve already seen how our \nmobile medical assistant\n for clinicians is helping patients and the clinicians looking after them, and we are looking forward to continuing our partnerships with The Royal Free London NHS Foundation Trust, Imperial College Healthcare NHS Trust and Taunton and Somerset NHS Foundation Trust.\nOn the research side, we\u2019ve seen major advances with Moorfields Eye Hospital NHS Foundation Trust \nin detecting eye disease from scans \nas accurately as experts; with University College London Hospitals NHS Foundation Trust on planning \ncancer radiotherapy treatment\n; and with the US Department of Veterans Affairs to \npredict patient deterioration\n up to 48 hours earlier than currently possible. We see enormous potential in continuing, and scaling, our work with all three partners in the coming years as part of Google Health. \nIt\u2019s clear that a \ntransition like this\n takes time. Health data is sensitive, and we gave proper time and care to make sure that we had the full consent and cooperation of our partners. This included giving them the time to ask questions and fully understand our plans and to choose whether to continue our partnerships. As has always been the case, our partners are in full control of all patient data and we will only use patient data to help improve care, under their oversight and instructions.\nI know DeepMind is proud of our healthcare work to date. With the expertise and reach of Google behind us, we\u2019ll now be able to develop tools and technology capable of helping millions of patients around the world. \nThis blog also appears on \nThe Keyword\n"}
{"title": "Causal Bayesian Networks: A flexible tool to enable fairer machine learning", "contents": "Decisions based on machine learning (ML) are potentially advantageous over human decisions, as they do not suffer from the same subjectivity, and can be more accurate and easier to analyse. At the same time, data used to train ML systems often contain human and societal biases that can lead to harmful decisions: extensive evidence in areas such as hiring, criminal justice, surveillance, and healthcare suggests that ML decision systems can treat individuals unfavorably (\nunfairly\n) on the basis of characteristics such as race, gender, disabilities, and sexual orientation \u2013 referred to as \nsensitive attributes\n. \u00a0\nCurrently, most fairness criteria used for evaluating and designing ML decision systems focus on the relationships between the sensitive attribute and the system output. However, the training data can display different patterns of unfairness depending on how and why the sensitive attribute influences other variables. Using such criteria without fully accounting for this could be problematic: it could, for example, lead to the erroneous conclusion that a model exhibiting harmful biases is fair and, vice-versa, that a model exhibiting harmless biases is unfair. The development of technical solutions to fairness also requires considering the different, potentially intricate, ways in which unfairness can appear in the data. \nUnderstanding how and why a sensitive attribute influences other variables in a dataset can be a challenging task, requiring both a technical and sociological analysis. The visual, yet mathematically precise, framework of \nCausal\n \nBayesian\n \nnetworks\n (CBNs) represents a flexible useful tool in this respect as it can be used to formalize, measure, and deal with different unfairness scenarios underlying a dataset. A CBN (Figure 1) is a graph formed by nodes representing random variables, connected by links denoting causal influence. By defining unfairness as the presence of a harmful influence from the sensitive attribute in the graph, CBNs provide us with a simple and intuitive \nvisual tool\n for describing different possible unfairness scenarios underlying a dataset. In addition, CBNs provide us with a powerful \nquantitative tool \nto measure unfairness in a dataset and to help researchers develop techniques for addressing it. \nConsider a hypothetical college admission example (\ninspired by the Berkeley case\n) in which applicants are admitted based on qualifications Q, choice of department D, and gender G; and in which female applicants apply more often to certain departments (for simplicity\u2019s sake, we consider gender as binary, but this is not a necessary restriction imposed by the framework). \nDefinition: In a CBN, a path from node X to node Z is defined as a sequence of linked nodes starting at X and ending at Z. X is a cause of (has an influence on) Z if there exists a causal path from X to Z, namely a path whose links are pointing from the preceding nodes toward the following nodes in the sequence. For example, in Figure 1, the path G\u2192D\u2192A is causal, whilst the path G\u2192D\u2192A\u2190Q is non causal.\nThe admission process is represented by the CBN in Figure 1. Gender has a direct influence on admission through the causal path G\u2192A and an indirect influence through the causal path G\u2192D\u2192A. The direct influence captures the fact that individuals with the same qualifications who are applying to the same department might be treated differently based on their gender. The indirect influence captures differing admission rates between female and male applicants due to their differing department choices. \nWhilst the direct influence of the sensitive attribute on admission is considered unfair for social and legal reasons, the indirect influence could be considered fair or unfair depending on contextual factors. In Figure 2a, 2b and 2c, we depict three possible scenarios, where total or partial red paths are used to indicate unfair and and partially-unfair links, respectively.\nThis simplified example shows how CBNs can provide us with a visual framework for describing different possible unfairness scenarios. Understanding which scenario underlies a dataset can be challenging or even impossible, and might require expert knowledge. It is nevertheless necessary to avoid pitfalls when evaluating or designing a decision system. \nAs an example, let\u2019s assume that a university uses historical data to train a decision system to decide whether a prospective applicant should be admitted, and that a regulator wants to evaluate its fairness. Two popular fairness criteria are \nstatistical parity \n(requiring the same \nadmission rates \namong female and male applicants) and \nequal false positive or negative rates\n (EFPRs/EFNRs, requiring the same \nerror rates \namong female and male applicants: i.e., the percentage of accepted applicants erroneously predicted as rejected, and vice-versa). In other words, statistical parity and EFPRs/EFNRs require all the predictions and the incorrect predictions to be independent of gender.\nFrom the discussion above, we can deduce that whether such criteria are appropriate or not strictly depends on the nature of the data pathways. Due to the presence of the unfair direct influence of gender on admission, it would be inappropriate for the regulator to use EFPRs/EFNRs to gauge fairness, because this criterion considers the influence that gender has on admission in the data as legitimate. This means that it would be possible for the system to be deemed fair, even if it carries the unfair influence: this would automatically be the case for an error-free decision system. On the other hand, if the path G\u2192D\u2192A was considered fair, it would be inappropriate to use statistical parity. In this case, it would be possible for the system to be deemed unfair, even if it does not contain the unfair direct influence of gender on admission through the path G\u2192A and only contains the fair indirect influence through the path G\u2192D\u2192A. In \nour first paper\n, we raise these concerns in the context of the fairness debate surrounding the COMPAS pretrial risk assessment tool, which has been central to the dialogue around the risks of using ML decision systems.\nCBNs can also be used to quantify unfairness in a dataset and to design techniques for alleviating unfairness in the case of complex relationships in the data. \nPath-specific techniques enable us to estimate the influence that a sensitive attribute has on other variables along specific sets of causal paths. This can be used to measure the degree of unfairness on a given dataset in complex scenarios in which some causal paths are considered unfair whilst other causal paths are considered fair. In the college admission example in which the path G\u2192D\u2192A is considered fair, path-specific techniques would enable us to measure the influence of G on A restricted to the direct path G\u2192A over the whole \npopulation\n, in order to obtain an estimate of the degree of unfairness contained in the dataset.\nSidenote: It's worth noting that, in our simple example, we do not consider the presence of confounders for the influence of G on A. In this case, as there are no unfair causal paths from G to A except the direct one, the degree of unfairness could simply be obtained by measuring the discrepancy between p(A | G=0, Q, D) and p(A | G=1, Q, D), where p(A | G=0, Q, D) indicates the distribution of A conditioned on the candidate being male, their qualifications, and department choice.\nThe additional use of counterfactual inference techniques would enable us to ask if a specific\n individual\n was treated unfairly, for example by asking whether a rejected female applicant (G=1, Q=q, D=d, A=0) would have obtained the same decision in a counterfactual world in which her gender were male along the direct path G\u2192A. In this simple example, assuming that the admission decision is obtained as the deterministic function f of G, Q, and D, i.e., A = f(G, Q, D), this corresponds to asking if f(G=0, Q=q, D=d) = 0, namely if a male applicant with the same department choice and qualifications would have also been rejected. We exemplify this in Figure 3 by re-computing the admission decision after changing the female candidate's photo to a male one in the profile. \nHowever, path-specific counterfactual inference is generally more complex to achieve, if some variables are unfairly influenced by G. Assume that G also has an influence on Q through a direct path G\u2192Q which is considered unfair. In this case, the CBN contains both variables that are fairly and unfairly influenced by G. Path-specific counterfactual inference would consist in performing a counterfactual correction of q, q_0, i.e of the variable which is unfairly influenced by G, and then computing the counterfactual decision as f(G=0, Q=q_0, D=d). The counterfactual correction q_0 is obtained by first using the information of the female applicant (G=1, Q=q, D=d, A=0) and knowledge about the CBN to get an estimate of the specific latent randomness in the makeup of the applicant, and then using this estimate to re-compute the value of Q as if G=0 along G\u2192Q. \nIn addition to answering questions of fairness in a dataset, path-specific counterfactual inference could be used to design methods to alleviate the unfairness of an ML system. In our \nsecond paper\n, we propose a method to perform path-specific counterfactual inference and suggest that it can be used to post-process the unfair decisions of a trained ML system by replacing them with counterfactual decisions. The resulting system is said to satisfy path-specific counterfactual fairness.\nAs machine learning continues to be embedded in more systems which have a significant impact on people\u2019s lives and safety, it is incumbent on researchers and practitioners to identify and address potential biases embedded in how training data sets are generated. Causal Bayesian Networks offer a powerful visual and quantitative tool for expressing the relationships among random variables in a dataset. While it is important to acknowledge the limitations and difficulties of using this tool \u2013 such as identifying a CBN that accurately describes the dataset\u2019s generation, dealing with confounding variables, and performing counterfactual inference in complex settings \u2013 this unique combination of capabilities could enable a deeper understanding of complex systems and allow us to better align decision systems with society's values.\nThis post is based on the following papers: \nA Causal Bayesian Networks Viewpoint on Fairness\nPath-Specific Counterfactual Fairness\nWith thanks to Niki Kilbertus, Ben Coppin, Tom Stepleton, \u00a0Ray Jiang, Christina Heinze-Deml, Tom Everitt, and Shira Mitchell.\n"}
{"title": "AlphaStar: Grandmaster level in StarCraft II using multi-agent reinforcement learning", "contents": "TL;DR: \nAlphaStar is the first AI to reach the top league of a widely popular esport without any game restrictions. \nThis January\n, a preliminary version of AlphaStar challenged two of the world's top players in StarCraft II, one of the most enduring and popular real-time strategy video games of all time. Since then, we have taken on a much greater challenge: playing the full game at a Grandmaster level under professionally approved conditions. \nWe chose to use general-purpose machine learning techniques \u2013 including neural networks, self-play via reinforcement learning, multi-agent learning, and imitation learning \u2013 to learn directly from game data with general purpose techniques. Using the advances described in our \nNature paper\n, AlphaStar was ranked above 99.8% of active players on Battle.net, and achieved a Grandmaster level for all three StarCraft II races: Protoss, Terran, and Zerg. We expect these methods could be applied to many other domains.\nLearning-based systems and self-play are elegant research concepts which have facilitated remarkable advances in artificial intelligence. In 1992, researchers at IBM developed TD-Gammon, combining a learning-based system with a neural network to play the game of backgammon. Instead of playing according to hard-coded rules or heuristics, TD-Gammon was designed to use \nreinforcement learning\n to figure out, through trial-and-error, how to play the game in a way that maximises its probability of winning. Its developers used the notion of \nself-play \nto make the system more robust: by playing against versions of itself, the system grew increasingly proficient at the game. When combined, the notions of learning-based systems and self-play provide a powerful paradigm of open-ended learning.\nMany advances since then have demonstrated that these approaches can be scaled to progressively challenging domains. For example, AlphaGo and AlphaZero established that it was possible for a system to \nlearn\n to achieve superhuman performance at Go, chess, and shogi, and \nOpenAI Five\n and \nDeepMind\u2019s FTW\n demonstrated the power of self-play in the modern games of Dota 2 and Quake III. \nAt DeepMind, we\u2019re interested in understanding the potential \u2013 and limitations \u2013 of open-ended learning, which enables us to develop robust and flexible agents that can cope with complex, real-world domains. Games like StarCraft are an excellent training ground to advance these approaches, as players must use limited information to make dynamic and difficult decisions that have ramifications on multiple levels and timescales. \nDespite its successes, self-play suffers from well known drawbacks. The most salient one is forgetting: an agent playing against itself may keep improving, but it also may forget how to win against a previous version of itself. Forgetting can create a cycle of an agent \u201cchasing its tail\u201d, and never converging or making real progress. For example, in the game rock-paper-scissors, an agent may currently prefer to play rock over other options. As self-play progresses, a new agent will then choose to switch to paper, as it wins against rock. Later, the agent will switch to scissors, and eventually back to rock, creating a cycle. Fictitious self-play - playing against a mixture of all previous strategies - is one solution to cope with this challenge.\nAfter first \nopen-sourcing StarCraft II\n as a research environment, we found that even fictitious self-play techniques were insufficient to produce strong agents, so we set out to develop a better, general-purpose solution. A central idea of our recently published Nature paper extends the notion of fictitious self-play to a group of agents \u2013 the League. Normally in self-play, every agent maximises its probability of winning against its opponents; however, this was only part of the solution. In the real world, a player trying to improve at StarCraft may choose to do so by partnering with friends so that they can train particular strategies. As such, their training partners are not playing to win against every possible opponent, but are instead exposing the flaws of their friend, to help them become a better and more robust player. The key insight of the League is that playing to win is insufficient: instead, we need both main agents whose goal is to win versus everyone, and also exploiter agents that focus on helping the main agent grow stronger by exposing its flaws, rather than maximising their own win rate against all players. Using this training method, the League learns all its complex StarCraft II strategy in an end-to-end, fully automated fashion.\nExploration is another key challenge in complex environments such as StarCraft. There are up to 1026 possible actions available to one of our agents at each time step, and the agent must make thousands of actions before learning if it has won or lost the game. Finding winning strategies is challenging in such a massive solution space. Even with a strong self-play system and a diverse league of main and exploiter agents, there would be almost no chance of a system developing successful strategies in such a complex environment without some prior knowledge. Learning human strategies, and ensuring that the agents keep exploring those strategies throughout self-play, was key to unlocking AlphaStar\u2019s performance. To do this, we used imitation learning \u2013 combined with advanced neural network architectures and techniques used for language modelling \u2013 to create an initial policy which played the game better than 84% of active players. We also used a latent variable which conditions the policy and encodes the distribution of opening moves from human games, which helped to preserve high-level strategies. AlphaStar then used a form of distillation throughout self-play to bias exploration towards human strategies. This approach enabled AlphaStar to represent many strategies within a single neural network (one for each race). During evaluation, the neural network was not conditioned on any specific opening moves.\nIn addition, we found that many prior approaches to reinforcement learning are ineffective in StarCraft, due to its enormous action space. In particular, AlphaStar uses a new algorithm for off-policy reinforcement learning, which allows it to efficiently update its policy from games played by an older policy.\nOpen-ended learning systems that utilise learning-based agents and self-play have achieved impressive results in increasingly challenging domains. Thanks to advances in imitation learning, reinforcement learning, and the League, we were able to train AlphaStar Final, an agent that reached Grandmaster level at the full game of StarCraft II without any modifications, as shown in the above video. This agent played online anonymously, using the gaming platform Battle.net, and achieved a Grandmaster level using all three StarCraft II races. AlphaStar played using a camera interface, with similar information to what human players would have, and with restrictions on its action rate to make it comparable with human players. The interface and restrictions were approved by a professional player. Ultimately, these results provide strong evidence that general-purpose learning techniques can scale AI systems to work in complex, dynamic environments involving multiple actors. The techniques we used to develop AlphaStar will help further the safety and robustness of AI systems in general, and, we hope, may serve to advance our research in real-world domains.\nRead more about this work\n in Nature\nPublicly available paper\n here\nView all\n Battle.net game replays\nAlphaStar team:\nOriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Micha\u00ebl Mathieu, Andrew Dudzik, Junyoung Chung, David H. Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, John P. Agapiou, Max Jaderberg, Alexander S. Vezhnevets, R\u00e9mi Leblond, Tobias Pohlen, Valentin Dalibard, David Budden, Yury Sulsky, James Molloy, Tom L. Paine, Caglar Gulcerhe, Ziyu Wang, Tobias Pfaff, Yuhuai Wu, Roman Ring, Dani Yogatama, Dario W\u00fcnsch, Katrina McKinney, Oliver Smith, Tom Schaul, Timothy Lillicrap, Koray Kavukcuoglu, Demis Hassabis, Chris Apps, David Silver\nAcknowledgements:\nWe\u2019re grateful to\n Dario W\u00fcnsch (TLO)\n ,\n Grzegorz Komincz (MaNa)\n, and\n Diego Schwimer (Kelazhur)\n for their advice, guidance, and immense skill. We are also grateful for the continued support of Blizzard and the StarCraft gaming and AI community for making this work possible\u2013especially those who played against AlphaStar on Battle.net. Thanks to Ali Razavi, Daniel Toyama, David Balduzzi, Doug Fritz, Eser Ayg\u00fcn, Florian Strub, Guillaume Alain, Haoran Tang, Jaume Sanchez, Jonathan Fildes, Julian Schrittwieser, Justin Novosad, Karen Simonyan, Karol Kurach, Philippe Hamel, \u00a0Ricardo Barreira, Scott Reed, Sergey Bartunov, Shibl Mourad, Steve Gaffney, Thomas Hubert, the\n team that created PySC2,\n and the whole DeepMind team, with special thanks to the research platform team, comms and events teams.\n*Agents were capped at a max of 22 agent actions per 5 seconds, where one agent action corresponds to a selection, an ability and a target unit or point, which counts as up to 3 actions towards the in-game APM counter. Moving the camera also counts as an agent action, despite not being counted towards APM.\n"}
{"title": "Advanced machine learning helps Play Store users discover personalised apps", "contents": "Over the past few years we've applied DeepMind's technology to Google products and infrastructure, with notable successes like reducing the amount of energy needed for \ncooling data centers\n, and extending \nAndroid battery performance\n. We're excited to share more about our work in the coming months.\nWe know users get the most out of their phone when they have apps and games they love, and that it\u2019s exciting to discover new favourites. In collaboration with Google Play, \nour team that leads on collaborations with Google\n has driven significant improvements in the Play Store's discovery systems, helping to deliver a more personalised and intuitive Play Store experience for users. \nEvery month, billions of users come to the \nGoogle Play Store\n to download apps for their mobile devices \u2013 the Play Store supports one of the largest recommendation systems in the world. \u00a0While some are looking for specific apps, like Snapchat, others are browsing the store to discover what\u2019s new and interesting. The Google Play discovery team strives to help users discover the most relevant apps and games by providing them with helpful app recommendations. To deliver a richer, personalised experience, apps are suggested according to past user preferences. This, however, requires nuance \u2013 both for understanding what an app does, and its relevance to a particular user. \u00a0For example, to an avid sci-fi gamer, similar game recommendations may be of interest, but if a user installs a travel app, recommending a translation app may be more relevant than five more travel apps. The collection and use of these user preferences is governed by \nGoogle's privacy policies\n.\nWe started collaborating with the Play store to help develop and improve systems that determine the relevance of an app with respect to the user. \u00a0In this post, we\u2019ll explore some of the cutting-edge machine learning techniques we developed to achieve this. Today, Google Play\u2019s recommendation system contains three main models: a candidate generator, a reranker, and a model to optimise for multiple objectives. \u00a0The candidate generator is a deep retrieval model that can analyse more than a million apps and retrieve the most suitable ones. For each app, a reranker, i.e. a user preference model, predicts the user's preferences along multiple dimensions. Next these predictions are the input to a multi-objective optimisation model whose solution gives the most suitable candidates to the user.\nTo improve how Google Play\u2019s recommendation system learns users\u2019 preferences, our first approach was to use an \nLSTM (Long Short-Term Memory)\n model, a recurrent neural network that performs well in real-world scenarios, owing to a powerful update equation and backpropagation dynamics. Whilst the LSTM led to significant accuracy gains, it also introduced a serving delay, because LSTMs can be computationally expensive when processing long sequences. To address this, we \u00a0replaced the LSTM with \na Transformer model\n, which is well-equipped for sequence-to-sequence prediction and has previously yielded strong results in natural language processing, as it\u2019s able to capture longer dependencies between words than other commonly used models. The Transformer improved the model performance, but also increased the training cost. Our third and final solution was to implement an efficient additive attention model that works for any combination of sequence features, while incurring low computational cost. \nOur model (called a candidate generator) learns what apps a user is more likely to install based on previous \u00a0apps they\u2019ve installed from the Play store. However, this can introduce a recommendation bias problem. For instance, if app A is shown in the Play store 10 times more than app B, it\u2019s more likely to be installed by the user, and thus more likely to be recommended by our model. \u00a0The model therefore learns a bias that favours the apps that are shown \u2013 and thus installed \u2013 more often.\nTo help correct for this bias, we introduced importance weighting in our model. An importance weight is based on the impression-to-install rate of each individual app in comparison with the median impression-to-install rate across the Play store. \u00a0An app with a below-median install rate will have an importance weight less than one. However, even \u201cniche\u201d apps that are installed less frequently can have a high importance weight if their install rate is higher than the median rate. Through importance weighting, our candidate generator can downweight or upweight apps based on their install rates, which mitigates the recommendation bias problem.\nRecommendation systems often provide a range of possibilities to a user, and present them in an order with the best or most relevant options at the top. But how do we ensure the most relevant apps make it to the top of the list, so the user doesn\u2019t have to scroll for pages, or potentially miss the best option? Many recommendation systems treat the ranking problem as a binary classification problem, where the training data is labeled with either a positive or negative class, and the ranker learns to predict a probability from this binary label alone. However, this type of \u201cpointwise\u201d model \u2013 which only ranks one item at a time \u2013 fails to capture the context of how apps perform relative to one another. To deliver a better user experience, the ranker could predict the relative order of presented items based on the context of other candidate apps.\nOur solution to this, the reranker model, learns the relative importance of a pair of apps that have been shown to the user at the same time. We built our reranker model on a core insight: if a user is presented with two apps in the store, the app that the user chooses to \u00a0install is more relevant to the user than the app that they didn't install. We can then assign each of the pair a positive or negative label, and the model tries to minimise the number of inversions in ranking, thus improving the relative ranking of the apps. This kind of \u201cpairwise\u201d model works better in practice than pointwise models because predicting relative order is closer to the nature of ranking than predicting class labels or install probabilities.\nMany recommendation systems must optimise for multiple objectives at the same time, such as relevance, popularity, or personal preferences. We formulated the multi-objective optimisation problem as a constrained optimisation problem: the overall objective is to maximise the expected value of a primary metric, subject to constraints in terms of expected values of secondary metrics. During online serving, the objectives may shift according to user\u2019s needs \u2013 for example, a user that had previously been interested in housing search apps might have found a new flat, and so is now interested in home decor apps \u2013 so we worked toward a dynamic solution.\nRather than solving the problem offline and bringing a fixed model online, we solved this problem on-line, per-request, based on the actual values of the objectives during serving time. We define the constraints to be relative constraints, meaning we would like to improve the secondary objective by a percentage rather than an absolute value. This way, any shifts in the secondary objectives didn\u2019t affect our solver.\nThe algorithm that we developed can be used to find tradeoffs between a number of metrics. Finding suitable points along the tradeoff curve, our algorithm can significantly raise secondary metrics with only minor effects on the primary metric.\nOne of our key takeaways from this \ncollaboration\n is that when implementing advanced machine learning techniques for use in the real world, we need to work within many practical constraints. Because the Play Store and DeepMind teams worked so closely together and communicated on a daily basis, we were able to take product requirements and constraints into consideration throughout the algorithm design, implementation, and final testing phases, resulting in a more successful product.\nOur collaborations with Google have so far reduced the electricity needed for \ncooling Google\u2019s data centres\n by up to 30%, boosted the value of Google\u2019s \nwind energy\n by roughly 20%, and created on-device learning systems to optimise \nAndroid\n battery performance. \nWaveNet\n is now in the hands of Google Assistant and Google Cloud Platform users around the world, and \nour research collaboration\n with Waymo has helped improve the performance of its models, as well as the efficiency of training its neural networks.\nWorking at Google scale presents a unique set of research challenges, and the opportunity to take our breakthroughs beyond the lab to address global, complex challenges. If you\u2019re interested in working on applying cutting edge research to real world problems, learn more about the team that led this project \nhere\n.\nIn collaboration with: Dj Dvijotham, Amogh Asgekar, Will Zhou, Sanjeev Jagannatha Rao, Xueliang Lu, Carlton Chu, Arun Nair, Timothy Mann, Bruce Chia, Ruiyang Wu, Natarajan Chendrashekar, Tyler Brabham, Amy Miao, Shelly Bensal, Natalie Mackraz, Praveen Srinivasan & Harish Chandran\n"}
{"title": "Strengthening the AI community", "contents": "Most people have at least one crossroads moment in their life - when the choice they make defines their personal or professional trajectory. For me, it was being awarded an internship at Intel, the first one ever through Purdue\u2019s Co-Op Engineering program in 1990.\nAt first, I questioned whether the then little-known Intel, California, or this internship (on the Pentium processor) was the right choice for me. I just didn\u2019t know if I had the right technical skills for the work, or if engineering was really my path. But I took a leap of faith because I didn\u2019t want to waste the opportunity. That internship gave me a fantastic insight into the day to day work of engineers, including a chance to prove to myself that I could do engineering! It grew into a very successful 18-year career at Intel and a 25-year career in tech. At that moment, I could have easily said \u201cengineering isn\u2019t for me\u201d had I not had the nudge in the right direction, the vote of confidence, that this practical experience provided. Nearly 30 years later, I returned to Purdue last week, for their Distinguished Engineering Lectures programme, and had a chance to talk about the incredible career journey I\u2019ve been on - in the hopes of inspiring a new generation of engineers as to what is possible.\nSometimes that extra support is the difference between saying yes or saying no - between following a path in STEM, or doing something completely different. Whether it\u2019s inspiring self-confidence, offering reassurance or providing a financial safety net, showing support and removing the barriers that prevent individuals achieving their full potential can have a powerful impact. \nAt DeepMind we want to build advanced AI to expand our knowledge and find answers to some of the fundamental questions facing society. It is an ambitious and long-term goal, and we will only achieve it if we can bring people together with different experiences, knowledge, backgrounds. This is a field where diversity is paramount, not only for the innovative work that diverse teams produce, but because it\u2019s vital that we mitigate the risks of bias in the development of algorithms and applications. We need as many perspectives as possible to make sure the important questions are being asked when it matters. \nThe DeepMind scholarship programme is one way we seek to broaden participation in science and AI. It gives talented students from underrepresented backgrounds the support they need to study at leading universities, and connect with our researchers and engineers. Scholars get their Masters' fees paid in full, as well as guidance and support from personal DeepMind mentors. \nThis week we announced\n the renewal and expansion of our scholarship programme with the University College London. Four more DeepMind graduate scholarships for students wishing to pursue a master\u2019s degree in the Department of Computer Science will be available for students starting courses in 2020\u201321. But UCL is just one example. We also work with numerous other universities, such as \nOxford\n, \nQueen Mary University London\n, the \nUniversity of Cambridge\n and \nNYU\n, to broaden participation in AI and computer science. \nI\u2019ve seen many examples of the impact that diverse perspectives can have in practice. Take Shaquille Momoh, one of our DeepMind scholars, who was inspired to research protein folding prediction while studying at UCL. Nearly every function our body performs\u2014contracting muscles, sensing light, or turning food into energy\u2014can be traced back to proteins and how they move and change. \u00a0Predicting their structure is fundamental to understanding the body, as well as diagnosing and treating diseases believed to be caused by misfolded proteins. Shaquille had a specific motivation for studying protein folding. He wanted to better understand sickle-cell anaemia, a painful inherited condition much more prevalent in black communities \u2013 and for which there is no current cure. \nTo ensure the next generation of researchers reach their full potential, protecting and strengthening the research and teaching capacity of our academic institutions is vital too. \nDeepMind partners with a range of world-leading universities with the aim of extending research excellence and teaching capacity. We\u2019ve established academic chairs in machine learning at the \nUniversity of Alberta\n, \nUniversity College London\n, and the \nUniversity of Cambridge\n, offering unrestricted funding for world-renowned researchers to freely pursue their academic interests. These chairs will be supported in their research and teaching efforts by PhDs students. And many of our researchers hold dual affiliations, allowing them to continue teaching or supervising students at Cambridge, Oxford, Imperial, MIT, McGill and elsewhere (you can access some of these courses on \nYouTube\n). \nOnly by investing the right way across the ecosystem will we able to ensure the highest quality AI research that benefits everyone. It\u2019s also why we partner with charities like \nChess in Schools and Communities\n and In2Science, and have become founding partners of the \nDeep Learning Indaba\n in Africa, \u00a0\nKhipu AI\n in South America, the \nEastern European Machine Learning Summer School\n, the \nSoutheast Asia Machine Learning School (SEAMLS)\n, and the \nAI4Good Summer Lab\n in Canada. \nAnd to research how the lack of diversity affects the development of AI \u2013 how companies work, what products get built, and who benefits \u2013 last month \nwe announced\n a research fellowship with the \nPartnership on AI\n to explore the pervasive challenge of developing AI for the benefit of people and society.\nHistory has shown us that hard problems are best solved with collective effort. Innovation happens when people with different experiences, knowledge, and backgrounds join together, break down boundaries, openly share ideas and collaborate for a common goal. Building advanced AI responsibly may be one of the hardest scientific challenges to solve. If our sector can provide the right support for researchers and foster an open, collaborative and diverse academic culture, the impact could be truly transformative.\n"}
{"title": "From unlikely start-up to major scientific organisation: Entering our tenth year at DeepMind", "contents": "Since we started DeepMind nearly 10 years ago, our mission has been to unlock answers to the world\u2019s biggest questions by understanding and recreating intelligence itself.\nAs we approach the end of 2019, we\u2019ve come a long way in building the organisation we need to achieve this long-term mission - from our environment for research, to our collaborations with other Alphabet companies, to our increasingly interdisciplinary and diverse team.\nA mission this ambitious requires pioneering research on many fronts over many years. We\u2019ve been privileged to make some significant advances over the past twelve months, from \nwinning a major international contest \nto predict the shapes of proteins - the building blocks of life - at the end of 2018, to developing AI agents that cooperate with each other and people in our \nCapture the Flag\n paper published by Science, to our latest work on mastering the complex strategy game StarCraft II, which led to a \nNature cover article\n last month. \nBeyond the moments that capture widespread imagination, we have a broad programme of \nfundamental research\n with hundreds of papers published each year. Many are substantial and exciting contributions to their domains, such as \nMuZero\n, which can not only master chess, Go, Shogi but also extend to Atari, without knowing the rules first. We\u2019ve developed new ways to open up and explain our work, such as our dedicated \nAI safety research\n blog, thematic summaries of \u00a0research concepts such as \nunsupervised learning\n, and our many \nopen source\n projects, which bring new frameworks and environments to the field. \u00a0\nAs our research matures, we\u2019ve been finding more opportunities to partner with others for social and commercial impact, often with our colleagues across Alphabet. This year, we demonstrated how AI could predict potentially fatal \npatient deterioration\n two days before existing tests, used machine learning to accelerate ecological research in the \nSerengeti\n, collaborated with \nWaymo\n on evolutionary selection to train more capable self-driving cars, learned how to boost the value of Google\u2019s \nwind energy\n, \nhelped Play Store users\n discover more relevant apps, and more.\nAs I discussed with Wired\n in the summer, this year feels like the start of a new phase for DeepMind as an established scientific organisation. We\u2019re no longer the tiny start-up working in a very unfashionable area, as was the case in 2010. Fuelled by breakthroughs like \nDQN \nand \nAlphaGo\n, and many exciting advances from colleagues at other labs, AI is once again one of the most vibrant areas of scientific research. \nOver the past year, we\u2019ve also been formalising a leadership team with the seasoned experience and skills for our second decade. We want to ensure DeepMind continues to be the best place in the world for fundamental breakthroughs in AI, and that we conduct this work thoughtfully and responsibly. Much of this work is led by Chief Operating Officer Lila Ibrahim and VP of Research Koray Kavukcuoglu. \nLila joined DeepMind in April 2018\n following a distinguished, global career including a long stint at Intel where she started as an engineer and took on management roles including Chief of Staff to the CEO and Chairman, and then leadership roles at investment firm Kleiner Perkins and education startup Coursera. Koray is one of the foremost research scientists in AI and deep learning, and a key contributor to many of DeepMind\u2019s biggest breakthroughs, from the \nDQN system that mastered Atari\n in 2013 to the \nresearch and deployment of\n \nWaveNet\n, which improves the experience of Google users around the world.\nAs we enter this next phase, Mustafa Suleyman is leaving DeepMind. I founded DeepMind back in 2010 along with Shane Legg (our Chief Scientist) and Mustafa. As a serial entrepreneur, Mustafa played a key role over the past decade helping to get DeepMind off the ground, and launched a series of innovative collaborations with Google to \nreduce energy consumption in data centres\n, improve \nAndroid battery performance\n, \noptimise Google Play\n, and find ways to \nimprove the lives of patients, nurses and doctors\n alike. Mustafa leaves DeepMind having helped set us up for long-term success, and I\u2019m looking forward to what he\u2019ll achieve in the years ahead as he joins Google in a new role. \nOf course, there\u2019s a long way left to go for DeepMind and for the AI field overall, and many grand challenges remain. Right back to our origins blending neuroscience with machine learning, we\u2019ve found that breakthroughs happen faster when different disciplines come together. In our offices across the world - soon to include our incredible new \npurpose-built HQ\n in London - we recruit and develop brilliant people with backgrounds in research and engineering, program management, games design, operations, ethics and safety research and beyond. You can get to know just a fraction of our team by listening to the \nDeepMind podcast\n. Increasing the diversity of our workforce - and doing what we can to \nimprove access to science \namong under-represented groups - remains a personal and organisational priority.\nThank you to the hundreds of amazing colleagues who have made DeepMind what it is today, and to all those at Alphabet and in the wider AI community for your long-term support and collaboration. I can\u2019t wait to show you what we have coming up\u2026 all I can say for now is, keep your eyes peeled for some very exciting advances in 2020!\n"}
{"title": "Learning human objectives by evaluating hypothetical behaviours", "contents": "TL;DR: We present a method for training reinforcement learning agents from human feedback in the presence of unknown unsafe states.\nWhen we train reinforcement learning (RL) agents in the real world, we don\u2019t want them to explore \nunsafe states\n, such as driving a mobile robot into a ditch or writing an embarrassing email to one\u2019s boss. Training RL agents in the presence of unsafe states is known as the \nsafe exploration\n problem\n. We tackle the hardest version of this problem, in which the agent initially doesn\u2019t know how the environment works or where the unsafe states are. The agent has one source of information: feedback about unsafe states from a human user.\nExisting\n \nmethods\n \nfor\n \ntraining\n \nagents\n from human feedback ask the user to evaluate data of the agent acting in the environment. That is \u2013 in order to learn about unsafe states, the agent first needs to visit these states, so the user can provide feedback on them. This makes prior work inapplicable to tasks that require safe exploration.\nIn our \nlatest paper\n, we propose a method for \nreward modeling\n that operates in two phases. First, the system is encouraged to explore a wide range of states through synthetically-generated, hypothetical behaviour. The user provides feedback on this hypothetical behaviour, and the system interactively learns a model of the user's reward function. Only after the model has successfully learned to predict rewards and unsafe states, we deploy an RL agent that safely performs the desired task.\nWe start with a generative model of initial states and a forward dynamics model, trained on off-policy data like random trajectories or safe expert demonstrations. Our method uses these models to synthesise hypothetical behaviours, asks the user to label the behaviours with rewards, and trains a neural network to predict these rewards. The key idea is to actively synthesise the hypothetical behaviours from scratch to make them as informative as possible, \nwithout interacting with the environment\n. We call this method \nreward query synthesis via trajectory optimisation\n (ReQueST).\nFor this approach to work, we need the system to simulate and explore a wide range of behaviours, in order to effectively train the reward model. To encourage exploration during reward model training, ReQueST synthesises \nfour different types of hypothetical behaviours\n using gradient descent trajectory optimisation. The first type of hypothetical behaviour \nmaximises the uncertainty of an ensemble of reward models\n, eliciting user labels for behaviours that have the highest information value. The second type of hypothetical behaviour \nmaximises predicted rewards\n, surfacing behaviours for which the reward model might be incorrectly predicting high rewards; i.e., \nreward hacking\n. The third type of hypothetical behaviour \nminimises predicted rewards\n, adding potentially unsafe hypothetical behaviours to the training data. This data enables the reward model to learn about unsafe states. The fourth type of hypothetical behaviour \nmaximises the novelty of trajectories\n, encouraging exploration of a wide range of states, regardless of predicted rewards. \nEach hypothetical behaviour consists of a sequence of state transitions (s, a, s\u2019). We ask the user to label each state transition with a reward, r. Then, given the labeled dataset of transitions (s, a, r, s\u2019), we train a neural network to predict rewards using a maximum-likelihood objective. We use standard supervised learning techniques based on gradient descent.\nOnce the user is satisfied with the reward model, we deploy a planning-based agent that uses model-predictive control (MPC) to pick actions that optimise the learned rewards. Unlike \nmodel-free\n RL algorithms like Q-learning or policy gradient methods that learn through trial and error, \nmodel-based\n RL algorithms like MPC enable the agent to avoid unsafe states during deployment by using the dynamics model to anticipate the consequences of its actions.\nWe evaluate ReQueST with simulated users on a state-based 2D navigation task and the image-based Car Racing video game. Our results show that ReQueST satisfies three important safety properties: it can \ntrain a reward model to detect unsafe states without visiting them\n; it can \ncorrect reward hacking \nbefore deploying the agent; and it tends to learn robust reward models that\n perform well when transferred to new environments\n.\nTo test the generalisation of the reward model, we set up a 2D navigation task with separate training and test environments.\nWe intentionally introduce a significant shift in the initial state distribution: the agent starts at the lower left corner (0, 0) in the training environment, and at the upper right corner (1, 1) in the test environment. Prior methods that collect data by deploying an agent in the training environment are unlikely to learn about the trap in the upper right corner, because they immediately find the goal, then fail to continue exploring. ReQueST synthesizes a variety of hypothetical states, including states in and around the trap. The user labels these states with rewards, using which ReQueST learns a robust reward model that enables the agent to navigate around the trap in the test environment.\nTo test whether ReQueST scales to domains with high-dimensional, continuous states like images, we use the \nCar Racing\n video game from the OpenAI Gym.\nIn addition to benchmarking ReQueST against prior methods, we ran a hyperparameter sweep and ablation study, where we varied the regularization strength of the dynamics model during trajectory optimisation as well as the subset of hypotheticals synthesized in order to measure ReQueST\u2019s sensitivity to these settings. We found that ReQueST can trade off between producing realistic vs. informative queries, and that the optimal trade-off varies across domains. We also found that the usefulness of each of the four hypothetical behaviours depends on the domain and the amount of training data collected.\nTo our knowledge, ReQueST is the first reward modeling algorithm that safely learns about unsafe states and scales to training neural network reward models in environments with high-dimensional, continuous states.\nReQueST relies on a generative model of initial states and a forward dynamics model, which can be hard to acquire for visual domains with complex dynamics. So far, we have only demonstrated the effectiveness of ReQueST in simulated domains with relatively simple dynamics. One direction for future work is to test ReQueST in 3D domains with more realistic physics and other agents acting in the environment.\nIf you want to learn more, check out our\n preprint on arXiv\n: \nSiddharth Reddy, Anca D. Dragan, Sergey Levine, Shane Legg, Jan Leike, Learning Human Objectives by Evaluating Hypothetical Behavior, arXiv, 2019.\nTo encourage replication and extensions, we have released our\n code\n.\nListen to our\n podcast\n to learn more about DeepMind's commitment to building safe AI.\nThanks to Zac Kenton and Kelly Clancy for feedback on early drafts of this post, and to Paulo Estriga for his design work.\n\u200d\n"}
{"title": "Using WaveNet technology to reunite speech-impaired users with their original voices", "contents": "This post details a recent project we undertook with Google and ALS campaigner Tim Shaw, as part of Google\u2019s Euphonia project. We demonstrate an early proof of concept of how text-to-speech technologies can synthesise a high-quality, natural sounding voice using minimal recorded speech data. \u00a0\nAs a teenager, Tim Shaw put everything he had into football practice: his dream was to join the NFL. After playing for Penn State in college, his ambitions were finally realised: the Carolina Panthers drafted him at age 23, and he went on to play for the Chicago Bears and Tennessee Titans, where he broke records as a linebacker. After six years in the NFL, on the cusp of greatness, his performance began to falter. He couldn\u2019t tackle like he once had; his arms slid off the pullup bar. At home, he dropped bags of groceries, and his legs began to buckle underneath him. In 2013 Tim was cut from the Titans but he resolved to make it onto another team. Tim practiced harder than ever, yet his performance continued to decline. Five months later, he finally discovered the reason: he was diagnosed with \nAmyotrophic lateral sclerosis\n (ALS, commonly known as Lou Gehrig\u2019s disease). In ALS, the neurons that control a person\u2019s voluntary muscles die, eventually leading to a total loss of control over one\u2019s body. ALS has no known cause, and, as of today, has no cure. \u00a0\nToday, Tim is a \npowerful advocate\n for ALS research. Earlier this year, he published a \nletter to his younger self\n advising acceptance\u2013\u201cotherwise, you\u2019ll grieve yourself to death.\u201d Now a wheelchair user, he lives under the constant care of his parents. People with ALS have trouble moving, and the disease makes speaking, swallowing, and even breathing on their own difficult and then impossible. Not being able to communicate can be one of the hardest aspects for people with ALS and their families. As Tim put it: \u201cit\u2019s beyond frustrating not to be able to express what\u2019s going on in my mind. I\u2019m smarter than ever but I just can\u2019t get it out.\u201d\nLosing one\u2019s voice can be socially devastating. Today, the main option available to people to preserve their voice is \nmessage banking,\n wherein people with ALS can digitally record and store personally meaningful phrases using their natural inflection and intonation. Message banking is a source of great comfort for people with ALS and their families, helping to preserve a core part of their identity - their voice - through a deeply challenging time. But message banking lacks flexibility, resulting in a static dataset of phrases. Imagine being told you will never be able to speak again. Now imagine that you were given the chance to preserve your voice by recording as much of it as possible. How would you decide what to record? How would you capture what you most want to be able to say in the future? \u00a0Would it be a meaningful story, a favorite phrase or a simple \u201cI love you\u201d? The process can be time consuming and emotionally draining, especially as someone\u2019s voice degrades. And people who aren\u2019t able to record phrases in time are left to choose a generic computer synthesized voice that lacks the same power of connection as their own.\nAt DeepMind, we\u2019ve been collaborating with Google and people like Tim Shaw to help develop technologies that can make it easier for people with speech difficulties to communicate. The challenges of this are two-fold. Firstly, we must have technology that can recognise the speech of people with non-standard pronunciation\u2013something Google AI has been researching through \nProject Euphonia\n. Secondly, we\u2019d ideally like people to be able to communicate using their original voice. Stephen Hawking, who also suffered from ALS, communicated with a famously unnatural sounding text-to-speech synthesiser. Thus, the second challenge is customising text-to-speech technology to the user\u2019s natural speaking voice. \nCreating natural sounding speech is considered a \u201c\ngrand challenge\n\u201d in the field of AI. With \u00a0\nWaveNet\n and \nTacotron\n, we\u2019ve seen tremendous breakthroughs in the quality of text-to-speech systems. However, whilst it is possible to create natural sounding voices that sound like specific people in certain contexts \u2013 as we demonstrated in collaboration with \nJohn Legend\n last year \u2013 \u00a0developing synthetic voices requires many hours of studio recording time with a very specific script \u2013 a luxury that many people with ALS simply don\u2019t have. Creating machine learning models that require less training data is an active area of research at DeepMind, and is crucial for use cases such as this where we need to recreate a voice with just a handful of audio recordings. \u00a0We\u2019ve helped do this by harnessing our \nWaveNet\n work and the novel approaches demonstrated in our paper, \nSample Efficient Adaptive Text-to-Speech\n (TTS) - where we showed that it\u2019s possible to create a high quality voice using small amounts of speech data. \nWhich brings us back to Tim. Tim and his family were instrumental in our recent research. Our goal was to provide Tim and his family an opportunity to hear his original speaking voice again. Thanks to Tim\u2019s time in the media spotlight, resulting in about thirty minutes of high-quality audio recordings, we were able to apply the methodologies from WaveNet and TTS to recreate his former voice. \nFollowing a six-month effort, Google\u2019s AI team visited Tim and his family to show him the results of their work. The meeting was captured for the new YouTube Originals learning series, \u201c\nThe Age of A.I.\n\u201d hosted by Robert Downey Jr. \u00a0Tim and his family were able to hear his old voice for the first time in years, as the model \u2013 trained on Tim\u2019s NFL audio recordings \u2013 read out the \nletter he\u2019d recently written to his younger self\n.\n\u201cI don\u2019t remember that voice,\u201d Tim remarked. His father responded, \u201cwe do.\u201d Later, Tim recounted\u2013\"it has been so long since I've sounded like that, I feel like a new person. I felt like a missing part was put back in place. It's amazing. I'm just thankful that there are people in this world that will push the envelope to help other people.\"\nYou can learn more about our project with Tim and the vital role he played in our research in \u201c\nThe Age of A.I.\n\u201d now available on \nYouTube.com/Learning\n.\nTo understand how the technology works, it\u2019s important to first understand \nWaveNet\n. WaveNet is a generative model trained on many hours of speech and text data from diverse speakers. It can then be fed arbitrary new text to be synthesized into a natural-sounding spoken sentence. \nLast year, in our \nSample Efficient Adaptive Text-to-Speech\n paper, we illustrated that it\u2019s possible to train a new voice with minutes, rather than hours, of voice recordings through a process called fine-tuning. This involves first training a large WaveNet model on up to thousands of speakers, which takes a few days, until it can produce the basics of natural sounding speech. Then, we take the small corpus of data for the target speaker and intelligently adapt the model, adjusting the weights so that we can create a single model that matches the target speaker. The concept of fine-tuning is similar to how people learn. For example, if you are attempting to learn calculus, you should first understand the foundations of basic algebra, and then apply these simpler concepts to help solve more complex equations. \nAfter this publication, we continued to iterate on our models. First, we migrated from WaveNet to \nWaveRNN\n, which is a more efficient text to speech model co-developed by Google AI and DeepMind. WaveNet requires a second distillation step to speed it up to serve requests in real-time, which makes fine-tuning more challenging. WaveRNN, on the other hand, does not require a second training step and can synthesize speech much faster than a WaveNet model that has not been distilled.\nIn addition to speeding up the models by switching to WaveRNN, we collaborated with Google AI to improve the quality of the models. Google AI researchers demonstrated that a similar fine-tuning approach could be applied to the related Google \nTacotron\n model, which we use in conjunction with WaveRNN to synthesise realistic voices. By combining these technologies trained on audio clips of Tim Shaw from his NFL days, we were able to generate an authentic sounding voice that resembles how Tim sounded before his speech degraded. While the voice is not yet perfect \u2013 lacking the expressiveness, quirks, and controllability of a real voice \u2013 we\u2019re excited that the combination of WaveRNN and Tacotron may help people like Tim preserve an important part of their identity, and we would like to one day integrate it into speech-generation devices. \u00a0 \nWe\u2019re honored to have briefly reunited Tim with his voice. At this stage, it\u2019s too early to know where our research will take us, but we are looking at ways to combine the Euphonia speech recognition systems with the speech synthesis technology so that people like Tim can more easily communicate. We hope that our research can eventually be shared more widely with those who need it most in order to communicate with their loved ones\u2013there are thousands of people in the world who this work might one day benefit. As Tim wrote in his letter to his younger self\u2013what matters, in the end, is \u201cthe relationships and the people you have in your life who love you and care about you.\u201d\nIn collaboration with: \nZachary Gleicher, Luis C. Cobo, Yannis Assael, Brendan Shillingford, Nando de Freitas, Julie Cattiau\u200e, Philip Nelson, Ye Jia, Heiga Zen, Ron Weiss, Zhifeng Chen, Yonghui Wu, Tejas Iyer\u200e, Hadar Shemtov, Tim Shaw, Fernando Vieira, Maeve McNally, John Shaw, Sharon Shaw, John Costello\n"}
{"title": "AlphaFold: Using AI for scientific discovery", "contents": "UPDATE: In July 2022, we released AlphaFold protein structure predictions for nearly all catalogued proteins known to science. Read the latest blog \nhere\n.\nIn our study \npublished in Nature\n, we demonstrate how artificial intelligence research can drive and accelerate new scientific discoveries. We\u2019ve built a dedicated, interdisciplinary team in hopes of using AI to push basic research forward: bringing together experts from the fields of structural biology, physics, and machine learning to apply cutting-edge techniques to predict the 3D structure of a protein based solely on its genetic sequence. \nOur system, AlphaFold \u2013 described in peer-reviewed papers now published in \nNature\n and \nPROTEINS\n \u2013 is the culmination of several years of work, and builds on decades of prior research using large genomic datasets to predict protein structure. The 3D models of proteins that AlphaFold generates are far more accurate than any that have come before - marking significant progress on one of the core challenges in biology. The AlphaFold code used at CASP13 is available on Github \nhere\n for anyone interested in learning more or replicating our results. We\u2019re also excited by the fact that this work has already inspired other, independent implementations, including the model described in \nthis paper\n, and a community - built, \nopen source implementation\n, described \nhere\n. \nProteins are large, complex molecules essential to all of life. Nearly every function that our body performs - contracting muscles, sensing light, or turning food into energy - relies on proteins, and how they move and change. What any given protein can do depends on its unique 3D structure. For example, antibody proteins utilised by our immune systems are \u2018Y-shaped\u2019, and form unique hooks. By latching on to viruses and bacteria, these antibody proteins are able to detect and tag disease - causing microorganisms for elimination. Collagen proteins are shaped like cords, which transmit tension between cartilage, ligaments, bones, and skin. Other types of proteins include Cas9, which, using CRISPR sequences as a guide, act like scissors to cut and paste sections of DNA; antifreeze proteins, whose 3D structure allows them to bind to ice crystals and prevent organisms from freezing; and ribosomes, which act like a programmed assembly line, helping to build proteins themselves.\nThe recipes for those proteins - called genes - are encoded in our DNA. An error in the genetic recipe may result in a malformed protein, which could result in disease or death for an organism. Many diseases, therefore, are fundamentally linked to proteins. But just because you know the genetic recipe for a protein doesn\u2019t mean you automatically know its shape. Proteins are comprised of chains of amino acids (also referred to as amino acid residues). But DNA only contains information about the \nsequence\n of amino acids - not how they fold into shape. The bigger the protein, the more difficult it is to model, because there are more interactions between amino acids to take into account. As demonstrated by \nLevinthal\u2019s paradox\n, it would take longer than the age of the known universe to randomly enumerate all possible configurations of a typical protein before reaching the true 3D structure - yet proteins themselves fold spontaneously, within milliseconds. Predicting how these chains will fold into the intricate 3D structure of a protein is what\u2019s known as the \u201cprotein-folding problem\u201d - a challenge that scientists have worked on for decades. This unsolved problem has already inspired countless developments, from spurring IBM\u2019s efforts in supercomputing (\nBlueGene\n), to novel citizen science efforts (\nFolding@Home\n and \nFoldIt\n) to new engineering realms, such as rational protein design.\nScientists have long been interested in determining the structures of proteins because a protein\u2019s form is thought to dictate its function. Once a protein\u2019s shape is understood, its role within the cell can be guessed at, and scientists can develop drugs that work with the protein\u2019s unique shape. \nOver the past five decades, researchers have been able to determine shapes of proteins in labs using experimental techniques like \ncryo-electron microscopy\n, \nnuclear magnetic resonance\n and \nX-ray crystallography\n, but each method depends on a lot of trial and error, which can take years of work, and cost tens or hundreds of thousands of dollars per protein structure. This is why biologists are turning to AI methods as an alternative to this long and laborious process for difficult proteins. The ability to predict a protein\u2019s shape computationally from its genetic code alone \u2013 rather than determining it through costly experimentation \u2013 could help accelerate research. \nFortunately, the field of genomics is quite rich in data thanks to the rapid reduction in the cost of genetic sequencing. As a result, deep learning \napproaches\n to the prediction problem that rely on genomic data have become increasingly popular in the last few years. To catalyse research and measure progress on the newest methods for improving the accuracy of predictions, a biennial global competition called CASP (\nCritical Assessment of protein Structure Prediction\n) was established in 1994, and has become the gold standard for assessing predictive techniques. We\u2019re indebted to decades of prior work by the CASP organisers, as well as to the thousands of experimentalists whose structures enable this kind of assessment.\nDeepMind\u2019s work on this problem resulted in AlphaFold, which we submitted to CASP13. We\u2019re proud to be part of what the CASP organisers have called \u201cunprecedented progress in the ability of computational methods to predict protein structure,\u201d placing \nfirst\n in rankings among the teams that entered (our entry is A7D).\nOur team focused specifically on the problem of modelling target shapes from scratch, without using previously solved proteins as templates. We achieved a high degree of accuracy when predicting the physical properties of a protein structure, and then used two distinct methods to construct predictions of full protein structures.\nBoth of these methods relied on deep neural networks that are trained to predict properties of the protein from its genetic sequence. The properties our networks predict are: (a) the distances between pairs of amino acids and (b) the angles between chemical bonds that connect those amino acids. The first development is an advance on commonly used techniques that estimate whether pairs of amino acids are near each other.\nWe trained a neural network to predict a distribution of distances between every pair of residues in a protein (visualised in Figure 2). These probabilities were then combined into a score that estimates how accurate a proposed protein structure is. We also trained a separate neural network that uses all distances in aggregate to estimate how close the proposed structure is to the right answer.\nBoth of these methods relied on deep neural networks that are trained to predict properties of the protein from its genetic sequence. The properties our networks predict are: (a) the distances between pairs of amino acids and (b) the angles between chemical bonds that connect those amino acids. The first development is an advance on commonly used techniques that estimate whether pairs of amino acids are near each other.\nWe trained a neural network to predict a distribution of distances between every pair of residues in a protein (visualised in Figure 2). These probabilities were then combined into a score that estimates how accurate a proposed protein structure is. We also trained a separate neural network that uses all distances in aggregate to estimate how close the proposed structure is to the right answer.\nUsing these scoring functions, we were able to search the protein landscape to find structures that matched our predictions. Our first method built on techniques commonly used in structural biology, and repeatedly replaced pieces of a protein structure with new protein fragments. We trained a generative neural network to invent new fragments, which were used to continually improve the score of the proposed protein structure.\nThe second method optimised scores through \ngradient descent \n- a mathematical technique commonly used in machine learning for making small, incremental improvements - which resulted in highly accurate structures. This technique was applied to entire protein chains rather than to pieces that must be folded separately before being assembled into a larger structure, to simplify the prediction process.\nThe AlphaFold version used at CASP13 is available \non Github\n for anyone interested in learning more, or replicating our protein-folding results. \nWhile we\u2019re thrilled by the success of our protein-folding model, there\u2019s still much to be done in the realm of protein biology, and we\u2019re excited to continue our efforts in this field. We\u2019re committed to establishing ways that AI can contribute to basic scientific discovery, with the hope of making real-world impact. This approach might serve to ultimately improve our understanding of the body and how it works, enabling scientists to target and design new, effective cures for diseases more efficiently. Scientists have only mapped structures for about half of all the proteins made by human cells. Some rare diseases involve mutations in a single gene, resulting in a malformed protein which can have profound effects on the health of an entire organism. A tool like AlphaFold might help rare disease researchers predict the shape of a protein of interest rapidly and economically. As scientists acquire more knowledge about the shapes of proteins and how they operate through simulations and models, this method may eventually help us contribute to efficient drug discovery, while also reducing the costs associated with experimentation. Our hope is that AI will be useful for disease research, and ultimately improve the quality of life for millions of patients around the world. \nBut potential benefits aren\u2019t restricted to health alone - understanding protein folding will assist in protein design, which could unlock a tremendous \nnumber of benefits\n. For example, advances in biodegradable enzymes - which can be enabled by protein design - could help manage pollutants like plastic and oil, helping us break down waste in ways that are more friendly to our environment. In fact, researchers have already begun \nengineering bacteria\n to secrete proteins that will make waste biodegradable, and easier to process.\nThe success of our first foray into protein folding is indicative of how machine learning systems can integrate diverse sources of information to help scientists come up with creative solutions to complex problems at speed. Just as we\u2019ve seen how AI can help people master complex games through systems like \nAlphaGo\n and \nAlphaZero\n, we similarly hope that one day, AI breakthroughs will help serve as a platform to advance our understanding of fundamental scientific problems, too.\nIt\u2019s exciting to see these early signs of progress in protein folding, demonstrating the utility of AI for scientific discovery. Even though there\u2019s a lot more work to do before we\u2019re able to have a quantifiable impact on treating diseases, managing waste, and more, we know the potential is enormous. With a \ndedicated team\n focused on delving into how machine learning can advance the world of science, we\u2019re looking forward to seeing the many ways our technology can make a difference.\nListen to our \npodcast\n featuring the researchers behind this work.\nThis blog post is based on the following work:\nAlphaFold: Improved protein structure prediction using potentials from deep learning\n (Nature)\nProtein structure prediction using multiple deep neural networks in CASP13\n (PROTEINS)\nThe AlphaFold version used at CASP13 is available \non Github\n for anyone interested in learning more, or replicating our protein folding results.\nThis work was done in collaboration with Andrew Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green, Chongli Qin, Augustin \u017d\u00eddek, Sandy Nelson, Alex Bridgland, Hugo Penedones, Stig Petersen, Karen Simonyan, Steve Crossan, Pushmeet Kohli, David Jones, David Silver, Koray Kavukcuoglu and Demis Hassabis\n\u200d\n"}
{"title": "A new model and dataset for long-range memory", "contents": "This blog introduces a new long-range memory model, the \nCompressive Transformer\n, alongside a new benchmark for book-level language modelling, \nPG19\n. We provide the conceptual tools needed to understand this new research in the context of recent developments in memory models and language modelling.\nThroughout our lives, we build up memories that are retained over a diverse array of timescales, from minutes to months to years to decades. When reading a book, we can recall characters who were introduced many chapters ago, or in an earlier book in a series, and reason about their motivations and likely actions in the current context. We can even put the book down during a busy week, and pick up from where we left off without forgetting the plotline. \nWe do not achieve such feats by storing every detail of sensory input we receive about the world throughout our lifetimes. \nOur brains select, filter, and integrate\n input stimuli based on factors of relevance, surprise, perceived danger, and repetition. In other words, we \ncompress\n lifelong experience to a set of salient memories which help us understand the past, and better anticipate the future. A major goal of AI researchers is discovering ways of implementing such abilities in computational systems and benchmarks which require complex reasoning over long time-spans. \nMemory systems for artificial neural networks have advanced considerably in the past two decades. In this post, we look to past advances to explore why this is such a difficult task and consider how natural language modelling could offer an effective means of designing better long range memory systems? We reflect on the necessity for better compressive memory architectures, and sparse memory access mechanisms, to work towards the goal of incorporating lifelong reasoning in our computational systems.\nOne of the earliest and most widely-used memory architectures in present day is a recurrent neural network (RNN) called the \nLong Short-Term-Memory\n (LSTM). The LSTM maintains a compact memory in the form of a vector of numbers, which it accesses and modifies with gated read, write, and forget operations. It was originally developed on a suite of synthetic tasks that involved learning logical operations on a stream of bits. However, it has since become a ubiquitous model of sequential data: from recognising handwritten notes to predicting the early onset of kidney injury.\nOne weakness of the LSTM, and of many contemporary RNNs, is capacity. They are designed so that each unit of memory can influence every other unit in memory with a learnable weight. But this results in a computationally inefficient system: \u00a0the number of learnable parameters in the model grows quadratically with the memory size. For example, an LSTM with a memory of size 64KB results in parameters of size 8GB. Circumventing this memory capacity bottleneck has been an active research area.\nResearchers at DeepMind proposed a novel architecture, the \nDifferentiable Neural Computer\n (DNC), which augments an LSTM with a much larger memory matrix to address these deficits. The DNC uses an \nattention\n operation to read from this memory matrix. In visual attention, our eyes are drawn by pertinent objects in a visual scene\u2013for example, one might typically spend more time observing a friend\u2019s face during an emotional conversation than on noticing their shoes. Here, memory models can attend to particular events/data in the past. This attention operation requires a fixed number of parameters, independent of the memory size, and so the memory capacity of the model can be significantly increased.\nAlongside the DNC, recurrent neural networks with an additional attention mechanism were showing promise in the domains of \ntranslation\n \u00a0and \nquestion answering\n. These models were able to reason over time using two memory structures: a small and compact LSTM memory and a large external memory. However, more recently researchers at Google Brain Team proposed \nthe\n \nTransformer\n which removes the LSTM, and only uses attention to transmit \ninformation across time\n. \nThe Transformer was originally shown to significantly outperform recurrent neural networks for machine translation. However it has since been applied to a range of applications in natural language processing, from question answering, document summarisation, sentiment classification and the modelling of natural language \u2013 a task that has seen particular exciting developments over the past year.\nFinding machine learning tasks which both drive the development of better memory architectures and push us further towards artificial general intelligence is challenging. Statistical \nlanguage modelling\n is one such task that we believe could be valuable for both purposes. Language models work by sequentially predicting the next word in a stream of text. They can be used to model existing texts and also to generate novel texts. As they get better at modelling the past, their predictions become more accurate, and the texts they generate become more realistic.\nIn Claude Shannon\u2019s seminal article \u201c\nA Mathematical Theory of Communication\n\u201d\n published in 1948, which founded the field of information theory, he discussed primitive language models and illustrated how adding more context improves the quality and realism of generated text. He does this by introducing the most simple model of English text, which has no contextual modelling at all \u2013 \u00a0a character-level model which treats each character independently. By sampling characters with their relative frequencies (8% of the time for \u2018a\u2019, 1.5% for \u2018b\u2019 etc.) we arrive with a nonsensical string :\nHowever, he remarks at the improvement in sample quality if one instead models the probability of words independently. Now the modelled context is approximately 7X larger (the average number of characters in a word): \nBy modelling the probability of word pairs, a further 2X in context length, even more realistic text emerges:\nIn other words, an increase in the length of context leads to an improvement in the quality of text generated. Shannon remarks on the quality of his produced samples and conjectures that natural text samples may emerge from a sufficiently complex statistical model, \n\u201cThe particular sequence of ten words \u201cattack on an English writer that the character of this\u201d is not at all unreasonable. It appears then that a sufficiently complex stochastic process will give a satisfactory representation of a discrete source\u201d\n. \nOne criticism of language modelling as a task for long-range reasoning is that models can capture a large portion of their predictions from the local context. Neural language models have traditionally ignored the wider context, focusing mostly on the short term. For example, in 2017 \nDailuk et al.\n found their neural language model rarely attends beyond the preceding five words. However in the past year large Transformer models have been shown to make use of hundreds of words of context to generate ever-more realistic text with a longer range of coherence. A demo from \nOpenAI\u2019s GPT-2\n, a 1.5B parameter Transformer, indicate that the model is able to generate realistic text and retain key entities (e.g. Dr Jorge P\u00e9rez and unicorns) across multiple paragraphs: \nSuch samples would likely astound Shannon, 70 years on from his early language model experiments. However the real benefit of powerful neural language models \u2013 and their relevance to the goal of AGI \u2013 is their ability to transfer knowledge to a suite of tasks. In the process of learning how to model text, neural language models appear to build up a knowledge-base of associations, and a plethora of skills. \nFor instance, researchers at OpenAI showed that GPT-2 can be applied to natural-language processing tasks such as question answering, paraphrasing, or sentiment analysis with surprisingly good performance \u2013 especially for a model that has never been explicitly trained to perform such tasks. When large Transformer language models are fine-tuned on particular tasks such as question answering, the resulting performance is significantly better than models that were designed and trained solely for question answering. Google\u2019s prominent natural language model, \nBERT\n, achieves state-of-the-art performance on a wide array of NLP benchmarks, and is now \na part of Google Search\n. And more recently, it was shown that GPT-2 can learn to play rudimentary chess by training it on strings of \ngame moves\n. \nA popular long-range language model benchmark is \nWikiText-103\n, which is comprised of English-language Wikipedia articles, and was developed by researchers at \nSalesforce AI\n. Articles are around 3,600 words on average, which, at the time of creation, was far beyond the memory window of state-of-the-art models. \nHowever researchers at Google recently showed that a Transformer variant called the TransformerXL \u2013 which maintains a memory of past network activations and recently obtained state-of-the-art results on WikiText-103 \u2013 can make use of contexts spanning over \none thousand words\n. This raises the question: will models soon saturate these benchmarks? As such, we\u2019ve compiled and released a new, longer-range language model benchmark based on books.\nTo support growing interest in long-range sequence models, we are releasing a new language modelling benchmark, \nPG-19\n, which is derived from books in the \nProject Gutenberg online library\n. \nBooks provide a rich context for the development of long-range memory models. \u00a0We selected a subset of approximately 28,000 books from Project Gutenberg published before 1919. Unlike prior language modeling dataset releases, we apply very little pre-processing to the text. For example, we do not limit the vocabulary size of the data or censor numbers, to avoid the filtering of useful information. \nPG-19 is over double the size of prior language modelling benchmarks, such as the \nBillion Word Benchmark\n, and contains text that is over 10X longer in context than the prior long-range language model benchmark, WikiText-103. We provide a comparative table of existing language modelling benchmarks, below:\nAlongside a new benchmark, we propose a long-range memory model called the \nCompressive Transformer\n. We take inspiration from the role of sleep in the formation of \nconsolidated episodic memories\n. Sleep is known to be crucial for memory, and it\u2019s thought that sleep serves to compress and consolidate memories, thereby improving reasoning abilities for memory tasks. In the Compressive Transformer, granular memories akin to episodic memories are collected online as the model passes over a sequence of inputs; over time, they are eventually compacted. \nThe Compressive Transformer uses attention to select information from the past, like the Transformer. It maintains a short-term memory of past activations, in the same style as the recently-proposed \nTransformerXL\n. Where the TransformerXL discards past activations when they become older, the Compressive Transformer instead compacts them into a \ncompressed memory\n. \u00a0The \ncompression\n is performed by a neural network guided by an auxiliary loss that guides it to keep around task-relevant information. It can learn to filter out irrelevant memories, as well as combine memories so that the salient information is preserved and retrievable over a longer period of time.\nWe find the Compressive Transformer has state-of-the-art performance in the modelling of natural language for two widely-used long-range benchmarks, \nWikiText-103\n and \nEnwik8, \ncompared to \npublished results\n that do not use additional sources of training data. We also show it can be used effectively to model speech, handles rare words especially well, and can be used within a reinforcement learning agent to solve a memory task. \nWe find the Compressive Transformer produces the largest performance gain in modelling long-context book text from the PG-19 benchmark. The model\u2019s conditional samples can be used to write book-like extracts. Below we show a sample that is fed a paragraph of text to be used as context, taken from \u201cThe Patrol of the Sun Dance\u201d by Ralph Connor, which the model has not previously seen.\nContext from \nThe Patrol of the Sun Dance\n Trail by Ralph Connor\n \nContinuation by the Compressive Transformer\nThe Compressive Transformer is able to produce narrative in a variety of styles, from multi-character dialogue, first-person diary entries, or third-person prose. Although the model does not have an understanding of language that\u2019s grounded in the real world, or the events that take place in it \u2013 by capturing longer-range correlations, we see the emergence of more coherent text. \nAs we strive to create agents that operate over days, weeks or even years, it will be impractical to compute over all raw input data at each timestep. Even with the current growth in computing power, we will need to develop compressive and sparse architectures for memory to build representations and reason about actions. \nModels which are able to capture relevant correlations across days, months, or years\u2019 worth of experience are on the horizon. We believe the route to more powerful reasoning over time will emerge from better selective attention of the past, and more effective mechanisms to compress it. As we explore ideas in this space, we need tasks and datasets that span longer and longer time intervals. The PG-19 dataset can help researchers move in this direction, presenting textual data in the longest form that we typically consume as humans: full-length books. We hope that its release will spur interest in new models that compress the past in order to predict the future and act effectively in the present.\nRead more\n"}
{"title": "Agent57: Outperforming the human Atari benchmark", "contents": "The Atari57 suite of games is a long-standing benchmark to gauge agent performance across a wide range of tasks. We\u2019ve developed \nAgent57,\n the first deep reinforcement learning agent to obtain a score that is above the \u00a0human baseline on all 57 Atari 2600 games. Agent57 combines an algorithm for efficient exploration with a meta-controller that adapts the exploration and long vs. short-term behaviour of the agent.\nAt DeepMind, \u00a0we\u2019re interested in building agents that do well on a wide range of tasks. An agent that performs \nsufficiently well\n on a \nsufficiently wide\n range of tasks is classified as \nintelligent\n. Games are an excellent testing ground for building adaptive algorithms: they provide a rich suite of tasks which players must develop sophisticated behavioural strategies to master, but they also provide an easy progress metric \u2013 game score \u2013 to optimise against. The ultimate goal is not to develop systems that excel at games, but rather to use games as a stepping stone for developing systems that learn to excel at a broad set of challenges. Typically, human performance is taken as a baseline for what doing \u201csufficiently well\u201d on a task means: the score obtained by an agent on each task can be measured relative to representative human performance, providing a human normalised score: \u00a00% indicates that an agent performs at random, while 100% or above indicates the agent is performing at human level or better.\nIn 2012, \nthe Arcade Learning environment\n \u2013 a suite of 57 Atari 2600 games (dubbed Atari57) \u2013 was proposed as a benchmark set of tasks: these canonical Atari games pose a broad range of challenges for an agent to master. The research community commonly uses this benchmark to measure progress in building successively more intelligent agents. \u00a0It\u2019s often desirable to summarise the performance of an agent on a wide range of tasks as a single number, and so average performance (either mean or median score across all games) on the Atari57 benchmark is often used to summarise an agents\u2019 abilities. Average scores have progressively increased over time. Unfortunately, the average performance can fail to capture how many tasks an agent is doing well on, and so is not a good statistic for determining how \ngeneral\n an agent is: it captures that an agent is doing \nsufficiently well,\n but not that it is doing sufficiently well on a \nsufficiently wide set of tasks\n. So although average scores have increased, until now, the number of above human games has not. As an illustrative example, consider a benchmark consisting of twenty tasks. Suppose agent A obtains a score of 500% on eight tasks, 200% on four tasks, and 0% on eight tasks \u00a0(mean = 240%, median = 200%), while agent B obtains a score of 150% on all tasks (mean = median = 150%). On average, agent A performs better than agent B. However, agent B possesses a more general ability: it obtains human-level performance on more tasks than agent A.\nThis issue is exacerbated if some tasks are much easier than others. By performing very well on very easy tasks, agent A can apparently outperform agent B, which performs well on both easy and hard tasks.\nThe median is less distorted by exceptional performance on a few easy games \u2013 it\u2019s a more \nrobust statistic\n than the mean for indicating the \ncenter of a distribution\n. However, in measuring generality, the tails of the distribution become more pertinent, particularly as the number of tasks becomes larger. For example, the measure of performance on the hardest 5th percentile of games can be much more representative of an agent\u2019s degree of generality.\nResearchers have focused on maximising agents\u2019 average performance on the Atari57 benchmark since its inception, and average performance has significantly increased over the past eight years. But, like the illustrative example above, not all Atari games are equal, with some games being much easier than others. Instead of examining the average performance, if we examine the performance of agents on the bottom 5% of games, we see that not much has changed since 2012: in fact, agents published in 2019 were struggling on the same games with which agents published in 2012 struggled. \u00a0Agent57 changes this, and is a more general agent in Atari57 than any agent since the inception of the benchmark. Agent57 finally obtains above human-level performance on the very hardest games in the benchmark set, as well as the easiest ones.\nBack in 2012, DeepMind developed the \nDeep Q-network agent \n(DQN) to tackle the Atari57 suite. Since then, the research community has developed many extensions and alternatives to DQN. Despite these advancements, however, all deep reinforcement learning agents have consistently failed to score in four games: Montezuma\u2019s Revenge, Pitfall, Solaris and Skiing. \nMontezuma\u2019s Revenge and Pitfall require extensive exploration to obtain good performance. A core dilemma in learning is the \nexploration-exploitation problem\n: should one keep performing behaviours one knows works (exploit), or should one try something new (explore) to discover new strategies that might be even more successful? For example, should one always order their same favourite dish at a local restaurant, or try something new that might surpass the old favourite? Exploration involves taking many suboptimal actions to gather the information necessary to discover an ultimately stronger behaviour.\nSolaris and Skiing are long-term credit assignment problems: in these games, it\u2019s challenging to match the consequences of an agents\u2019 actions to the rewards it receives. Agents must collect information over long time scales to get the feedback necessary to learn.\nPlaylist: Agent57 playing the four most challenging Atari57 games \u2013 Montezuma's Revenge, Pitfall, Solaris and Skiing\nFor Agent57 to tackle these four challenging games in addition to the other Atari57 games, several changes to DQN were necessary.\nEarly improvements to DQN enhanced its learning efficiency and stability, including \ndouble DQN\n, \nprioritised experience replay\n and \ndueling architecture\n. These changes allowed agents to make more efficient and effective use of their experience.\nNext, researchers introduced \ndistributed\n variants of DQN, \nGorila DQN\n and \nApeX\n, \u00a0that could be run on many computers simultaneously. This allowed agents to acquire and learn from experience more quickly, enabling researchers to rapidly iterate on ideas. Agent57 is also a distributed RL agent that decouples the data collection and the learning processes. Many actors interact with independent copies of the environment, feeding data to a central \u2018memory bank\u2019 in the form of a prioritized replay buffer. A learner then samples training data from this replay buffer, as shown in Figure 4, similar to how a person might recall memories to better learn from them. \u00a0The learner uses these replayed experiences to construct loss functions, by which it estimates the cost of actions or events. Then, it updates the parameters of its neural network by minimizing losses. Finally, each actor shares the same network architecture as the learner, but with its own copy of the weights. The learner weights are sent to the actors frequently, allowing them to update their own weights in a manner determined by their individual priorities, as we\u2019ll discuss later. \nAgents need to have memory in order to take into account previous observations into their decision making. This allows the agent to not only base its decisions on the present observation (which is usually partial, that is, an agent only sees some of its world), but also on past observations, which can reveal more information about the environment as a whole. Imagine, for example, a task where an agent goes from room to room in order to count the number of chairs in a building. Without memory, the agent can only rely on the observation of one room. With memory, the agent can remember the number of chairs in previous rooms and simply add the number of chairs it observes in the present room to solve the task. Therefore the role of memory is to aggregate information from past observations to improve the decision making process. In deep RL and deep learning, recurrent neural networks such as \nLong-Short Term Memory\n (LSTM) are used as short term memories.\nInterfacing memory with behaviour is crucial for building systems that self-learn. In reinforcement learning, an agent can be an on-policy learner, which can only learn the value of its direct actions, or an off-policy learner, which can learn about optimal actions even when not performing those actions \u2013 e.g., it might be taking random actions, but can still learn what the best possible action would be. \u00a0Off-policy learning is therefore a desirable property for agents, helping them learn the best course of action to take while thoroughly exploring their environment. Combining off-policy learning with memory is challenging because you need to know what you might remember when executing a different behaviour. For example, what you might choose to remember when looking for an apple (e.g., where the apple is located), is different to what you might choose to remember if looking for an orange. But if you were looking for an orange, you could still learn how to find the apple if you came across the apple by chance, in case you need to find it in the future. The first deep RL agent combining memory and off-policy learning was\n Deep Recurrent Q-Network\n (DRQN). More recently, a significant speciation in the lineage of Agent57 occurred with \nRecurrent Replay Distributed DQN\n (R2D2), combining a neural network model of short-term memory with off-policy learning and distributed training, and achieving a very strong average performance on Atari57. \u00a0R2D2 modifies the replay mechanism for learning from past experiences to work with short term memory. All together, this helped R2D2 efficiently learn profitable behaviours, and \nexploit\n them for reward.\nWe designed \nNever Give Up\n (NGU) to augment R2D2 with another form of memory: episodic memory. This enables NGU to detect when new parts of a game are encountered, so the agent can explore these newer parts of the game in case they yield rewards. This makes the agent\u2019s behaviour (\nexploration\n) deviate significantly from the policy the agent is trying to learn (obtaining a high score in the game); thus, off-policy learning again plays a critical role here. NGU was the first agent to obtain positive rewards, without domain knowledge, on Pitfall, a game on which no agent had scored any points since the introduction of the Atari57 benchmark, and other challenging Atari games. Unfortunately, NGU sacrifices performance on what have historically been the \u201ceasier\u201d games and so, on average, underperforms relative to R2D2. \nIn order to discover the most successful strategies, agents must explore their environment\u2013but some exploration strategies are more efficient than others. With DQN, researchers attempted to address the exploration problem by using an undirected exploration strategy known as epsilon-greedy: with a fixed probability (epsilon), take a random action, otherwise pick the current best action. However, this family of techniques do not scale well to hard exploration problems: in the absence of rewards, they require a prohibitive amount of time to explore large state-action spaces, as they rely on undirected random action choices to discover unseen states. In order to overcome this limitation, many directed exploration strategies have been proposed. Among these, one strand has focused on developing \nintrinsic motivation\n \nrewards\n that encourage an agent to explore and visit as many states as possible by providing more dense \u201cinternal\u201d rewards for novelty-seeking behaviours. Within that strand, we distinguish two types of rewards: firstly, \nlong-term novelty \nrewards encourage visiting many states throughout training, across many episodes. Secondly, \nshort-term novelty \nrewards encourage visiting many states over a short span of time (e.g., within a single episode of a game).\nLong-term novelty rewards\n signal when a previously unseen state is encountered in the agent\u2019s lifetime, and is a function of the density of states seen so far in training: that is, it\u2019s adjusted by how often the agent has seen a state similar to the current one relative to states seen overall. When the density is high (indicating that the state is \nfamiliar\n), the long term novelty reward is low, and vice versa. When all the states are familiar, the agent resorts to an undirected exploration strategy. However, learning density models of high dimensional spaces is fraught with problems due to the \ncurse of dimensionality\n. In practice, when agents use deep learning models to learn a density model, they suffer from \ncatastrophic forgetting\n (forgetting information seen previously as they encounter new experiences), as well as an inability to produce precise outputs for all inputs. For example, in Montezuma\u2019s Revenge, unlike undirected exploration strategies, long-term novelty rewards allow the agent to surpass the human baseline. However, even the\n best performing methods on Montezuma\u2019s Revenge\n need to carefully train a density model at the \nright\n speed: when the density model indicates that the states in the first room are \nfamiliar\n, the agent should be able to consistently get to unfamiliar territory.\nPlaylist: DQN vs. Agent57 playing Montezuma's revenge\nShort-term novelty rewards\n can be used to encourage an agent to explore states that have not been encountered in its recent past. Recently, neural networks that mimic some properties of \nepisodic memory\n have been used \u00a0to speed up learning in reinforcement learning agents. Because episodic memories are also thought to be important for \nrecognising novel experiences\n, we adapted these models to give Never Give Up a notion of short-term novelty. Episodic memory models are efficient and reliable candidates for computing short-term novelty rewards, as they can quickly learn a non-parametric density model that can be adapted on the fly (without needing to learn or adapt parameters of the model). In this case, the magnitude of the reward is determined by measuring the distance between the present state and previous states recorded in episodic memory. \nHowever, not all notions of distance encourage meaningful forms of exploration. For example, consider the task of navigating a busy city with many pedestrians and vehicles. If an agent is programmed to use a notion of distance wherein every tiny visual variation is taken into account, that agent would visit a large number of different states simply by passively observing the environment, even standing still \u2013 a fruitless form of exploration. To avoid this scenario, the agent should instead learn features that are seen as important for exploration, such as controllability, and compute a distance with respect to those features only. Such models have previously been used for exploration, and combining them with episodic memory is one of the main advancements of the \nNever Give Up exploration method\n, which resulted in above-human performance in Pitfall!\nPlaylist: NGU vs. Agent57 playing Pitfall!\nNever Give Up (NGU) used this short-term novelty reward based on \ncontrollable states\n, mixed with a long term novelty reward, using \nRandom Network Distillation\n. The mix was achieved by multiplying both rewards, where the long term novelty is bounded. This way the short-term novelty reward\u2019s effect is preserved, but can be down-modulated as the agent becomes more familiar with the game over its lifetime. The other core idea of NGU is that it learns a family of policies that range from purely exploitative to highly exploratory. This is achieved by leveraging a distributed setup: by building on top of \nR2D2\n, actors produce experience with different policies based on different importance weighting on the total novelty reward. This experience is produced uniformly with respect to each weighting in the family.\nAgent57 is built on the following observation: what if an agent can learn when it\u2019s better to exploit, and when it\u2019s better to explore? We introduced the notion of a meta-controller that adapts the exploration-exploitation trade-off, as well as a time horizon that can be adjusted for games requiring longer temporal credit assignment. With this change, Agent57 is able to get the best of both worlds: above human-level performance on both easy games and hard games.\nSpecifically, intrinsic motivation methods have two shortcomings:\nThis motivated the use of an online adaptation mechanism that controls the amount of experience produced with different policies, with a variable-length time horizon and importance attributed to novelty. Researchers have tried tackling this with multiple methods, including \ntraining a population of agents\n with different hyperparameter values, \ndirectly learning the values of the hyperparameters by gradient descent\n, or using a \ncentralized bandit to learn the value of hyperparameters\n.\nWe used a bandit algorithm to select which policy our agent should use to generate experience. Specifically, we trained a \nsliding-window UCB bandit\n for each actor to select the degree of preference for exploration and time horizon its policy should have.\nPlaylist: NGU vs. Agent57 playing Skiing\nTo achieve Agent57, we combined our previous exploration agent, Never Give Up, with a meta-controller. This agent computes a mixture of long and short term intrinsic motivation to explore and learn a family of policies, where the choice of policy is selected by the meta-controller. The meta-controller allows each actor of the agent to choose a different trade-off between near vs. long term performance, as well as exploring new states vs. exploiting what\u2019s already known (Figure 4). Reinforcement learning is a feedback loop: the actions chosen determine the training data. Therefore, the meta-controller also determines what data the agent learns from.\nWith Agent57, we have succeeded in building a more generally intelligent agent that has above-human performance on all tasks in the Atari57 benchmark. It builds on our previous agent Never Give Up, and instantiates an adaptive meta-controller that helps the agent to know when to explore and when to exploit, as well as what time-horizon it would be useful to learn with. A wide range of tasks will naturally require different choices of both of these trade-offs, therefore the meta-controller provides a way to dynamically adapt such choices.\nAgent57 was able to scale with increasing amounts of computation: the longer it trained, the higher its score got. While this enabled Agent57 to achieve strong general performance, it takes a lot of computation and time; the data efficiency can certainly be improved. Additionally, this agent shows better 5th percentile performance on the set of Atari57 games. This by no means marks the end of Atari research, not only in terms of data efficiency, but also in terms of general performance. We offer two views on this: firstly, analyzing the performance among percentiles gives us new insights on how general algorithms are. While Agent57 achieves strong results on the first percentiles of the 57 games and holds better mean and median performance than NGU or R2D2, as illustrated by \nMuZero\n, it could still obtain a higher average performance. Secondly, all current algorithms are \nfar from achieving optimal performance\n in some games. To that end, key improvements to use might be enhancements in the representations that Agent57 uses for exploration, planning, and credit assignment.\nRead the paper \nhere\n.\nWork done by: Adri\u00e0 Puigdom\u00e8nech, Bilal Piot, Steven Kapturowski, Pablo Sprechmann, Alex Vitvitskyi, Daniel Guo, Charles Blundell\nFigure design by Paulo Estriga and Adam Cain\n"}
{"title": "Dopamine and temporal difference learning: A fruitful relationship between neuroscience and AI", "contents": "Learning and motivation are driven by internal and external rewards. Many of our day-to-day behaviours are guided by predicting, or anticipating, whether a given action will result in a positive (that is, rewarding) outcome. The study of how organisms learn from experience to correctly anticipate rewards has been a productive research field for well over a century, since Ivan Pavlov's seminal psychological work. In his most famous experiment, dogs were trained to expect food some time after a buzzer sounded. These dogs began salivating as soon as they heard the sound, before the food had arrived, indicating they'd learned to predict the reward. In the original experiment, Pavlov estimated the dogs\u2019 anticipation by measuring the volume of saliva they produced. But in recent decades, scientists have begun to decipher the inner workings of how the brain learns these expectations. Meanwhile, in close contact with this study of reward learning in animals, computer scientists have developed algorithms for reinforcement learning in artificial systems. These algorithms enable AI systems to learn complex strategies without external instruction, guided instead by reward predictions. \nThe contribution of our new work, \npublished in Nature\n (\nPDF\n), is finding that a recent development in computer science \u2013 which yields significant improvements in performance on reinforcement learning problems \u2013 may provide a deep, parsimonious explanation for several previously unexplained features of reward learning in the brain, and opens up new avenues of research into the brain\u2019s dopamine system, with potential implications for learning and motivation disorders.\nReinforcement learning is one of the oldest and most powerful ideas linking neuroscience and AI. In the late 1980s, computer science researchers were trying to develop algorithms that could learn how to perform complex behaviours on their own, using only rewards and punishments as a teaching signal. These rewards would serve to reinforce whatever behaviours led to their acquisition. To solve a given problem, it\u2019s necessary to understand how current actions result in future rewards. For example, a student might learn by reinforcement that studying for an exam leads to better scores on tests. In order to predict the total future reward that will result from an action, it's often necessary to reason many steps into the future. \nAn important breakthrough in solving the problem of reward prediction was the \ntemporal difference learning (TD) algorithm\n. TD uses a mathematical trick to replace complex reasoning about the future with a very simple learning procedure that can produce the same results. This is the trick: instead of trying to calculate total future reward, TD simply tries to predict the combination of immediate reward and \nits own reward prediction at the next moment in time\n. Then, when the next moment comes, bearing new information, the new prediction is compared against what it was expected to be. If they\u2019re different, the algorithm calculates how different they are, and uses this \u201ctemporal difference\u201d to adjust the old prediction toward the new prediction. By always striving to bring these numbers closer together at every moment in time \u2013 matching expectations to reality \u2013 the entire chain of prediction gradually becomes more accurate. \nAround the same time, in the late 80s and early 90s, neuroscientists were \nstruggling\n to understand the behaviour of dopamine neurons. Dopamine neurons are clustered in the midbrain, but send projections to many brain areas, potentially broadcasting some globally relevant message. It was clear that the firing of these neurons had some relationship to reward, but their responses also depended on sensory input, and changed as the animals became more experienced in a given task.\nFortuitously, some researchers were versed in the recent developments of both neuroscience and AI. These scientists \nnoticed\n, in the mid-1990s, that responses in some dopamine neurons represented reward prediction errors\u2013their firing signalled when the animal got more reward, or less reward, than it was trained to expect. These researchers therefore proposed that the brain uses a TD learning algorithm: a reward prediction error is calculated, broadcast to the brain via the dopamine signal, and used to drive learning. Since then, the \nreward prediction error theory of dopamine\n has been tested and validated in thousands of experiments, and has become one of the most successful quantitative theories in neuroscience.\nComputer scientists have continued to improve the algorithms for learning from rewards and punishments. \nSince 2013\n, there\u2019s been a focus on \ndeep\n reinforcement learning: using deep neural networks to learn powerful representations in reinforcement learning. This has enabled reinforcement learning algorithms to \nsolve\n \ntremendously\n \nmore\n \nsophisticated\n and \nuseful\n problems.\nOne of the algorithmic developments that has made reinforcement learning work better with neural networks is \ndistributional reinforcement learning\n. In many situations (\nespecially\n in the \nreal world\n), the amount of future reward that will result from a particular action is not a perfectly known quantity, but instead involves some randomness. An example is shown in Figure 1. This is a stylised representation of a situation where a computer-controlled avatar, \ntrained\n to traverse an obstacle course, jumps across a gap. The agent is uncertain about whether it will fall, or reach the other side. Therefore, the distribution of predicted rewards has two bumps: one representing the possibility of falling, and one representing the possibility of successfully reaching the other side.\nIn such situations, a standard TD algorithm learns to predict the future reward that will be received \non average\n\u2013in this case, failing to capture the two-peaked distribution of potential returns. A distributional reinforcement learning algorithm, on the other hand, learns to predict the full spectrum of future rewards. Figure 1 depicts the reward prediction learned by a distributional agent. \u00a0\nOne of the simplest distributional reinforcement learning algorithms is very closely related to standard TD, and is called distributional TD. Whereas standard TD learns a single prediction \u2013 the average expected reward \u2013 a distributional TD network learns a set of distinct predictions. Each of these is learned through the same method as standard TD \u2013 by computing a reward prediction error that describes the difference between consecutive predictions. But the crucial ingredient is that each predictor applies a different transformation to its reward prediction errors. Some predictors \"amplify\" or \"overweight\" their reward prediction errors (RPE) selectively when the reward prediction error is positive (Figure 2a). This causes the predictor to learn a more \noptimistic\n reward prediction, corresponding to a higher part of the reward distribution. Other predictors amplify their negative reward prediction errors (Figure 2a), and so learn more \npessimistic\n predictions. All together, a set of predictors with a diverse set of pessimistic and optimistic weightings map out the full reward distribution (Figure 2b, 2c).\nAside from its simplicity, another benefit of distributional reinforcement learning is that it\u2019s very powerful when combined with deep neural networks. In the last 5 years, there\u2019s been a great deal of progress in algorithms based around the original deep reinforcement learning \nDQN agent\n, and these are frequently evaluated on the \nAtari-57 benchmark\n set of \nAtari 2600 games\n. Figure 3 compares many standard and distributional RL algorithms, trained and evaluated under the same conditions, on this benchmark. Distributional reinforcement learning agents are shown in blue, and illustrate the significant pattern of improvements. Three of these algorithms (QR-DQN, IQN, and FQF) are variants of the distributional TD algorithm we\u2019ve been discussing.\nWhy are distributional reinforcement learning algorithms so effective? Although this is still an active topic of research, a key ingredient is that learning about the distribution of rewards gives the neural network a more powerful signal for \nshaping its representation\n in a way that\u2019s robust to changes in the environment or changes in the policy. \nBecause distributional TD is so powerful in artificial neural networks, a natural question arises: Is distributional TD used in the brain? This was the driving question behind our paper recently published in \nNature\n.\nIn this work, we collaborated with an \nexperimental lab at Harvard\n to analyse their recordings of dopamine cells in mice. The recordings were made while the mice performed a well-learned task in which they received rewards of unpredictable magnitude (indicated by the dice illustration in Figure 4). We evaluated whether the activity of dopamine neurons was more consistent with standard TD or distributional TD.\nAs described above, distributional TD relies on a set of distinct reward predictions. Our first question was whether we could see such genuinely diverse reward predictions in the neural data.\nFrom previous work, we know that dopamine cells change their firing rate to indicate a prediction error \u2013 that is, if an animal receives more or less reward than it expected. We know that there should be zero prediction error when a reward is received that is the exact size as what a cell had predicted, and therefore no change in firing rate. For each dopamine cell, we determined the reward size for which it didn\u2019t change its baseline firing rate. We call this the cell's \"reversal point\". We wanted to know whether these reversal points were different between cells. In Figure 4c, we show that there were marked differences between cells, with some cells predicting very large amounts of reward, and other cells predicting very little reward. These differences were above and beyond the amount of difference we would expect to see from random variability inherent in the recordings. \nIn distributional TD, these differences in reward prediction arise from selective amplification of positive or negative reward prediction errors. Amplifying positive reward prediction errors causes more optimistic reward predictions to be learned; amplifying negative reward prediction errors causes pessimistic predictions. So we next measured the degree to which different dopamine cells exhibited different relative amplifications of positive versus negative expectations. Between cells, we found reliable diversity which, again, could not be explained by noise. And, crucially, we found that the \nsame cells\n which amplified their positive reward prediction errors also had higher reversal points (Figure 4c, bottom-right panels) \u2013 that is, they were apparently tuned to expect higher reward volumes. \nFinally, distributional TD theory predicts that these diverse reversal points and diverse asymmetries, across cells, should collectively encode the learned reward distribution. So our final question was if we could \ndecode\n the reward distribution from the firing rates of dopamine cells. As shown in Figure 5, we found that it was indeed possible, using only the firing rates of dopamine cells, to reconstruct a reward distribution (blue trace) which was a very close match to the actual distribution of rewards (grey area) in the task that the mice were engaged in. This reconstruction relied on interpreting the firing rates of dopamine cells as the reward prediction errors of a distributional TD model, and performing inference to determine what distribution that model had learned about.\nIn summary, we found that dopamine neurons in the brain were each tuned to different levels of pessimism or optimism. If they were a choir, they wouldn\u2019t all be singing the same note, but harmonizing \u2013 each with a consistent vocal register, like bass and soprano singers. In artificial reinforcement learning systems, this diverse tuning creates a richer training signal that greatly speeds learning in neural networks, and we speculate that the brain might use it for the same reason.\nThe existence of distributional reinforcement learning in the brain has interesting implications both for AI and neuroscience. Firstly, this discovery validates distributional reinforcement learning \u2013 it gives us increased confidence that AI research is on the right track, since this algorithm is already being used in the most intelligent entity we're aware of: the brain. \nSecondly, it raises new questions for neuroscience, and new insights for understanding mental health and motivation. What happens if an individual's brain \u201clistens\u201d selectively to optimistic versus pessimistic dopamine neurons? Does this give rise to impulsivity, or depression? A strength of the brain is its powerful representations \u2013 how are these sculpted by distributional learning? Once an animal learns about the distribution of rewards, how is that representation used downstream? How does the variability of optimism across dopamine cells relate to \nother\n \nknown\n \nforms\n of diversity in the brain? \nFinally, we hope that asking and answering these questions will stimulate progress in neuroscience that will feed back to benefit AI research, completing the \nvirtuous circle\n. \nRead the paper \nhere\n.\nListen to our \npodcast\n on the virtuous circle between AI and neuroscience.\n"}
{"title": "Towards understanding glasses with graph neural networks", "contents": "Under a microscope, a pane of window glass doesn\u2019t look like a collection of orderly molecules, as a crystal would, but rather a jumble with no discernable structure. Glass is made by starting with a glowing mixture of high-temperature melted sand and minerals. Once cooled, its viscosity (a measure of the friction in the fluid) increases a trillion-fold, and it becomes a solid, resisting tension from stretching or pulling. Yet the molecules in the glass remain in a seemingly disordered state, much like the original molten liquid \u2013 almost as though the disordered liquid state had been flash-frozen in place. The \nglass transition\n, then, first appears to be a dramatic arrest in the movement of the glass molecules. Whether this process corresponds to a structural phase transition (as in water freezing, or the \nsuperconducting transition\n) is a major open question in the field. Understanding the nature of the dynamics of glass is fundamental to understanding how the atomic-scale properties define the visible features of many solid materials. \u00a0In the words of the \nrecently deceased\n Nobel Prize laureate \nPhilip W. Anderson\n, whose pioneering work shaped the field of solid-state physics:\nThe glass transition is a \nubiquitous phenomenon\n which manifests in more than window (silica) glasses. For instance, when \nironing\n, polymers in a fabric are heated, become mobile, and then oriented by the weight of the iron. More broadly, a similar and related transition, the \njamming\n transition, can be found in colloidal suspensions (such as ice cream), \ngranular materials \n(such as a static pile of sand), and also biological systems (e.g., for modelling \ncell migration\n during embryonic development) as well as social behaviours (for instance traffic jams). These systems all operate under local constraints where the position of some elements inhibits the motion of others (termed\n frustration\n). Their dynamics are complex and cooperative, taking the form of large-scale, collective rearrangements which propagate through space in a heterogeneous manner. Glasses are considered to be archetypal of these kinds of complex systems, and so better understanding them will have implications across many research areas. This understanding might yield practical benefits \u2013 for example, creating materials that have a more stable glass structure, instead of a crystalline one, would allow them to dissolve quickly, which could lead to new \ndrug delivery\n methods. \u00a0Understanding the glass transition may result in other applications of disordered materials, in fields as diverse as biorenewable polymers and food processing. The study of glasses has also already led to insights in apparently very different domains such as \nconstraint satisfaction problems\n in computer science and, more recently, the training dynamics of \nunder-parameterized neural networks\n. \nA deeper understanding of glasses may lead to practical advances in the future, but their mysterious properties also raise many fundamental research questions. Though humans have been making silica glasses for at least four thousand years, they remain enigmatic to scientists: there are many unknowns about the underlying physical correlates of, for example, the trillion-fold increase in viscosity that happens over the cooling process. Our interest in this field was also motivated by the fact that glasses are also an excellent testbed for applying modern machine learning methods to physical problems: they\u2019re easy to simulate, and easy to input to particle-based machine learning models. Crucially, we can then go in and examine these models to understand what they\u2019ve learned about the system, to gain deeper qualitative insights about the nature of glass, and the structural quantities which underpin its mysterious dynamical qualities. Our \nnew work\n, published in Nature Physics, could help us gain an understanding of the structural changes that may occur near the glass transition. More practically, this research could lead to insights about the mechanical constraints of glasses (e.g., where a glass will break). \nGlasses can be modelled as particles interacting via a short-range repulsive potential which essentially prevents particles from getting too close to each other. This potential is \nrelational\n (only pairs of particles interact) and \nlocal\n (only nearby particles interact with each other), which suggests that a model that respects this local and relational structure should be effective. In other words, given the system is underpinned by a graph-like structure, we reasoned it would be best modeled by a graph structured network, and set out to apply \nGraph Neural Networks\n to predict physical aspects of a glass. \nWe first created an input graph where the nodes represent particles, and edges represent interactions between particles, and are labelled with their relative distance. \u00a0A particle was connected to its neighboring particles within a certain radius (in this case, 2 particle diameters). We then trained a neural network, described below, to predict a single real number for each node of the graph. This prediction was ultimately regressed towards the \nmobilities \nof particles obtained from computer simulations of glasses. Mobility is a measure of how much a particle \ntypically \nmoves (more technically, this corresponds to the average distance travelled when averaging over initial velocities). \nOur network architecture was a typical graph network architecture, consisting of several neural networks. We first embedded the node and edge labels in a high-dimensional vector-space using two encoder networks (we used standard \nmulti-layer perceptrons\n). Next, we iteratively updated the embedded node and edge labels using two update networks visualized in Fig. 2b. At first, each edge updated based on its previous embedding and the embeddings of the two nodes it connected to. After all edges were updated in parallel using the same network, the nodes were also updated based on the sum of their neighboring edge embeddings and their previous embeddings, using a second network. We repeated this procedure several times (typically 7), allowing local information to propagate throughout the graph, as shown in Fig. 2c. Finally, we extracted the mobility for each particle from the final embeddings of the corresponding node using a decoder network. The resulting network has all the required properties: it is inherently relational, it is invariant under permutation of the nodes and edges of the graph, and it updates embedding in a way that is a composition of local operations. The network parameter training was done via stochastic gradient descent.\nTo study the full dynamical evolution of glasses, we constructed several datasets corresponding to predictions of mobilities on different time horizons and for different temperatures. We note that each particle will have collided several thousands of times over those timescales. Thus, the network must find a way to coarsely represent the long-term dynamics of the system.\nAfter applying graph networks to the three dimensional glasses that we simulated, we found that they strongly outperformed existing models, ranging from standard \nphysics-inspired baselines\n to \nstate-of-the-art machine learning models\n. Comparing the predicted mobilities (colour gradients, Figure 3) with the ground truth simulation (dots, Figure 3), we see that the agreement is extremely good on short times and remains well matched up to the relaxation time of the glass. Looking at a glass over the timescale of its relaxation time \u2013 for actual glass, this would be thousands of years \u2013 is like looking at a liquid over about a picosecond (10-12): the relaxation time is loosely when particles have collided enough to start losing information about their initial position. In numbers, the correlation between our prediction and the simulation's ground truth is 96% for very short timescales, and remains high at 64% for the relaxation time of the glass (an improvement of 40% compared to the previous state of the art).\nWe don\u2019t want to simply model glass, however: we want to understand it. \u00a0We therefore explored what factors were important to our model\u2019s success in order to infer what properties are important in the underlying system. A central unsolved question in the dynamics of glass is how particles influence one another as a function of distance, and how this evolves over time. We investigated this by designing an experiment leveraging the specific architecture of the graph network. Recall that repeated applications of the edge and node updates define shells of particles around any given particle: the first shell consists of all particles one step away from this \"marked\" particle, the second shell consists of all particles one step away from the first shell, and so on (see the different shades of blue on Figure 2c). By measuring the sensitivity of the prediction that the network makes for the central particle when the \nn-\nth shell is modified, we can measure how large an area the network uses to extract its prediction, which provides an estimate of the distance over which particles influence each other in the physical system.\nWe found that when predicting what happens in the near future or in the liquid phase, drastic modifications of the third shell (for instance, removing it altogether, Figure 4, left) did \nnot\n modify the prediction that the network would make for the marked particle. On the other hand, when making predictions at low temperature and in the far future, after the glass starts to relax, even tiny perturbations (Figure 4, right) of the 5-th shell affect the prediction for the marked particle. These findings are consistent with a physical picture where a \ncorrelation length\n (a measure of the distance over which particles influence each other) grows upon approaching the glass transition. The definition and study of correlation lengths is a cornerstone of the study of phase transition in physics, and one that is still an open point of debate when studying glasses. While this \"machine learned\" correlation length cannot be directly transformed into a physically measurable quantity, it provides compelling evidence that growing spatial correlations are present in the system upon approaching the glass transition, and that our network has learned to extract them.\nOur results show that graph networks constitute a powerful tool to predict the long term dynamics of glassy systems, leveraging the structure hidden in a local neighborhood of particles. We expect our technique to be useful for predicting other physical quantities of interest in glasses, and hope that it will lead to more insights for glassy system theorists \u2013 we are \nopen-sourcing our models and trained networks\n to aid this effort. More generally, graph networks are a versatile tool that are being applied to many other physical systems that consist of many-body interactions, in contexts including \ntraffic\n, crowd simulations, and cosmology. The network analysis methods used here also yield a deeper understanding in other fields: graph networks may not only help us make better predictions for a range of systems, but indicate what physical correlates are important for modeling them \u2013 in this work, how interactions between local particles in a glassy material evolve over time. \nWe believe that our results advocate using structured models when applying machine learning to the physical sciences; in our case, the ability to analyse the inner workings of a neural network indicated that it had discovered a quantity that correlates with an elusive physical quantity. This demonstrates that machine learning can be used not only to make quantitative predictions, but also to gain qualitative understanding of physical systems. This could mean that machine learning systems might be able to eventually assist researchers in deriving fundamental physical theories, ultimately helping to augment, rather than replace, human understanding.\n\u200d\nRead more\nWe\u2019re continuing to develop methods for applying machine learning to a broad range of fundamental science questions. We\u2019re always looking to hire more scientists\u2013read about our \nscience programme\n and \nopenings\n for more information.\nWork done in collaboration with: E. D. Cubuk, S. S. Schoenholz, A. Obika, A. W. R. Nelson, T. Back, D. Hassabis and P. Kohli\nFigure design by Paulo Estriga and Adam Cain\n"}
{"title": "DeepMind Blog", "contents": ""}
{"title": "Using AI to plan head and neck cancer treatments", "contents": "Early results\n from our partnership with \nthe Radiotherapy Department at University College London Hospitals NHS Foundation Trust\n suggest that we are well on our way to developing an artificial intelligence (AI) system that can analyse and segment medical scans of head and neck cancer to a similar standard as expert clinicians. This segmentation process is an essential but time-consuming step when planning radiotherapy treatment. \nThe findings\n also show that our system can complete this process in a fraction of the time.\nMore than half a million people are diagnosed each year with cancers of the head and neck worldwide. Radiotherapy is a key part of treatment, but clinical staff have to plan meticulously so that healthy tissue doesn\u2019t get damaged by radiation: a process which involves radiographers, oncologists and/or dosimetrists manually outlining the areas of anatomy that need radiotherapy, and those areas that should be avoided.\nAlthough our work is still at an early stage, we hope it could one day reduce the waiting time between diagnosis and treatment, which could potentially improve outcomes for cancer patients. We also hope that accurate auto-segmentation could speed up the \nadaptive radiotherapy process\n, whereby radiotherapy treatments are adapted as the tumour shrinks - although more work is needed to investigate how this would work in practice.\nAs well as changing patients\u2019 lives, this research could also free up time for the clinicians who treat them, meaning they get to spend more time on patient care, education and research.\nWe\u2019ve taken steps to ensure our work is clinically applicable. This includes the development of a \nnew performance metric\n used to assess model performance that we believe is more representative of clinical processes, and a test set with new \nhigh-quality segmentations\n of scans selected from sites previously unseen to the model which demonstrates generalisability. Both of these have been open sourced to the research community. But for our system to have an impact on real people diagnosed with cancer, we need to expand it and demonstrate that it works in real clinical environments.\nThat\u2019s why we\u2019re looking forward to moving into the next phase of work with UCLH, where we will be exploring a human evaluation of these AI algorithms to test how they might perform in a clinical environment.\nAt DeepMind Health, we think it is important to share our work with others in the community. As such, Professor Olaf Ronneberger, Senior Research Scientist at DeepMind Health, will be presenting these initial findings at \nMICCAI\n, the world leading conference on medical imaging, this Sunday.\nUltimately, we believe that advanced technologies can and should help change lives, and we\u2019re excited for the next steps of this project. We\u2019ll continue to keep you updated as we make progress.\n"}
{"title": "Scaling Streams with Google", "contents": "We\u2019re excited to announce that the team behind \nStreams - \nour mobile app that supports doctors and nurses to deliver faster, better care to patients - will be joining Google.\nIt\u2019s been a phenomenal journey to see Streams go from initial idea to live deployment, and to hear how it\u2019s helped \nchange the lives of patients\n and the nurses and doctors who treat them. The arrival of world-leading health expert \nDr. David Feinberg at Google\n will accelerate these efforts, helping to make a difference to the lives of millions of patients around the world.\nThis is a major milestone for DeepMind! One of the reasons for joining forces with Google in 2014 was the opportunity to use Google\u2019s scale and experience in building billion-user products to bring our breakthroughs more rapidly to the wider world. It\u2019s been amazing to put this into practice in \ndata centre efficiency\n, \nAndroid battery life\n, \ntext-to-speech applications\n, and now the work of our Streams team.\nOver the past three years we\u2019ve built a team of experts in what it takes to deploy clinical tools in practice - engineers, clinicians, translational researchers and more. In that time, we\u2019ve also made major advances in healthcare AI research: \ndetecting eye disease more quickly\n and accurately than experts; \nplanning cancer radiotherapy treatment\n in seconds rather than hours; and working to \ndetect patient deterioration from electronic records\n.\nOur vision is for Streams to now become an AI-powered assistant for nurses and doctors everywhere - combining the best algorithms with intuitive design, all backed up by rigorous evidence. The team working within Google, alongside brilliant colleagues from across the organisation, will help make this vision a reality.\nThe Streams team will remain in London, under the leadership of former NHS surgeon and researcher Dr Dominic King. We\u2019re fully committed to all our NHS partners, and to delivering on our current projects and more. We\u2019ll be working closely with them as we plan for the team\u2019s transition, and information governance and safety remain our top priorities. Patient data remains under our partners\u2019 strict control, and all decisions about its use will continue to lie with them.\nAs a research organisation, DeepMind will continue to lead the way in fundamental research applying AI to important science and medical research questions, in collaboration with academic partners, to accelerate scientific progress for the benefit of everyone.\nOver the coming years, we expect AI to help scientists make transformative advances on problems ranging from protein folding to image analysis, potentially improving medical diagnosis, drug discovery and much more. We\u2019re excited to play our part in that journey both at DeepMind and at Google, in the service of patients and clinicians around the world.\nUpdate: On September 18 2019, we confirmed DeepMind Health had joined the Google Health family. Find out more \nhere\n.\n"}
{"title": "AlphaFold: Using AI for scientific discovery", "contents": "In July 2022, we released AlphaFold protein structure predictions for nearly all catalogued proteins known to science. Read the latest blog \nhere\n.\nWe\u2019re excited to share DeepMind\u2019s first significant milestone in demonstrating how artificial intelligence research can drive and accelerate new scientific discoveries. With a strongly interdisciplinary approach to our work, DeepMind has brought together experts from the fields of structural biology, physics, and machine learning to apply cutting-edge techniques to predict the 3D structure of a protein based solely on its genetic sequence.\nOur system, \nAlphaFold\n, which we have been working on for the past two years, builds on years of prior research in using vast genomic data to predict protein structure. The 3D models of proteins that AlphaFold generates are far more accurate than any that have come before\u2014making significant progress on one of the core challenges in biology.\nProteins are large, complex molecules essential in sustaining life. Nearly every function our body performs\u2014contracting muscles, sensing light, or turning food into energy\u2014can be traced back to one or more proteins and how they move and change. The recipes for those proteins\u2014called genes\u2014are encoded in our DNA.\nWhat any given protein can do depends on its unique 3D structure. For example, antibody proteins that make up our immune systems are \u2018Y-shaped\u2019, and are akin to unique hooks. By latching on to viruses and bacteria, antibody proteins are able to detect and tag disease-causing microorganisms for extermination. Similarly, collagen proteins are shaped like cords, which transmit tension between cartilage, ligaments, bones, and skin. Other types of proteins include Cas9, which, using CRISPR sequences as a guide, act like scissors to cut and paste sections of DNA; antifreeze proteins, whose 3D structure allows them to bind to ice crystals and prevent organisms from freezing; and ribosomes that act like a programmed assembly line, which help build proteins themselves.\nBut figuring out the 3D shape of a protein purely from its genetic sequence is a complex task that scientists have found challenging for decades. The challenge is that DNA only contains information about the sequence of a protein\u2019s building blocks called amino acid residues, which form long chains. Predicting how those chains will fold into the intricate 3D structure of a protein is what\u2019s known as the \u201cprotein-folding problem\u201d.\nThe bigger the protein, the more complicated and difficult it is to model because there are more interactions between amino acids to take into account. As noted in \nLevinthal\u2019s paradox\n, it would take longer than the age of the universe to enumerate all the possible configurations of a typical protein before reaching the right 3D structure.\nThe ability to predict a protein\u2019s shape is useful to scientists because it is fundamental to understanding its role within the body, as well as diagnosing and treating diseases believed to be caused by misfolded proteins, such as \nAlzheimer\u2019s\n, \nParkinson\u2019s\n, \nHuntington\u2019s\n and \ncystic fibrosis\n.\nWe are especially excited about how it might improve our understanding of the body and how it works, enabling scientists to design new, effective cures for diseases more efficiently. As we acquire more knowledge about the shapes of proteins and how they operate through simulations and models, it opens up new potential within drug discovery while also reducing the costs associated with experimentation. That could ultimately improve the quality of life for millions of patients around the world.\nAn understanding of protein folding will also assist in protein design, which could unlock a tremendous number of benefits. For example, advances in biodegradable enzymes\u2014which can be enabled by protein design\u2014could help manage pollutants like plastic and oil, helping us break down waste in ways that are more friendly to our environment. In fact, researchers have already begun \nengineering bacteria \nto secrete proteins that will make waste biodegradable, and easier to process.\nTo catalyse research and measure progress on the newest methods for improving the accuracy of predictions, a global biennial competition called CASP (\nCritical Assessment of protein Structure Prediction\n) was established in 1994, and has become the gold standard for assessing techniques.\nOver the past five decades, scientists have been able to determine shapes of proteins in labs using experimental techniques like \ncryo-electron microscopy\n, \nnuclear magnetic resonance\n or \nX-ray crystallography\n, but each method depends on a lot of trial and error, which can take years and cost tens of thousands of dollars per structure. This is why biologists are turning to AI methods as an alternative to this long and laborious process for difficult proteins.\nFortunately, the field of genomics is quite rich in data thanks to the rapid reduction in the cost of genetic sequencing. As a result, deep learning \napproaches\n to the prediction problem that rely on genomic data have become increasingly popular in the last few years. DeepMind\u2019s work on this problem resulted in AlphaFold, which we submitted to CASP this year. We\u2019re proud to be part of what the CASP organisers have called \u201cunprecedented progress in the ability of computational methods to predict protein structure,\u201d placing \nfirst\n in rankings among the teams that entered (our entry is A7D).\nOur team focused specifically on the hard problem of modelling target shapes from scratch, without using previously solved proteins as templates. We achieved a high degree of accuracy when predicting the physical properties of a protein structure, and then used two distinct methods to construct predictions of full protein structures.\nBoth of these methods relied on deep neural networks that are trained to predict properties of the protein from its genetic sequence. The properties our networks predict are: (a) the distances between pairs of amino acids and (b) the angles between chemical bonds that connect those amino acids. The first development is an advance on commonly used techniques that estimate whether pairs of amino acids are near each other.\nWe trained a neural network to predict a separate distribution of distances between every pair of residues in a protein. These probabilities were then combined into a score that estimates how accurate a proposed protein structure is. We also trained a separate neural network that uses all distances in aggregate to estimate how close the proposed structure is to the right answer.\nUsing these scoring functions, we were able to search the protein landscape to find structures that matched our predictions. Our first method built on techniques commonly used in structural biology, and repeatedly replaced pieces of a protein structure with new protein fragments. We trained a generative neural network to invent new fragments, which were used to continually improve the score of the proposed protein structure. \u00a0 \u00a0 \nThe second method optimised scores through \ngradient descent\n\u2014a mathematical technique commonly used in machine learning for making small, incremental improvements\u2014which resulted in highly accurate structures. This technique was applied to entire protein chains rather than to pieces that must be folded separately before being assembled, reducing the complexity of the prediction process.\nThe success of our first foray into protein folding is indicative of how machine learning systems can integrate diverse sources of information to help scientists come up with creative solutions to complex problems at speed. Just as we\u2019ve seen how AI can help people master complex games through systems like \nAlphaGo\n and \nAlphaZero\n, we similarly hope that one day, AI breakthroughs will help us master fundamental scientific problems, too.\nIt\u2019s exciting to see these early signs of progress in protein folding, demonstrating the utility of AI for scientific discovery. Even though there\u2019s a lot more work to do before we\u2019re able to have a quantifiable impact on treating diseases, managing the environment, and more, we know the potential is enormous. With a dedicated team focused on delving into how machine learning can advance the world of science, we\u2019re looking forward to seeing the many ways our technology can make a difference.\nUntil we have published a paper on this work, please cite it as:\nDe novo structure prediction with deep-learning based scoring \nR.Evans, \u00a0J.Jumper, J.Kirkpatrick, L.Sifre, T.F.G.Green, C.Qin, A.Zidek, A.Nelson, A.Bridgland, H.Penedones, S.Petersen, K.Simonyan, S.Crossan, D.T.Jones, D.Silver, K.Kavukcuoglu, D.Hassabis, A.W.Senior\nIn Thirteenth Critical Assessment of Techniques for Protein Structure Prediction (Abstracts) 1-4 December 2018. Retrieved from here \nhere\n.\nThis work was done in collaboration with Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green, Chongli Qin, Augustin Zidek, Sandy Nelson, Alex Bridgland, Hugo Penedones, Stig Petersen, Karen Simonyan, Steve Crossan, David Jones, David Silver, Koray Kavukcuoglu, Demis Hassabis, and Andrew Senior.\n"}
{"title": "Expanding our research on breast cancer screening to Japan", "contents": "Six months ago, we joined a groundbreaking new research partnership led by the Cancer Research UK Imperial Centre at Imperial College London to explore whether AI technology could help clinicians diagnose breast cancers on mammograms quicker and more effectively.\nBreast cancer is a huge global health problem. Around the world, over 1.6 million people are diagnosed with the disease every single year, and 500,000 lose their life to it \u2013 partly because accurately detecting and diagnosing breast cancer still remains a huge challenge.\nWorking alongside leading breast cancer experts, clinicians and academics in the UK, we\u2019ve been exploring whether machine learning (a form of AI) could help address this issue.\nToday, we\u2019re delighted to announce that this project is expanding internationally, with The Jikei University Hospital, one of Japan\u2019s foremost medical institutions, \njoining the collaboration\n as part of a wider five year partnership they have signed with DeepMind Health.\nFor the purposes of this research, they will be working with us to analyse historic, de-identified mammograms from around 30,000 women taken at the hospital between 2007 and 2018. These will be analysed with AI technology alongside the historic de-identified mammograms already provided by the UK OPTIMAM mammography database in order to investigate whether the technology can spot signs of cancerous tissue on these X-rays more effectively than current screening techniques allow. De-identified breast ultrasounds from around 30,000 women and 3,500 de-identified breast MRI scans will also be shared by The Jikei University Hospital during the course of the project.\nWorking with partners and data from multiple countries in a single project is a first for DeepMind Health. We hope that doing so will help us work towards our ambition to create technology that works for everyone around the world, because it will help us minimise bias.\nBias can occur when you train an AI system on data which doesn\u2019t accurately reflect the people it is being designed for, and it\u2019s a serious problem. By under-representing or even excluding certain groups from a dataset \u2013 be it by age, ethnicity, or gender \u2013 you create technology which doesn\u2019t best meet their needs.\nIn health, where genetic and biological differences between certain groups are commonplace, this could have a huge impact on patient care. In the field of mammography, for example, there can be considerable variations in breast density between ethnic groups. Bias in our AI system could therefore result in breast cancers being misidentified or even missed altogether if the technology is not set up to reflect these differences.\nTraining our algorithm on representative datasets from the UK and Japan is one of a number of ways we\u2019re looking to overcome this. In time, we hope to extend this research further to other international partners too.\nAs with all of our research, DeepMind is committed to treating the data from The Jikei University Hospital with the utmost care and respect. As is standard practice, the data being used in the research remains in the full control of our partners, and is being stored to world-class standards of security and encryption. Additionally, all medical information will be de-identified before it is transferred, with any information that could identify an individual being removed before researchers can conduct their analysis. More details on the de-identification process that has been applied to this data can be found \nhere\n.\nProfessor Ara Darzi, Director of the Cancer Research UK Imperial Centre, said: \u201cThe involvement of The Jikei University Hospital in a global research partnership will help take us one step closer to developing technology that could ultimately transform care for the millions of people who develop breast cancer around the world every year.\u201d\nIt\u2019s early days for this work, but we\u2019re optimistic about the long-term potential for AI technology in this area and hope, in time, to explore how it could help in the analysis of other forms of breast imaging. It\u2019s a hugely exciting opportunity to make a difference to breast cancer treatment across the world, and we\u2019ll keep you updated as we continue on this journey.\n\u534a\u5e74\u524d\u3001DeepMind \u306f\u3001\u30a4\u30f3\u30da\u30ea\u30a2\u30eb\u30fb\u30ab\u30ec\u30c3\u30b8\u30fb\u30ed\u30f3\u30c9\u30f3\u306e Cancer Research UK Imperial Centre \u304c\u4e3b\u5c0e\u3059\u308b\u9769\u65b0\u7684\u306a\u30ea\u30b5\u30fc\u30c1\u30d1\u30fc\u30c8\u30ca\u30fc\u30b7\u30c3\u30d7\u3078\u306e\u53c2\u753b\u3092\u767a\u8868\u3057\u307e\u3057\u305f\u3002\u672c\u7814\u7a76\u306f\u3001\u81e8\u5e8a\u533b\u306b\u3088\u308b\u30de\u30f3\u30e2\u30b0\u30e9\u30d5\u30a3\u3092\u7528\u3044\u305f\u4e73\u304c\u3093\u30b9\u30af\u30ea\u30fc\u30cb\u30f3\u30b0\u306b\u304a\u3044\u3066\u3001AI \u6280\u8853\u304c\u30d7\u30ed\u30bb\u30b9\u306e\u8fc5\u901f\u5316\u3084\u52b9\u7387\u5316\u306b\u3044\u304b\u306b\u8ca2\u732e\u3067\u304d\u308b\u304b\u3092\u63a2\u308b\u3053\u3068\u3092\u76ee\u7684\u3068\u3057\u3066\u3044\u307e\u3059\u3002\n\u4e73\u304c\u3093\u306f\u4e16\u754c\u898f\u6a21\u306e\u91cd\u8981\u306a\u5065\u5eb7\u554f\u984c\u3067\u3059\u3002\u305d\u306e\u6b63\u78ba\u306a\u691c\u51fa\u3068\u8a3a\u65ad\u306b\u306f\u672a\u3060\u5927\u304d\u306a\u8ab2\u984c\u304c\u6b8b\u3063\u3066\u304a\u308a\u3001\u4e16\u754c\u3067\u306f\u6bce\u5e74 160 \u4e07\u4eba\u3092\u8d85\u3048\u308b\u4eba\u3005\u304c\u4e73\u304c\u3093\u3068\u8a3a\u65ad\u3055\u308c\u300150 \u4e07\u4eba\u304c\u547d\u3092\u843d\u3068\u3057\u3066\u3044\u307e\u3059\u3002\n\u79c1\u305f\u3061\u306f\u3001\u82f1\u56fd\u306b\u304a\u3051\u308b\u4e73\u304c\u3093\u306e\u512a\u308c\u305f\u5c02\u9580\u533b\u3084\u81e8\u5e8a\u533b\u3001\u7814\u7a76\u8005\u3068\u5354\u529b\u3057\u3001\u3053\u306e\u8ab2\u984c\u89e3\u6c7a\u306b\u5bfe\u3057\u3066\u3001\u6a5f\u68b0\u5b66\u7fd2\uff08AI \u306e\u4e00\u5f62\u5f0f\uff09\u306f\u3069\u306e\u3088\u3046\u306b\u8ca2\u732e\u3067\u304d\u308b\u306e\u304b\u3092\u63a2\u308b\u7814\u7a76\u3092\u884c\u3063\u3066\u3044\u307e\u3059\u3002\n\u672c\u65e5\u3001\u65e5\u672c\u306e\u5148\u9032\u7684\u306a\u533b\u7642\u6a5f\u95a2\u306e\u4e00\u3064\u3067\u3042\u308b\u6771\u4eac\u6148\u6075\u4f1a\u533b\u79d1\u5927\u5b66\u9644\u5c5e\u75c5\u9662\u304c\u3001\u672c\u7814\u7a76\u3078\u306e\u53c2\u52a0\u3092\u767a\u8868\u3055\u308c\u307e\u3057\u305f\u3002\u3053\u308c\u306f\u3001DeepMind Health \u3068\u6771\u4eac\u6148\u6075\u4f1a\u533b\u79d1\u5927\u5b66\u9644\u5c5e\u75c5\u9662\u306e 5 \u5e74\u9593\u306e\u30d1\u30fc\u30c8\u30ca\u30fc\u30b7\u30c3\u30d7\u5951\u7d04\u306b\u57fa\u3065\u304f\u3082\u306e\u3067\u3001\u540c\u75c5\u9662\u306e\u53c2\u753b\u306b\u3088\u308a\u672c\u7814\u7a76\u306e\u56fd\u969b\u5316\u304c\u9032\u3080\u3053\u3068\u3092\u559c\u3070\u3057\u304f\u601d\u3063\u3066\u3044\u307e\u3059\u3002\n\u672c\u7814\u7a76\u3067\u306f\u3001\u6771\u4eac\u6148\u6075\u4f1a\u533b\u79d1\u5927\u5b66\u9644\u5c5e\u75c5\u9662\u3068 DeepMind \u304c\u5171\u540c\u3067\u30012007 \u5e74\uff5e 2018 \u5e74\u306b\u540c\u75c5\u9662\u3067\u3001\u904e\u53bb\u306b\u64ae\u5f71\u3055\u308c\u3001\u304b\u3064\u533f\u540d\u52a0\u5de5\u3055\u308c\u305f \u7d04 30,000 \u4eba\u306e\u5973\u6027\u306e\u30de\u30f3\u30e2\u30b0\u30e9\u30d5\u30a3\u306e\u5206\u6790\u3092\u884c\u3044\u307e\u3059\u3002\u3053\u308c\u3089\u306e\u30c7\u30fc\u30bf\u306f\u3001UK OPTIMAM \uff08\u82f1\u56fd\uff09\u304c\u4fdd\u6709\u3059\u308b\u30de\u30f3\u30e2\u30b0\u30e9\u30d5\u30a3\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u306b\u4fdd\u7ba1\u3055\u308c\u3066\u3044\u308b\u904e\u53bb\u306b\u64ae\u5f71\u3055\u308c\u3001\u533f\u540d\u52a0\u5de5\u3055\u308c\u305f\u30de\u30f3\u30e2\u30b0\u30e9\u30d5\u30a3\u306e\u30c7\u30fc\u30bf\u3068\u5408\u308f\u305b\u3066 AI \u6280\u8853\u306b\u3088\u308b\u89e3\u6790\u3092\u884c\u3044\u3001AI \u6280\u8853\u304c\u73fe\u5728\u306e\u30b9\u30af\u30ea\u30fc\u30cb\u30f3\u30b0\u6280\u8853\u3088\u308a\u3082\u52b9\u679c\u7684\u306b \u00a0X \u7dda\u753b\u50cf\u4e0a\u3067\u304c\u3093\u6027\u7d44\u7e54\u306e\u5146\u5019\u3092\u691c\u51fa\u3067\u304d\u308b\u304b\u691c\u8a0e\u3057\u307e\u3059\u3002\u307e\u305f\u3001\u672c\u7814\u7a76\u306e\u904e\u7a0b\u3067\u306f\u3001\u6771\u4eac\u6148\u6075\u4f1a\u533b\u79d1\u5927\u5b66\u9644\u5c5e\u75c5\u9662\u3088\u308a\u3001\u7d04 3 \u4e07\u4eba\u306e\u5973\u6027\u306e\u533f\u540d\u52a0\u5de5\u3055\u308c\u305f\u4e73\u623f\u8d85\u97f3\u6ce2\u691c\u67fb\u753b\u50cf\u304a\u3088\u3073 3,500 \u306e\u533f\u540d\u52a0\u5de5\u3055\u308c\u305f\u4e73\u623f MRI \u30b9\u30ad\u30e3\u30f3\u753b\u50cf\u306e\u5171\u6709\u3092\u4e88\u5b9a\u3057\u3066\u3044\u307e\u3059\u3002\nDeepMind \u306f\u300c\u4e16\u754c\u4e2d\u306e\u4eba\u3005\u306b\u3068\u3063\u3066\u5f79\u7acb\u3064\u30c6\u30af\u30ce\u30ed\u30b8\u30fc\u3092\u5275\u9020\u3059\u308b\u300d\u3053\u3068\u3092\u305d\u306e\u30df\u30c3\u30b7\u30e7\u30f3\u306b\u63b2\u3052\u3066\u3044\u307e\u3059\u3002DeepMind Health \u306b\u3068\u3063\u3066\u3001\u30d1\u30fc\u30c8\u30ca\u30fc\u3084\u30c7\u30fc\u30bf\u304c\u8907\u6570\u56fd\u306b\u307e\u305f\u304c\u308b\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306f\u521d\u306e\u8a66\u307f\u3067\u3059\u304c\u3001\u307e\u3055\u306b\u3053\u306e\u3088\u3046\u306a\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u304c\u30d0\u30a4\u30a2\u30b9\uff08\u504f\u308a\uff09\u306e\u6975\u5c0f\u5316\u3092\u52a9\u3051\u3001\u76ee\u6a19\u306e\u5b9f\u73fe\u3078\u3068\u524d\u9032\u3055\u305b\u308b\u3053\u3068\u306b\u7e4b\u304c\u308b\u3082\u306e\u3068\u8003\u3048\u3066\u3044\u307e\u3059\u3002\n\u300c\u30d0\u30a4\u30a2\u30b9\u300d\u306f\u3001\u6b63\u3057\u304f\u5bfe\u8c61\u3092\u53cd\u6620\u3057\u306a\u3044\u30c7\u30fc\u30bf\uff08\u504f\u3063\u305f\u30c7\u30fc\u30bf\uff09\u3092\u7528\u3044\u3066 AI \u30b7\u30b9\u30c6\u30e0\u3092\u8a13\u7df4\u3057\u305f\u5834\u5408\u306b\u8d77\u304d\u308b\u6df1\u523b\u306a\u554f\u984c\u3067\u3059\u3002\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u304b\u3089\u5e74\u9f62\u3001\u4eba\u7a2e\uff08\u6c11\u65cf\uff09\u3001\u6027\u5225\u7b49\u306e\u7279\u5b9a\u306e\u30b0\u30eb\u30fc\u30d7\u3092\u904e\u5c0f\u8a55\u4fa1\u3057\u305f\u308a\u3001\u9664\u5916\u3057\u305f\u308a\u3059\u308b\u3068\u3001\u30cb\u30fc\u30ba\u306b\u5408\u308f\u306a\u3044\u6280\u8853\u3092\u751f\u307f\u51fa\u3059\u3053\u3068\u306b\u306a\u308a\u304b\u306d\u307e\u305b\u3093\u3002\n\u533b\u7642\u306e\u4e16\u754c\u306b\u304a\u3044\u3066\u3001\u7279\u5b9a\u30b0\u30eb\u30fc\u30d7\u9593\u306b\u907a\u4f1d\u7684\u304a\u3088\u3073\u751f\u7269\u5b66\u7684\u306a\u76f8\u9055\u304c\u3042\u308b\u3053\u3068\u306f\u5f53\u7136\u3067\u3001\u60a3\u8005\u306e\u30b1\u30a2\u306b\u3082\u5927\u304d\u306a\u5f71\u97ff\u3092\u4e0e\u3048\u307e\u3059\u3002\u30de\u30f3\u30e2\u30b0\u30e9\u30d5\u30a3\u306e\u5206\u91ce\u3092\u3068\u3063\u3066\u307f\u308b\u3068\u3001\u4eba\u7a2e\uff08\u6c11\u65cf\uff09\u306b\u3088\u3063\u3066\u4e73\u817a\u6fc3\u5ea6\u306b\u304b\u306a\u308a\u306e\u5909\u52d5\u304c\u3042\u308b\u3053\u3068\u304c\u308f\u304b\u3063\u3066\u3044\u307e\u3059\u3002\u3064\u307e\u308a\u3001\u3053\u308c\u3089\u306e\u9055\u3044\u304c AI \u30b7\u30b9\u30c6\u30e0\u306b\u53cd\u6620\u3055\u308c\u306a\u3051\u308c\u3070\u3001\u305d\u306e\u30b7\u30b9\u30c6\u30e0\u306f\u4e73\u304c\u3093\u3092\u6b63\u3057\u304f\u691c\u51fa\u3067\u304d\u306a\u304b\u3063\u305f\u308a\u3001\u898b\u9003\u3059\u3068\u3044\u3063\u305f\u7d50\u679c\u3092\u62db\u304f\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002\n\u4eca\u56de\u306e\u767a\u8868\u306e\u3088\u3046\u306b\u3001\u82f1\u56fd\u3068\u65e5\u672c\u306e\u4ee3\u8868\u7684\u306a\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u7528\u3044\u3066\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u8a13\u7df4\u3059\u308b\u65b9\u6cd5\u306f\u3001\u30d0\u30a4\u30a2\u30b9\u3092\u514b\u670d\u3059\u308b\u305f\u3081\u306e\u6709\u52b9\u306a\u624b\u6bb5\u306e\u4e00\u3064\u3067\u3042\u308b\u3068\u8003\u3048\u3066\u3044\u307e\u3059\u3002\u4eca\u5f8c\u3001\u672c\u7814\u7a76\u3092\u4ed6\u56fd\u306e\u30d1\u30fc\u30c8\u30ca\u30fc\u6a5f\u95a2\u306b\u3082\u62e1\u5927\u3057\u3066\u3044\u304d\u305f\u3044\u3068\u8003\u3048\u3066\u3044\u307e\u3059\u3002\nDeepMind \u306b\u304a\u3051\u308b\u5168\u3066\u306e\u7814\u7a76\u3068\u540c\u69d8\u306b\u3001\u79c1\u305f\u3061\u306f\u6771\u4eac\u6148\u6075\u4f1a\u533b\u79d1\u5927\u5b66\u9644\u5c5e\u75c5\u9662\u306e\u30c7\u30fc\u30bf\u3092\u7d30\u5fc3\u306e\u6ce8\u610f\u3068\u656c\u610f\u306e\u3082\u3068\u306b\u53d6\u6271\u3046\u3053\u3068\u3092\u304a\u7d04\u675f\u3057\u307e\u3059\u3002\u6a19\u6e96\u7684\u306a\u624b\u9806\u306b\u5f93\u3044\u3001\u7814\u7a76\u3067\u4f7f\u7528\u3055\u308c\u3066\u3044\u308b\u30c7\u30fc\u30bf\u306f\u3001\u5f15\u304d\u7d9a\u304d\u5b8c\u5168\u306b\u30d1\u30fc\u30c8\u30ca\u30fc\u6a5f\u95a2\u306e\u7ba1\u7406\u4e0b\u306b\u3042\u308a\u3001\u4e16\u754c\u6a19\u6e96\u306e\u30bb\u30ad\u30e5\u30ea\u30c6\u30a3\u3068\u6697\u53f7\u5316\u306b\u3088\u308a\u4fdd\u8b77\u3055\u308c\u307e\u3059\u3002\u307e\u305f\u3001\u3059\u3079\u3066\u306e\u533b\u7642\u60c5\u5831\u306f\u3001DeepMind \u3078\u306e\u63d0\u4f9b\u524d\u306b\u533f\u540d\u52a0\u5de5\u51e6\u7406\u304c\u884c\u308f\u308c\u3001\u500b\u4eba\u304c\u8b58\u5225\u3067\u304d\u308b\u60c5\u5831\u306f\u7814\u7a76\u8005\u304c\u5206\u6790\u3092\u884c\u3046\u524d\u306b\u524a\u9664\u3055\u308c\u307e\u3059\u3002\u3053\u306e\u30c7\u30fc\u30bf\u306b\u9069\u7528\u3055\u308c\u308b\u533f\u540d\u52a0\u5de5\u306e\u30d7\u30ed\u30bb\u30b9\u306b\u3064\u3044\u3066\u3001\u8a73\u7d30\u306f \nPIPA\n\u3092\u3054\u53c2\u7167\u304f\u3060\u3055\u3044\u3002\nCancer Research UK Imperial Centre \u30c7\u30a3\u30ec\u30af\u30bf\u30fc\u3067\u3042\u308b Ara Darzi \u6559\u6388\u306f\u3001\u6b21\u306e\u3088\u3046\u306b\u8ff0\u3079\u3066\u3044\u307e\u3059\u3002\u300c\u30b0\u30ed\u30fc\u30d0\u30eb\u306a\u30ea\u30b5\u30fc\u30c1\u30d1\u30fc\u30c8\u30ca\u30fc\u30b7\u30c3\u30d7\u3078\u306e\u6771\u4eac\u6148\u6075\u4f1a\u533b\u79d1\u5927\u5b66\u9644\u5c5e\u75c5\u9662\u306e\u53c2\u753b\u306f\u3001\u6bce\u5e74\u4e16\u754c\u4e2d\u3067\u4e73\u304c\u3093\u3092\u767a\u75c7\u3059\u308b\u6570\u767e\u4e07\u4eba\u306e\u4eba\u3005\u306e\u6cbb\u7642\u3092\u5909\u9769\u3059\u308b\u304b\u3082\u3057\u308c\u306a\u3044\u6280\u8853\u306e\u958b\u767a\u306b\u4e00\u6b69\u8fd1\u3065\u304f\u3053\u3068\u3092\u610f\u5473\u3057\u3066\u3044\u307e\u3059\u3002\n\u672c\u53d6\u7d44\u307f\u306f\u7dd2\u306b\u5c31\u3044\u305f\u3070\u304b\u308a\u3067\u3059\u304c\u3001\u3053\u306e\u5206\u91ce\u306b\u304a\u3051\u308b AI \u30c6\u30af\u30ce\u30ed\u30b8\u30fc\u306e\u9577\u671f\u7684\u306a\u53ef\u80fd\u6027\u306b\u3064\u3044\u3066\u3001\u79c1\u305f\u3061\u306f\u697d\u89b3\u7684\u3067\u3042\u308a\u3001\u5c06\u6765\u7684\u306b\u306f\u4ed6\u306e\u30bf\u30a4\u30d7\u306e\u4e73\u623f\u753b\u50cf\u8a3a\u65ad\u306b\u304a\u3044\u3066\u3082\u5f79\u7acb\u3064\u65b9\u6cd5\u3092\u63a2\u6c42\u3067\u304d\u308b\u53ef\u80fd\u6027\u306b\u671f\u5f85\u3092\u6301\u3063\u3066\u3044\u307e\u3059\u3002\u3053\u308c\u306f\u4e16\u754c\u306e\u4e73\u304c\u3093\u6cbb\u7642\u3092\u5909\u3048\u308b\u304b\u3082\u3057\u308c\u306a\u3044\u6311\u6226\u3067\u3059\u3002\u7814\u7a76\u306e\u9032\u6357\u306b\u3042\u308f\u305b\u3066\u3001\u7686\u69d8\u306b\u6700\u65b0\u306e\u60c5\u5831\u3092\u304a\u5c4a\u3051\u3057\u3066\u53c2\u308a\u307e\u3059\u3002\n"}
{"title": "Open sourcing TRFL: a library of reinforcement learning building blocks", "contents": "Today we are open sourcing a \nnew library\n of useful building blocks for writing reinforcement learning (RL) agents in TensorFlow. Named TRFL (pronounced \u2018truffle\u2019), it represents a collection of key algorithmic components that we have used internally for a large number of our most successful agents such as DQN, DDPG and the Importance Weighted Actor Learner Architecture.\nA typical deep reinforcement learning agent consists of a large number of interacting components: at the very least, these include the environment and some deep network representing values or policies, but they often also include components such as a learned model of the environment, pseudo-reward functions or a replay system.\nThese parts tend to interact in subtle ways (often not well-documented in papers, as highlighted by \nHenderson and colleagues\n), thus making it difficult to identify bugs in such large computational graphs. A recent \nblog post by OpenAI\n highlighted this issue by analysing some of the most popular open-source implementations of reinforcement learning agents and finding that six out of 10 \u201chad subtle bugs found by a community member and confirmed by the author\u201d.\nOne approach to addressing this issue, and helping those in the research community attempting to reproduce results from papers, is through open-sourcing complete agent implementations. For example, this is what we did recently with our \nscalable distributed implementation of the v-trace agent\n. These large agent codebases can be very useful for reproducing research, but also hard to modify and extend. A different and complementary approach is to provide reliable, well-tested implementations of common building blocks, that can be used in a variety of different RL agents. Moreover, having these core components abstracted away in a single library, with a consistent API, makes it simpler to combine ideas originating from various different publications.\nThe TRFL library includes functions to implement both classical RL algorithms as well as more cutting-edge techniques. \u00a0The loss functions and other operations provided here are implemented in pure TensorFlow. They are not complete algorithms, but implementations of RL-specific mathematical operations needed when building fully-functional RL agents.\nFor value-based reinforcement learning we provide TensorFlow ops for learning in discrete action spaces, such as TD-learning, Sarsa, Q-learning and their variants, as well as ops for implementing continuous control algorithms, such as DPG. We also include ops for learning distributional value functions. These ops support batches, and return a loss that can be minimised by feeding it to a TensorFlow Optimiser. Some losses operate over batches of transitions (e.g. Sarsa, Q learning, ...), and others over batches of trajectories (e.g. Q lambda, Retrace, \u2026). For policy-based methods, we have utilities to easily implement both online methods such as A2C, as well as supporting off-policy correction techniques, such as v-trace. The computation of policy gradients in continuous action spaces is also supported. \u00a0Finally, TRFL also provides an implementation of the auxiliary pseudo-reward functions used by UNREAL, which we have found to improve data efficiency in a variety of domains.\nThis is not a one-time release. Since this library is used extensively within DeepMind, we will continue to maintain it as well as add new functionalities over time. We are also eager to \nreceive contributions\n to the library by the wider RL community.\nThis library was created by the Research Engineering team at DeepMind.\n"}
{"title": "Predicting eye disease with Moorfields Eye Hospital", "contents": "In August, we \nannounced the first stage\n of our \njoint research partnership with Moorfields Eye Hospital\n, which showed how AI could match world-leading doctors at recommending the correct course of treatment for over 50 eye diseases, and also explain how it arrives at its recommendations.\nNow we\u2019re excited to start working on the next research challenge \u2013 whether we can help clinicians predict eye diseases before symptoms set in.\nThere are two types of age-related macular degeneration (AMD), one of the most common blinding eye diseases, with 170 million sufferers worldwide. The \u2018dry\u2019 form is relatively common among those over 65, and often only causes mild sight loss. However, about 15% of patients with dry AMD go on to develop the more serious form of the disease \u2013 \u2018wet\u2019 AMD \u2013 which can cause permanent, blinding sight loss.\nCurrently, ophthalmologists diagnose wet AMD by analysing highly detailed 3D scans of the back of the eye, called OCT scans. The first phase of our research suggested that our AI technology could help clinicians analyse these scans more quickly to detect the symptoms of patients who need urgent treatment \u2013 ultimately saving their sight.\nThat could help patients who already display symptoms: but what about those who haven\u2019t yet developed them? If AI could help predict severe eye diseases in advance, that could help clinicians prevent sight loss before it even occurs.\nCollaborating with the clinicians at Moorfields Eye Hospital, we\u2019ll be analysing de-identified scans of up to 7,000 patients at Moorfields who had previously received treatment for wet AMD in one eye, to try to predict deterioration in the other, seemingly healthy eye.\nPredicting potential indicators of disease is a much more complicated \u2013 and computationally intense \u2013 task than identifying existing known symptoms. To carry out this research reliably, efficiently and at scale, we have agreed with Moorfields to use Google\u2019s world-class cloud computing infrastructure, which is already being used in our partnership with Cancer Research UK on research to improve the diagnosis of breast cancer. Moorfields and DeepMind researchers will initially access this in the UK and US, but it may one day include cloud facilities around the world.\nThe benefits of using the cloud have been endorsed by \nNHS Digital\n as well as other regulatory bodies. We believe that using this infrastructure \u2013 with its greater size and scalability, reliability and processing power \u2013 will help us achieve the best possible results, which could take us one step closer to improving patient care.\nAs with all of our work, we\u2019re committed to treating the data used in this research with the utmost care and respect. Data is encrypted and de-identified, accessible only to a limited number of researchers who are conducting this research. All access to data is automatically audited and logged, and granted only for officially approved research purposes, with Moorfields\u2019 permission.\nWe\u2019re thrilled to be working with our partners at Moorfields to take this research a step further. This is the first phase of a number of new and exciting research projects that we will be working on in the coming months. We will keep you updated as we make progress.\n"}
{"title": "AlphaStar: Mastering the real-time strategy game StarCraft II", "contents": "Games have been used for decades as an important way to test and evaluate the performance of artificial intelligence systems. As capabilities have increased, the research community has sought games with increasing complexity that capture different elements of intelligence required to solve scientific and real-world problems. In recent years, StarCraft, considered to be one of the most challenging Real-Time Strategy (RTS) games and one of the longest-played esports of all time, has emerged by consensus as a \u201cgrand challenge\u201d for AI research.\nNow, we introduce our \nStarCraft II\n program AlphaStar, the first Artificial Intelligence to defeat a top professional player. In a series of test matches held on 19 December, AlphaStar decisively beat \nTeam Liquid\u2019s\n Grzegorz \"\nMaNa\n\" Komincz, \none of the world\u2019s strongest professional StarCraft players\n, 5-0, following a successful benchmark match against his team-mate Dario \u201c\nTLO\n\u201d W\u00fcnsch. The matches took place under professional match conditions on a competitive ladder \nmap\n and without any game restrictions.\nAlthough there have been significant successes in video games such as \nAtari\n, \nMario\n, \nQuake III Arena Capture the Flag\n, and \nDota 2\n, until now, AI techniques have struggled to cope with the complexity of StarCraft. The best \nresults\n were made possible by hand-crafting major elements of the system, imposing significant restrictions on the game rules, giving systems superhuman capabilities, or by playing on simplified maps. Even with these modifications, no system has come anywhere close to rivalling the skill of professional players. In contrast, AlphaStar plays the full game of StarCraft II, using a deep neural network that is trained directly from raw game data by \nsupervised learning \nand \nreinforcement learning\n.\nStarCraft II, created by \nBlizzard Entertainment\n, is set in a fictional sci-fi universe and features rich, multi-layered gameplay designed to challenge human intellect. Along with the original title, it is among the biggest and most successful games of all time, with players competing in esports tournaments for more than 20 years.\nThere are several different ways to play the game, but in esports the most common is a 1v1 tournament played over five games. To start, a player must choose to play one of three different alien \u201craces\u201d - Zerg, Protoss or Terran, all of which have distinctive characteristics and abilities (although professional players tend to specialise in one race). Each player starts with a number of worker units, which gather basic resources to build more units and structures and create new technologies. These in turn allow a player to harvest other resources, build more sophisticated bases and structures, and develop new capabilities that can be used to outwit the opponent. To win, a player must carefully balance big-picture management of their economy - known as macro - along with low-level control of their individual units - known as micro.\nThe need to balance short and long-term goals and adapt to unexpected situations, poses a huge challenge for systems that have often tended to be brittle and inflexible. Mastering this problem requires breakthroughs in several AI research challenges including:\nDue to these immense challenges, StarCraft has emerged as a \u201cgrand challenge\u201d for AI research. Ongoing competitions in both StarCraft and StarCraft II have assessed progress since the launch of the BroodWar API in 2009, including the \nAIIDE StarCraft AI Competition\n, CIG StarCraft Competition, \nStudent StarCraft AI Tournament\n, and the \nStarcraft II AI Ladder\n. To help the community explore these problems further, \nwe worked with Blizzard in 2016 and 2017 to release an open-source set of tools known as PySC2\n, including the largest set of anonymised game replays ever released. We have now built on this work, combining engineering and algorithmic breakthroughs to produce AlphaStar.\nAlphaStar\u2019s behaviour is generated by a deep \nneural network\n that receives input data from the raw game interface (a list of units and their properties), and outputs a sequence of instructions that constitute an action within the game. More specifically, the neural network architecture applies a \ntransformer\n torso to the units (similar to \nrelational deep reinforcement learning\n), combined with a \ndeep LSTM core\n, an \nauto-regressive policy head\n with a \npointer network\n, and a \ncentralised value baseline\n. We believe that this advanced model will help with many other challenges in machine learning research that involve long-term sequence modelling and large output spaces such as translation, language modelling and visual representations.\nAlphaStar also uses a novel multi-agent learning algorithm. The neural network was initially trained by supervised learning from anonymised human games \nreleased by Blizzard\n. This allowed AlphaStar to learn, by imitation, the basic micro and macro-strategies used by players on the StarCraft ladder. This initial agent defeated the built-in \u201cElite\u201d level AI - around gold level for a human player - in 95% of games.\nThese were then used to seed a multi-agent reinforcement learning process. A continuous league was created, with the agents of the league - competitors - playing games against each other, akin to how humans experience the game of StarCraft by playing on the \nStarCraft ladder\n. New competitors were dynamically added to the league, by branching from existing competitors; each agent then learns from games against other competitors. This new form of training takes the ideas of \npopulation-based\n and \nmulti-agent\n reinforcement learning further, creating a process that continually explores the huge strategic space of StarCraft gameplay, while ensuring that each competitor performs well against the strongest strategies, and does not forget how to defeat earlier ones.\nAs the league progresses and new competitors are created, new counter-strategies emerge that are able to defeat the earlier strategies. While some new competitors execute a strategy that is merely a refinement of a previous strategy, others discover drastically new strategies consisting of entirely new build orders, unit compositions, and micro-management plans. For example, early on in the AlphaStar league, \u201ccheesy\u201d strategies such as very quick rushes with \nPhoton Cannons\n or \nDark Templars\n were favoured. These risky strategies were discarded as training progressed, leading to other strategies: for example, gaining economic strength by over-extending a base with more workers, or sacrificing two \nOracles\n to disrupt an opponent's workers and economy. This process is similar to the way in which players have discovered new strategies, and were able to defeat previously favoured approaches, over the years since StarCraft was released.\nTo encourage diversity in the league, each agent has its own learning objective: for example, which competitors should this agent aim to beat, and any additional internal motivations that bias how the agent plays. One agent may have an objective to beat one specific competitor, while another agent may have to beat a whole distribution of competitors, but do so by building more of a particular game unit. These learning objectives are adapted during training.\nThe neural network weights of each agent are updated by reinforcement learning from its games against competitors, to optimise its personal learning objective. The weight update rule is an efficient and novel \noff-policy actor-critic\n reinforcement learning algorithm with \nexperience replay\n, \u00a0\nself-imitation learning\n and \npolicy distillation\n.\nIn order to train AlphaStar, we built a highly scalable distributed training setup using \nGoogle's v3 TPUs that\n supports a population of agents learning from many thousands of parallel instances of StarCraft II. The AlphaStar league was run for 14 days, using 16 TPUs for each agent. During training, each agent experienced up to 200 years of real-time StarCraft play. The final AlphaStar agent consists of the components of the \nNash distribution of the league\n - in other words, the most effective mixture of strategies that have been discovered - that run on a single desktop GPU.\nA full technical description of this work is being prepared for publication in a peer-reviewed journal.\nProfessional StarCraft players such as TLO and MaNa are able to issue hundreds of \nactions per minute\n (APM) on average. This is far fewer than the majority of \nexisting bots\n, which control each unit independently and consistently maintain thousands or even tens of thousands of APMs.\nIn its games against TLO and MaNa, AlphaStar had an average APM of around 280, significantly lower than the professional players, although its actions may be more precise. This lower APM is, in part, because AlphaStar starts its training using replays and thus mimics the way humans play the game. Additionally, AlphaStar reacts with a delay between observation and action of 350ms on average.\nDuring the matches against TLO and MaNa, AlphaStar interacted with the StarCraft game engine directly via its raw interface, meaning that it could observe the attributes of its own and its opponent\u2019s visible units on the map directly, without having to move the camera - effectively playing with a zoomed out view of the game. In contrast, human players must explicitly manage an \"economy of attention\" to decide where to focus the camera. However, analysis of AlphaStar\u2019s games suggests that it manages an implicit focus of attention. On average, agents \u201cswitched context\u201d about 30 times per minute, similar to MaNa or TLO.\nAdditionally, and subsequent to the matches, we developed a second version of AlphaStar. Like human players, this version of AlphaStar chooses when and where to move the camera, its perception is restricted to on-screen information, and action locations are restricted to its viewable region.\nWe trained two new agents, one using the raw interface and one that must learn to control the camera, against the AlphaStar league. Each agent was initially trained by supervised learning from human data followed by the reinforcement learning procedure outlined above. The version of AlphaStar using the camera interface was almost as strong as the raw interface, exceeding 7000 MMR on our internal leaderboard. In an exhibition match, MaNa defeated a prototype version of AlphaStar using the camera interface, that was trained for just 7 days. We hope to evaluate a fully trained instance of the camera interface in the near future. \nThese results suggest that AlphaStar\u2019s success against MaNa and TLO was in fact due to superior macro and micro-strategic decision-making, rather than superior click-rate, faster reaction times, or the raw interface.\nThe game of StarCraft allows players to select one of three alien races: Terran, Zerg or Protoss. We elected for AlphaStar to specialise in playing a single race for now - Protoss - to reduce training time and variance when reporting results from our internal league. Note that the same training pipeline could be applied to any race. Our agents were trained to play StarCraft II (v4.6.2) in Protoss v Protoss games, on the CatalystLE ladder map. To evaluate AlphaStar\u2019s performance, we initially tested our agents against \nTLO\n: a \ntop professional Zerg player\n and a GrandMaster level Protoss player. AlphaStar won the match 5-0, using a wide variety of units and build orders. \u201cI was surprised by how strong the agent was,\u201d he said. \u201cAlphaStar takes well-known strategies and turns them on their head. The agent demonstrated strategies I hadn\u2019t thought of before, which means there may still be new ways of playing the game that we haven\u2019t fully explored yet.\u201d\nAfter training our agents for an additional week, we played against MaNa, \none of the world\u2019s strongest StarCraft II players\n, and among the 10 strongest Protoss players. AlphaStar again won by 5 games to 0, demonstrating strong micro and macro-strategic skills. \u201cI was impressed to see AlphaStar pull off advanced moves and different strategies across almost every game, using a very human style of gameplay I wouldn\u2019t have expected,\u201d he said. \u201cI\u2019ve realised how much my gameplay relies on forcing mistakes and being able to exploit human reactions, so this has put the game in a whole new light for me. We\u2019re all excited to see what comes next.\u201d\nWhile StarCraft is just a game, albeit a complex one, we think that the techniques behind AlphaStar could be useful in solving other problems. For example, its neural network architecture is capable of modelling very long sequences of likely actions - with games often lasting up to an hour with tens of thousands of moves - based on imperfect information. Each frame of StarCraft is used as one step of input, with the neural network predicting the expected sequence of actions for the rest of the game after every frame. The fundamental problem of making complex predictions over very long sequences of data appears in many real world challenges, such as weather prediction, climate modelling, language understanding and more. We\u2019re very excited about the potential to make significant advances in these domains using learnings and developments from the AlphaStar project.\nWe also think some of our training methods may prove useful in the study of safe and robust AI. One of the great challenges in AI is the number of ways in which systems could go wrong, and StarCraft pros have previously found it easy to beat AI systems by finding inventive ways to provoke these mistakes. AlphaStar\u2019s innovative league-based training process finds the approaches that are most reliable and least likely to go wrong. We\u2019re excited by the potential for this kind of approach to help improve the safety and robustness of AI systems in general, particularly in safety-critical domains like energy, where it\u2019s essential to address complex edge cases.\nAchieving the highest levels of StarCraft play represents a major breakthrough in one of the most complex video games ever created. We believe that these advances, alongside other recent progress in projects such as \nAlphaZero\n and \nAlphaFold\n, represent a step forward in our mission to create intelligent systems that will one day help us unlock novel solutions to some of the world\u2019s most important and fundamental scientific problems.\nWe are thankful for the support and immense skill of Team Liquid\u2019s TLO and MaNa. We are also grateful for the continued support of Blizzard and the StarCraft community for making this work possible.\nAlphaStar Team:\nOriol Vinyals, Igor Babuschkin, Junyoung Chung, Michael Mathieu, Max Jaderberg, Wojtek Czarnecki, Andrew Dudzik, Aja Huang, Petko Georgiev, Richard Powell, Timo Ewalds, Dan Horgan, Manuel Kroiss, Ivo Danihelka, John Agapiou, Junhyuk Oh, Valentin Dalibard, David Choi, Laurent Sifre, Yury Sulsky, Sasha Vezhnevets, James Molloy, Trevor Cai, David Budden, Tom Paine, Caglar Gulcehre, Ziyu Wang, Tobias Pfaff, Toby Pohlen, Yuhuai Wu, Dani Yogatama, Julia Cohen, Katrina McKinney, Oliver Smith, Tom Schaul, Timothy Lillicrap, Chris Apps, Koray Kavukcuoglu, Demis Hassabis, David Silver\nWith thanks to:\nAli Razavi, Daniel Toyama, David Balduzzi, Doug Fritz, Eser Ayg\u00fcn, Florian Strub, Guillaume Alain, Haoran Tang, Jaume Sanchez, Jonathan Fildes, Julian Schrittwieser, Justin Novosad, Karen Simonyan, Karol Kurach, Philippe Hamel, Remi Leblond, Ricardo Barreira, Scott Reed, Sergey Bartunov, Shibl Mourad, Steve Gaffney, Thomas Hubert, the \nteam that created PySC2\n and the whole DeepMind Team, with special thanks to the research platform team, comms and events teams.\n"}
{"title": "TF-Replicator: Distributed Machine Learning for Researchers", "contents": "At DeepMind, the Research Platform Team builds infrastructure to empower and accelerate our AI research. Today, we are excited to share how we developed TF-Replicator, a software library that helps researchers deploy their TensorFlow models on GPUs and \nCloud TPUs\n with minimal effort and no previous experience with distributed systems. TF-Replicator\u2019s programming model has now been open sourced as part of TensorFlow\u2019s \ntf.distribute.Strategy\n. This blog post gives an overview of the ideas and technical challenges underlying TF-Replicator. For a more comprehensive description, please read our \narXiv paper\n.\nA recurring theme in recent AI breakthroughs - from \nAlphaFold\n to \nBigGAN\n to \nAlphaStar\n - \u00a0is the need for effortless and reliable scalability. Increasing amounts of computational capacity allow researchers to train ever-larger neural networks with new capabilities. To address this, the Research Platform Team developed TF-Replicator, which allows researchers to target different hardware accelerators for Machine Learning, scale up workloads to many devices, and seamlessly switch between different types of accelerators. While it was initially developed as a library on top of TensorFlow, TF-Replicator\u2019s API has since been integrated into TensorFlow 2.0\u2019s new \ntf.distribute.Strategy\n.\nWhile TensorFlow provides direct support for CPU, GPU, and TPU (\nTensor Processing Unit\n) devices, switching between targets requires substantial effort from the user. This typically involves specialising code for a particular hardware target, constraining research ideas to the capabilities of that platform. Some existing frameworks built on top of TensorFlow, e.g. \nEstimators\n, seek to address this problem. However, they are typically targeted at production use cases and lack the expressivity and flexibility required for rapid iteration of research ideas.\nOur original motivation for developing TF-Replicator was to provide a simple API for DeepMind researchers to use \nTPUs\n. TPUs provide scalability for Machine Learning workloads, enabling research breakthroughs such as state-of-the-art image synthesis with our \nBigGAN\n model. \nTensorFlow\u2019s native API for TPUs\n differs from how GPUs are targeted, forming a barrier to TPU adoption. TF-Replicator provides a simpler, more user-friendly API that hides the complexity of TensorFlow\u2019s TPU API. Critically, the Research Platform Team developed the TF-Replicator API in close collaboration with researchers across various machine learning disciplines to ensure the necessary flexibility and ease-of-use.\nCode written using TF-Replicator looks similar to code written in TensorFlow for a single device, allowing users the freedom to define their own model run loop. The user simply needs to define (1) an input function that exposes a Dataset, and (2) a step function that defines the logic of their model (e.g. a single step of gradient descent):\nScaling computation to multiple devices requires the devices to communicate with each other. In the context of training Machine Learning models, the most common form of communication is to accumulate gradients for use in optimisation algorithms such as \nStochastic Gradient Descent\n. We therefore provide a convenient method to wrap \nTensorFlow Optimizers\n, so that gradients are accumulated across devices before updating the model\u2019s parameters. For more general communication patterns we provide \nMPI\n-like primitives, such as `all_reduce` and `broadcast`. These make it trivial to implement operations such as global batch normalisation, a technique that is crucial to scale up training of our \nBigGAN\n models (see Section 3 of the paper).\nFor multi-GPU computation TF-Replicator relies on an \u201cin-graph replication\u201d pattern, where the computation for each device is replicated in the same TensorFlow graph. Communication between devices is achieved by connecting nodes from the devices\u2019 corresponding sub-graphs. Implementing this in TF-Replicator was challenging, as communication can occur at any point in the data-flow graph. The order in which computations are constructed is therefore critical.\nOur first idea was to build each device\u2019s sub-graph concurrently in a separate Python thread. When encountering a communication primitive, the threads synchronise and the main thread inserts the required cross-device computation. After that, each thread would continue building its device\u2019s computation. However, at the time we considered this approach, TensorFlow\u2019s graph building API was not thread-safe which made concurrently building sub-graphs in different threads very difficult. Instead, we used \ngraph rewriting\n to insert the communication after all devices\u2019 sub-graphs had been built. When constructing the sub-graphs, placeholders are inserted in places where communication is required. We then collect all matching placeholders across devices and replace them with the appropriate cross-device computation.\nBy collaborating closely with researchers throughout the design and implementation of TF-Replicator, we were able to build a library that allows users to easily scale computation across many hardware accelerators, while leaving them with the control and flexibility required to do cutting-edge AI research. For example, we added MPI-style communication primitives such as all-reduce following discussion with researchers. TF-Replicator and other shared infrastructure allows us to build increasingly complex experiments on robust foundations and quickly spread best practices throughout DeepMind.\nAt the time of writing, TF-Replicator is the most widely used interface for TPU programming at DeepMind. While the library itself is not constrained to training neural networks, it is most commonly used for training on large batches of data. The \nBigGAN\n model, for example, was trained on batches of size 2048 across up to 512 cores of a TPUv3 pod. In Reinforcement Learning agents with a distributed actor-learner setup, such as \nour importance weighted actor-learner architectures\n, scalability is achieved by having many actors generating new experiences by interacting with the environment. This data is then processed by the learner to improve the agent\u2019s policy, represented as a neural network. To cope with an increasing number of actors, TF-Replicator can be used to easily distribute the learner across many hardware accelerators. These and other examples are described in more detail in \nour arXiv paper\n.\nTF-Replicator is just one of many examples of impactful technology built by DeepMind\u2019s Research Platform Team. Many of DeepMind\u2019s breakthroughs in AI, from AlphaGo to AlphaStar, were enabled by the team. If you share our mission and are excited about accelerating state-of-the-art AI research, look out for open Software Engineering positions in Research Platform at \nhttps://deepmind.com/careers\n \n(machine learning experience is optional for these roles).\nThis work was completed by the Research Platform Team at DeepMind. We\u2019d like to thank Frederic Besse, Fabio Viola, John Aslanides, Andy Brock, Aidan Clark, Sergio G\u00f3mez Colmenarejo, Karen Simonyan, Sander Dieleman, Lasse Espeholt, Akihiro Matsukawa, Tim Harley, Jean-Baptiste Lespiau, Koray Kavukcuoglu, Dan Belov and many others at DeepMind for their valuable feedback throughout the development of TF-Replicator. We'd also like to thank Priya Gupta, Jonathan Hseu, Josh Levenberg, Martin Wicke and others at Google for making these ideas available to all TensorFlow users as part of tf.distribute.Strategy.\n"}
{"title": "AlphaZero: Shedding new light on chess, shogi, and Go", "contents": "In late 2017 we \nintroduced AlphaZero\n, a single system that taught itself from scratch how to master the games of chess, \nshogi\n(Japanese chess), and \nGo\n, beating a world-champion program in each case. We were excited by the preliminary results and thrilled to see the response from members of the chess community, who saw in AlphaZero\u2019s games a ground-breaking, highly dynamic and \u201c\nunconventional\n\u201d style of play that differed from any chess playing engine that came before it.\nToday, we are delighted to introduce the full evaluation of AlphaZero, \npublished in the journal Science\n (\nOpen Access version here\n), that confirms and updates those preliminary results. It describes how AlphaZero quickly learns each game to become the strongest player in history for each, despite starting its training from random play, with no in-built domain knowledge but the basic rules of the game.\nThis ability to learn each game afresh, unconstrained by the norms of human play, results in a distinctive, unorthodox, yet creative and dynamic playing style. Chess Grandmaster Matthew Sadler and Women\u2019s International Master Natasha Regan, who have analysed thousands of AlphaZero\u2019s chess games for \ntheir forthcoming book Game Changer\n (New in Chess, January 2019), say its style is unlike any traditional chess engine.\u201d It\u2019s like discovering the secret notebooks of some great player from the past,\u201d says Matthew.\nTraditional chess engines \u00a0\u2013 including the world computer chess champion \nStockfish\n and \nIBM\u2019s ground-breaking Deep Blue\n \u2013 rely on thousands of rules and heuristics handcrafted by strong human players that try to account for every eventuality in a game. Shogi programs are also game specific, using similar search engines and algorithms to chess programs.\nAlphaZero takes a totally different approach, replacing these hand-crafted rules with a deep \nneural network\n and general purpose algorithms that know nothing about the game beyond the basic rules.\nTo learn each game, an untrained neural network plays millions of games against itself via a process of trial and error called \nreinforcement learning\n. At first, it plays completely randomly, but over time the system learns from wins, losses, and draws to adjust the parameters of the neural network, making it more likely to choose advantageous moves in the future. The amount of training the network needs depends on the style and complexity of the game, taking approximately 9 hours for chess, 12 hours for shogi, and 13 days for Go.\nThe trained network is used to guide a search algorithm \u2013 known as Monte-Carlo Tree Search (MCTS) \u2013 to select the most promising moves in games. For each move, AlphaZero searches only a small fraction of the positions considered by traditional chess engines. In Chess, for example, it searches only 60 thousand positions per second in chess, compared to roughly 60 million for Stockfish.\nThe fully trained systems were tested against the strongest hand-crafted engines for chess (\nStockfish\n) and shogi (\nElmo\n), along with our previous self-taught system \nAlphaGo Zero\n, the strongest Go player known.\nIn each evaluation, AlphaZero convincingly beat its opponent:\nHowever, it was the style in which AlphaZero plays these games that players may find most fascinating. In Chess, for example, AlphaZero independently discovered and played common human motifs during its self-play training such as openings, king safety and pawn structure. But, being self-taught and therefore unconstrained by conventional wisdom about the game, it also developed its own intuitions and strategies adding a new and expansive set of exciting and novel ideas that augment centuries of thinking about chess strategy.\nThe first thing that players will notice is AlphaZero's style, says Matthew Sadler \u2013 \u201cthe way its pieces swarm around the opponent\u2019s king with purpose and power\u201d. Underpinning that, he says, is AlphaZero\u2019s highly dynamic game play that maximises the activity and mobility of its own pieces while minimising the activity and mobility of its opponent\u2019s pieces. Counterintuitively, AlphaZero also seems to place less value on \u201cmaterial\u201d, an idea that underpins the modern game where each piece has a value and if one player has a greater value of pieces on the board than the other, then they have a material advantage. Instead, AlphaZero is willing to sacrifice material early in a game for gains that will only be recouped in the long-term.\n\u201cImpressively, it manages to impose its style of play across a very wide range of positions and openings,\u201d says Matthew, who also observes that it plays in a very deliberate style from its first move with a \u201cvery human sense of consistent purpose\u201d.\n\u201cTraditional engines are exceptionally strong and make few obvious mistakes, but can drift when faced with positions with no concrete and calculable solution,\u201d he says. \u201cIt's precisely in such positions where \u2018feeling\u2019, \u2018insight\u2019 or \u2018intuition\u2019 is required that AlphaZero comes into its own.\"\nThis unique ability, not seen in other traditional chess engines, has already been harnessed to give chess fans \nfresh insight and commentary\n on the recent\n World Chess Championship\n match between\n Magnus Carlsen\n and\n Fabiano Caruana\n and will be explored further in \nGame Changer\n. \u201cIt was fascinating to see how AlphaZero's analysis differed from that of top chess engines and even top grandmaster play,\u201d says Natasha Regan. \"AlphaZero could be a powerful teaching tool for the whole community.\"\nAlphaZero\u2019s teachings echo what we saw when \nAlphaGo\n played the legendary champion \nLee Sedol\n in 2016. During \nthe games\n, AlphaGo played a number of \nhighly inventive winning moves,\n including move 37 in game two, which overturned hundreds of years of thinking. These moves - and many others - have since been studied by players at all levels including Lee Sedol himself, who said of Move 37: \u201cI thought AlphaGo was based on probability calculation and it was merely a machine. But when I saw this move I changed my mind. Surely AlphaGo is creative.\u201d\nAs with Go, we are excited about AlphaZero\u2019s creative response to chess, which has been a grand challenge for artificial intelligence since the dawn of the computing age with early pioneers including Babbage, Turing, Shannon, and von Neumann all trying their hand at designing chess programs. But AlphaZero is about more than chess, shogi or Go. To create intelligent systems capable of solving a wide range of real-world problems we need them to be flexible and generalise to new situations. While there has been some progress towards this goal, it remains a major challenge in AI research with systems capable of mastering specific skills to a very high standard, but often failing when presented with even slightly modified tasks.\nAlphaZero\u2019s ability to master three different complex games \u2013 and potentially any perfect information game \u2013 is an important step towards overcoming this problem. It demonstrates that a single algorithm can learn how to discover new knowledge in a range of settings. And, while it is still early days, AlphaZero\u2019s creative insights coupled with the encouraging results we see in other projects such as \nAlphaFold\n, \u00a0give us confidence in \nour mission\n to create general purpose learning systems that will one day help us find novel solutions to some of the most important and complex scientific problems.\nThis work was done by David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, and Demis Hassabis.\n"}
{"title": "Unsupervised learning: The curious pupil", "contents": "One in a series of posts explaining the theories underpinning our research.\nOver the last decade, machine learning has made unprecedented progress in areas as diverse as image recognition, self-driving cars and playing complex games like Go. These successes have been largely realised by training deep neural networks with one of two learning paradigms\u2014supervised learning and reinforcement learning. Both paradigms require training signals to be designed by a human and passed to the computer. In the case of supervised learning, these are the \u201ctargets\u201d (such as the correct label for an image); in the case of reinforcement learning, they are the \u201crewards\u201d for successful behaviour (such as getting a high score in an Atari game). The limits of learning are therefore defined by the human trainers.\nWhile some scientists contend that a sufficiently inclusive training regime\u2014for example, the ability to complete a very wide variety of tasks\u2014should be enough to give rise to general intelligence, others believe that true intelligence will require more independent learning strategies. Consider how a toddler learns, for instance. Her grandmother might sit with her and patiently point out examples of ducks (acting as the instructive signal in supervised learning), or reward her with applause for solving a woodblock puzzle (as in reinforcement learning). But the vast majority of a toddler\u2019s time is spent naively exploring the world, making sense of her surroundings through curiosity, play, and observation. Unsupervised learning is a paradigm designed to create autonomous intelligence by rewarding agents (that is, computer programs) for learning about the data they observe without a particular task in mind. In other words, the agent learns for the sake of learning.\nA key motivation for unsupervised learning is that, while the data passed to learning algorithms is extremely rich in internal structure (e.g., images, videos and text), the targets and rewards used for training are typically very sparse (e.g., the label \u2018dog\u2019 referring to that particularly protean species, or a single one or zero to denote success or failure in a game). This suggests that the bulk of what is learned by an algorithm must consist of understanding the data itself, rather than applying that understanding to particular tasks.\n2012 was a landmark year for deep learning, when AlexNet (named after its lead architect Alex Krizhnevsky) swept the \nImageNet classification competition\n. AlexNet\u2019s abilities to recognize images were unprecedented, but even more striking is what was happening under the hood. When researchers analysed what AlexNet was doing, they discovered that it interprets images by building increasingly complex \ninternal representations of its inputs\n. Low-level features, such as textures and edges, are represented in the bottom layers, and these are then combined to form high-level concepts such as wheels and dogs in higher layers.\nThis is remarkably similar to how information is processed in our brains, where simple edges and textures in primary sensory processing areas are assembled into complex objects like faces in higher areas. The representation of a complex scene can therefore be built out of visual primitives, in much the same way that meaning emerges from the individual words comprising a sentence. Without explicit guidance to do so, the layers of AlexNet had discovered a fundamental \u2018vocabulary\u2019 of vision in order to solve its task. In a sense, it had learned to play what Wittgenstein called a \n\u2018language game\u2019\n that iteratively translates from pixels to labels.\nFrom the perspective of general intelligence, the most interesting thing about AlexNet\u2019s vocabulary is that it can be reused, or transferred, to visual tasks other than the one it was trained on, such as recognising \nwhole scenes rather than individual objects\n. Transfer is essential in an ever-changing world, and humans excel at it: we are able to rapidly adapt the skills and understanding we\u2019ve gleaned from our experiences (our \u2018world model\u2019) to whatever situation is at hand. For example, a classically-trained pianist can pick up jazz piano with relative ease. Artificial agents that form the right internal representations of the world, the reasoning goes, should be able to do similarly.\nNonetheless, the representations learned by classifiers such as AlexNet have limitations. In particular, as the network was only trained to label images with a single class (cat, dog, car, volcano), any information not required to infer the label\u2014no matter how useful it might be for other tasks\u2014is liable to be ignored. For example, the representations may fail to capture the background of the image if the label always refers to the foreground. A possible solution is to provide more comprehensive training signals, like \ndetailed captions describing the images\n: not just \u201cdog,\u201d but \u201cA Corgi catching a frisbee in a sunny park.\u201d However, such targets are laborious to provide, especially at scale, and still may be insufficient to capture all the information needed to complete a task. The basic premise of unsupervised learning is that the best way to learn rich, broadly transferable representations is to attempt to learn everything that can be learned about the data.\nIf the notion of transfer through representation learning seems too abstract, consider a child who has learned to draw people as stick figures. She has discovered a representation of the human form that is both highly compact and rapidly adaptable. By augmenting each stick figure with specifics, she can create portraits of all her classmates: glasses for her best friend, her deskmate in his favorite red tee-shirt. And she has developed this skill not in order to complete a specific task or receive a reward, but rather in response to her basic urge to reflect the world around her.\nPerhaps the simplest objective for unsupervised learning is to train an algorithm to generate its own instances of data. So-called generative models should not simply reproduce the data they are trained on (an uninteresting act of memorisation), but rather build a model of the underlying class from which that data was drawn: not a particular photograph of a horse or a rainbow, but the set of all photographs of horses and rainbows; not a specific utterance from a specific speaker, but the general distribution of spoken utterances. The guiding principle of generative models is that being able to construct a convincing example of the data is the strongest evidence of having understood it: as Richard Feynman put it, \"what I cannot create, I do not understand.\"\nFor images, the most successful generative model so far has been the \nGenerative Adversarial Network\n (GAN for short), in which two networks\u2014a generator and a discriminator\u2014engage in a contest of discernment akin to that of an artistic forger and a detective. The generator produces images with the goal of tricking the discriminator into believing they are real; the discriminator, meanwhile, is rewarded for spotting the fakes. The generated images, first messy and random, are refined over many iterations, and the ongoing dynamic between the networks leads to ever-more realistic images that are in many cases \nindistinguishable from real photographs\n. Generative adversarial networks can also dream details of landscapes \ndefined by the rough sketches of users\n.\nA glance at the images below is enough to convince us that the network has learned to represent many of the key features of the photographs they were trained on, such as the structure of animal\u2019s bodies, the texture of grass, and detailed effects of light and shade (even when refracted through a soap bubble). Close inspection reveals slight anomalies, such as the white dog\u2019s apparent extra leg and the oddly right-angled flow of one of the jets in the fountain. While the creators of generative models strive to avoid such imperfections, their visibility highlights one of the benefits of recreating familiar data such as images: by inspecting the samples, researchers can infer what the model has and hasn\u2019t learned.\nAnother notable family within unsupervised learning are autoregressive models, in which the data is split into a sequence of small pieces, each of which is predicted in turn. Such models can be used to generate data by successively guessing what will come next, feeding in a guess as input and guessing again. Language models, where each word is predicted from the words before it, are perhaps the best known example: these models power the text predictions that pop up on some email and messaging apps. Recent advances in language modelling have enabled the generation of strikingly plausible passages, such as the one shown below from \nOpenAI\u2019s GPT-2\n.\nOne interesting inconsistency in the text is that the unicorns are described as \u201cfour-horned\u201d: again, it is fascinating to probe the limitations of the network\u2019s understanding.\nBy controlling the input sequence used to condition the out predictions, autoregressive models can also be used to transform one sequence into another. This \ndemo\n uses a conditional autoregressive model to transform text into realistic handwriting. \nWaveNet\n transforms text into natural sounding speech, and is now used to \ngenerate voices for Google Assistant\n. A similar process of conditioning and autoregressive generation can be used to \ntranslate from one language to another\n.\nAutoregressive models learn about data by attempting to predict each piece of it in a particular order. A more general class of unsupervised learning algorithms can be built by predicting any part of the data from any other. For example, this could mean removing a word from a sentence, and \nattempting to predict it from whatever remains\n. By learning to make lots of localised predictions, the system is forced to learn about the data as a whole.\nOne concern around generative models is their potential for misuse. While manipulating evidence with photo, video, and audio editing has been possible for a long time, generative models could make it even easier to edit media with malicious intent. We have already seen demonstrations of so-called \u2018deepfakes\u2019\u2014for instance, this\n fabricated video footage of President Obama\n. It\u2019s encouraging to see that several major efforts to address these challenges are already underway, including using statistical techniques to help\n detect\n synthetic media\n and verify authentic media, \nraising public awareness\n, and discussions around limiting the availability of trained generative models. Furthermore, generative models can themselves be used to detect synthetic media and anomalous data\u2014for example when \ndetecting fake speech\n or identifying payment abnormalities to protect customers against fraud. Researchers need to work on generative models in order to better understand them and mitigate downstream risks.\nGenerative models are fascinating in their own right, but our principal interest in them at DeepMind is as a stepping stone towards general intelligence. Endowing an agent with the ability to generate data is a way of giving it an imagination, and hence the ability to \nplan and reason about the future\n. Even without explicit generation, our studies show that \nlearning to predict\n different \naspects of the environment\n enriches the agent\u2019s \nworld model\n, and thereby improves its ability to solve problems.\nThese results resonate with our intuitions about the human mind. Our ability to learn about the world without explicit supervision is fundamental to what we regard as intelligence. On a train ride we might listlessly gaze through the window, drag our fingers over the velvet of the seat, regard the passengers sitting across from us. We have no agenda in these studies: we almost can\u2019t help but gather information, our brains ceaselessly working to understand the world around us, and our place within it.\n"}
{"title": "Machine learning can boost the value of wind energy", "contents": "Carbon-free technologies like renewable energy help combat climate change, but many of them have not reached their full potential. Consider wind power: over the past decade, wind farms have become an important source of carbon-free electricity as the cost of turbines has plummeted and adoption has surged. However, the variable nature of wind itself makes it an unpredictable energy source\u2014less useful than one that can reliably deliver power at a set time.\nIn search of a solution to this problem, last year, DeepMind and Google started applying machine learning algorithms to 700 megawatts of wind power capacity in the central United States. These wind farms\u2014part of Google\u2019s global fleet of \nrenewable energy projects\n\u2014collectively generate as much electricity as is needed by a medium-sized city.\nUsing a neural network trained on widely available weather forecasts and historical turbine data, we configured the DeepMind system to predict wind power output 36 hours ahead of actual generation. Based on these predictions, our model recommends how to make optimal hourly delivery commitments to the power grid a full day in advance. This is important, because energy sources that can be scheduled (i.e. can deliver a set amount of electricity at a set time) are often more valuable to the grid.\nAlthough we continue to refine our algorithm, our use of machine learning across our wind farms has produced positive results. To date, machine learning has boosted the value of our wind energy by roughly 20 percent, compared to the baseline scenario of no time-based commitments to the grid.\nWe can\u2019t eliminate the variability of the wind, but our early results suggest that we can use machine learning to make wind power sufficiently more predictable and valuable. This approach also helps bring greater data rigor to wind farm operations, as machine learning can help wind farm operators make smarter, faster and more data-driven assessments of how their power output can meet electricity demand.\nOur hope is that this kind of machine learning approach can strengthen the business case for wind power and drive further adoption of carbon-free energy on electric grids worldwide. Researchers and practitioners across the energy industry are developing novel ideas for how society can make the most of variable power sources like solar and wind. We\u2019re eager to join them in exploring general availability of these cloud-based machine learning strategies.\nGoogle recently achieved \n100 percent renewable energy purchasing\n and is now striving to \nsource carbon-free energy \non a 24x7 basis. The partnership with DeepMind to make wind power more predictable and valuable is a concrete step toward that aspiration. While much remains to be done, this step is a meaningful one\u2014for Google, and more importantly, for the environment.\nThis article is cross-posted from \nThe Keyword\n.\n"}
{"title": "Identifying and eliminating bugs in learned predictive models", "contents": "One in a series of posts explaining the theories underpinning our research. \nBugs and software have gone hand in hand since the beginning of computer programming. Over time, software developers have established a set of best practices for testing and debugging before deployment, but these practices are not suited for modern deep learning systems. Today, the prevailing practice in machine learning is to train a system on a training data set, and then test it on another set. While this reveals the average-case performance of models, it is also crucial to ensure robustness, or acceptably high performance even in the worst case. \u00a0In this article, we describe three approaches for rigorously identifying and eliminating bugs in learned predictive models: adversarial testing, robust learning, and formal verification.\nMachine learning systems are not robust by default. Even systems that outperform humans in a particular domain can fail at solving simple problems if subtle differences are introduced. For example, consider the problem of image perturbations: a neural network that can classify images better than a human can be easily fooled into believing that sloth is a race car if a small amount of carefully calculated noise is added to the input image.\nThis is not an entirely new problem. Computer programs have always had bugs. Over decades, software engineers have assembled an impressive toolkit of techniques, ranging from unit testing to formal verification. These methods work well on traditional software, but adapting these approaches to rigorously test machine learning models like neural networks is extremely challenging due to the scale and lack of structure in these models, which may contain hundreds of millions of parameters. This necessitates the need for developing novel approaches for ensuring that machine learning systems are robust at deployment.\nFrom a programmer\u2019s perspective, a bug is any behaviour that is inconsistent with the \nspecification\n, i.e. the intended functionality, of a system. As part of our mission of solving intelligence, we conduct research into techniques for evaluating whether machine learning systems are consistent not only with the train and test set, but also with a list of specifications describing desirable properties of a system. Such properties might include robustness to sufficiently small perturbations in inputs, safety constraints to avoid catastrophic failures, or producing predictions consistent with the laws of physics.\nIn this article, we discuss three important technical challenges for the machine learning community to take on, as we collectively work towards rigorous development and deployment of machine learning systems that are reliably consistent with desired specifications:\nRobustness to adversarial examples is a relatively well-studied problem in deep learning. One major theme that has come out of this work is the importance of evaluating against strong attacks, and designing transparent models which can be efficiently analysed. Alongside other researchers from the community, we have found that many models appear robust when evaluated against weak adversaries. However, they show essentially 0% adversarial accuracy when evaluated against stronger adversaries (\nAthalye et al., 2018\n, \nUesato et al., 2018\n, \nCarlini and Wagner, 2017\n).\nWhile most work has focused on rare failures in the context of supervised learning (largely image classification), there is a need to extend these ideas to other settings. In recent work on adversarial approaches for uncovering catastrophic failures, we apply these ideas towards testing reinforcement learning agents intended for use in safety-critical settings. One challenge in developing autonomous systems is that because a single mistake may have large consequences, very small failure probabilities are unacceptable.\nOur objective is to design an \u201cadversary\u201d to allow us to detect such failures in advance (e.g., in a controlled environment). If the adversary can efficiently identify the worst-case input for a given model, this allows us to catch rare failure cases before deploying a model. As with image classifiers, evaluating against a weak adversary provides a false sense of security during deployment. This is similar to the software practice of red-teaming, though extends beyond failures caused by malicious adversaries, and also includes failures which arise naturally, for example due to lack of generalization.\nWe developed two complementary approaches for adversarial testing of RL agents. In the first, we use a derivative-free optimisation to directly minimise the expected reward of an agent. In the second, we learn an adversarial value function which predicts from experience which situations are most likely to cause failures for the agent. We then use this learned function for optimisation to focus the evaluation on the most problematic inputs. These approaches form only a small part of a rich, growing space of potential algorithms, and we are excited about future development in rigorous evaluation of agents.\nAlready, both approaches result in large improvements over random testing. Using our method, failures that would have taken days to uncover, or even gone undetected entirely, can be detected in minutes (\nUesato et al., 2018b\n). We also found that adversarial testing may uncover qualitatively different behaviour in our agents from what might be expected from evaluation on a random test set. In particular, using adversarial environment construction we found that agents performing a 3D navigation task, which match human-level performance on average, still failed to find the goal completely on surprisingly simple mazes (\nRuderman et al., 2018\n). Our work also highlights that we need to design systems that are secure against natural failures, not only against adversaries.\nAdversarial testing aims to find a counter example that violates specifications. As such, it often leads to overestimating the consistency of models with respect to these specifications. Mathematically, a specification is some relationship that has to hold between the inputs and outputs of a neural network. This can take the form of upper and lower bounds on certain key input and output parameters.\nMotivated by this observation, several researchers (\nRaghunathan et al., 2018\n; \nWong et al., 2018\n; \nMirman et al., 2018\n; \nWang et al., 2018\n) including our team at DeepMind (\nDvijotham et al., 2018\n; \nGowal et al., 2018\n), have worked on algorithms that are agnostic to the adversarial testing procedure (used to assess consistency with the specification). This can be understood geometrically - we can bound (e.g., using interval bound propagation; \nEhlers 2017\n, \nKatz et al. 2017\n, \nMirman et al., 2018\n) the worst violation of a specification by bounding the space of outputs given a set of inputs. If this bound is differentiable with respect to network parameters and can be computed quickly, it can be used during training. The original bounding box can then be propagated through each layer of the network.\nWe show that interval bound propagation is fast, efficient, and \u2014 contrary to prior belief \u2014 can achieve strong results (\nGowal et al., 2018\n). In particular, we demonstrate that it can decrease the provable error rate (i.e., maximal error rate achievable by any adversary) over state-of-the-art in image classification on both MNIST and CIFAR-10 datasets.\nGoing forward, the next frontier will be to learn the right geometric abstractions to compute \ntighter overapproximations\n of the space of outputs. We also want to train networks to be consistent with more complex specifications capturing desirable behavior, such as above mentioned invariances and consistency with physical laws.\nRigorous testing and training can go a long way towards building robust machine learning systems. However, no amount of testing can formally guarantee that a system will behave as we want. In large-scale models, enumerating all possible outputs for a given set of inputs (for example, infinitesimal perturbations to an image) is intractable due to the astronomical number of choices for the input perturbation. However, as in the case of training, we can find more efficient approaches by setting geometric bounds on the set of outputs. Formal verification is a subject of ongoing research at DeepMind.\nThe machine learning community has developed several interesting ideas on how to compute precise geometric bounds on the space of outputs of the network (\nKatz et al. 2017\n, \nWeng et al., 2018\n; \nSingh et al., 2018\n). Our approach (\nDvijotham et al., 2018\n), based on optimisation and duality, consists of formulating the verification problem as an optimisation problem that tries to find the largest violation of the property being verified. By using ideas from duality in optimisation, the problem becomes computationally tractable. This results in additional constraints that refine the bounding boxes computed by interval bound propagation, using so-called cutting planes. This approach is sound but incomplete: there may be cases where the property of interest is true, but the bound computed by this algorithm is not tight enough to prove the property. However, once we obtain a bound, this formally guarantees that there can be no violation of the property. The figure below graphically illustrates the approach.\nThis approach enables us to extend the applicability of verification algorithms to more general networks (activation functions, architectures), general specifications and more sophisticated deep learning models (generative models, neural processes, etc.) and specifications beyond adversarial robustness (\nQin, 2018\n).\nDeployment of machine learning in high-stakes situations presents unique challenges, and requires the development of evaluation techniques that reliably detect unlikely failure modes. More broadly, we believe that learning consistency with specifications can provide large efficiency improvements over approaches where specifications only arise implicitly from training data. We are excited about ongoing research into adversarial evaluation, learning robust models, and verification of formal specifications.\nMuch more work is needed to build automated tools for ensuring that AI systems in the real world will do the \u201cright thing\u201d. In particular, we are excited about progress in the following directions:\nDeepMind is dedicated to positive social impact through responsible development and deployment of machine learning systems. To make sure that the contributions of developers are reliably positive, we need to tackle many technical challenges. We are committed to taking part in this effort and are excited to work with the community on solving these challenges.\nThis post describes the work of the Robust and Verified Deep Learning group (Pushmeet Kohli, DJ Krishnamurthy, Jonathan Uesato, Sven Gowal, Chongli Qin, Robert Stanforth, Po-Sen Huang) performed in collaboration with various contributors across DeepMind including Avraham Ruderman, Alhussein Fawzi, Ananya Kumar, Brendan O'Donoghue, Bristy Sikder, Chenglong Wang, Csaba Szepesvari, Hubert Soyer, Relja Arandjelovic, Richard Everett, Rudy Bunel, Timothy Mann, Grzegorz Swirszcz, and Tom Erez.\nThanks to Vishal Maini, Ale\u0161 Flidr, Damien Boudot, and Jan Leike for their contributions to the post.\n"}
{"title": "Capture the Flag: the emergence of complex cooperative agents", "contents": "Mastering the strategy, tactical understanding, and team play involved in multiplayer video games represents a critical challenge for AI research. In our latest paper, \nnow published in the journal Science\n, we present new developments in reinforcement learning, resulting in human-level performance in Quake III Arena Capture the Flag. This is a complex, multi-agent environment and one of the canonical 3D first-person multiplayer games. The agents successfully cooperate with both artificial and human teammates, and demonstrate high performance even when trained with reaction times comparable to human players. Furthermore, we show how these methods have managed to scale beyond research Capture the Flag environments to the full game of Quake III Arena.\nBillions of people inhabit the planet, each with their own individual goals and actions, but still capable of coming together through teams, organisations and societies in impressive displays of collective intelligence. This is a setting we call multi-agent learning: many individual agents must act independently, yet learn to interact and cooperate with other agents. This is an immensely difficult problem - because with co-adapting agents the world is constantly changing.\nTo investigate this problem, we look at 3D first-person multiplayer video games. These games represent the most popular genre of video game, and have captured the imagination of millions of gamers because of their immersive game play, as well as the challenges they pose in terms of strategy, tactics, hand-eye coordination, and team play. The challenge for our agents is to learn directly from raw pixels to produce actions. This complexity makes first-person multiplayer games a fruitful and active area of research within the AI community.\nThe game we focused on in this work is Quake III Arena (which we aesthetically modified, though all game mechanics remain the same). Quake III Arena has laid the foundations for many modern first-person video games, and has attracted a long-standing competitive e-sports scene. We train agents that learn and act as individuals, but which must be able to play on teams with and against any other agents, artificial or human.\nThe rules of CTF are simple, but the dynamics are complex. Two teams of individual players compete on a given map with the goal of capturing the opponent team\u2019s flag while protecting their own. To gain tactical advantage they can tag the opponent team members to send them back to their spawn points. The team with the most flag captures after five minutes wins.\nFrom a multi-agent perspective, CTF requires players to both successfully cooperate with their teammates as well as compete with the opposing team, while remaining robust to any playing style they might encounter.\nTo make things even more interesting, we consider a variant of CTF in which the map layout changes from match to match. As a consequence, our agents are forced to acquire general strategies rather than memorising the map layout. Additionally, to level the playing field, our learning agents experience the world of CTF in a similar way to humans: they observe a stream of pixel images and issue actions through an emulated game controller.\nOur agents must learn from scratch how to see, act, cooperate, and compete in unseen environments, all from a single reinforcement signal per match: whether their team won or not. This is a challenging learning problem, and its solution is based on three general ideas for reinforcement learning:\nThe resulting agent, dubbed the For The Win (FTW) agent, learns to play CTF to a very high standard. Crucially, the learned agent policies are robust to the size of the maps, the number of teammates, and the other players on their team. Below, you can explore some games on both the outdoor procedural environments, where FTW agents play against each other, as well as games in which humans and agents play together on indoor procedural environments.\nWe ran a tournament including 40 human players, in which humans and agents are randomly matched up in games - both as opponents and as teammates.\nThe FTW agents learn to become much stronger than the strong baseline methods, and exceed the win-rate of the human players. In fact, in a survey among participants they were rated more collaborative than human participants.\nGoing beyond mere performance evaluation, it is important to understand the emergent complexity in the behaviours and internal representations of these agents.\nTo understand how agents represent game state, we look at activation patterns of the agents\u2019 neural networks plotted on a plane. Dots in the figure below represent situations during play with close by dots representing similar activation patterns. These dots are coloured according to the high-level CTF game state in which the agent finds itself: In which room is the agent? What is the status of the flags? What teammates and opponents can be seen? We observe clusters of the same colour, indicating that the agent represents similar high-level game states in a similar manner.\nThe agents are never told anything about the rules of the game, yet learn about fundamental game concepts and effectively develop an intuition for CTF. In fact, we can find particular neurons that code directly for some of the most important game states, such as a neuron that activates when the agent\u2019s flag is taken, or a neuron that activates when an agent\u2019s teammate is holding a flag. The paper provides further analysis covering the agents\u2019 use of memory and visual attention.\nHow did our agents perform as well as they did? First, we noticed that the agents had very fast reaction times and were very accurate taggers, which might explain their performance (tagging is a tactical action that sends opponents back to their starting point). Humans are comparatively slow to process and act on sensory input, due to our slower biological signalling. \u00a0\nHere\u2019s an example of a reaction time test you can try yourself\n. Thus, our agents\u2019 superior performance might be a result of their faster visual processing and motor control. However, by artificially reducing this accuracy and reaction time, we saw that this was only one factor in their success. In a further study, we trained agents which have an inbuilt delay of a quarter of a second (267 ms) \u2013 that is, agents have a 267ms lag before observing the world \u2013 comparable with reported reaction times of human video game players. These response-delayed agents still outperformed human participants, with strong humans only winning 21% of the time.\nThrough unsupervised learning we established the prototypical behaviours of agents and humans to discover that agents in fact learn human-like behaviours, such as following teammates and camping in the opponent\u2019s base.\nThese behaviours emerge in the course of training, through reinforcement learning and population-level evolution, with behaviours - such as teammate following - falling out of favour as agents learn to cooperate in a more complementary manner.\nThe training progression of a population of FTW agents. Top left: the 30 agents\u2019 Elo ratings as they train and evolve from each other. Top right: the genetic tree of these evolution events. The lower graph shows the progression of knowledge, some of the internal rewards, and behaviour probability throughout the training of the agents.\nWhile this paper focuses on Capture the Flag, the research contributions are general and we are excited to see how others build upon our techniques in different complex environments. Since initially publishing these results, we have found success in extending these methods to the full game of Quake III Arena, which includes professionally played maps, more multiplayer game modes in addition to Capture the Flag, and more gadgets and pickups. Initial results indicate that agents can play multiple game modes and multiple maps competitively, and are starting to challenge the skills of our human researchers in test matches. Indeed, ideas introduced in this work, such as population based multi-agent RL, form a foundation of the \nAlphaStar agent in our work on StarCraft II\n.\nIn general, this work highlights the potential of multi-agent training to advance the development of artificial intelligence: exploiting the natural curriculum provided by multi-agent training, and forcing the development of robust agents that can even team up with humans.\nFor more details, please see the \npaper\n (\nPDF\n) and the \nfull supplementary video\n.\nThis work was done by Max Jaderberg, Wojciech M. Czarnecki, Iain Dunning, Luke Marris, Brendan Tracey, Guy Lever, Antonio Garcia Castaneda, Charles Beattie, Neil Rabinowitz, Ari Morcos, Avraham Ruderman, Nicolas Sonnerat, Tim Green, Louise Deason, Joel Z. Leibo, David Silver, Demis Hassabis, Koray Kavukcuoglu, and Thore Graepel.\nVisualisations were created by Adam Cain, Damien Boudot, Doug Fritz, Jaume Sanchez Elias, Paul Lewis, Max Jaderberg, Wojciech M. Czarnecki, and Luke Marris.\nWe would like to thank Patrick Howard and Dan \u201cScancode\u201d Gold for allowing us to use the Quake III Arena maps they designed.\nUpdated 30/5/19. Read about our new work below, in \u201cHuman Comparable Agents\u201d and \u201cGoing Further\u201d.\n"}
{"title": "How evolutionary selection can train more capable self-driving cars", "contents": "Waymo\u2019s self-driving vehicles employ neural networks to perform many driving tasks, from detecting objects and predicting how others will behave, to planning a car's next moves. Training an individual neural net has traditionally required weeks of fine-tuning and experimentation, as well as enormous amounts of computational power. Now, Waymo, in a research collaboration with DeepMind, has taken inspiration from Darwin\u2019s insights into evolution to make this training more effective and efficient.\nAt a high level, neural nets learn through trial and error. A network is presented with a task, and is \u201cgraded\u201d on whether it performs the task correctly or not. The network learns by continually attempting these tasks and adjusting itself based on its grades, such that it becomes more likely to perform correctly in the future.\nA network\u2019s performance depends heavily on its training regimen. For example, a researcher can tweak how much a network adjusts itself after each task\u2013referred to as its learning rate. The higher the learning rate, the more dramatic the adjustments. The goal is to find a learning rate high enough that the network gets better after each iteration, but not so high that the network's performance fluctuates wildly.\nFinding the best training regimen (or \u201chyperparameter schedule\u201d) is commonly achieved through an engineer\u2019s experience and intuition, or through extensive searching. In random search, researchers apply many random hyperparameter schedules over multiple types of hyperparameters in order to train different networks independently and in parallel\u2013after which it\u2019s possible to settle on the best performing model. This\n blog post\n covers how Waymo engineers apply reinforcement learning to the search for better neural net architectures.\nBecause training numerous models in parallel is computationally expensive, researchers typically hand-tune random search by monitoring networks while they\u2019re training, periodically culling the weakest performers and freeing resources to train new networks from scratch with new random hyperparameters. This type of manual tuning produces better results faster, but it\u2019s labor intensive.\nTo make this process more efficient, researchers at DeepMind devised a way to automatically determine good hyperparameter schedules based on evolutionary competition (called \u201cPopulation Based Training\u201d or PBT), which combines the advantages of \nhand-tuning and random search\n.\nLike random search, PBT also starts with multiple networks initiated with random hyperparameters. Networks are evaluated periodically and compete with each other for \u201csurvival\u201d in an evolutionary fashion. If a member of the population is underperforming, it\u2019s replaced with the \u201cprogeny\u201d of a better performing member. The progeny is a copy of the better performing member, with slightly mutated hyperparameters. PBT doesn\u2019t require us to restart training from scratch, because each progeny inherits the full state of its parent network, and hyperparameters are updated actively throughout training, not at the end of training. Compared to random search, PBT spends more of its resources training with good hyperparameter values.\n\u200d\nThe first experiments that DeepMind and Waymo collaborated on involved training a network that generates boxes around pedestrians, bicyclists, and motorcyclists detected by our sensors\u2013named a \u201cregion proposal network.\u201d The aim was to investigate whether PBT could improve a neural net's ability to detect pedestrians along two measures: recall (the fraction of pedestrians identified by the neural net over total number of pedestrians in the scene) and precision (the fraction of detected pedestrians that are actually pedestrians, and not spurious \u201cfalse positives\u201d). Waymo\u2019s vehicles detect these road users using multiple neural nets and other methods, but the goal of this experiment was to train this single neural net to maintain recall over 99%, while reducing false positives using population-based training.\nWe learned a lot from this experiment. Firstly, we discovered that we needed to create a realistic and robust evaluation for the networks so that we\u2019d know if a neural net would truly perform better when deployed across a variety of situations in the real world. This evaluation formed the basis of the competition that PBT employs to pick one winning neural net over another. To ensure neural nets perform well generally, and don\u2019t simply memorise answers to examples they've seen during training, our PBT competition evaluation uses a set of examples (the \"validation set\") that is different from those used in training (the \"training set.\") To verify final performance, we also use a third set of examples (the \"evaluation set\") that the neural nets have never seen in training or competition.\nSecondly, we learned that we needed fast evaluation to support frequent evolutionary competition. Researchers seldom evaluate their models during training, and when they do, the evaluation is done infrequently. PBT required models be evaluated every 15 minutes. To achieve this, we took advantage of Google\u2019s data centres to parallelise the evaluation across hundreds of distributed machines.\nDuring these experiments, we noticed that one of PBT\u2019s strengths\u2013allocating more resources to the progeny of better performing networks\u2013can also be a weakness, because PBT optimises for the present and fails to consider long-term outcomes. This can be a problem because it disadvantages late-bloomers, so neural nets with hyperparameters that perform better over the long term don\u2019t have the chance to mature and succeed. One way to combat this is to increase population diversity, which can be achieved by simply training a larger population. If the population is large enough, there is a greater chance for networks with late-blooming hyperparameters to survive and catch up in later generations.\nIn these experiments, we were able to increase diversity by creating sub-populations called \u201cniches,\u201d where neural nets were only allowed to compete within their own sub-groups\u2013similar to how species evolve when isolated on islands. We also tried to directly reward diversity through a technique called \u201cfitness sharing,\u201d where we measure the difference between members of the population and give more unique neural nets an edge in the competition. Greater diversity allows PBT to explore a larger hyperparameter space.\nPBT enabled dramatic improvements in model performance. For the experiment above, our PBT models were able to achieve higher precision by reducing false positives by 24% compared to its hand-tuned equivalent, while maintaining a high recall rate. A chief advantage of evolutionary methods such as PBT is that they can optimise arbitrarily complex metrics. Traditionally, neural nets can only be trained using simple and smooth loss functions, which act as a proxy for what we really care about. PBT enabled us to go beyond the update rule used for training neural nets, and towards the more complex metrics optimising for features we care about, such as maximising precision under high recall rates.\nPBT also saves time and resources. The hyperparameter schedule discovered with PBT-trained nets outperformed Waymo\u2019s previous net with half the training time and resources. Overall, PBT uses half the computational resources used by random parallel search to efficiently discover better hyperparameter schedules. \u00a0It also saves time for researchers\u2013by incorporating PBT directly into Waymo\u2019s technical infrastructure, researchers from across the company can apply this method with the click of a button, and spend less time tuning their learning rates. Since the completion of these experiments, PBT has been applied to many different Waymo models, and holds a lot of promise for helping to create more capable vehicles for the road.\nContributors: The work described here was a research collaboration between Yu-hsin Chen and Matthieu Devin of Waymo, and Ali Razavi, Ang Li, Sibon Li, Ola Spyra, Pramod Gupta and Oriol Vinyals of DeepMind. Advisors to the project include Max Jaderberg, Valentin Dalibard, Meire Fortunato and Jackson Broshear from DeepMind.\n"}
{"title": "Using AI to give doctors a 48-hour head start on life-threatening illness", "contents": "Artificial intelligence can now predict one of the leading causes of avoidable patient harm up to two days before it happens, as demonstrated by \nour latest research published in Nature\n. Working alongside experts from the US Department of Veterans Affairs (VA), we have developed technology that, in the future, could give doctors a 48-hour head start in treating acute kidney injury (AKI), a condition that is associated with over \n100,000 people \nin the UK every year. These findings come alongside a peer-reviewed service evaluation of Streams, our mobile assistant for clinicians, which shows that patient care can be improved, and health care costs reduced, through the use of digital tools. Together, they form the foundation for a transformative advance in medicine, helping to move from reactive to preventative models of care.\nMillions of people die every year from diseases that could have been prevented with earlier detection. One such disease is acute kidney injury (AKI), a condition where a patient\u2019s kidney suddenly stops working properly. Affecting up to one in five hospitalised patients in \nthe UK\n and \nthe US\n, the condition is notoriously difficult to spot, and deterioration can happen quickly. \nExperts\n believe that up to 30% of cases could be prevented if a doctor intervenes early enough.\nOver the last few years, our team at DeepMind has focused on finding an answer to the complex problem of avoidable patient harm, building digital tools that can spot serious conditions earlier and helping doctors and nurses deliver faster, better care to patients in need. This is our team\u2019s biggest healthcare research breakthrough to date, demonstrating the ability to not only spot deterioration more effectively, but actually predict it before it happens.\nWorking with the VA, the DeepMind team applied AI technology to a comprehensive de-identified electronic health record dataset collected from a network of over a hundred VA sites. The \nresearch\n shows that the AI could accurately predict AKI in patients up to 48 hours earlier than it is currently diagnosed. Importantly, the model correctly predicted 9 out of 10 patients whose condition deteriorated so severely that they then required dialysis. This could provide a window in the future for earlier preventative treatment and avoid the need for more invasive procedures like kidney dialysis. The model has also been designed so that it might, in the future, generalise to other major causes of diseases and deterioration such as sepsis, a life-threatening infection.\nTo address the \u2018black box\u2019 problem \u2013 one of the key barriers for the implementation of AI in clinical practice \u2013 the model also provides the clinical information that was most important in making its predictions of deteriorating kidney function. It also provides predicted future results for several relevant blood tests. This information may help clinicians understand the reasoning behind the AI-enabled alert and anticipate future patient deterioration.\nHowever, these predictions can\u2019t help real patients without the right tools to \u00a0alert specialists. Clinicians routinely use pagers, paper records and fax machines to communicate with each other, but better technology is desperately needed so that critical information can be delivered to the right specialist at the right time. That\u2019s why we\u2019re also pleased to report that the results of a peer-reviewed evaluation of our mobile medical assistant Streams have also been published today. This work was conducted by researchers at University College London.\nStreams is a mobile medical assistant for clinicians, and has been in use at the Royal Free London NHS Foundation Trust since early 2017. The app uses the existing national AKI algorithm to flag patient deterioration, supports the review of medical information at the bedside, and enables instant communication between clinical teams. Shortly after rolling out at the Royal Free, clinicians \nsaid\n that Streams was saving them up to two hours a day. We also heard about patients, like \nAfia Ahmad\n, whose treatment was escalated thanks to the app. But we wanted to quantify these benefits through robust clinical evaluation. Today\u2019s results show that the app saved clinicians\u2019 time, improved care and reduced the number of AKI cases being missed at the hospital.\nBy using Streams, \nspecialists reviewed urgent cases within 15 minutes or less\n(a process that might otherwise have taken several hours) and fewer cases of AKI were missed (3.3% rather than 12.4%). The app also \nreduced the average cost of admission\n for a patient with AKI by 17%, demonstrating a huge potential cost saving for hospitals in the future, considering that AKI costs the NHS \nmore than \u00a31 billion \neach year.\nFeedback from \nthe qualitative study\n was positive, with healthcare professionals emphasising the ways in which the app accelerated the detection of patients in need, saved them time in performing administrative tasks, and improved team communication. One respondent said the app \u201cstreamlines care, and speeds up the time in which they get a specialist renal review.\u201d Another clinician from the nephrology team stated that \u201cBeing able to look up the blood results for anyone in the hospital wherever you are is unparalleled...it must save at least \u2013 I don\u2019t know if you could analyse it \u2013 but it must save at least a couple of hours in a day.\u201d\nGetting the right information about the right patient at the right time is a huge problem for healthcare systems across the globe. Critically, these early findings from the Royal Free suggest that, in order to improve patient outcomes even further, clinicians need to be able to intervene before AKI can be detected by the current NHS algorithm \u2013 which is why our research on AKI is so promising. These results comprise the building blocks for our long-term vision of preventative healthcare, helping doctors to intervene in a proactive, rather than reactive, manner.\nStreams doesn\u2019t use artificial intelligence at the moment, but the team now intends to find ways to safely integrate predictive AI models into Streams in order to provide clinicians with intelligent insights into patient deterioration.\nThis is a major milestone for the DeepMind Health team, who will be carrying this work forward as part of Google Health, led by Dr David Feinberg. As we \nannounced\n in November 2018, the Streams team, and colleagues working on translational research in healthcare, will be joining Google in order to make a positive impact on a global scale. The combined experience, infrastructure and expertise of DeepMind Health teams alongside Google\u2019s will help us continue to develop mobile tools that can support more clinicians, address critical patient safety issues and could, we hope, save thousands of lives globally.\n"}
{"title": "Using machine learning to accelerate ecological research", "contents": "The Serengeti is one of the last remaining sites in the world that hosts an intact community of large mammals. These animals roam over vast swaths of land, some migrating thousands of miles across multiple countries following seasonal rainfall. As human encroachment around the region becomes more intense, these species are forced to alter their behaviours in order to survive. Increasing agriculture, poaching, and climate abnormalities contribute to changes in animal behaviours and population dynamics, but these changes have occurred at spatial and temporal scales which are difficult to monitor using traditional research methods. There is a great urgency to understand how these animal communities function as human pressures grow, both in order to understand the dynamics of these last pristine ecosystems, and to formulate effective management plans to conserve and protect the integrity of this unique biodiversity hotspot.\nTo this end, DeepMind is collaborating with ecologists and conservationists to develop machine learning methods to help study the behavioural dynamics of an entire African animal community in the Serengeti National Park and Grumeti Reserve in Tanzania. The Serengeti-Mara ecosystem is globally unparalleled in its biodiversity, hosting an estimated 70 large mammal species and 500 bird species, thanks in part to its unique geology and varied habitat types. Almost a decade ago, the Serengeti Lion Research program installed hundreds of motion-sensitive cameras within the core of the protected area. The cameras are triggered by passing wildlife, capturing animal images frequently, across vast spatial scales, allowing researchers to study animal behaviour, distribution, and demography with great spatial and temporal resolution.\nOver the last nine years, the team has collected and stored millions of photos like the one above. Until now, volunteers from across the world have helped to identify and count the species in the photos by hand using the \nZooniverse\n web-based platform, which hosts many similar projects for citizen-scientists. This has resulted in a rich \ndataset\n, \nSnapshot Serengeti\n, featuring labels and counts for around 50 different species. Currently, the annotation process is labor intensive and time-consuming: it takes up to a year from the time a camera is triggered until labels are collected from volunteers. This bottleneck has not only impeded scientists\u2019 ability to perform basic research, but has made it hard for conservationists to react adaptively to challenges and perturbations disrupting the ecosystem. To help researchers unlock this data with greater efficiency, we\u2019ve used the \nSnapshot Serengeti dataset\n to train machine learning models to automatically detect, identify, and count animals.\nUsing machine learning for conservation is not new. For example, researchers have previously leveraged \ntourist photos\n and \nYouTube videos\n to track animals, and \naudio recordings\n to identify species by their calls. Camera trap data can be hard to work with\u2013animals may appear out of focus, and can be at many different distances and positions with respect to the camera (as in the image above). With expert input from leading ecologist and conservationist Dr. Meredith Palmer, our project quickly took shape, and we now have a model that can perform on par with, or better than, human annotators for most of the species in the region. Importantly, this method shortens the data processing pipeline by up to 9 months, which has immense potential to help researchers in the field.\nOf course, field work is challenging, and fraught with unexpected hazards such as failing power lines and limited or no internet access. We are currently preparing the software for deployment in the field, and looking at ways to safely run our pre-trained model with modest hardware requirements and little Internet access. We\u2019ve worked closely with our collaborators in the field to be sure that our technology is used \nresponsibly\n. Once in place, researchers in the Serengeti will be able to make direct use of this tool, helping provide them with up-to-date species information to better support their conservation efforts.\nWe will be talking further about the project and related work at the \nDeep Learning Indaba\n in Kenya later this August. DeepMind is a founding partner of the Deep Learning Indaba, a continent-wide movement to strengthen the research and application of ML and AI in Africa, and several DeepMind researchers serve as key organisers of this unique event. \u201cIndaba\u201d is a Zulu word indicating an important community gathering. This year, community-led IndabaX AI conferences were held in 26 African countries as part of the runup to the main \nDeep Learning Indaba\n at Kenyatta University in Kenya in late August. For a week, researchers, students and community members will meet to share their knowledge and best practices, and experts will host panels, workshops and discussions covering many topics in machine learning and AI. During this meeting, DeepMind and other Indaba volunteers will co-host a hackathon for anyone interested in ML and conservation to develop their own models using the \nSnapshot Serengeti\n dataset. Ecology students will be equipped to understand and use ML models for conservation, and taught how to develop their own models. Through gatherings like Indaba, we hope to empower more local experts to use AI techniques for addressing problems in their own communities. The AI community in Africa is growing, and the hackathon will help to train local experts, centering on conservation as part of the core dialogue.\nThe DeepMind Science Team works to leverage AI to tackle key scientific challenges that impact the world. We\u2019ve developed a robust model for detecting and analysing animal populations in field data, and have helped to consolidate data to enable the growing machine learning community in Africa to build AI systems for conservation which, we hope, will scale to other parks. We\u2019ll next be validating our models by deploying them in the field and tracking their progress. Our hope is to contribute towards making AI research more inclusive\u2013both in terms of the kinds of domains we apply it to, and the people developing it. Hence, participating in meetings like Indaba are key for helping build a global team of AI practitioners who can deploy machine learning for diverse projects.\nProject credits:\nJean-baptiste Alayrac, Sam Blackwell, Joao Carreira, Reena Chopra, Sander Dieleman, Brian McWilliams, Sofia Mi\u00f1ano, Sanjana Narayanan, Meredith Palmer, Ulrich Paquet, Stig Petersen, Roman Werpachowski, Michal Zielinski.\nAdditional Credits:\nRazia Ahamed, Andrea Banino, Pushmeet Kohli, Drew Purves, Andrew Zisserman\nThis work was made possible by data from Snapshot Serengeti. Images are available via a \nCreative Commons Attribution 4.0 International License\n and can be found \nhere\n. Please contact \nDr Meredith Palmer\n with data inquiries.\nSwanson AB, Kosmala M, Lintott CJ, Simpson RJ, Smith A, Packer C (2015) Snapshot Serengeti, high-frequency annotated camera trap images of 40 mammalian species in an African savanna. Scientific Data 2: 150026\n"}
{"title": "Welcome to the DeepMind podcast", "contents": "What\u2019s AI? What can it be used for? Is it safe? And how do I get involved? These are the kinds of questions we often get asked at public events like science festivals, talks and workshops. We love answering them and really value the conversations and thinking they provoke.\nSadly, we can\u2019t have face-to-face conversations with everyone who is interested in AI. So, to help us bridge that gap, we\u2019re now launching DeepMind: The Podcast, a new series that we hope will answer these questions and more, while also giving listeners an inside look at how AI research is done at an organisation like DeepMind. You can subscribe now on your favourite podcast app.\nCommunicating a complex topic like AI research is not easy, and we\u2019re grateful that this series is hosted by the brilliant \nDr Hannah Fry\n, a mathematician and broadcaster with a talent for making technical topics accessible and interesting. We\u2019ve worked together over the last 12 months to choose topics that we hope will convey the excitement of AI research, whilst also highlighting some of the questions and challenges the whole field is wrestling with today. The result is an eight-part series that explores topics such as the link between neuroscience and AI, why we use games in our research, building safe AI and how AI can be used to solve scientific problems.\nIn it, you will hear from our researchers, engineers, program managers and collaborators, many of whom are talking to the public about their work for the very first time.\nSubscribe on\n \nApple podcasts\n, \nGoogle podcasts\n, \nSpotify\n, \nDeezer\n \nor your favourite podcast app by searching for \u201cDeepMind: The Podcast\u201d.\nWe\u2019re really proud of the programmes and hope they'll spark the curiosity of listeners to explore the world of AI further. To help, we have compiled a list of further reading for each episode in the show notes, drawing on the work of other labs and organisations in the AI community. We want to make this useful to anyone, so if you know of other resources we should link to, please help other listeners by either replying to us on Twitter (#DMpodcast) or emailing us at \npodcast@deepmind.com.\n You can also use that address to send us feedback on the series.\nThis is our first foray into podcasting and we've wrestled with lots of questions, some of which we thought it would be useful to share: \nWe\u2019d like to say a huge thank you to all of the brilliant contributors, creative producers, and our talented presenter, Hannah, all of whom helped craft our initial idea into the final programmes you'll hear over the coming weeks. \nWe really enjoyed making this podcast and hope you enjoy listening too. \nCredits:\nPresenter:\n Hannah Fry\nEditor:\n David Prest\nSenior Producer:\n Louisa Field\nProducers:\n Amy Racs, Dan Hardoon\nBinaural Sound:\n Lucinda Mason-Brown\nMusic composition:\n Eleni Shaw (with help from Sander Dieleman and WaveNet)\n"}
{"title": "The Podcast: Episode 1: AI and neuroscience - The virtuous circle", "contents": "In the first episode of the \nDeepMind podcast\n, Hannah meets the DeepMind Neuroscience team to explore these connections and discovers how our brains are like birds\u2019 wings, what training a dog and an AI agent have in common, and why the simplest things for people to do are, paradoxically, often the hardest for machines. \nInterviewees: \nDeepmind CEO and co-founder, Demis Hassabis; Matt Botvinick, Director of Neuroscience Research; research scientists Jess Hamrick and Greg Wayne; and Director of Research Koray Kavukcuoglu.\nListen to this episode and subscribe to the whole series on\n \nApple podcasts\n, \nGoogle podcasts\n, \nSpotify\n, \nDeezer\n \nor your favourite podcast app by searching for \u201cDeepMind: The Podcast\u201d.\nIf you know of other resources we should link to, please help other listeners by either replying to us on \nTwitter\n (#DMpodcast) or emailing us at \npodcast@deepmind.com.\n You can also use that address to send us questions or feedback on the series.\nPresenter:\n Hannah Fry\nEditor:\n David Prest\nSenior Producer:\n Louisa Field\nProducers:\n Amy Racs, Dan Hardoon\nBinaural Sound:\n Lucinda Mason-Brown\nMusic composition:\n Eleni Shaw (with help from Sander Dieleman and \nWaveNet\n)\n"}
{"title": "The Podcast: Episode 2: Go to Zero", "contents": "In March 2016, more than 200 million people watched AlphaGo become first computer program to defeat a professional human player at the game of Go, a milestone in AI research that was considered to be a decade ahead of its time.\nSince then the team has continued to develop the system and recently unveiled \u00a0AlphaZero: a program that has taught itself how to play chess, Go, and shogi. Hannah explores the inside story of both with Lead Researcher David Silver and finds out why games are a useful proving ground for AI researchers. She also meets Chess Grandmaster Matthew Sadler and women\u2019s international master Natasha Regan, who have written a book on AlphaZero and its unique gameplay.\nInterviewees: \nDeepMind CEO Demis Hassabis, Matthew Sadler, chess Grandmaster; Lead Researcher David Silver, Matt Botvinick, Director of Neuroscience Research; and Natasha Regan, women\u2019s international chess master.\nIf you know of other resources we should link to, please help other listeners by either replying to us on \nTwitter\n (#DMpodcast) or emailing us at \npodcast@deepmind.com.\n You can also use that address to send us questions or feedback on the series.\nPresenter:\n Hannah Fry\nEditor:\n David Prest\nSenior Producer:\n Louisa Field\nProducers:\n Amy Racs, Dan Hardoon\nBinaural Sound:\n Lucinda Mason-Brown\nMusic composition:\n Eleni Shaw (with help from Sander Dieleman and \nWaveNet\n)\n"}
{"title": "Researching patient deterioration with the US Department of Veterans Affairs", "contents": "We\u2019re excited to announce a medical research partnership with the US Department of Veterans Affairs (VA), one of the world\u2019s leading healthcare organisations responsible for providing high-quality care to veterans and their families across the United States.\nThis project will see us analyse patterns from historical, depersonalised medical records to predict patient deterioration.\nPatient deterioration is a significant global health problem that often has fatal consequences. \nStudies\nestimate that 11% of all in-hospital deaths are due to patient deterioration not being recognised early enough or acted on in the right way.\nAlongside world-renowned clinicians and researchers at the VA, we are analysing patterns from approximately 700,000 historical, depersonalised medical records in order to determine if machine learning can accurately identify the risk factors for patient deterioration and correctly predict its onset.\nWe\u2019re focusing on Acute Kidney Injury (AKI), one of the most common conditions associated with patient deterioration, and an area where \nDeepMind\n and the VA both have expertise. This is a complex challenge, because predicting AKI is far from easy. Not only is the onset of AKI sudden and often asymptomatic, but the risk factors associated with it are commonplace throughout hospitals. AKI can also strike people of any age, and frequently occurs following routine procedures and operations like a hip replacement.\nOur goal is to find ways to improve the algorithms currently used to detect AKI and allow doctors and nurses to intervene sooner. Eventually, we hope to apply similar approaches to other signs of patient deterioration as well, leading to improved care for many more patients, with fewer people developing serious infections and conditions\u2014ultimately saving lives.\nAs with all of our research work, we are committed to treating the data for this project with the utmost care and respect. The data being used in the research are depersonalised, meaning that any information that could be used to identify individuals has been removed before DeepMind receives it. You can read more about our own approach to information governance \nhere\n.\nThe work we\u2019re currently conducting is exploratory, but we\u2019re optimistic about the long term potential for machine learning technology in this area. In a world where nearly all hospital resources go toward managing symptoms after people are already ill, we hope that predictive techniques can pave the way for more preventive healthcare and help keep people from getting sick in the first place. We\u2019ll keep you updated as we continue this work.\n\u200d\n"}
{"title": "Understanding deep learning through neuron deletion", "contents": "Deep neural networks are composed of many individual neurons, which combine in complex and counterintuitive ways to solve a wide range of challenging tasks. This complexity grants neural networks their power but also earns them their reputation as confusing and opaque black boxes. \nUnderstanding how deep neural networks function is critical for explaining their decisions and enabling us to build more powerful systems. For instance, imagine the difficulty of trying to build a clock without understanding how individual gears fit together. One approach to understanding neural networks, both in neuroscience and deep learning, is to investigate the role of individual neurons, especially those which are easily interpretable. \nOur investigation into the \nimportance of single directions for generalisation\n, soon to appear at the Sixth International Conference on Learning Representations (\nICLR\n), uses an approach inspired by decades of experimental neuroscience \u2014 exploring the impact of damage \u2014 to determine: how important are small groups of neurons in deep neural networks? Are more \neasily interpretable\n neurons also more important to the network\u2019s computation? \nWe measured the performance impact of damaging the network by deleting individual neurons as well as groups of neurons. Our experiments led to two surprising findings: \nIn both neuroscience and deep learning, easily interpretable neurons (\u201cselective\u201d neurons) which are only active in response to images of a single input category, such as dogs, have been analysed extensively. In deep learning, this has led to the emphasis on \ncat neurons\n, \nsentiment neurons\n, and \nparentheses neurons\n; in neuroscience, \nJennifer Aniston neurons\n, among others. However, the relative importance of these few highly selective neurons compared to the majority of neurons which have low selectivity and more puzzling, hard-to-interpret activity has remained unknown.\nTo evaluate neuron importance, we measured how network performance on image classification tasks changes when a neuron is deleted. If a neuron is very important, deleting it should be highly damaging and substantially decrease network performance, while the deletion of an unimportant neuron should have little impact. Neuroscientists routinely perform similar experiments, although they cannot achieve the fine-grained precision which is necessary for these experiments and readily available in artificial neural networks. \nSurprisingly, we found that there was little relationship between selectivity and importance. In other words, \u201ccat neurons\u201d were no more important than confusing neurons. This finding echoes recent work in neuroscience which has demonstrated that confusing neurons can actually be quite informative, and suggests that we must look beyond the most easily interpretable neurons in order to understand deep neural networks.\nAlthough interpretable neurons are easier to understand intuitively (\u201cit likes dogs\u201d), they are no more important than confusing neurons with no obvious preference. \nWe seek to build intelligent systems, and we can only call a system intelligent if it can generalise to new situations. For example, an image classification network which can only classify specific dog images that it has seen before, but not new images of the same dog, is useless. It is only in the intelligent categorisation of new examples that these systems gain their utility. A \nrecent collaborative paper from Google Brain, Berkeley, and DeepMind\n which won best paper at ICLR 2017 showed that deep nets can simply memorise each and every image on which they are trained instead of learning in a more human-like way (e.g., understanding the abstract notion of a \"dog\"). \nHowever, it is often unclear whether a network has learned a solution which will generalise to new situations or not. By deleting progressively larger and larger groups of neurons, we found that networks which generalise well were much more robust to deletions than networks which simply memorised images that were previously seen during training. In other words, networks which generalise better are harder to break (although they can definitely still be broken). \nBy measuring network robustness in this way, we can evaluate whether a network is exploiting undesirable memorisation to \u201ccheat.\u201d Understanding how networks change when they memorise will help us to build new networks which memorise less and generalise more.\nTogether, these findings demonstrate the power of using techniques inspired by experimental neuroscience to understand neural networks. Using these methods, we found that highly selective individual neurons are no more important than non-selective neurons, and that networks which generalise well are much less reliant on individual neurons than those which simply memorise the training data. These results imply that individual neurons may be much less important than a first glance may suggest. \nBy working to explain the role of all neurons, not just those which are easy-to-interpret, we hope to better understand the inner workings of neural networks, and critically, to use this understanding to build more intelligent and general systems.\nRead the full paper \nhere\n.\nThis work was done by Ari S Morcos, David GT Barrett, Neil C Rabinowitz, and Matthew Botvinick.\nVisualisations were created by Paul Lewis, Adam Cain, and Doug Fritz.\n"}
{"title": "Learning to write programs that generate images", "contents": "Through a human\u2019s eyes, the world is much more than just the images reflected in our corneas. For example, when we look at a building and admire the intricacies of its design, we can appreciate the craftsmanship it requires. This ability to interpret objects through the tools that created them gives us a richer understanding of the world and is an important aspect of our intelligence.\nWe would like our systems to create similarly rich representations of the world. For example, when observing an image of a painting we would like them to understand the brush strokes used to create it and not just the pixels that represent it on a screen.\nIn this work\n, we equipped artificial agents with the same tools that we use to generate images and demonstrate that they can reason about how digits, characters and portraits are constructed. Crucially, they learn to do this by themselves and without the need for human-labelled datasets. This contrasts with \nrecent research\n which has so far relied on learning from human demonstrations, which can be a time-intensive process.\nWe designed a deep reinforcement learning \nagent \nthat interacts with a computer \npaint program\n, placing strokes on a digital canvas and changing the brush size, pressure and colour. The untrained agent starts by drawing random strokes with no visible intent or structure. To overcome this, we had to create a way to reward the agent that encourages it to produce meaningful drawings.\nTo this end, we trained a second neural network, called the \ndiscriminator\n, whose sole purpose is to predict whether a particular drawing was produced by the agent, or if it was sampled from a dataset of real photographs. The painting agent is rewarded by how much it manages to \u201cfool\u201d the discriminator into thinking its drawings are real. In other words, the agent\u2019s reward signal is itself learned. While this is similar to the approach used in Generative Adversarial Networks (GANs), it differs because the generator in GAN setups is typically a neural network that directly outputs pixels. In contrast, our agent produces images by writing graphics programs to interact with a paint environment.\nIn the first set of experiments, the agent was trained to generate images resembling \nMNIST\n digits: it was shown what the digits look like, but not how they are drawn. By attempting to generate images that fool the discriminator, the agent learns to control the brush and to manoeuvre it to fit the style of different digits, a technique known as visual \nprogram synthesis\n.\nWe also trained it to reproduce specific images. Here, the discriminator\u2019s aim is to determine if the reproduced image is a copy of the target image, or if it has been produced by the agent. The more difficult this distinction becomes for the discriminator, the more the agent is rewarded.\nCrucially, this framework is also interpretable because it produces a sequence of motions that control a simulated brush. This means that the model can apply what it has learnt on the simulated paint program to re-create characters in other similar environments, for instance on a simulated or real robot arm. A video of this can be seen \nhere\n. \nThere is also potential to scale this framework to real datasets. When trained to paint \ncelebrity faces\n, the agent is capable of capturing the main traits of the face, such as shape, tone and hair style, much like a street artist would when painting a portrait with a limited number of brush strokes:\nRecovering structured representations from raw sensations is an ability that humans readily possess and frequently use. In this work we show it is possible to guide artificial agents to \u00a0produce similar representations by giving them access to the same tools that we use to recreate the world around us. In doing so they learn to produce visual programs that succinctly express the causal relationships that give rise to their observations.\nAlthough our work only represents a small step towards flexible program synthesis, we anticipate that similar techniques may be necessary to enable artificial agents with human-like cognitive, generalisation and communication abilities.\nWatch the video \nhere\n, read more about the method in the \npaper\n.\nThis work was done by Yaroslav Ganin, Tejas Kulkarni, Igor Babuschkin, S. M. Ali Eslami and Oriol Vinyals, with thanks to Oleg Sushkov, David Barker, Matej Vecerik and Jon Scholz for their help with the robot.\n"}
{"title": "Retour \u00e0 Paris / A return to Paris", "contents": "English version follows\nLorsque nous avons \u00e9tabli notre si\u00e8ge \u00e0 Londres en 2010, nous voulions faire de DeepMind le nec plus ultra de la recherche de pointe dans le domaine de l\u2019intelligence artificielle. Nous voulions \u00e9galement aider la communaut\u00e9 de l\u2019intelligence artificielle \u00e0 se d\u00e9velopper. Nous avons ainsi publi\u00e9 des articles dans les conf\u00e9rences et journaux les plus s\u00e9lectifs (plus de 180 \u00e0 ce jour !) et partag\u00e9 nos connaissances dans ce domaine ; nous avons incit\u00e9 nos experts \u00e0 enseigner dans les universit\u00e9s locales, et \u0153uvr\u00e9 avec les \u00e9coles et les ONG \u00e0 former la prochaine g\u00e9n\u00e9ration de scientifiques. \nNous avons eu non seulement la chance de contribuer au succ\u00e8s scientifique du Royaume-Uni, mais avons aussi grandement b\u00e9n\u00e9fici\u00e9 de l\u2019ouverture et de la diversit\u00e9 de cette ville ainsi que de son influence culturelle. L\u2019intelligence artificielle doit \u00eatre d\u00e9velopp\u00e9e en accordant la plus grande attention aux diff\u00e9rents besoins de la soci\u00e9t\u00e9 et \u2013 pour autant qu\u2019une ville puisse r\u00e9unir \u00e0 elle seule ces conditions \u2013 une capitale multiculturelle comme Londres, ma ville natale, est \u00e0 cet \u00e9gard l\u2019endroit id\u00e9al. \nJe suis, donc, tr\u00e8s heureux d\u2019annoncer notre d\u00e9cision d\u2019ouvrir notre premier laboratoire en Europe continentale, dans une autre grande capitale culturelle et scientifique : Paris. Et je me r\u00e9jouis d\u2019autant plus que R\u00e9mi Munos, l\u2019un des principaux chercheurs de DeepMind et auteur de 150 articles scientifiques, fera son retour dans son pays, la France, o\u00f9 il dirigera ce nouveau laboratoire. \nLe laboratoire DeepMind de Paris se consacrera \u00e0 la recherche fondamentale en intelligence artificielle, en s\u2019appuyant sur les pr\u00e9c\u00e9dentes contributions scientifiques de R\u00e9mi Munos. Ces travaux portent aussi bien sur les m\u00e9thodes d\u2019apprentissage permettant \u00e0 un \nalgorithme unique\n d\u2019apprendre \u00e0 ex\u00e9cuter plusieurs t\u00e2ches diff\u00e9rentes \u2013 un \u00e9l\u00e9ment-cl\u00e9 de l\u2019intelligence \u2013 que sur les d\u00e9couvertes algorithmiques fondamentales comme l\u2019apprentissage par renforcement distributionnel.\nCertains \u00e9tablissements de recherche en intelligence artificielle, parmi les plus renomm\u00e9s au monde, dont des organismes publics de recherche comme l\u2019INRIA, o\u00f9 R\u00e9mi Munos est directeur de recherche, le CNRS et les Grandes \u00c9coles, sans parler d\u2019un exceptionnel r\u00e9seau d\u2019universit\u00e9s, ont d\u00e9j\u00e0 \u00e9lu domicile \u00e0 Paris. Avec l\u2019int\u00e9r\u00eat que porte le gouvernement fran\u00e7ais \u00e0 la fois \u00e0 l\u2019intelligence artificielle et \u00e0 l\u2019excellence en mati\u00e8re de recherche, l'Hexagone est devenu une destination privil\u00e9gi\u00e9e pour les scientifiques et ing\u00e9nieurs de stature internationale, contribuant \u00e0 cr\u00e9er une communaut\u00e9 de recherche prosp\u00e8re et performante.\nNous nous r\u00e9jouissons que R\u00e9mi Munos et son \u00e9quipe aient ainsi la chance d\u2019apporter plus encore leur contribution aux communaut\u00e9s de recherche dans lesquelles ils ont d\u00e9but\u00e9 leur carri\u00e8re. Comme nous l\u2019avons fait dans d\u2019autres laboratoires de recherche \u00e0 \nLondres\n et au \nCanada\n, o\u00f9 nos scientifiques ont \napport\u00e9 leur soutien \u00e0 des cours universitaires\n ainsi qu\u2019\u00e0 des\n chaires universitaires ind\u00e9pendantes\n, nous nous r\u00e9jouissons \u00e0 l\u2019id\u00e9e de collaborer \navec les chercheurs en intelligence artificielle de Google France\n mais aussi avec une communaut\u00e9 plus large, la capitale regorgeant de start-ups et de scientifiques prometteurs.\nJ'ai h\u00e2te de voir ce que R\u00e9mi et son \u00e9quipe vont r\u00e9aliser \u00e0 Paris, et suis tout aussi impatient de prendre plus souvent l\u2019Eurostar entre nos deux grandes villes!\nVoici ce que l\u2019on dit de DeepMind Paris\nWhen we set up our headquarters in London in 2010, we wanted to make DeepMind the best possible place to do cutting-edge AI research. We also wanted to help the wider AI community grow - publishing peer-reviewed papers (over 180 to date, and counting!) and sharing our insights to advance the field, supporting our staff to teach at local universities, and working with schools and NGOs to foster the next generation of scientists.\nAlongside enjoying the chance to support the UK\u2019s scientific success, we\u2019ve benefited a lot from the cultural influence of being in such a diverse and open city. AI needs to be developed with thoughtful consideration of the different needs of society, and - as much as any one city can provide that context - a multicultural capital like London, my hometown, is a great place to be.\nSo, I\u2019m excited to announce that we\u2019ve decided to open our first lab in continental Europe, in another of the world\u2019s great cultural and scientific capitals: Paris. And I\u2019m doubly excited that Remi Munos, one of DeepMind\u2019s principal research scientists, author of 150 research papers, and former professor at \u00c9cole Polytechnique will be returning home to France to lead this new lab.\nThe DeepMind Paris lab will focus on fundamental AI research, building on Remi\u2019s previous scientific contributions. These include new state-of-the-art methods that enable \nsingle AI systems\n to learn how to perform many different tasks - a core component of intelligence - as well as fundamental algorithmic breakthroughs such as \ndistributional reinforcement learning\n.\nParis is already home to some of the world\u2019s most influential AI research institutions, including public research centres like INRIA - where Remi is a senior researcher - as well as CNRS, the Grandes \u00c9coles, and an outstanding network of universities. With the French government\u2019s focus on AI and research excellence, the country has emerged as a leading destination for world-class scientists and engineers, creating a thriving and productive research community.\nWe\u2019re excited at the chance for Remi and his team to contribute even more to the communities where they started their careers. As we\u2019ve done in our other research labs in \nLondon\n and \nCanada\n, where our scientists have \nsupported academic courses\n and \nindependent university chairs\n, we\u2019re looking forward to collaborating with \nother AI researchers in Google France\n and the wider community, start-ups, and Paris\u2019 brilliant scientists of tomorrow.\nI can\u2019t wait to see what Remi and his team achieve in Paris, and am even looking forward to spending more time on the Eurostar train between our two great cities!\n"}
{"title": "Learning to navigate in cities without a map", "contents": "How did you learn to navigate the neighborhood of your childhood, to go to a friend\u2019s house, to your school or to the grocery store? Probably without a map and simply by remembering the visual appearance of streets and turns along the way. As you gradually explored your neighborhood, you grew more confident, mastered your whereabouts and learned new and increasingly complex paths. You may have gotten briefly lost, but found your way again thanks to landmarks, or perhaps even by looking to the sun for an impromptu compass.\nNavigation is an important cognitive task that enables humans and animals to traverse, without maps, over long distances in a complex world. Such long-range navigation can simultaneously support self-localisation (\u201cI am here\u201d) and a representation of the goal (\u201cI am going there\u201d).\nIn \nLearning to Navigate in Cities Without a Map\n, we present an interactive navigation environment that uses first-person perspective photographs from \nGoogle Street View\n, approved for use by the StreetLearn project and academic research, and gamify that environment to train an AI. As standard with Street View images, faces and license plates have been blurred and are unrecognisable. We build a neural network-based artificial agent that learns to navigate multiple cities using visual information (pixels from a Street View image). Note that this research is about navigation in general rather than driving; we did not use traffic information nor try to model vehicle control.\nThe agent is rewarded when it reaches a target destination (specified, for instance, as pair of latitude and longitude coordinates), like a courier tasked with an endless set of deliveries but without a map. Over time, the AI agent learns to cross entire cities in this way. We also demonstrate that our agent can learn the task in multiple cities, and then robustly adapt to a new city.\nWe depart from the traditional approaches which rely on explicit mapping and exploration (like a cartographer who tries to localise themselves and draw a map at the same time). Our approach, in contrast, is to learn to navigate as humans used to do, without maps, GPS localisation, or other aids, using only visual observations. We build a neural network agent that inputs images observed from the environment and predicts the next action it should take in that environment. We train it end-to-end using deep reinforcement learning, similarly to some recent work on \nlearning to navigate in complex 3D mazes\n and \nreinforcement learning with unsupervised auxiliary tasks\n for playing games. Unlike those studies, which were conducted on small-scale simulated maze environments, we utilise city-scale real-world data, including complex intersections, footpaths, tunnels, and diverse topology across London, Paris, and New York City. Moreover, the approach we use support city-specific learning and optimisation as well as general, transferable navigation behaviours.\nThe neural network inside our agent consists of three parts: 1) a convolutional network that can process images and extract visual features, 2) a locale-specific recurrent neural network that is implicitly tasked with memorising the environment as well as learning a representation of \u201chere\u201d (current position of the agent) and of \u201cthere\u201d (location of the goal) and 3) a locale-invariant recurrent network that produces the navigation policy over the agent\u2019s actions. The locale-specific module is designed to be interchangeable and, as its name indicates, unique to each city where the agent navigates, whereas the vision module and the policy module can be locale-invariant.\nJust as in the Google Street View interface, the agent can rotate in place or move forward to the next panorama, when possible. Unlike the Google Maps and Street View environment, the agent does not see the little arrows, the local or global map, or the famous Pegman: it needs to learn to differentiate open roads from sidewalks. The target destinations may be kilometres away in the real world and require the agent to step through hundreds of panoramas to reach them.\nWe demonstrate that our proposed method can provide a mechanism for transferring knowledge to new cities. As with humans, when our agent visits a new city, we would expect it to have to learn a new set of landmarks, but not to have to re-learn its visual representations or its behaviours (e.g., zooming forward along streets or turning at intersections). Therefore, using the MultiCity architecture, we train first on a number of cities, then we freeze both the policy network and the visual convolutional network and only a new locale-specific pathway on a new city. This approach enables the agent to acquire new knowledge without forgetting what it has already learned, similarly to the\n progressive neural networks\narchitecture.\nStudying navigation is fundamental in the study and development of artificial intelligence, and trying to replicate navigation in artificial agents can also help scientists understand its biological underpinnings.\n"}
{"title": "Our first COO Lila Ibrahim takes DeepMind to the next level", "contents": "One of the greatest pleasures of coming to work every day at DeepMind is the chance to collaborate with brilliant researchers and engineers from so many different fields and perspectives - with machine learning experts alongside neuroscientists, physicists, mathematicians, roboticists, ethicists and more.\nThis level of interdisciplinary collaboration is both challenging and unusual, and it requires a unique type of organisation. We built DeepMind to combine the rigour and long-term thinking of the world\u2019s best scientific institutions, along with the focus, pace and energy common to the best tech startups. I believe this is essential if we\u2019re to fulfil the scientific and social promise of AI, and I\u2019m proud of all that \nwe\u2019ve achieved so far\n. But there\u2019s still a very long way to go!\nSo I\u2019m really pleased to welcome Lila Ibrahim to DeepMind as our first ever Chief Operating Officer, partnering with me to design, build and manage our next phase of growth. Having started out as a microprocessor designer and assembler programmer at Intel, Lila went on to lead the company\u2019s emerging markets product group, as well as working with Intel CEO Craig Barrett and then the legendary investor John Doerr at Kleiner Perkins as Chief of Staff. Most recently, Lila led the growth of the brilliant startup \nCoursera\n as their COO, helping to bring new educational opportunities to millions of people around the world.\nAlongside her organisational and technical experience, Lila is a passionate advocate for social impact in her work and her personal life, has been \nrecognised by the Anita Borg Institute for her commitment and vision\n, and is a Henry Crown Fellow at the \nAspen Institute\n. I can\u2019t wait for Lila to get started, and I know we\u2019ll gain a lot from her insight as we take DeepMind to the next level.\nHere\u2019s what Lila has to say:\nIt\u2019s a privilege to be working on this mission with so many world-class people, and Lila, Shane, Mustafa and I can\u2019t wait to see what we achieve together in the years ahead. And if you\u2019d like to be part of our amazing adventure \nthen please get in touch\n!\n"}
{"title": "DeepMind papers at ICLR 2018", "contents": "Between 30 April and 03 May, hundreds of researchers and engineers will gather in Vancouver, Canada, for the \nSixth International Conference on Learning Representations.\nHere you can read details of all DeepMind\u2019s accepted papers and find out where you can see the accompanying poster sessions and talks.\nAuthors: \nAbbas Abdolmaleki, Jost Tobias Springenberg, Nicolas Heess, Yuval Tassa, Remi Munos\nWe introduce a new algorithm for reinforcement learning called Maximum a posteriori Policy Optimisation (MPO) based on coordinate ascent on a relative entropy objective. We show that several existing methods can directly be related to our derivation. We develop two off-policy algorithms and demonstrate that they are competitive with the state-of-the-art in deep reinforcement learning. In particular, for continuous control, our method outperforms existing methods with respect to sample efficiency, premature convergence and robustness to hyperparameter settings.\nAuthors:\n Hanxiao Liu (CMU), Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, Koray Kavukcuoglu\nWe explore efficient neural architecture search methods and show that a simple yet powerful evolutionary algorithm can discover new architectures with excellent performance. Our approach combines a novel hierarchical genetic representation scheme that imitates the modularized design pattern commonly adopted by human experts, and an expressive search space that supports complex topologies. Our algorithm efficiently discovers architectures that outperform a large number of manually designed models for image classification, obtaining top-1 error of 3.6% on CIFAR-10 and 20.3% when transferred to ImageNet, which is competitive with the best existing neural architecture search approaches. We also present results using random search, achieving 0.3% less top-1 accuracy on CIFAR-10 and 0.1% less on ImageNet whilst reducing the search time from 36 hours down to 1 hour.\nAuthors: \nKarol Hausman, Jost Tobias Springenberg, Ziyu Wang, Nicolas Heess, Martin Riedmiller\nWe present a method for reinforcement learning of closely related skills that are parameterized via a skill embedding space. \u00a0We learn such skills by taking advantage of latent variables and exploiting a connection between reinforcement learning and variational inference.\nThe main contribution of our work is an entropy-regularized policy gradient formulation for hierarchical policies, and an associated, data-efficient and robust off-policy gradient algorithm based on stochastic value gradients. We demonstrate the effectiveness of our method on several simulated robotic manipulation tasks.\nAuthors: \nBrandon Amos, Laurent Dinh, Serkan Cabi, Thomas Roth\u00f6rl, Sergio G\u00f3mez Colmenarejo, Alistair M Muldal, Tom Erez, Yuval Tassa, Nando de Freitas, Misha Denil\nWe show that models trained to predict proprioceptive information about an agent's body come to represent objects in the external world. The models able to successfully predict sensor readings over 100 steps into the future and continue to represent the shape of external objects even after contact is lost. We show that active data collection by maximizing uncertainty over future sensor readings leads to models that show superior performance when used for control. We also collect data from a real robotic hand and show that the same models can be used to answer questions about the properties of objects in the real world.\nAuthors: \nJames Martens, Jimmy Ba (Vector Institute), \u00a0Matthew Johnson (Google)\nKronecker-factor Approximate Curvature (Martens & Grosse, 2015) (K-FAC) is a 2nd-order optimization method which has been shown to give state-of-the-art performance on large-scale neural network optimization tasks (Ba et al., 2017). \u00a0It is based on an approximation to the Fisher information matrix (FIM) that makes assumptions about the particular structure of the network and the way it is parameterized. The original K-FAC method was applicable only to fully-connected networks, although it has been recently extended by Grosse & Martens (2016) to handle convolutional networks as well. In this work we extend the method to handle RNNs by introducing a novel approximation to the FIM for RNNs. This approximation works by modelling the covariance structure between the gradient contributions at different time-steps using a chain-structured linear Gaussian graphical model, summing the various cross-covariances, and computing the inverse in closed form. We demonstrate in experiments that our method significantly outperforms general purpose state-of-the-art optimizers like SGD with momentum and Adam on several challenging RNN training tasks.\nAuthors: \nGabriel Barth-maron, Matthew Hoffman, David Budden, Will Dabney, Daniel Horgan, Dhruva Tirumala Bukkapatnam, Alistair M Muldal, Nicolas Heess, Timothy Lillicrap\nThis work adopts the very successful distributional perspective on reinforcement learning and adapts it to the continuous control setting. We combine this within a distributed framework for off-policy learning in order to develop what we call the Distributed Distributional Deep Deterministic Policy Gradient algorithm, D4PG. We also combine this technique with a number of additional, simple improvements such as the use of N-step returns and prioritized experience replay. Experimentally we examine the contribution of each of these individual components, and show how they interact, as well as their combined contributions. Our results show that across a wide variety of simple control tasks, difficult manipulation tasks, and a set of hard obstacle-based locomotion tasks the D4PG algorithm achieves state of the art performance.\nAuthors: \nYan Wu, Greg Wayne, Alex Graves, Timothy Lillicrap\nWe present an end-to-end trained memory system that quickly adapts to new data and generates samples like them. The memory is analytically tractable, which enables optimal on-line compression via a Bayesian update-rule. We formulate it as a hierarchical conditional generative model, where memory provides a rich data-dependent prior distribution. Consequently, the top-down memory and bottom-up perception are combined to produce the code representing an observation.\nAuthors: \nPablo Sprechmann, Siddhant Jayakumar, Jack Rae, Alexander Pritzel, Adria P Badia \u00b7 Benigno Uria, Oriol Vinyals, Demis Hassabis, Razvan Pascanu, Charles Blundell\nHumans and animals are able to incorporate new knowledge quickly from a few examples, continually throughout much of their lifetime. In contrast, neural network-based models rely on the data distribution being stationary and a gradual training procedure to obtain good generalisation. \u00a0Drawing inspiration from the theory of complementary learning systems, we propose Memory-based Parameter Adaptation (MbPA), a method for augmenting neural networks with an episodic memory to allow for rapid acquisition of new knowledge while preserving the high performance and good generalisation of standard deep models. MbPA, stores examples in memory and then uses a context-based lookup to directly modify the weights of a neural network. It alleviates several shortcomings of neural networks, such as catastrophic forgetting, fast, stable acquisition of new knowledge, and fast learning during evaluation.\nAuthors: \nIrina Higgins, Nicolas Sonnerat, Loic Matthey, Arka Pal, Christopher P Burgess, Matko Bo\u0161njak, Murray Shanahan, Matthew Botvinick, \u00a0Alexander Lerchner\nWe propose a novel theoretical approach to address the problem of abstract compositionality - how can we learn a small number of grounded building blocks and use them to create a vast number of new abstract concepts on the fly? We present a new neural network architecture called the Symbol-Concept Association Network (SCAN), that can learn a grounded visual concept hierarchy, enabling it to imagine novel concepts guided by language instructions.\nAuthors: \nAngeliki Lazaridou, Karl M Hermann, Karl Tuyls, Stephen Clark\nThe ability of algorithms to evolve or learn (compositional) communication protocols has traditionally been studied in the language evolution literature through the use of emergent communication tasks. Here we scale up this research by using contemporary deep learning methods and by training reinforcement-learning neural network agents on referential communication games. We extend previous work, in which agents were trained in symbolic environments, by developing agents which are able to learn from raw pixel data, a more challenging and realistic input representation. We find that the degree of structure found in the input data affects the nature of the emerged protocols, and thereby corroborate the hypothesis that structured compositional language is most likely to emerge when agents perceive the world as being structured.\nAuthors:\n William Fedus (Universit\u00e9 de Montr\u00e9al), Mihaela Rosca, Balaji Lakshminarayanan, Andrew Dai (Google), Shakir Mohamed, \u00a0Ian Goodfellow (Google Brain)\nThe field of generative adversarial networks research has grown, fueled by the successes of their application in computer vision. In an attempt to solve training instability in generative adversarial networks, multiple theoretical justifications for training dynamics have been suggested and new training methods proposed. By focusing on the divergence minimization view of generative adversarial networks and regularizers such as gradient penalties, we empirically show that the success of some of these approaches cannot be solely explained by the accompanying underlying theory. This motivates the need for new theoretical framework that can encompass and explains the presented results.\nAuthors: \nRichard Evans, David Saxton, David Amos, Pushmeet Kohli, Edward Grefenstette\nWe introduce a new dataset of logical entailments for the purpose of measuring models' ability to capture and exploit the structure of logical expressions against an entailment prediction task. We use this task to compare a series of architectures which are ubiquitous in the sequence-processing literature, in addition to a new model class\u2013PossibleWorldNets\u2013which computes entailment as a \"convolution over possible worlds\". Results show that convolutional networks present the wrong inductive bias for this class of problems relative to LSTM RNNs, tree-structured neural networks outperform LSTM RNNs due to their enhanced ability to exploit the syntax of logic, and PossibleWorldNets outperform all benchmarks.\nAuthors: \nDaniel Horgan, John Quan, David Budden, Gabriel Barth-maron, Matteo Hessel, Hado van Hasselt, David Silver\nWe propose a distributed architecture for deep reinforcement learning at scale, that enables agents to learn effectively from orders of magnitude more data than previously possible. The algorithm decouples acting from learning: the actors interact with their own instances of the environment by selecting actions according to a shared neural network, and accumulate the resulting experience in a shared experience replay memory; the learner replays samples of experience and updates the neural network. The architecture relies on prioritized experience replay to focus only on the most significant data generated by the actors. Our architecture substantially improves the state of the art on the Arcade Learning Environment, achieving better final performance in a fraction of the wall-clock training time.\nAuthors:\n Audrunas Gruslys, Will Dabney, Mohammad Gheshlaghi Azar, Bilal Piot, Marc G Bellemare, Remi Munos\nWe propose multiple algorithmic and architectural improvements producing an agent with a higher sample-efficiency than Prioritized Dueling DQN and Categorical DQN, while giving better run-time performance than A3C. Distributional Retrace policy evaluation algorithm brings multi-step off-policy updates to the distributional reinforcement learning setting. Our approach can be used to convert several classes of multi-step policy evaluation algorithms into distributional ones. \u03b2-leave-one-out policy gradient algorithm uses action values as a baseline. A new prioritized replay algorithm exploits temporal locality for more efficient replay prioritization. Reactor reaches state-of-the-art performance after 200 million frames in less than a day.\nAuthors:\n David Pfau, Christopher P Burgess\nSpectral algorithms for learning low-dimensional data manifolds have largely been supplanted by deep learning methods in recent years. One reason is that classic spectral manifold learning methods often learn collapsed embeddings that do not fill the embedding space. We show that this is a natural consequence of data where different latent dimensions have dramatically different scaling in observation space. We present a simple extension of Laplacian Eigenmaps to fix this problem based on choosing embedding vectors which are both orthogonal and \\textit{minimally redundant} to other dimensions of the embedding. In experiments on NORB and similarity-transformed faces we show that Minimally Redundant Laplacian Eigenmap (MR-LEM) significantly improves the quality of embedding vectors over Laplacian Eigenmaps, accurately recovers the latent topology of the data, and discovers many disentangled factors of variation of comparable quality to state-of-the-art deep learning methods.\nAuthors: \nAri Morcos, David GT Barrett, Neil C Rabinowitz, Matthew Botvinick\nOur investigation into the importance of single directions for generalisation...uses an approach inspired by decades of experimental neuroscience - exploring the impact of damage - to determine: how important are small groups of neurons in deep neural networks? Are more easily interpretable neurons also more important to the network\u2019s computation? We measured the performance impact of damaging the network by deleting individual neurons as well as groups of neurons. Our experiments led to two surprising findings: 1. Although many previous studies have focused on understanding easily interpretable individual neurons (e.g. \u201ccat neurons\u201d, or neurons in the hidden layers of deep networks which are only active in response to images of cats), we found that these interpretable neurons are no more important than confusing neurons with difficult-to-interpret activity. 2. Networks which correctly classify unseen images are more resilient to neuron deletion than networks which can only classify images they have seen before. In other words, networks which generalise well are much less reliant on single directions than those which memorise.\nAuthors:\n Dani Yogatama, Yishu Miao, G\u00e1bor Melis, Wang Ling, Adhiguna Kuncoro, Chris Dyer, Phil Blunsom\nGenerating fluent, grammatical language requires keeping track of what words have been generated in the past. In this paper, we compare three memory architectures (sequential, random access, and stack-based) and find that a stack-structured memory demonstrates the best performance in terms of held-out perplexity. To give the stack memory more power and better match the phenomena encountered in language, we introduce a generalization of existing differentiable stack memories enabling them to execute multiple pop operations at each timestep, which further improves performance. Finally, we show that our stack-augmented language model correctly learns to predict difficult long-range agreement patterns which are difficult for conventional LSTM language models.\nAuthors: \nScott Reed, Yutian Chen, Thomas Paine, Aaron van den Oord, S. M. Ali Eslami, Danilo J Rezende, Oriol Vinyals, Nando de Freitas\nCurrent image density models require large amounts of data and computation time for training. In this paper, we show how 1) neural attention and 2) meta learning techniques can be used in combination with autoregressive models to enable effective few-shot density estimation. Our modified PixelCNNs result in state-of-the art few-shot density estimation on Omniglot. We visualize the learned attention policy and find that it learns intuitive algorithms for simple tasks such as image mirroring and digit drawing on Omniglot without supervision. Finally, we demonstrate few-shot image generation on the Stanford Online Products dataset.\nAuthors: \nG\u00e1bor Melis, Chris Dyer, Phil Blunsom\nOngoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing code bases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.\nAuthors:\n Kris Cao, Angeliki Lazaridou, Marc Lanctot, Joel Z Leibo, Karl Tuyls, Stephen Clark\nMulti-agent reinforcement learning offers a way to study how communication could emerge in communities of agents needing to solve specific problems. In this paper, we study the emergence of communication in the negotiation environment, a semi-cooperative model of agent interaction. We introduce two communication protocols \u2013 one grounded in the semantics of the game, and one which is a priori ungrounded and is a form of cheap talk. We show that self-interested agents can use the pre-grounded communication channel to negotiate fairly, but are unable to effectively use the ungrounded channel. However, prosocial agents do learn to use cheap talk to find an optimal negotiating strategy, suggesting that cooperation is necessary for language to emerge. We also study communication behaviour in a setting where one agent interacts with agents in a community with different levels of prosociality and show how agent identifiability can aid negotiation.\nAuthors:\n Edward Choi, Angeliki Lazaridou, Nando de Freitas\nOne of the distinguishing aspects of human language is its compositionality, which allows us to describe complex environments with limited vocabulary. Previously, it has been shown that neural network agents can learn to communicate in a highly structured, possibly compositional language based on disentangled input (e.g. hand-engineered features). Humans, however, do not learn to communicate based on well-summarized features. In this work, we train neural agents to simultaneously develop visual perception from raw image pixels, and learn to communicate with a sequence of discrete symbols. The agents play an image description game where the image contains factors such as colors and shapes. We train the agents using the obverter technique where an agent introspects to generate messages that maximize its own understanding. Through qualitative analysis, visualization and a zero-shot test, we show that the agents can develop, out of raw image pixels, a language with compositional properties, given a proper pressure from the environment.\nAuthors: \nMeire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Matteo Hessel, Ian Osband, Alex Graves, Volodymyr Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, Charles Blundell, Shane Legg\nWe introduce NoisyNet, a deep reinforcement learning agent with parametric noise added to its weights, and show that the induced stochasticity of the agent's policy can be used to aid efficient exploration. The parameters of the noise are learned with gradient descent along with the remaining network weights. NoisyNet is straightforward to implement and adds little computational overhead. We find that replacing the conventional exploration heuristics for A3C, DQN and dueling agents (entropy reward and \u03f5-greedy respectively) with NoisyNet yields substantially higher scores for a wide range of Atari games, in some cases advancing the agent from sub to super-human performance.\n"}
{"title": "Navigating with grid-like representations in artificial agents", "contents": "Most animals, including humans, are able to flexibly navigate the world they live in \u2013 exploring new areas, returning quickly to remembered places, and taking shortcuts. Indeed, these abilities feel so easy and natural that it is not immediately obvious how complex the underlying processes really are. In contrast, spatial navigation remains a substantial challenge for artificial agents whose abilities are far outstripped by those of mammals.\nIn 2005, a potentially crucial part of the neural circuitry underlying spatial behaviour was revealed by an astonishing discovery: neurons that fire in a strikingly regular hexagonal pattern as animals explore their environment. This lattice of points is believed to facilitate spatial navigation, similarly to the gridlines on a map. In addition to equipping animals with an internal coordinate system, these neurons - known as \ngrid cells\n - have recently been hypothesised to support \nvector-based navigation\n. That is: enabling the brain to calculate the distance and direction to a desired destination, \u201c\nas the crow flies\n,\u201d allowing animals to make direct journeys between different places even if that exact route had not been followed before.\nThe group that first discovered grid cells was jointly awarded the \n2014 Nobel Prize in Physiology or Medicine\n for shedding light on how cognitive representations of space might work. But after more than 10 years of theorising since their discovery, the computational functions of grid cells - and whether they support vector-based navigation - has remained largely a mystery.\nIn our \nmost recent paper\n [\nPDF here\n]published in Nature, we developed an artificial agent to test the theory that grid cells support vector-based navigation, in keeping with our \noverarching philosophy\n that algorithms used for AI can meaningfully approximate elements of the brain.\nAs a first step, we trained a recurrent network to perform the task of localising itself in a virtual environment, using predominantly movement-related velocity signals. This ability is commonly used by mammals when moving through unfamiliar places or in situations where it is not easy to spot familiar landmarks (e.g. when navigating in the dark).\nWe found that grid-like representations (hereafter grid units) spontaneously emerged within the network - providing a striking convergence with the neural activity patterns observed in foraging mammals, and consistent with the notion that grid cells provide an efficient code for space.\nWe next sought to test the theory that grid cells support vector-based navigation by creating an artificial agent to be used as an experimental guinea pig. This was done by combining the initial \u201cgrid network\u201d with a larger network architecture, forming an agent that could be trained using deep reinforcement learning to navigate to goals in challenging virtual reality game environments.\nThis agent performed at a super-human level, exceeding the ability of a professional game player, and exhibited the type of flexible navigation normally associated with animals, taking novel routes and shortcuts when they became available.\nThrough a series of experimental manipulations, we showed that grid-like representations were critical for vector-based navigation. For example, when grid cells in the network were silenced, the agent\u2019s ability to navigate was impaired, and the representation of key metrics such as distance and direction to the goal became less accurate.\nWe believe our study constitutes an important step in understanding the fundamental computational purpose of grid cells in the brain and also highlights the benefits they afford to artificial agents. The evidence provides compelling support for the theory that grid cells provide a Euclidean spatial framework - a concept of space - enabling vector-based navigation.\nMore broadly, our work reaffirms the potential of utilising algorithms thought to be used by the brain as \ninspiration for machine learning architectures\n. The extensive previous neuroscience research into grid cells makes the agent's interpretability - which is itself a major topic in AI research - significantly easier, by giving us clues about what to look for when trying to understand its internal representations. The work also showcases the potential of using artificial agents actively engaging in complex behaviours within realistic virtual environments to test theories of how the brain works.\nTaking this principle further, a similar approach could be used to test theories concerning brain areas that are important for perceiving sound or controlling limbs, for example. In the future such networks may well provide a new way for scientists to conduct \u2018experiments\u2019, suggesting new theories and even complementing some of the work that is currently conducted in animals.\nUPDATE 14.05.18: \nWe\u2019d encourage you to read \nThe emergence of grid-like representations by training recurrent neural networks to perform spatial localization\n by Cueva and Wei, which was published contemporaneously at ICLR. While different in scope and findings, it shows interesting results. In brief, the authors found periodic firing that conformed to the shape of the enclosure, e.g rectangular grids in a square environment and triangular in a triangular environment (fig. 2 of Cueva and Wei). This differs from our study, where we found grid-like units whose firing pattern closely resembles rodent grid cells which typically show hexagonal firing patterns across different shaped environments (e.g. square and circular arena).\nRead the Nature paper: [\nPDF\n]\nDownload the original paper (unformatted): [\nPDF\n]\nRead Nobel Prize Laureate Edvard Moser's \nreview of the paper\n.\nThis work was done by Andrea Banino, Caswell Barry, Benigno Uria, Charles Blundell, Timothy Lillicrap, Piotr Mirowski, Alexander Pritzel, Martin Chadwick, Thomas Degris, Joseph Modayil, Greg Wayne, Hubert Soyer, Fabio Viola, Brian Zhang, Ross Goroshin, Neil Rabinowitz, Razvan Pascanu, Charlie Beattie, Stig Petersen, Amir Sadik, Stephen Gaffney, Helen King, Koray Kavukcuoglu, Demis Hassabis, Raia Hadsell, and Dharshan Kumaran.\n"}
{"title": "DeepMind, meet Android", "contents": "We\u2019re delighted to announce a new collaboration between \nDeepMind for Google\n and Android, the world\u2019s most popular mobile operating system. Together, we\u2019ve created two new features that will be available to people with devices running Android P later this year:\nThis is an exciting first for us. Our previous work with Google has been built on massive-scale infrastructure, including projects to \nreduce energy use\n in data centres, optimise recommendations in Google Play, and bring WaveNet voices to the \nGoogle Assistant\n and to \nGoogle Cloud Platform\ncustomers across the world.\nBut this time we\u2019re deploying techniques that run on the compute power of a single mobile device\u2014that\u2019s orders of magnitude less than typical machine learning applications. Here\u2019s how they work:\nAndroid has worked to improve battery life with each release of its operating system. That shouldn\u2019t be surprising, since \nsurveys\n of smartphone users show that battery life is a top priority.\nToday, battery power is spent keeping apps up-to-date in the \nbackground\n so that they\u2019re fresh when users open them next. But nobody uses all the apps on their phone with the same frequency, so in many cases that battery use might not be necessary.\nTo help tackle this, we\u2019ve partnered with the Android team to develop a feature called Adaptive Battery that uses a deep \nconvolutional neural net\n to predict which apps you\u2019ll use in the next few hours and which you probably won\u2019t use until later.\nUsing that knowledge, Android adapts to your usage patterns so that it only spends battery power on the apps you\u2019ll need. The initial results have been very promising, and we\u2019ve seen a significant reduction in background activity in our internal testing.\nSometimes your screen can be frustratingly dim on a bright, sunny day or too bright when you reach over to check your phone in the middle of the night. That\u2019s because the system is one size fits all, and doesn't account for your personal preferences. So you need to make adjustments manually\u2014for example, turning screen brightness down to read in bed at night and back up when you wake up in the morning.\nTo improve this experience, we've partnered with Android to incorporate machine learning into a feature called Adaptive Brightness. The feature now learns how you set the brightness slider for the ambient light of your surroundings, and then adjusts the screen brightness according to your preferences. During our internal testing, a considerable proportion of Android P users made fewer manual brightness adjustments.\nWe\u2019re excited to be working with the amazing Android team to try to save energy while making people\u2019s lives easier, and look forward to more to come. And if you\u2019re interested in working on real-world machine learning challenges, from global-scale infrastructure to on-device optimisation, then the DeepMind for Google team is \nalways looking for exceptional people\n!\n"}
{"title": "Prefrontal cortex as a meta-reinforcement learning system", "contents": "Recently, AI systems have mastered a range of video-games such as Atari classics Breakout and Pong. \u00a0But as impressive as this performance is, AI still relies on the equivalent of thousands of hours of gameplay to reach and surpass the performance of human video game players. In contrast, we can usually grasp the basics of a video game we have never played before in a matter of minutes.\nThe question of why the brain is able to do so much more with so much less has given rise to the theory of meta-learning, or \u2018learning to learn\u2019. It is thought that we learn on two timescales \u2014 in the short term we focus on learning about specific examples while over longer timescales we learn the abstract skills or rules required to complete a task. It is this combination that is thought to help us learn efficiently and apply that knowledge rapidly and flexibly on new tasks. Recreating this meta-learning structure in AI systems \u2014 called meta-reinforcement learning \u2014 has proven very fruitful in facilitating fast, one-shot, learning in our agents (see \nour paper\n and closely related \nwork\n from OpenAI). However, the specific mechanisms that allow this process to take place in the brain are still largely unexplained in neuroscience.\nIn \nour new paper\n in Nature Neuroscience (Download a \nPDF here\n), we use the meta-reinforcement learning framework developed in AI research to investigate the role of dopamine in the brain in helping us to learn. Dopamine\u2014commonly known as the brain\u2019s pleasure signal\u2014has often been thought of as analogous to the reward prediction error signal used in AI reinforcement learning algorithms. These systems learn to act by trial and error guided by the reward. We propose that dopamine\u2019s role goes beyond just using reward to learn the value of past actions and that it plays an integral role, specifically within the prefrontal cortex area, in allowing us to learn efficiently, rapidly and flexibly on new tasks.\nWe tested our theory by virtually recreating six meta-learning experiments from the field of neuroscience\u2014each requiring an agent to perform tasks that use the same underlying principles (or set of skills) but that vary in some dimension. We trained a recurrent neural network (representing the prefrontal cortex) using standard deep reinforcement learning techniques (representing the role of dopamine) and then compared the activity dynamics of the recurrent network with real data taken from previous findings in neuroscience experiments. Recurrent networks are a good proxy for meta-learning because they are able to internalise past actions and observations and then draw on those experiences while training on a variety of tasks.\nOne experiment we recreated is known as the Harlow Experiment, a psychology test from the 1940s used to explore the concept of meta-learning. In the original test, a group of monkeys were shown two unfamiliar objects to select from, only one of which gave them a food reward. They were shown these two objects six times, each time the left-right placement was randomised so the monkey had to learn which object gave a food reward. They were then shown two brand new objects, again only one would result in a food reward. Over the course of this training, the monkey developed a strategy to select the reward associated-object: it learnt to select randomly the first time, and then based on the reward feedback to choose the particular object, rather than the left or right position, from then on. The experiment shows that monkeys could internalise the underlying principles of the task and learn an abstract rule structure \u2014 in effect, learning to learn.\nWhen we simulated a very similar test using a \nvirtual computer screen\n and randomly selected images, we found that our \u2018meta-RL agent\u2019 appeared to learn in a manner analogous to the animals in the Harlow Experiment, even when presented with entirely new images never seen before.\nIn fact, we found that the meta-RL agent could learn to quickly adapt in a wide domain of tasks with different rules and structures. And because the network learned how to adapt to a variety of \u00a0tasks, it also learned general principles about how to learn efficiently.\nImportantly, we saw that the majority of learning took place in the recurrent network, which supports our proposal that dopamine plays a more integral role in the meta-learning process than previously thought. Dopamine is traditionally understood to strengthen synaptic links in the prefrontal system, reinforcing particular behaviours. In AI, this means the dopamine-like reward signal adjusts the artificial synaptic weights in a neural network as it learns the right way to solve a task. However, in our experiments the weights of the neural network were frozen, meaning they couldn\u2019t be adjusted during the learning process, yet, the meta-RL agent was still able to solve and adapt to new tasks. This shows us that dopamine-like reward isn't only used to adjust weights, but it also conveys and encodes important information about abstract task and rule structure, allowing faster adaptation to new tasks.\nNeuroscientists have long observed similar patterns of neural activations in the prefrontal cortex, which is quick to adapt and flexible, but have struggled to find an adequate explanation for why that\u2019s the case. The idea that the prefrontal cortex isn\u2019t relying on slow synaptic weight changes to learn rule structures, but is using abstract model-based information directly encoded in dopamine, offers a more satisfactory reason for its versatility.\nIn demonstrating that the key ingredients thought to give rise to meta-reinforcement learning in AI also exist in the brain, we\u2019ve posed a theory that not only fits with what is known about both dopamine and prefrontal cortex but that also explains a range of mysterious findings from neuroscience and psychology. In particular, the theory sheds new light on how structured, model-based learning emerges in the brain, why dopamine itself contains model-based information, and how neurons in the prefrontal cortex become tuned to learning-related signals. Leveraging insights from AI which can be applied to explain findings in neuroscience and psychology highlights the value each field can offer the other. Going forward, we anticipate that much benefit can be gained in the reverse direction, by taking guidance from specific organisation of brain circuits in designing new models for learning in reinforcement learning agents.\nDownload the Nature Neuroscience paper \nhere\n.\nDownload an Open Access version of the paper \nhere\n.\nThis work was completed by Jane X. Wang, Zeb Kurth-Nelson, Dharshan Kumaran, Dhruva Tirumala, Hubert Soyer, Joel Z. Leibo, Demis Hassabis and Matthew Botvinick.\n"}
{"title": "Neural scene representation and rendering", "contents": "There is more than meets the eye when it comes to how we understand a visual scene: our brains draw on prior knowledge to reason and to make inferences that go far beyond the patterns of light that hit our retinas. For example, when entering a room for the first time, you instantly recognise the items it contains and where they are positioned. If you see three legs of a table, you will infer that there is probably a fourth leg with the same shape and colour hidden from view. Even if you can\u2019t see everything in the room, you\u2019ll likely be able to sketch its layout, or imagine what it looks like from another perspective.\nThese visual and cognitive tasks are seemingly effortless to humans, but they represent a significant challenge to our artificial systems. Today, state-of-the-art visual recognition systems are trained using large datasets of annotated images produced by humans. Acquiring this data is a costly and time-consuming process, requiring individuals to label every aspect of every object in each scene in the dataset. As a result, often only a small subset of a scene\u2019s overall contents is captured, which limits the artificial vision systems trained on that data. As we develop more complex machines that operate in the real world, we want them to fully understand their surroundings: where is the nearest surface to sit on? What material is the sofa made of? Which light source is creating all the shadows? Where is the light switch likely to be?\nIn this work, \npublished in Science\n (\nOpen Access version\n), we introduce the Generative Query Network (GQN), a framework within which machines learn to perceive their surroundings by training only on data obtained by themselves as they move around scenes. Much like infants and animals, the GQN learns by trying to make sense of its observations of the world around it. In doing so, the GQN learns about plausible scenes and their geometrical properties, without any human labelling of the contents of scenes.\nThe GQN model is composed of two parts: a representation network and a generation network. The representation network takes the agent's observations as its input and produces a representation (a vector) which describes the underlying scene. The generation network then predicts (\u2018imagines\u2019) the scene from a previously unobserved viewpoint.\nThe representation network does not know which viewpoints the generation network will be asked to predict, so it must find an efficient way of describing the true layout of the scene as accurately as possible. It does this by capturing the most important elements, such as object positions, colours and the room layout, in a concise distributed representation. During training, the generator learns about typical objects, features, relationships and regularities in the environment. This shared set of \u2018concepts\u2019 enables the representation network to describe the scene in a highly compressed, abstract manner, leaving it to the generation network to fill in the details where necessary. For instance, the representation network will succinctly represent \u2018blue cube\u2019 as a small set of numbers and the generation network will know how that manifests itself as pixels from a particular viewpoint.\nWe performed controlled experiments on the GQN in a collection of procedurally-generated environments in a simulated 3D world, containing multiple objects in random positions, colours, shapes and textures, with randomised light sources and heavy occlusion. After training on these environments, we used GQN\u2019s representation network to form representations of new, previously unobserved scenes. We showed in our experiments that the GQN exhibits several important properties:\nGQN builds upon a large literature of recent related work in multi-view geometry, generative modelling, unsupervised learning and predictive learning, which we discuss \nhere\n, in the \nScience paper\n and the \nOpen Access version\n. It illustrates a novel way to learn compact, grounded representations of physical scenes. Crucially, the proposed approach does not require domain-specific engineering or time-consuming labelling of the contents of scenes, allowing the same model to be applied to a range of different environments. It also learns a powerful neural renderer that is capable of producing accurate images of scenes from new viewpoints.\nOur method still has many limitations when compared to more traditional computer vision techniques, and has currently only been trained to work on synthetic scenes. However, as new sources of data become available and advances are made in our hardware capabilities, we expect to be able to investigate the application of the GQN framework to higher resolution images of real scenes. In future work, it will also be important to explore the application of GQNs to broader aspects of scene understanding, for example by querying across space and time to learn a common sense notion of physics and movement, as well as applications in virtual and augmented reality.\nWhile there is still much more research to be done before our approach is ready to be deployed in practice, we believe this work is a sizeable step towards fully autonomous scene understanding.\nThis work was done by S. M. Ali Eslami, Danilo J. Rezende, Frederic Besse, Fabio Viola, Ari S. Morcos, Marta Garnelo, Avraham Ruderman, Andrei A. Rusu, Ivo Danihelka, Karol Gregor, David P. Reichert, Lars Buesing, Theophane Weber, Oriol Vinyals, Dan Rosenbaum, Neil Rabinowitz, Helen King, Chloe Hillier, Matt Botvinick, Daan Wierstra, Koray Kavukcuoglu and Demis Hassabis.\n"}
{"title": "DeepMind Health Response to Independent Reviewers' Report 2018", "contents": "When we set up DeepMind Health we believed that pioneering technology should be matched with pioneering oversight. That\u2019s why when we launched in February 2016, we did so with an unusual and additional mechanism: a panel of Independent Reviewers, who meet regularly throughout the year to scrutinise our work. This is an innovative approach within tech companies - one that forces us to question not only what we are doing, but how and why we are doing it - and we believe that their robust challenges make us better.\nIn their report last year, the Independent Reviewers asked us important questions about our engagement with stakeholders, data governance, and the behavioural elements that need to be considered when deploying new technologies in clinical environments. We\u2019ve done a lot over the past twelve months to address these questions, and we\u2019re really proud that this year\u2019s Annual Report recognises the progress we\u2019ve made.\nOf course, \nthis year\u2019s report\n includes a series of new recommendations for areas where we can continue to improve, which we\u2019ll be working on in the coming months. In particular:\nWe want to take this opportunity to thank the Independent Reviewers for their thoughtful and committed engagement with our work. By holding us to account, recognising where we\u2019re getting it right and challenging us where we can improve, they\u2019ll help us to do a better job for patients, nurses, doctors, carers, families, and all those who rely on healthcare systems around the world.\n"}
{"title": "Measuring abstract reasoning in neural networks", "contents": "Neural network-based models continue to achieve impressive results on longstanding machine learning problems, but establishing their capacity to reason about abstract concepts has proven difficult. Building on previous efforts to solve this important feature of general-purpose learning systems, our \nlatest paper\n sets out an approach for measuring abstract reasoning in learning machines, and reveals some important insights about the nature of generalisation itself.\nTo understand why abstract reasoning is critical for general intelligence, consider Archimedes\u2019 famous \u201cEureka!\u201d moment: by noticing that the volume of an object is equivalent to the volume of water that the object displaces, he understood volume at a conceptual level, and was therefore able to reason about the volume of other irregularly shaped objects.\nWe would like AI to have similar capabilities. While current systems can defeat world champions in complicated strategic games, they often struggle on other apparently simple tasks, especially when an abstract concept needs to be discovered and reapplied in a new setting. For example, if specifically trained to only count triangles, then even our best AI systems can still fail to count squares, or any other previously unencountered object.\nTo build better, more intelligent systems it is therefore important to understand the ways in which neural networks are currently able to process abstract concepts, and where they still need improvement. To begin doing this, we took inspiration from the methods used to measure abstract reasoning in human IQ tests.\nStandard human IQ tests often require test-takers to interpret perceptually simple visual scenes by applying principles that they have learned through everyday experience. For example, human test-takers may have already learned about \u2018progressions\u2019 (the notion that some attribute can increase) by watching plants or buildings grow, by studying addition in a mathematics class, or by tracking a bank balance as interest accrues. They can then apply this notion in the puzzles to infer that the number of shapes, their sizes, or even the intensity of their colour will increase along a sequence.\nWe do not yet have the means to expose machine learning agents to a similar stream of \u2018everyday experiences\u2019, meaning we cannot easily measure their ability to transfer knowledge from the real world to visual reasoning tests. Nonetheless, we can create an experimental set-up that still puts human visual reasoning tests to good use. Rather than study knowledge transfer from everyday life to visual reasoning problems (as in human testing), we instead studied knowledge transfer from one controlled set of visual reasoning problems to another.\nTo achieve this, we built a generator for creating matrix problems, involving a set of abstract factors, including relations like \u2018progression\u2019 and attributes like \u2018colour\u2019 and \u2018size\u2019. While the question generator uses a small set of underlying factors, it can nonetheless create an enormous number of unique questions.\nNext, we constrained the factors or combinations available to the generator to create different sets of problems for training and testing our models, to measure how well our models can generalise to held-out test sets. For instance, we created a training set of puzzles in which the progression relation is only encountered when applied to the colour of lines, and a test set when it is applied to the size of shapes. If a model performs well on this test set, it would provide evidence for an ability to infer and apply the abstract notion of progression, even in situations in which it had never previously seen a progression.\nIn the typical generalisation regime applied in machine learning evaluations, where training and test data are sampled from the same underlying distribution, all of the networks we tested exhibited good generalisation error, with some achieving impressive absolute performance at just above 75%. The best performing network explicitly computed relations between different image panels and evaluated the suitability of each potential answer in parallel. We call this architecture a Wild Relation Network (WReN).\nWhen required to reason using attribute values \u2018interpolated\u2019 between previously seen attribute values, and also when applying known abstract relations in unfamiliar combinations, the models generalised notably well. However, the same network performed much worse in the \u2018extrapolation\u2019 regime, where attribute values in the test set did not lie within the same range as those seen during training. An example of this occurs for puzzles that contain dark coloured objects during training and light coloured objects during testing. Generalisation performance was also worse when the model was trained to apply a previously seen relation, such as a progression on the number of shapes, to a new attribute, such as size.\nFinally, we observed improved generalisation performance when the model was trained to predict not only the correct answer, but also the \u2018reason\u2019 for the answer (i.e. the particular relations and attributes that should be considered to solve the puzzle). Interestingly, in the neutral split, the model\u2019s accuracy was strongly correlated with its ability to infer the correct relation underlying the matrix: when the explanation was right, the model would choose the correct answer 87% of the time, but when its explanation was wrong this performance dropped to only 32%. This suggests that models which achieved better performance when they correctly inferred the abstract concepts underlying the task.\nRecent literature has focussed on the strengths and weaknesses of neural network-based approaches to machine learning problems, often based around their capacity or failure to generalise. Our results show that it might be unhelpful to draw universal conclusions about generalisation: the neural networks we tested performed well in certain regimes of generalisation and very poorly in others. Their success was determined by a range of factors, including the architecture of the model used and whether the model was trained to provide an interpretable \u201creason\u201d for its answer choices. In almost all cases, the systems performed poorly when required to extrapolate to inputs beyond their experience, or to deal with entirely unfamiliar attributes; creating a clear focus for future work in this critical, and important area of research.\nTo encourage and support further research towards improved generalisation and abstract reasoning, we have made our dataset publicly available \nhere\n.\nDownload the paper [\nPDF\n] and supplementary material [\nPDF\n].\nThis work was done by David G. T. Barrett*, Felix Hill*, Adam Santoro*, Ari Morcos and Timothy Lillicrap.\n\u200d\n"}
{"title": "DeepMind papers at ICML 2018", "contents": "The \n2018 International Conference on Machine Learning\n will take place in Stockholm, Sweden from 10-15 July.\nFor those attending and planning the week ahead, we are sharing a schedule of DeepMind presentations at ICML (\nyou can download a pdf version \nhere\n). We look forward to the many engaging discussions, ideas, and collaborations that are sure to arise from the conference!\nAuthors:\n Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Nouri, Norman Casagrande, Edward Lockhart, Sander Dieleman, Aaron van den Oord, Koray Kavukcuoglu\nSequential models achieve state-of-the-art results in audio, visual and textual domains with respect to both estimating the data distribution and generating desired samples. Efficient sampling for this class of models at the cost of little to no loss in quality has however remained an elusive task. With a focus on text-to-speech synthesis, we show that compact recurrent architectures, a remarkably high degree of weight sparsification and a novel reordering of the variables greatly reduce sampling latency while maintaining high audio fidelity. We first describe a compact single-layer recurrent neural network, the WaveRNN, with a novel dual softmax layer that matches the quality of the state-of-the-art WaveNet model. Persistent GPU kernels for the WaveRNN are able to synthesize 24kHz 16-bit audio 4 times faster than real time. We then apply a weight sparsification technique to the model. We show that, given a constant number of weights, large sparse networks perform better than small dense networks. Using a large Sparse WaveRNN, we demonstrate the feasibility of real-time synthesis of high-fidelity audio on a low-power mobile phone CPU. We use a large Sparse WaveRNN to demonstrate the first instance of real-time synthesis of high-fidelity audio on low-resource mobile phone CPU. Finally, we introduce a novel reordering of the variables in the factorization of the joint distribution. The reordering makes it possible to trade vacuous dependencies on samples from the distant future for the ability to generate in batches. The Batch WaveRNN produces up to 16 samples per step maintaining high quality and enables audio synthesis that is up to 40 times faster than real time.\nPresentations:\nAuthors: \nArthur Guez*, Theophane Weber*, Ioannis Antonoglou, Karen Simonyan, Oriol Vinyals, Daan Wierstra, Remi Munos, David Silver\nPlanning problems are among the most important and well-studied problems in artificial intelligence. They are most typically solved by tree search algorithms that simulate ahead into the future, evaluate future states, and back-up those evaluations to the root of a search tree. Among these algorithms, Monte-Carlo tree search (MCTS) is one of the most general, powerful and widely used. A typical implementation of MCTS uses cleverly designed rules, optimised to the particular characteristics of the domain. These rules control where the simulation traverses, what to evaluate in the states that are reached, and how to back-up those evaluations. In this paper we instead learn where, what and how to search. Our architecture, which we call an MCTSnet, incorporates simulation-based search inside a neural network, by expanding, evaluating and backing-up a vector embedding. The parameters of the network are trained end-to-end using gradient-based optimisation. When applied to small searches in the well-known planning problem Sokoban, the learned search algorithm significantly outperformed MCTS baselines.\nPresentations:\nAuthors:\n Gellert Weisz, Andras Gyorgy, and Csaba Szepesvari\nWe consider the problem of configuring general-purpose solvers to run efficiently on problem instances drawn from an unknown distribution. The goal of the configurator is to find a configuration that runs fast on average on most instances, and do so with the least amount of total work. It can run a chosen solver on a random instance until the solver finishes or a timeout is reached. We propose LEAPSANDBOUNDS, an algorithm that tests configurations on randomly selected problem instances for longer and longer time. We prove that the capped expected runtime of the configuration returned by LEAPSANDBOUNDS is close to the optimal expected runtime, while our algorithm\u2019s running time is near-optimal. Our results show that LEAPSANDBOUNDS is more efficient than the recent algorithm of Kleinberg et al. (2017), which, to our knowledge, is the only other algorithm configuration method that claims to have non-trivial theoretical guarantees. Experimental results on configuring a public SAT solver on a public benchmark also stand witness to the superiority of our method.\nPresentations: \nAuthors:\n Will Dabney*, Georg Ostrovski*, David Silver, Remi Munos\nIn this work, we build on recent advances in distributional reinforcement learning to give a generally applicable, flexible, and state-of-the-art distributional variant of DQN. We achieve this by using quantile regression to approximate the full quantile function for the state-action return distribution. By reparameterizing a distribution over the sample space, this yields an implicitly defined return distribution and gives rise to a large class of risk-sensitive policies. We demonstrate improved performance on the 57 Atari 2600 games in the ALE, and use our algorithms implicitly defined distributions to study the effects of risk-sensitive policies in Atari games.\nPresentations:\nAuthors: \nAlvaro Sanchez, Nicolas Heess, Jost Tobias Springenberg, Josh Merel, Martin Riedmiller, Raia Hadsell, Peter Battaglia\nUnderstanding and interacting with everyday physical scenes requires rich knowledge about the structure of the world, represented either implicitly in a value or policy function, or explicitly in a transition model. Here we introduce a new class of learnable models\u2014based on graph networks\u2014which implement an inductive bias for object- and relation-centric representations of complex, dynamical systems. Our results show that as a forward model, our approach supports accurate predictions, and surprisingly strong and efficient generalization, across eight distinct physical systems which we varied parametrically and structurally. We also found that our inference model can perform system identification from real and simulated data. Our models are also differentiable, and support online planning via gradient based trajectory optimization, as well as offline policy optimization. Our framework offers new opportunities for harnessing and exploiting rich knowledge about the world, and takes a key step toward building machines with more human-like representations of the world.\nPresentations:\nAuthors:\n Mehrdad Farajtabar, Yinlam Chow, and Mohammad Ghavamzadeh\nWe study the problem of off-policy evaluation (OPE) in reinforcement learning (RL), where the goal is to estimate the performance of a policy from the data generated by another policy(ies). In particular, we focus on the doubly robust (DR) estimators that consist of an importance sampling (IS) component and a performance model, and utilize the low (or zero) bias of IS and low variance of the model at the same time. Although the accuracy of the model has a huge impact on the overall performance of DR, most of the work on using the DR estimators in OPE has been focused on improving the IS part, and not much on how to learn the model. In this paper, we propose alternative DR estimators, called more robust doubly robust (MRDR), that learn the model parameter by minimizing the variance of the DR estimator. We first present a formulation for learning the DR model in RL. We then derive formulas for the variance of the DR estimator in both contextual bandits and RL, such that their gradients w.r.t. the model parameters can be estimated from the samples, and propose methods to efficiently minimize the variance. We prove that the MRDR estimators are strongly consistent and asymptotically optimal. Finally, we evaluate MRDR in bandits and RL benchmark problems, and compare its performance with the existing methods.\nPresentations:\nAuthors: \n Marta Garnelo, Dan Rosenbaum, Christopher Maddison, Tiago Ramalho, David Saxton, Murray Shanahan, Yee Whye Teh, Danilo Rezende, S. M. Ali Eslami\nDeep neural networks excel at function approximation, yet they are typically trained from scratch for each new function. On the other hand, Bayesian methods, such as Gaussian Processes (GPs), exploit prior knowledge to quickly infer the shape of a new function at test time. Yet GPs are computationally expensive, and it can be hard to design appropriate priors. In this paper we propose a family of neural models, Conditional Neural Processes (CNPs), that combine the benefits of both. CNPs are inspired by the flexibility of stochastic processes such as GPs, but are structured as neural networks and trained via gradient descent. CNPs make accurate predictions after observing only a handful of training data points, yet scale to complex functions and large datasets. We demonstrate the performance and versatility of the approach on a range of canonical machine learning tasks, including regression, classification\nand image completion.\nPresentations:\nAuthors: \nMarco \u00a0Fraccaro, Danilo \u00a0Jimenez Rezende, Yori \u00a0Zwols, Alexander Pritzel, S. M. Ali Eslami, Fabio Viola\nIn model-based reinforcement learning, generative and temporal models of environments can be leveraged to boost agent performance, either by tuning the agent\u2019s representations during training or via use as part of an explicit planning mechanism. However, their application in practice has been limited to simplistic environments, due to the difficulty of training such models in larger, potentially partially-observed and 3D environments. In this work we introduce a novel action-conditioned generative model of such challenging environments. The model features a non-parametric spatial memory system in which we store learned, disentangled representations of the environment. Low-dimensional spatial updates are computed using a state-space model that makes use of knowledge on the prior dynamics of the moving agent, and high-dimensional visual observations are modelled with a Variational Auto-Encoder. The result is a scalable architecture capable of performing coherent predictions over hundreds of time steps across a range of partially observed 2D and 3D environments.\nPresentations:\nAuthors: \nHyunjik Kim, Andriy Mnih\nWe define and address the problem of unsupervised learning of disentangled representations on data generated from independent factors of variation. We propose FactorVAE, a method that disentangles by encouraging the distribution of rep-resentations to be factorial and hence independent across the dimensions. We show that it improves upon \u03b2-VAE by providing a better trade-off between disentanglement and reconstruction quality and being more robust to the number of training iterations. Moreover, we highlight the problems of a commonly used disentanglement metric and introduce a new metric that does not suffer from them.\nPresentations: \nAuthors: \nMartin Riedmiller, Roland Hafner, Thomas Lampe, Michael Neunert, Jonas Degrave, Tobias Springenberg\nWe propose Scheduled Auxiliary Control (SAC), a new learning paradigm in the context of Reinforcement Learning (RL) . SAC enables learning of complex behaviors \u2013 from scratch \u2013 in the presence of multiple sparse reward signals. To achieve this the agent is equipped with a set of general auxiliary tasks, that it attempts to learn simultaneously via off-policy RL. The key idea behind our method is that active (learned) scheduling and execution of auxiliary policies allows the agent to efficiently explore its environment \u2013 enabling it\nto excel at sparse reward RL. Our experiments in several challenging robotic manipulation settings demonstrate the power of our approach.\nRead more on the DeepMind \nblog\n.\nPresentations:\nAuthors:\n Jack W Rae, Chris Dyer, Peter Dayan, Timothy P Lillicrap\nNeural networks trained with backpropagation often struggle to identify classes that have been observed a small number of times. In applications where most class labels are rare, such as language modelling, this can become a performance bottleneck. One potential remedy is to augment the network with a fast-learning non-parametric model which attends over recent activations. We explore a simplified architecture where we treat a subset of the model parameters as fast memory stores. This can help retain information over longer time intervals than a traditional memory, and does not require additional space or compute. In the case of image classification, we display faster binding of novel classes on an Omniglot image curriculum task. We also show improved performance for word-based language models on news reports (GigaWord), books (Project Gutenberg) and Wikipedia articles (WikiText-103) \u2014 the latter achieving state-of-the-art perplexity.\nPresentations:\nAuthors: \nSuman Ravuri, Shakir Mohamed, Mihaela Rosca, and Oriol Vinyals\nWe propose a method of moments (MoM) algorithm for training large-scale implicit generative models. Moment estimation in this setting encounters two problem: it is often difficult to define the millions of moments needed to learn the model parameters, and it is hard to determine which properties are useful when specifying moments. To address the first issue, we introduce a moment network, and define the moments as the gradient of the network\u2019s output with respect to its parameters and the network\u2019s hidden units. To tackle the second problem, we use asymptotic theory to highlight desiderata for moments \u2013 namely they should minimize the asymptotic variance of estimated model parameters \u2013 and introduce an objective to learn better moments. The sequence of objectives created by this Method of Learned Moments (MoLM) can train high-quality neural image samplers. On CIFAR-10, we demonstrate that MoLM-trained generators achieve significantly higher Inception Scores and lower Frechet Inception Distances than those trained with gradient penalty regularized adversarial objectives. These generators also achieve nearly perfect Multi-Scale Structural Similarity Scores on CelebA, and can create high-quality samples of resolutions up to 128\u00d7128.\nPresentations: \nAuthors:\n David Held, Xinyang Geng, Carlos Florensa, Pieter Abbeel\nReinforcement learning is a powerful technique to train an agent to perform a task. However, an agent that is trained using reinforcement learning is only capable of achieving the single task that is specified via its reward function. Such an approach does not scale well to settings in which an agent needs to perform a diverse set of tasks, such as navigating to varying positions in a room or moving objects to varying locations. Instead, we propose a method that allows an agent to automatically discover the range of tasks that it is capable of performing. We use a generator network to propose tasks for the agent to try to achieve, specified as goal states. The generator network is optimized using adversarial training to produce tasks that are always at the appropriate level of difficulty for the agent. Our method thus automatically produces a curriculum of tasks for the agent to learn. We show that, by using this framework, an agent can efficiently and automatically learn to perform a wide set of tasks without requiring any prior knowledge of its environment. Our method can also learn to achieve tasks with sparse rewards, which traditionally pose significant challenges.\nPresentations: \nAuthors:\n \u00a0Neil C. Rabinowitz, Frank Perbet, H. Francis Song, Chiyuan Zhang, S. M. Ali Eslami, Matthew Botvinick\nTheory of mind (ToM) broadly refers to humans\u2019 ability to represent the mental states of others, including their desires, beliefs, and intentions. We propose to train a machine to build such models too. We design a Theory of Mind neural network \u2013 a ToMnet \u2013 which uses meta-learning to build models of the agents it encounters. The ToMnet learns a strong prior model for agents\u2019 future behaviour, and, using only a small number of behavioural observations, can bootstrap to richer predictions about agents\u2019 characteristics and mental states. We apply the ToMnet to agents behaving in simple gridworld environments, showing that it learns to model random, algorithmic, and deep RL agents from varied populations, and that it passes classic ToM tasks such as the \u201cSally-Anne\u201d test of recognising that others can hold false beliefs about the world.\nPresentations:\nAuthors:\n Ofir Nachum, Yinlam Chow, and Mohammad Ghavamzadeh\nWe study the sparse entropy-regularized reinforcement learning (ERL) problem in which the entropy term is a special form of the Tsallis entropy. The optimal policy of this formulation is sparse, i.e., at each state, it has non-zero probability for only a small number of actions. This addresses the main drawback of the standard Shannon entropy-regularized RL (soft ERL) formulation, in which the optimal policy is {\\em softmax}, and thus, may assign a non-negligible probability mass to non-optimal actions. This problem is aggravated as the number of actions is increased. In this paper, we follow the work of Nachum et al. (2017) in the soft ERL setting, and propose a class of novel path consistency learning (PCL) algorithms, called sparse PCL, for the sparse ERL problem that can work with both on-policy and off-policy data. We first derive a sparse consistency equation that specifies a relationship between the optimal value function and policy of the sparse ERL along any system trajectory. Crucially, a weak form of the converse is also true, and we quantify the sub-optimality of a policy which satisfies sparse consistency, and show that as we increase the number of actions, this sub-optimality is better than that of the soft ERL optimal policy. We then use this result to derive the sparse PCL algorithms. We empirically compare sparse PCL with its soft counterpart, and show its advantage, especially in problems with a large number of actions.\nPresentations:\nAuthors: \nAndre Barreto, Diana Borsa, John Quan, Tom Schaul, David Silver, Matteo Hessel, Daniel Makowitz, Augustin Zidek, Remi Munos\nThe ability to transfer skills across tasks has the potential to scale up reinforcement learning (RL) agents to environments currently out of reach. Recently, a framework based on two ideas, successor features (SFs) and generalised policy improvement (GPI), has been introduced as a principled way of transferring skills. In this paper we investigate the feasibility of combining SF&GPI with the representation power of deep learning. Since in deep RL we are interested in learning all the components of SF&GPI concurrently, the existing inter-dependencies between them can lead to instabilities. In this work we propose a solution for this problem that makes it possible to use SF & GPI online, at scale. In order to empirically verify this claim, we apply the proposed method to a complex 3D environment that requires hundreds of millions of transitions to be solved. We show that the transfer promoted by SF&GPI leads to reasonable policies on unseen tasks almost instantaneously. We also show how to build on the transferred policies to learn policies that are specialised to the new tasks, which can then be added to the agent\u2019s set of skills to be used in the future.\nPresentations: \nAuthors:\n Samuel Ritter, Jane Wang, Sid Jayakumar, Zeb Kurth-Nelson, Charles Blundell, Razvan Pascanu, Matt Botvinick\nMeta-learning agents have demonstrated the ability to rapidly explore and exploit new tasks sampled from the task distribution on which they were trained. However, when these agents encounter situations that they explored in the distant past, they are not able to remember the results of their past exploration. Thus, instead of immediately exploiting previously discovered solutions, they must again explore from scratch. In this work, we argue that the necessity to remember the results of past exploration is ubiquitous in naturalistic environments. We propose a formalism for modeling this kind of recurring environment structure, then develop a meta-learning architecture for solving such environments. This architecture melds the standard LSTM working memory with a differentiable neural episodic memory. We explore the capabilities of this episodic LSTM in four recurrent-state stochastic process environments: 1.) episodic contextual bandits, 2.) compositional contextual bandits, 3.) episodic two-step task, and 4.) contextual water-maze navigation.\nPresentations: \nAuthors:\n Jonathan Uesato, Brendan O'Donoghue, Aaron van den Oord, and Pushmeet Kohli.\nThis paper investigates recently proposed approaches for defending against adversarial examples and evaluating adversarial robustness. The existence of adversarial examples in trained neural networks reflects the fact that expected risk alone does not capture the model\u2019s performance against worst-case inputs. We motivate the use of advKarol Hausmaersarial risk as an objective, although it can not easily be computed exactly. We then frame commonly used attacks and evaluation metrics as defining a tractable surrogate objective to the true adversarial risk. This suggests that models may be obscured to adversaries, by optimizing this surrogate rather than the true adversarial risk. We demonstrate that this is a significant problem in practice by repurposing gradient-free optimization techniques into adversarial attacks, which we use to decrease the accuracy of several recently proposed defenses to near zero. Our hope is that\nour formulations and results will help researchers to develop more powerful defenses.\nPresentations: \nLearning to Coordinate with Coordination Graphs in Repeated Single-Stage Multi-Agent Decision Problems\nAuthors:\n Eugenio Bargiacchi (Vrije Universiteit Brussel), Timothy Verstraeten (Vrije Universiteit Brussel), Diederik Roijers (Vrije Universiteit Brussel / Vrije Universiteit Amsterdam), Ann Now\u00e9 (Vrije Universiteit Brussel), Hado van Hasselt\nLearning to coordinate between multiple agents is an important problem in many reinforcement learning problems. Key to learning to coordinate is exploiting loose couplings, i.e., conditional independences between agents. In this paper we study learning in repeated fully cooperative games, multi-agent multi-armed bandits (MAMABs), in which the expected rewards can be expressed as a coordination graph. We propose multi-agent upper confidence exploration (MAUCE), a new algorithm for MAMABs that exploits loose couplings, which enables us to prove a regret bound that is logarithmic in the number of arm pulls and only linear in the number of agents. We empirically compare MAUCE to sparse cooperative Q-learning, and a state-of-the-art combinatorial bandit approach, and show that it performs much better on a variety of settings, including learning control policies for wind farms.\nPresentations:\nAuthors:\n Ciara Pike-Burke, Shipra Agrawal, Csaba Szepesvari, Steffen Grunewalder\nWe study a variant of the stochastic K-armed bandit problem, which we call \"bandits with delayed, aggregated anonymous feedback\". In this problem, when the player pulls an arm, a reward is generated, however it is not immediately observed. Instead, at the end of each round the player observes only the sum of a number of previously generated rewards which happen to arrive in the given round. The rewards are stochastically delayed and due to the aggregated nature of the observations, the information of which arm led to a particular reward is lost. The question is what is the cost of the information loss due to this delayed, aggregated anonymous feedback? Previous works have studied bandits with stochastic, non-anonymous delays and found that the regret increases only by an additive factor relating to the expected delay. In this paper, we show that this additive regret increase can be maintained in the harder delayed, aggregated anonymous feedback setting when the expected delay (or a bound on it) is known. We provide an algorithm that matches the worst case regret of the non-anonymous problem exactly when the delays are bounded, and up to logarithmic factors or an additive variance term for unbounded delays.\nPresentations:\nBest Paper Runner Up\nAuthors:\n David Balduzzi, S\u00e9bastien Racaniere, James Martens, Jakob Foerster, Karl Tuyls, Thore Graepel\nThe cornerstone underpinning deep learning is the guarantee that gradient descent on an objective converges to local minima. Unfortunately, this guarantee fails in settings, such as generative adversarial nets, where there are multiple interacting losses. The behavior of gradient-based methods in games is not well understood \u2013 and is becoming increasingly important as adversarial and multi-objective architectures proliferate. In this paper, we develop new techniques to understand and control the dynamics in general games. The key result is to decompose the second-order dynamics into two components. The first is related to potential games, which reduce to gradient descent on an implicit function; the second relates to Hamiltonian games, a new class of games that obey a conservation law, akin to conservation laws in classical mechanical systems. The decomposition motivates Symplectic Gradient Adjustment (SGA), a new algorithm for finding stable fixed points in general games. Basic experiments show SGA is competitive with recently proposed algorithms for finding local Nash equilibria in GANs \u2013 whilst at the same time being applicable to \u2013 and having guarantees in \u2013 much more general games.\nPresentations:\nAuthors\n: Danny Karmon (Bar Ilan University), Daniel Zoran, Yoav Goldberg (Bar Ilan University)\nMost works on adversarial examples for deep-learning based image classifiers use noise that, while small, covers the entire image. We explore the case where the noise is allowed to be visible but confined to a small, localized patch of the image, without covering any of the main object(s) in the image. We show that it is possible to generate localized adversarial noises that cover only 2% of the pixels in the image, none of them over the main object, and that are transferable across images and locations, and successfully fool a state-of-the-art Inception v3 model with very high success rates.\nPresentations:\nAuthors:\n Yaroslav Ganin, Tejas Kulkarni, Igor Babuschkin, S.M. Ali Eslami, Oriol Vinyals\nAdvances in deep generative networks have led to impressive results in recent years. Nevertheless, such models can often waste their capacity on the minutiae of datasets, presumably due to weak inductive biases in their decoders. This is where graphics engines may come in handy since they abstract away low-level details and represent images as high-level programs. Current methods that combine deep learning and renderers are limited by hand-crafted likelihood or distance functions, a need for large amounts of supervision, or difficulties in scaling their inference algorithms to richer datasets. To mitigate these issues, we present SPIRAL, an adversarially trained agent that generates a program which is executed by a graphics engine to interpret and sample images. The goal of this agent is to fool a discriminator network that distinguishes between real and rendered data, trained with a distributed reinforcement learning setup without any supervision. A surprising finding is that using the discriminator's output as a reward signal is the key to allow the agent to make meaningful progress at matching the desired output rendering. To the best of our knowledge, this is the first demonstration of an end-to-end, unsupervised and adversarial inverse graphics agent on challenging real world (MNIST, Omniglot, CelebA) and synthetic 3D datasets.\nPresentations: \nAuthors: \nDavid Barrett*, Felix Hill*, Adam Santoro*, Ari Morcos, Tim Lillicrap\nWhether neural networks can learn abstract reasoning or whether they merely rely on superficial statistics is a topic of recent debate. Here, we propose a dataset and challenge designed to probe abstract reasoning, inspired by a well-known human IQ test. To succeed at this challenge, models must cope with various generalisation \u2018regimes\u2019 in which the training and test data differ in clearly defined ways. We show that popular models such as ResNets perform poorly, even when the training and test sets differ only minimally, and we present a novel architecture, with a structure designed to encourage reasoning, that does significantly better. When we vary the way in which the test questions and training data differ, we find that our model is notably proficient at certain forms of generalisation, but notably weak at others. We further show that the model\u2019s ability to generalise improves markedly if it is trained to predict symbolic explanations for its answers. Altogether, we introduce and explore ways to both measure and induce stronger abstract reasoning in neural networks. Our freely-available dataset should motivate further progress in this direction.\nPresentations:\nRead more on the DeepMind \nblog\n.\nAuthors:\n Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, Koray Kavukcuoglu\nIn this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called\nV-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari-57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach.\nRead more on the DeepMind \nblog\n, and see our open-source implementation on \nGitHub\n.\nPresentations:\nAuthors: \nWojtek Czarnecki*, Siddhant Jayakumar*, Max Jaderberg, Leonard Hasenclever, Yee Whye Teh, Nicolas Heess, Simon \u00a0Osindero, Razvan Pascanu\nWe introduce Mix & Match (M&M) \u2013 a training framework designed to facilitate rapid and effective learning in RL agents, especially those that would be too slow or too challenging to train otherwise. The key innovation is a procedure that allows us to automatically form a curriculum over agents. Through such a curriculum we can progressively train more complex agents by, effectively, bootstrapping from solutions found by simpler agents. In contradistinction to typical curriculum learning approaches, we do not gradually modify the tasks or environments presented, but instead use a process to gradually alter how the policy is represented internally. We show the broad applicability of our method by demonstrating significant performance gains in three different experimental setups: (1) We train an agent able to control more than 700 actions in a challenging 3D first-person task; using our method to progress through an action-space curriculum we achieve both faster training and better final performance than one obtains using traditional methods. (2) We further show that M&M can be used successfully to progress through a curriculum of architectural variants defining an agents internal state. (3) Finally, we illustrate how a variant of our method can be used to improve agent performance in a multitask setting.\nPresentations: \nAuthors: \nAaron van den Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, Koray Kavukcuoglu\nThe recently-developed WaveNet architecture is the current state of the art in realistic speech synthesis, consistently rated as more natural sounding for many different languages than any previous system. However, because WaveNet relies on sequential generation of one audio sample at a time, it is poorly suited to today\u2019s massively parallel computers, and therefore hard to deploy in a real-time production setting. This paper introduces Probability Density Distillation, a new method for training a parallel feed-forward network from a trained WaveNet with no significant difference in quality. The resulting system is capable of generating high-fidelity speech samples at more than 20 times faster than real-time, and is deployed online by Google Assistant, including serving multiple English and Japanese voices.\nPresentations:\nRead more on the DeepMind \nblog\n.\nAuthors: \nJonathan Schwarz, Jelena Luketina, Wojciech M. Czarnecki, Agnieszka Grabska-Barwinska, Yee Whye Teh, Razvan Pascanu* Raia Hadsell*\nWe introduce a conceptually simple and scalable framework for continual learning domains where tasks are learned sequentially. Our method is constant in the number of parameters and is designed to preserve performance on previously encountered tasks while accelerating learning progress on subsequent problems. This is achieved through\ntraining two neural networks: A knowledge base, capable of solving previously encountered problems, which is connected to an active column that is employed to efficiently learn the current task. After learning a new task, the active column is distilled into the knowledge base, taking care to protect any previously learnt tasks. This cycle of active learning (progression) followed by consolidation (compression) requires no architecture growth, no access to or storing of previous data or tasks, and no task-specific parameters. Thus, it is a learning process that may be sustained over a lifetime of tasks while supporting forward transfer and minimising forgetting. We demonstrate the progress & compress approach on sequential classification of handwritten alphabets as well as two reinforcement learning domains: Atari games and 3D maze navigation.\nPresentations:\nAuthors: \nGeorg Ostrovski*, Will Dabney*, Remi Munos\nWe introduce autoregressive implicit quantile networks (AIQN), a fundamentally different approach to generative modeling than those commonly used, that implicitly captures the distribution using quantile regression. AIQN is able to achieve superior perceptual quality and improvements in evaluation metrics, without incurring a loss of sample diversity. The method can be applied to many existing models and architectures. In this work we extend the PixelCNN model with AIQN and demonstrate results on CIFAR-10 and ImageNet using Inception scores, FID, non-cherry-picked samples, and inpainting results. We consistently observe that AIQN yields a highly stable algorithm that improves perceptual quality while maintaining a highly diverse distribution.\nPresentations:\nAuthors:\n Brendan O'Donoghue, Ian Osband, Remi Munos, Volodymyr Mnih\nWe consider the exploration/exploitation problem in reinforcement learning. For exploitation, it is well known that the Bellman equation connects the value at any time-step to the expected value at subsequent time-steps. In this paper we consider a similar \\textit{uncertainty} Bellman equation (UBE), which connects the uncertainty at any time-step to the expected uncertainties at subsequent time-steps, thereby extending the potential exploratory benefit of a policy beyond individual time-steps. We prove that the unique fixed point of the UBE yields an upper bound on the variance of the posterior distribution of the Q-values induced by any policy. This bound can be much tighter than traditional count-based bonuses that compound standard deviation rather than variance. Importantly, and unlike several existing approaches to optimism, this method scales naturally to large systems with complex generalization. Substituting our UBE-exploration strategy for \u03f5-greedy improves DQN performance on 51 out of 57 games in the Atari suite.\nPresentations:\nAuthors: \nYao Ma (Boston University), Alexander Olshevsky (Boston University), Csaba Szepesvari, Venkatesh Saligrama (Boston University)\nWe consider estimation of worker skills from worker-task interaction data (with unknown labels) for the single-coin crowd-sourcing binary classification model in symmetric noise. We define the (worker) interaction graph whose nodes are workers and an edge between two nodes indicates whether or not the two workers participated in a common task. We show that skills are asymptotically identifiable if and only if an appropriate limiting version of the interaction graph is irreducible and has odd-cycles. We then formulate a weighted rank-one optimization problem to estimate skills based on observations on an irreducible, aperiodic interaction graph. We propose a gradient descent scheme and show that for such interaction graphs estimates converge asymptotically to the global minimum. We characterize noise robustness of the gradient scheme in terms of spectral properties of signless Laplacians of the interaction graph. We then demonstrate that a plug-in estimator based on the estimated skills achieves state-of-art performance on a number of real-world datasets. Our results have implications for rank-one matrix completion problem in that gradient descent can provably recover W\u00d7W rank-one matrices based on W+1 off-diagonal observations of a connected graph with a single odd-cycle.\nPresentations:\n"}
{"title": "Preserving Outputs Precisely while Adaptively Rescaling Targets", "contents": "Multi-task learning - allowing a single agent to learn how to solve many different tasks - is a longstanding objective for artificial intelligence research. Recently, there has been a lot of excellent progress, with agents like \nDQN\n able to use the same algorithm to learn to play multiple games including Breakout and Pong. These algorithms were used to train individual expert agents for each task. As artificial intelligence research advances to more complex real world domains, building a single general agent - as opposed to multiple expert agents - to learn to perform multiple tasks will be crucial. However, so far, this has proven to be a significant challenge. \nOne reason is that there are often differences in the reward scales our reinforcement learning agents use to judge success, leading them to focus on tasks where the reward is arbitrarily higher. For example, in the Atari game Pong, the agent receives a reward of either -1, 0, or +1 per step. In contrast, an agent playing Ms. Pac-Man can obtain hundreds or thousands of points in a single step. Even if the size of individual rewards is comparable, the frequency of rewards can change over time as the agent gets better. This means agents tend to focus on those tasks which have large scores, leading to better performance on certain tasks, and far worse on others.\nTo resolve these kinds of issues, we developed \nPopArt\n, a technique that can adapt the scale of scores in each game so the agent judges the games to be of equal learning value, no matter the scale of rewards available in each specific game. We applied a PopArt normalisation to a state-of-the-art reinforcement learning agent, resulting in a single agent that can play a whole set of 57 diverse Atari video games, with above-human median performance across the set.\nBroadly speaking, deep learning relies on the weights of a neural network being updated so that its output moves closer to the desired target output. \u00a0This also applies when neural networks are used in the context of deep reinforcement learning. PopArt works by estimating the mean and the spread of these targets (such as the score in a game). It then uses these statistics to normalise the targets before they are used to update the network\u2019s weights. Using normalised targets makes learning more stable and robust to changes in scale and shift. To obtain accurate estimates - of expected future scores for example - the outputs of the network can then be rescaled back to the true target range by inverting the normalisation process. If done naively, each update to the statistics would change all unnormalised outputs, including those that were already very good. We prevent this from happening by updating the network in the opposite direction whenever we update the statistics, this can be done exactly. This means we get the benefit of well-scaled updates, while keeping the previously learnt outputs intact. It is for these reasons that we call our method PopArt: it works by Preserving Outputs Precisely while Adaptively Rescaling Targets.\nTraditionally, researchers have overcome the problem of varying reward scales by using reward clipping in their reinforcement learning algorithms. This clips big and small scores at 1 or -1, roughly normalising the expected rewards. Although this makes learning easier, it also changes the goal of the agent. For instance, in Ms. Pac-Man the goal is to collect pellets, each of which is worth 10 points each, and eat ghosts worth between 200 and 1600 points. With clipped rewards, there is no apparent difference for the agent between eating a pellet or eating a ghost and results \u00a0in agents that only eat pellets, and never bothers to chase ghosts, as \nthis video\n shows. \u00a0When we remove reward clipping and use PopArt\u2019s adaptive normalisation to stabilise learning, it results in quite different behaviour, with the agent chasing ghosts, and achieving a higher score, as shown in \nthis video\n.\nWe applied PopArt to the \nImportance-weighted Actor-Learner Architecture\n (IMPALA), one of the most popular deep reinforcement learning agents used at DeepMind. In our experiments, PopArt greatly improved the performance of the agent compared to the baseline agent without PopArt. Both with clipped and unclipped rewards, the median score of the PopArt agent across games was above the human median. This is much higher than the baseline with clipped rewards, while the baseline with unclipped rewards fails to reach meaningful performance at all because it cannot effectively deal with the large variation in reward scales across games.\nThis is the first time we\u2019ve seen superhuman performance on this kind of multi-task environment using a single agent, suggesting PopArt could provide some answers to the open research question of how to balance varied objectives without manually clipping or scaling them. Its ability to adapt the normalisation automatically while learning may become important as we apply AI to more complex multi-modal domains where an agent must learn to trade-off a number of different objectives with varying rewards.\nFor more details, please see our latest\n paper\n and the original \npaper\n. \nThis work was done by Matteo Hessel, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki, Simon Schmitt, and Hado van Hasselt. \n"}
{"title": "Objects that Sound", "contents": "Visual and audio events tend to occur together: a musician plucking guitar strings and the resulting melody; a wine glass shattering and the accompanying crash; the roar of a motorcycle as it accelerates. These visual and audio stimuli are concurrent because they share a common cause. Understanding the relationship between visual events and their associated sounds is a fundamental way that we make sense of the world around us.\nIn \nLook, Listen, and Learn\n and \nObjects that Sound\n (to appear at \nECCV 2018\n), we explore this observation by asking: what can be learnt by looking at and listening to a large number of unlabelled videos? By constructing an audio-visual correspondence learning task that enables visual and audio networks to be jointly trained from scratch, we demonstrate that:\nLearning from multiple modalities is not new; historically, researchers have largely focused on image-text or audio-vision pairings. However, a common approach has been to train a \u201cstudent\u201d network in one modality using the automatic supervision provided by a \u201cteacher\u201d network in the other modality (\u201c\nteacher-student supervision\n\u201d), where the \u201cteacher\u201d has been trained using a large number of human annotations.\nFor instance, a vision network trained on ImageNet can be used to annotate frames of a YouTube video as \u201cacoustic guitar\u201d, which provides training data to the \u201cstudent\u201d audio network for learning what an \u201cacoustic guitar\u201d sounds like. In contrast, we train both visual and audio networks from scratch, where the concept of the \u201cacoustic guitar\u201d naturally emerges in both modalities. Somewhat surprisingly, this approach achieves superior audio classification compared to teacher-student supervision. As described below, this also equips us to localise the object making the sound, which was not possible with previous approaches.\nOur core idea is to use a valuable source of information contained in the video itself: the correspondence between visual and audio streams available by virtue of them appearing together at the same time in the same video. By seeing and hearing many examples of a person playing a violin and examples of a dog barking, and rarely or never seeing a violin being played while hearing a dog bark and vice versa, it should be possible to conclude what a violin and a dog look and sound like. This approach is, in part, motivated by the way an infant might learn about the world as their visual and audio capabilities develop.\nWe apply learning by audio-visual correspondence (AVC), a simple binary classification task: given an example video frame and a short audio clip, decide whether they correspond to each other or not.\nThe only way for a system to solve this task is by learning to detect various semantic concepts in both the visual and the audio domain. To tackle the AVC task, we propose the following network architecture\nThe image and the audio subnetworks extract visual and audio \nembeddings\n and the correspondence score is computed as a function of the distance between the two embeddings. If the embeddings are similar, the (image, audio) are deemed to correspond.\nWe show that the networks learn useful semantic representations, as, for example, our audio network sets the new state-of-the-art on two sound classification benchmarks. Since the correspondence score is computed purely based on the distance, the two embeddings are forced to be aligned (i.e. the vectors live in the same space, and so can be compared meaningfully), thus facilitating cross-modal retrieval:\nThe AVE-Net recognises semantic concepts in the audio and visual domains, but it cannot answer the question, \u201cWhere is the object that is making the sound?\u201d We again make use of the AVC task and show that it is possible to learn to localise sounding objects, while still not using any labels whatsoever.\nTo localise a sound in the image, we compute the correspondence scores between the audio embedding and a grid of region-level image descriptors. The network is trained with multiple instance learning \u2013 the image-level correspondence score is computed as the maximum of the correspondence score map:\nFor corresponding (image, audio) pairs, the method encourages at least one region to respond highly and therefore localise the object. In the below video (left - input frame, right - localisation output, middle - overlay), frames are processed completely independently \u2013 motion information is not used, and there is no temporal smoothing:\nFor mismatched pairs the maximal score should be low, thus making the entire score map dark, indicating, as desired, there is no object which makes the input sound:\nThe unsupervised audio-visual correspondence task enables, with appropriate network design, two entirely new functionalities to be learnt: cross-modal retrieval, and semantic-based localisation of objects that sound. Furthermore, it facilitates learning of powerful features, setting the new state-of-the-art on two sound classification benchmarks.\nThese techniques may prove useful in reinforcement learning, enabling agents to make use of large amounts of unlabelled sensory information. Our work may also have implications for other multimodal problems beyond audio-visual tasks in the future.\nRead the full papers:\nLook, Listen, and Learn\nObjects that Sound\nThis work was done by Relja Arandjelovi\u0107 and Andrew Zisserman. Graphics by Adam Cain and Damien Boudot.\n"}
{"title": "A major milestone for the treatment of eye disease", "contents": "We are delighted to announce the results of the first phase of our joint research partnership with \nMoorfields Eye Hospital\n, which could potentially transform the management of sight-threatening eye disease.\nThe results, published online in \nNature Medicine\n (open access full text, see end of blog), show that our AI system can quickly interpret eye scans from routine clinical practice with unprecedented accuracy. It can correctly recommend how patients should be referred for treatment for over 50 sight-threatening eye diseases as accurately as world-leading expert doctors.\nThese are early results, but they show that our system could handle the wide variety of patients found in routine clinical practice. In the long term, we hope this will help doctors quickly prioritise patients who need urgent treatment \u2013 which could ultimately save sight.\nCurrently, eyecare professionals use optical coherence tomography (OCT) scans to help diagnose eye conditions. These 3D images provide a detailed map of the back of the eye, but they are hard to read and need expert analysis to interpret.\nThe time it takes to analyse these scans, combined with the sheer number of scans that healthcare professionals have to go through (over 1,000 a day at Moorfields alone), can lead to lengthy delays between scan and treatment \u2013 even when someone needs urgent care. If they develop a sudden problem, such as a bleed at the back of the eye, these delays could even cost patients their sight.\nThe system we have developed seeks to address this challenge. Not only can it automatically detect the features of eye diseases in seconds, but it can also prioritise patients most in need of urgent care by recommending whether they should be referred for treatment. This instant triaging process should drastically cut down the time elapsed between the scan and treatment, helping sufferers of diabetic eye disease and age-related macular degeneration avoid sight loss.\nWe don\u2019t just want this to be an academically interesting result \u2013 we want it to be used in real treatment. So our paper also takes on one of the key barriers for AI in clinical practice: the \u201cblack box\u201d problem. For most AI systems, it\u2019s very hard to understand exactly why they make a recommendation. That\u2019s a huge issue for clinicians and patients who need to understand the system\u2019s reasoning, not just its output \u2013 the why as well as the what.\nOur system takes a novel approach to this problem, combining two different neural networks with an easily interpretable representation between them. The first neural network, known as the segmentation network, analyses the OCT scan to provide a map of the different types of eye tissue and the features of disease it sees, such as haemorrhages, lesions, irregular fluid or other symptoms of eye disease. This map allows eyecare professionals to gain insight into the system\u2019s \u201cthinking.\u201d The second network, known as the classification network, analyses this map to present clinicians with diagnoses and a referral recommendation. Crucially, the network expresses this recommendation as a percentage, allowing clinicians to assess the system\u2019s confidence in its analysis.\nThis functionality is critically important, since eyecare professionals are always going to play a key role in deciding the type of care and treatment a patient receives. Enabling them to scrutinise the technology\u2019s recommendations is key to making the system usable in practice.\nOn top of this, our technology can be easily applied to different types of eye scanners, and not just the specific type of device it was trained on at Moorfields. This might seem inconsequential, but it means that the technology could be applied across the world with relative ease, massively increasing the number of patients who could potentially benefit. This also ensures the system can still be used in hospitals and other clinical settings even as OCT scanners are upgraded or replaced over time.\nWhile we\u2019re incredibly proud of this progress, this \ninitial research\n [PDF] would need to be turned into a product and then undergo rigorous clinical trials and regulatory approval before being used in practice. But we\u2019re confident that, in time, this system could transform the diagnosis, treatment and management of eye disease.\nOur partners at Moorfields want our research to help them improve care, reduce some of the strain on clinicians, and lower costs - all at the same time. So we\u2019ve also worked hard on what comes next.\nIf this technology is validated for general use by clinical trials, Moorfields\u2019 clinicians will be able to use it for free across all 30 of their UK hospitals and community clinics, for an initial period of five years. These clinics serve 300,000 patients a year and receive over 1,000 OCT scan referrals every day \u2013 each of which could benefit from improved accuracy and speed of diagnosis.\nWe\u2019re also proud that the work we\u2019ve put into this project will help accelerate many other NHS research efforts. The original dataset held by Moorfields was suitable for clinical use, but not for machine learning research. So we\u2019ve invested significantly in cleaning up, curating and labelling the dataset to create one of the best AI-ready databases for eye research in the world.\nThis improved database is owned by Moorfields as a non-commercial public asset, and it\u2019s already been used by hospital researchers for nine separate studies into a wide range of conditions - with many more to come. Moorfields can also use DeepMind\u2019s trained AI model for their future non-commercial research efforts.\nFor all of us who have worked on this since we signed our agreement with Moorfields in 2016, this is a hugely exciting milestone, and another indication of what is possible when clinicians and technologists work together. We\u2019ll continue to keep you updated as we make progress.\nRead the full text open access paper on Nature Medicine \nhere\nDownload the Author's version \nhere\n [PDF]\n"}
{"title": "Safety-first AI for autonomous data centre cooling and industrial control", "contents": "Many of society\u2019s most pressing problems have grown increasingly complex, so the search for solutions can feel overwhelming. At DeepMind and Google, we believe that if we can use AI as a tool to discover new knowledge, solutions will be easier to reach.\nIn 2016, we jointly developed an \nAI-powered recommendation system\n to improve the energy efficiency of Google\u2019s already highly-optimised data centres. Our thinking was simple: even minor improvements would provide significant energy savings and reduce CO2 emissions to help combat climate change.\nNow we\u2019re taking this system to the next level: instead of human-implemented recommendations, our AI system is directly controlling data centre cooling, while remaining under the expert supervision of our data centre operators. This first-of-its-kind cloud-based control system is now safely delivering energy savings in multiple Google data centres.\nEvery five minutes, our cloud-based AI pulls a snapshot of the data centre cooling system from thousands of sensors and feeds it into our deep neural networks, which predict how different combinations of potential actions will affect future energy consumption. The AI system then identifies which actions will minimise the energy consumption while satisfying a robust set of safety constraints. Those actions are sent back to the data centre, where the actions are verified by the local control system and then implemented.\nThe idea evolved out of feedback from our data centre operators who had been using our AI recommendation system. They told us that although the system had taught them some new best practices\u2014such as spreading the cooling load across more, rather than less, equipment\u2014implementing the recommendations required too much operator effort and supervision. Naturally, they wanted to know whether we could achieve similar energy savings without manual implementation.\nWe\u2019re pleased to say the answer was yes!\nGoogle's data centres contain thousands of servers that power popular services including Google Search, Gmail and YouTube. Ensuring that they run reliably and efficiently is mission-critical. We've designed our AI agents and the underlying control infrastructure from the ground up with safety and reliability in mind, and use eight different mechanisms to ensure the system will behave as intended at all times.\nOne simple method we\u2019ve implemented is to estimate uncertainty. For every potential action\u2014and there are billions\u2014our AI agent calculates its confidence that this is a good action. Actions with low confidence are eliminated from consideration.\nAnother method is two-layer verification. Optimal actions computed by the AI are vetted against an internal list of safety constraints defined by our data centre operators. Once the instructions are sent from the cloud to the physical data centre, the local control system verifies the instructions against its own set of constraints. This redundant check ensures that the system remains within local constraints and operators retain full control of the operating boundaries.\nMost importantly, our data centre operators are always in control and can choose to exit AI control mode at any time. In these scenarios, the control system will transfer seamlessly from AI control to the on-site rules and heuristics that define the automation industry today.\nFind out about the other safety mechanisms we\u2019ve developed, below:\nWhereas our original recommendation system had operators vetting and implementing actions, our new AI control system directly implements the actions. We\u2019ve purposefully constrained the system\u2019s optimisation boundaries to a narrower operating regime to prioritise safety and reliability, meaning there is a risk/reward trade off in terms of energy reductions.\nDespite being in place for only a matter of months, the system is already delivering consistent energy savings of around 30 percent on average, with further expected improvements. That\u2019s because these systems get better over time with more data, as the graph below demonstrates. Our optimisation boundaries will also be expanded as the technology matures, for even greater reductions.\nOur direct AI control system is finding yet more novel ways to manage cooling that have surprised even the data centre operators. Dan Fuenffinger, one of Google\u2019s data centre operators who has worked extensively alongside the system, remarked: \"It was amazing to see the AI learn to take advantage of winter conditions and produce colder than normal water, which reduces the energy required for cooling within the data centre. Rules don\u2019t get better over time, but AI does.\"\nWe\u2019re excited that our direct AI control system is operating safely and dependably, while consistently delivering energy savings. However, data centres are just the beginning. In the long term, we think there's potential to apply this technology in other industrial settings, and help tackle climate change on an even grander scale.\n"}
{"title": "DeepMind and Blizzard open StarCraft II as an AI research environment", "contents": "DeepMind's scientific mission is to push the boundaries of AI by developing systems that can learn to solve complex problems. To do this, we design agents and test their ability in a wide range of environments from the purpose-built \nDeepMind Lab\n to established games, such as \nAtari\n and \nGo\n.\nTesting our agents in games that are not specifically designed for AI research, and where humans play well, is crucial to benchmark agent performance. That is why we, along with our \npartner Blizzard Entertainment\n, are excited to announce the release of SC2LE, a set of tools that we hope will accelerate AI research in the real-time strategy game StarCraft II. The SC2LE release includes:\nStarCraft and StarCraft II are among the biggest and most successful games of all time, with players competing in tournaments for more than 20 years. The original game is also already used by AI and ML researchers, who compete annually in the \nAIIDE bot competition\n. Part of StarCraft\u2019s longevity is down to the rich, multi-layered gameplay, which also makes it an ideal environment for AI research.\nFor example, while the objective of the game is to beat the opponent, the player must also carry out and balance a number of sub-goals, such as gathering resources or building structures. In addition, a game can take from a few minutes to one hour to complete, meaning actions taken early in the game may not pay-off for a long time. Finally, the map is only partially observed, meaning agents must use a combination of memory and planning to succeed.\nThe game also has other qualities that appeal to researchers, such as the large pool of avid players that compete online every day. \u00a0This ensures that there is a large quantity of replay data to learn from - as well as a large quantity of extremely talented opponents for AI agents.\nEven StarCraft\u2019s action space presents a challenge with a choice of more than 300 basic actions that can be taken. Contrast this with Atari games, which only have about 10 (e.g. up, down, left, right etc). On top of this, actions in StarCraft are hierarchical, can be modified and augmented, with many of them requiring a point on the screen. Even assuming a small screen size of 84x84 there are roughly 100 million possible actions available.\nThis release means researchers can now tackle some of these challenges using Blizzard\u2019s own tools to build their own tasks and models.\nOur \nPySC2\n environment wrapper helps by offering a flexible and easy-to-use interface for RL agents to play the game. In this initial release, we break the game down into \u201cfeature layers\u201d, where elements of the game such as unit type, health and map visibility are isolated from each other, whilst preserving the core visual and spatial elements of the game.\nThe release also contains a series of \u2018mini-games\u2019 - an established technique for breaking down the game into manageable chunks that can be used to test agents on \nspecific\ntasks\n, such as moving the camera, \ncollecting mineral shards\n or selecting units. \u00a0We hope that researchers can test their techniques on these as well as propose new mini-games for other researchers to compete and evaluate on.\nOur initial investigations show that our agents perform well on these mini-games. But when it comes to the full game, even strong baseline agents, such as \nA3C\n, cannot win a single game against even the easiest built-in AI. For instance, the following video shows an early-stage training agent (left) which fails to keep its workers mining, a task that humans find trivial. After training (right), the agents perform more meaningful actions, but if they are to be competitive, we will need further breakthroughs in deep RL and related areas.\nOne technique that we know allows our agents to learn stronger policies is imitation learning. This kind of training will soon be far easier thanks to Blizzard, which has committed to ongoing releases of hundreds of thousands of anonymised replays gathered from the StarCraft II ladder. These will not only allow researchers to train supervised agents to play the game, but also opens up other interesting areas of research such as sequence prediction and long-term memory.\nOur hope is that the release of these new tools will build on the work that the AI community has already done in StarCraft, encouraging more DeepRL research and making it easier for researchers to focus on the frontiers of our field.\nWe look forward to seeing what the community discovers.\nRead more on the \nBlizzard blog\n.\nPySC2 is available from \nDeepMind\u2019s github page\n.\nBlizzard\u2019s StarCraft API is available \nhere\n, with details on how to get the linux version, replays and other elements.\nIf you use our environment in your research, please cite \nthe release paper.\n"}
{"title": "The hippocampus as a predictive map", "contents": "Think about how you choose a route to work, where to move house, or even which move to make in a game like Go. All of these scenarios require you to estimate the likely future reward of your decision. This is tricky because the number of possible scenarios explodes as one peers farther and farther into the future. Understanding how we do this is a major research question in neuroscience, while building systems that can effectively predict rewards is a major focus in AI research.\nIn our new paper, \nin Nature Neuroscience\n, we apply a neuroscience lens to a longstanding mathematical theory from machine learning to provide new insights into the nature of learning and memory. Specifically, we propose that the area of the brain known as the hippocampus offers a unique solution to this problem by compactly summarising future events using what we call a \u201cpredictive map.\u201d\nThe hippocampus has traditionally been thought to only represent an animal\u2019s current state, particularly in spatial tasks, such as navigating a maze. This view gained significant traction with the \ndiscovery of \u201cplace cells\u201d in the rodent hippocampus\n, which fire selectively when the animal is in specific locations. While this theory accounts for many neurophysiological findings, it does not fully explain why the hippocampus is also involved in other functions, such as memory, relational reasoning, and decision making.\nOur new theory thinks about navigation as part of the more general problem of computing plans that maximise future reward. Our insights were derived from reinforcement learning, the subdiscipline of AI research that focuses on systems that learn by trial and error. The key computational idea we drew on is that to estimate future reward, an agent must first estimate how much immediate reward it expects to receive in each state, and then weight this expected reward by how often it expects to visit that state in the future. By summing up this weighted reward across all possible states, the agent obtains an estimate of future reward.\nSimilarly, we argue that the hippocampus represents every situation - or state - in terms of the future states which it predicts. For example, if you are leaving work (your current state) your hippocampus might represent this by predicting that you will likely soon be on your commute, picking up your kids from school or, more distantly, at home. By representing each current state in terms of its anticipated successor states, the hippocampus conveys a compact summary of future events, known formally as the \u201c\nsuccessor representation\n\u201d. We suggest that this specific form of predictive map allows the brain to adapt rapidly in environments with changing rewards, but without having to run expensive simulations of the future.\nThis approach combines the strengths of two algorithms that are already well known in reinforcement learning and are also believed to exist in humans and rodents. \u201cModel-based\u201d algorithms learn models of the environment that can then be simulated to produce estimates of future reward, while \u201cmodel-free\u201d algorithms learn future reward estimates directly from experience in the environment. Model-based algorithms are flexible but computationally expensive, while model-free algorithms are computationally cheap but inflexible.\nThe algorithm that inspired our theory combines some of the flexibility of model-based algorithms with the efficiency of model-free algorithms. Because the calculation is a simple weighted sum, it is computationally efficient, much like a model-free algorithm. At the same time, by separating reward expectations and state expectations (the predictive map), it can rapidly adapt to changes in reward by simply updating the reward expectations while leaving the state expectations intact (\nsee our recent paper for further detail\n).\nIn future work, we plan to test the theory further. Since the predictive map theory can be translated into a \nneural network architecture\n, we want to explore the extent to which this learning strategy can promote flexible, rapid planning \nin silico\n.\nMore generally, a major future task will be to look at how the brain integrates different types of learning. While we posed this model as an alternative to model-based and model-free learning in the brain, \na more realistic view\n is that many types of learning are simultaneously coordinated by the brain during learning and planning. Understanding how these learning algorithms are combined is an important step towards understanding human and animal brains, and could provide key insights for designing equally complex, multifaceted AI.\nRead paper: \nThe hippocampus as a predictive map\n"}
{"title": "Why we launched DeepMind Ethics & Society", "contents": "At DeepMind, we\u2019re proud of the role we\u2019ve played in pushing forward the science of AI, and our track record of exciting breakthroughs and major publications. We believe AI can be of extraordinary benefit to the world, but only if held to the highest ethical standards. Technology is not value neutral, and technologists must take responsibility for the ethical and social impact of their work. \u00a0\nAs history attests, technological innovation in itself is no guarantee of broader social progress. The development of AI creates important and complex questions. Its impact on society\u2014and on all our lives\u2014is not something that should be left to chance. Beneficial outcomes and protections against harms must be actively fought for and built-in from the beginning. But in a field as complex as AI, this is easier said than done.\nAs scientists developing AI technologies, we have a responsibility to conduct and support open research and investigation into the wider implications of our work. At DeepMind, we start from the premise that all AI applications should remain under meaningful human control, and be used for socially beneficial purposes. Understanding what this means in practice requires rigorous scientific inquiry into the most sensitive challenges we face.\nSo today we\u2019re launching a new research unit, DeepMind Ethics & Society, to complement our work in AI science and application. This new unit will help us explore and understand the real-world impacts of AI. It has a dual aim: to help technologists put ethics into practice, and to help society anticipate and direct the impact of AI so that it works for the benefit of all. \nOf course, we\u2019re far from alone in thinking about these topics. The ethical and social impact of AI is a thriving field of study, home to groundbreaking work from Julia Angwin\u2019s \nstudy of racism in criminal justice algorithms\n, to Kate Crawford and Ryan Calo's \nexamination of the broader consequences\n of AI for social systems, and many others besides. That\u2019s why we plan to conduct interdisciplinary research that brings together experts from the humanities, social sciences and beyond, along with voices from civil society and technical insights from our team at DeepMind to conduct and fund interdisciplinary research.\nWe\u2019re grateful that this effort will benefit from the advice and guidance of our DeepMind Ethics & Society \nFellows\n, a respected group of independent thinkers. These Fellows are important not only for the expertise that they bring but for the diversity of thought they represent.\nTo guarantee the rigor, transparency and social accountability of our work, we've developed a set of \nprinciples\n together with our Fellows, other academics and civil society. We welcome feedback on these and on the \nkey ethical challenges\n we have identified. Please \nget in touch\n if you have any thoughts, ideas or contributions.\nIf AI technologies are to serve society, they must be shaped by society\u2019s priorities and concerns. This isn\u2019t a quest for closed solutions but rather an attempt to scrutinise and help design collective responses to the future impacts of AI technologies. With the creation of DeepMind Ethics & Society, we hope to challenge assumptions\u2014including our own\u2014and pave the way for truly beneficial and responsible AI.\n"}
{"title": "WaveNet launches in the Google Assistant", "contents": "Just over a year ago we presented \nWaveNet\n, a new deep neural network for generating raw audio waveforms that is capable of producing better and more realistic-sounding speech than existing techniques. At that time, the model was a research prototype and was too computationally intensive to work in consumer products. \u00a0\nBut over the last 12 months we have worked hard to significantly improve both the speed and quality of our model and today we are proud to announce that an updated version of WaveNet is being used to generate the \nGoogle Assistant\n voices for US English and Japanese across all platforms.\nUsing the new WaveNet model results in a range of more natural sounding voices for the Assistant.\nTo understand why WaveNet improves on the current state of the art, it is useful to understand how text-to-speech (TTS) - or \nspeech synthesis\n - systems work today.\nThe majority of these are based on so-called \nconcatenative TTS\n, which uses a large database of high-quality recordings, collected from a single voice actor over many hours. These recordings are split into tiny chunks that can then be combined - or concatenated - to form complete utterances as needed. However, these systems can result in unnatural sounding voices and are also difficult to modify because a whole new database needs to be recorded each time a set of changes, such as new emotions or intonations, are needed.\nTo overcome some of these problems, an alternative model known as \nparametric TTS\n is sometimes used. This does away with the need for concatenating sounds by using a series of rules and parameters about grammar and mouth movements to guide a computer-generated voice. Although cheaper and quicker, this method creates less natural sounding voices.\nWaveNet takes a totally different approach. In the \noriginal paper\n we described a deep generative model that can create individual waveforms from scratch, one sample at a time, with 16,000 samples per second and seamless transitions between individual sounds.\nIt was built using a \nconvolutional neural network\n, which was trained on a large dataset of speech samples. During this training phase, the network determined the underlying structure of the speech, such as which tones followed each other and what waveforms were realistic (and which were not). The trained network then synthesised a voice one sample at a time, with each generated sample taking into account the properties of the previous sample. The resulting voice contained natural intonation and other features such as lip smacks. Its \u201caccent\u201d depended on the voices it had trained on, opening up the possibility of creating any number of unique voices from blended datasets. As with all text-to-speech systems, WaveNet used a text input to tell it which words it should generate in response to a query.\nBuilding up sound waves at such high-fidelity using the original model was computationally expensive, meaning WaveNet showed promise but was not something we could deploy in the real world. But over the last 12 months our teams have worked hard to develop a new model that is capable of more quickly generating waveforms. It is also now capable of running at scale and is the first product to launch on \nGoogle\u2019s latest TPU cloud infrastructure\n.\nThe WaveNet team will now turn their focus to preparing a publication detailing the research behind the new model, but the results speak for themselves. The new, improved WaveNet model still generates a raw waveform but at speeds 1,000 times faster than the original model, meaning it requires just 50 milliseconds to create one second of speech. In fact, the model is not just quicker, but also higher-fidelity, capable of creating waveforms with 24,000 samples a second. We have also increased the resolution of each sample from 8 bits to 16 bits, the same resolution used in compact discs.\nThis makes the new model more natural sounding according to tests with human listeners. For example, the new US English voice I gets a mean-opinion-score (MOS) of 4.347 on a scale of 1-5, where even human speech is rated at just 4.667.\nThe new model also retains the flexibility of the original WaveNet, allowing us to make better use of large amounts of data during the training phase. Specifically, we can train the network using data from multiple voices. This can then be used to generate high-quality, nuanced voices even where there is little training data available for the desired output voice.\nWe believe this is just the start for WaveNet and we are excited by the possibilities that the power of a voice interface could now unlock for all the world's languages.\n\u200d\nThis work was done by the DeepMind WaveNet research and engineering teams and the Google Text-to-Speech team.\nRead the \noriginal WaveNet blog post\n.\nRead the original \nWaveNet paper.\nRead more about \nthe updated Google Assistant\n. \nUpdate: \nAn earlier version of this blog incorrectly put the MOS score for the US English 3rd Party Voice as 4.326. This has now been corrected.\n"}
{"title": "Strengthening our commitment to Canadian research", "contents": "(French translation below)\nThree months ago we \nannounced\n the opening of DeepMind\u2019s first ever international AI research laboratory in Edmonton, Canada. Today, we are thrilled to announce that we are strengthening our commitment to the Canadian AI community with the opening of a DeepMind office in Montreal, in close collaboration with \nMcGill University\n.\nOpening a second office is a natural next step for us in Canada, a country that is globally recognised as a leader in artificial intelligence research. We have always had strong links with the thriving research community in Canada and Montreal, where large companies, startups, incubators and government come together with ground-breaking teams, such as those at the Montreal Institute for Learning Algorithms (\nMILA\n) and McGill University.\nWe are delighted that DeepMind Montreal will be led by one of the pioneers of this community, \nDoina Precup\n, Associate Professor in the School of Computer Science at McGill, Senior Fellow of the Canadian Institute for Advanced Research, and a member of MILA. Doina\u2019s expertise is in reinforcement learning - one of DeepMind\u2019s specialities - which is critical for areas such as reasoning and planning.\nIn her new position, Doina will continue to focus on fundamental research at McGill, MILA, and DeepMind. She will also maintain her role as Associate Professor at McGill, lecturing and supervising the community\u2019s leaders of tomorrow. As part of our collaboration with McGill University, we intend to provide funding to support AI research, and also sponsorship of PhD students in order to support the Canadian AI ecosystem.\nDoina will be supported by Shibl Mourad who will head up the engineering and program management teams at DeepMind Montreal and DeepMind Alberta.\nWe see open collaboration between company research labs and academia as central to the future of AI, which is why we will work closely with the broader community in Montreal, collaborating on research, openly publishing our own research papers and participating in community events - just as we do in \nLondon\n and Edmonton.\nWe hope that our partnership with McGill can help build on Montreal\u2019s world-leading position, attracting more gifted researchers to this globally-recognised technology and research hub, and further nurturing local talent.\nIt\u2019s an honour for us to work with Doina, Shibl and McGill University, and we look forward to the many scientific breakthroughs that we will achieve together.\nHere's what others have to say about DeepMind Montreal:\nNous \nannoncions\n il y a trois mois l\u2019ouverture du tout premier laboratoire international DeepMind de recherche en intelligence artificielle (IA) \u00e0 Edmonton, au Canada. Aujourd\u2019hui, nous sommes heureux d\u2019annoncer que nous consolidons notre engagement envers la communaut\u00e9 IA canadienne gr\u00e2ce \u00e0 l\u2019ouverture d\u2019un bureau de DeepMind \u00e0 Montr\u00e9al, en \u00e9troite collaboration avec \nl\u2019Universit\u00e9 McGill\n.\nPour nous, il \u00e9tait tout naturel d\u2019ouvrir un second bureau au Canada, un pays qui est internationalement reconnu comme un leader de la recherche en mati\u00e8re d\u2019intelligence artificielle. Nous avons toujours eu des liens tr\u00e8s solides avec la communaut\u00e9 dynamique de la recherche au Canada et \u00e0 Montr\u00e9al, o\u00f9 les grandes soci\u00e9t\u00e9s, les entreprises en d\u00e9marrage, les incubateurs et les gouvernements s\u2019unissent pour former des \u00e9quipes innovantes comme celles de l\u2019Institut des algorithmes d\u2019apprentissage de Montr\u00e9al (\nMILA\n) et de l\u2019Universit\u00e9 McGill.\nNous sommes enchant\u00e9s que \nDoina Precup\n, pionni\u00e8re de cette communaut\u00e9, professeure agr\u00e9g\u00e9e \u00e0 la \u00c9cole d'Informatique de l\u2019Universit\u00e9 McGill, \u201cSenior Fellow\u201d \u00e0 l\u2019Institut canadien de recherches avanc\u00e9es et membre de MILA, dirigera DeepMind Montr\u00e9al. L\u2019expertise de Doina est concentr\u00e9e en apprentissage par renforcement, l\u2019une des sp\u00e9cialit\u00e9s de DeepMind et un \u00e9l\u00e9ment essentiel pour des domaines comme le raisonnement et la planification.\nDans son nouveau poste, Doina continuera de se concentrer sur la recherche fondamentale \u00e0 McGill, \u00e0 MILA et chez DeepMind. Elle conservera \u00e9galement son poste de professeure agr\u00e9g\u00e9e \u00e0 McGill, enseignant les leaders de demain pour la communaut\u00e9. Dans le cadre de notre collaboration avec l\u2019Universit\u00e9 McGill, nous pr\u00e9voyons offrir du financement pour soutenir la recherche en intelligence artificielle et aussi de commanditer des \u00e9tudiants au doctorat afin de soutenir l\u2019\u00e9cosyst\u00e8me canadien en intelligence artificielle.\nDoina sera second\u00e9e par Shibl Mourad, qui dirigera les \u00e9quipes d\u2019ing\u00e9nierie et de gestion de programmes chez DeepMind Montr\u00e9al et DeepMind Alberta.\nPuisque nous consid\u00e9rons la collaboration ouverte entre les laboratoires de recherche des soci\u00e9t\u00e9s et le monde acad\u00e9mique comme \u00e9tant au c\u0153ur de l\u2019avenir de l\u2019IA, nous nous engageons \u00e0 collaborer \u00e9troitement avec l\u2019ensemble de la communaut\u00e9 montr\u00e9alaise dans le domaine de la recherche, \u00e0 publier ouvertement nos propres documents de recherche et \u00e0 participer aux \u00e9v\u00e9nements du milieu \u2013 exactement comme nous le faisons \u00e0 \nLondres\n et \u00e0 Edmonton.\nNous esp\u00e9rons que notre partenariat avec McGill puisse contribuer \u00e0 rehausser le profil de Montr\u00e9al comme leader mondial et \u00e0 attirer d\u2019autres chercheurs dou\u00e9s dans ce p\u00f4le de technologie et de recherche mondialement reconnu afin de favoriser encore davantage l\u2019essor des talents locaux.\nC\u2019est un honneur pour nous de travailler avec Doina, Shibl et l\u2019Universit\u00e9 McGill, et nous esp\u00e9rons faire ensemble de nombreuses perc\u00e9es scientifiques.\nVoici ce que d\u2019autres avaient \u00e0 dire au sujet de DeepMind Montr\u00e9al:\n"}
{"title": "AlphaGo Zero: Starting from scratch", "contents": "Artificial intelligence research has made rapid progress in a wide variety of domains from speech recognition and image classification to genomics and drug discovery. In many cases, these are specialist systems that leverage enormous amounts of human expertise and data.\nHowever, for some problems this human knowledge may be too expensive, too unreliable or simply unavailable. As a result, a long-standing ambition of AI research is to bypass this step, creating algorithms that achieve superhuman performance in the most challenging domains with no human input. In our most recent \npaper\n, published in the \njournal Nature\n, we demonstrate a significant step towards this goal.\nThe paper introduces AlphaGo Zero, the latest evolution of \nAlphaGo\n, the first computer program to defeat a world champion at the ancient Chinese game of Go. Zero is even more powerful and is arguably the strongest Go player in history.\nPrevious versions of AlphaGo initially trained on thousands of human amateur and professional games to learn how to play Go. AlphaGo Zero skips this step and learns to play simply by playing games against itself, starting from completely random play. In doing so, it quickly surpassed human level of play and defeated the \npreviously published\n champion-defeating version of AlphaGo by 100 games to 0.\nIt is able to do this by using a novel form of \nreinforcement learning\n, in which AlphaGo Zero becomes its own teacher. The system starts off with a neural network that knows nothing about the game of Go. It then plays games against itself, by combining this neural network with a powerful search algorithm. As it plays, the neural network is tuned and updated to predict moves, as well as the eventual winner of the games.\nThis updated neural network is then recombined with the search algorithm to create a new, stronger version of AlphaGo Zero, and the process begins again. In each iteration, the performance of the system improves by a small amount, and the quality of the self-play games increases, leading to more and more accurate neural networks and ever stronger versions of AlphaGo Zero.\nThis technique is more powerful than previous versions of AlphaGo because it is no longer constrained by the limits of human knowledge. Instead, it is able to learn tabula rasa from the strongest player in the world: AlphaGo itself.\nIt also differs from previous versions in other notable ways.\nAll of these differences help improve the performance of the system and make it more general. But it is the algorithmic change that makes the system much more powerful and efficient.\nAfter just three days of self-play training, AlphaGo Zero emphatically defeated the previously \npublished version of AlphaGo\n - which had itself \ndefeated 18-time world champion Lee Sedol\n - by 100 games to 0. After 40 days of self training, AlphaGo Zero became even stronger, outperforming the version of AlphaGo known as \u201cMaster\u201d, which has defeated the world's best players and \nworld number one Ke Jie\n.\nOver the course of millions of AlphaGo vs AlphaGo games, the system progressively learned the game of Go from scratch, accumulating thousands of years of human knowledge during a period of just a few days. AlphaGo Zero also discovered new knowledge, developing unconventional strategies and creative new moves that echoed and surpassed the novel techniques it played in the games against Lee Sedol and Ke Jie.\nThese moments of creativity give us confidence that AI will be a multiplier for human ingenuity, helping us with \nour mission\n to solve some of the most important challenges humanity is facing.\nWhile it is still early days, AlphaGo Zero constitutes a critical step towards this goal. If similar techniques can be applied to other structured problems, such as protein folding, reducing energy consumption or searching for revolutionary new materials, the resulting breakthroughs have the potential to positively impact society.\nRead \nthe paper\nRead the accompanying \nNature News and Views article\nDownload \nAlphaGo Zero games\nRead \nmore about AlphaGo\nThis work was done by David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel and Demis Hassabis.\n"}
{"title": "Bringing Streams to Yeovil District Hospital NHS Foundation Trust", "contents": "We\u2019re excited to announce that we\u2019ve agreed a five year partnership with Yeovil District Hospital NHS Foundation Trust. We\u2019ll be providing them with Streams, our secure mobile app that helps nurses and doctors access important clinical information and get the right care to the right patient \nas quickly as possible\n.\nThis will be our fourth Streams partnership, following on from our work with Taunton and Somerset NHS Foundation Trust, Imperial College Healthcare NHS Trust and the Royal Free London NHS Foundation Trust.\nDoctors and nurses at Yeovil will be able to use Streams for a range of functions, including vital signs observations - entering information such as heart rate, blood pressure, respiratory rate and temperature into the app, and making that information instantly available to other Streams users involved in caring for that patient. These types of vital signs are used by clinicians to identify the earliest signs of patient deterioration from many serious conditions like sepsis.\nThe Trust will also be able to use Streams to enable clinicians to view blood and other test results on their mobile phones, so they can check results at the patient\u2019s bedside or from anywhere in the hospital, without having to log on to a computer. The app will also in time generate alerts for patients at risk of deterioration from conditions such as acute kidney injury and provide a range of teamwork and collaboration tools across devices and operating systems.\nHaving quick and secure access to important patient information in this way has already proved popular with Streams users elsewhere, with some nurses saying that the app saves them up to two hours each day. While Streams doesn\u2019t currently use AI technology, we hope that it will help to accelerate the pace of innovation in NHS IT and make it possible for more advanced services to be introduced in future, subject to strict regulation and oversight.\nDr Tony Smith, Consultant Anaesthetist and Chief Clinical Information Officer at the Trust, said:\n\u201cTechnology has an increasingly important role to play in the way our hospital provides care, which goes far beyond the move from paper to digital records.\n\u201cWe are therefore very pleased to be working with DeepMind; a company at the forefront of the application of digital technologies and better use of data to improve healthcare.\n\u201cThe Streams application will enhance the expertise and experience of our clinicians, by processing and presenting existing data, applied to existing NHS algorithms, in more accessible and more responsive ways. The results will be quicker, more informed clinical decisions and better outcomes for patients.\n\u201cEverything about our partnership with DeepMind and the products we implement is about improving our ability to care for the patient. The information that will enable the Streams application to work and support staff to make vital decisions about treatment and care is data that is already collected and held by our hospital, and remains under our control. It will be used for no other purpose than to inform the delivery of care to our patients, in the same way as existing observations and patient record products in use in our hospital.\n\u201cDescribing how data will be used will form a central element of the forthcoming engagement which will happen with patients, the public and staff in the coming weeks and months.\u201d\nDr Dominic King, clinician lead at DeepMind, said: \u201cWe are really pleased to be working with the team at Yeovil District Hospital NHS Foundation Trust. They want to use health technologies in a way that benefits patients and the doctors and nurses who care for them. This is a goal we share, and it\u2019s the reason why we built Streams. By putting together all the information a clinician needs, we think Streams can reduce the time nurses and doctors currently spend on administrative tasks. This should help them make quicker and better decisions about the right care for patients. It\u2019s a tool I wish I\u2019d had when I worked as a doctor in the NHS.\n\u201cWe\u2019re also looking forward to working with the Trust to talk to patients and the public about our partnership, and hearing from them what they think about how these types of technology should be used in care.\u201d\nIn line with our commitment to transparency in our work with the NHS, we are publishing \nour legal agreement with Yeovil\n, and Yeovil has published the \nPrivacy Impact Assessment\n, so that patients and the public can see exactly what we\u2019ll be doing for the Trust. As with all our partnerships with the NHS, the Trust will remain in full control of all patient data at all times, and DeepMind will only process data according to the strict instructions of the Trust and in accordance with our \u00a0contract. No patient data has yet been processed by DeepMind, and we expect this work to begin in 2018.\nAll our work is done with patients and their needs at its heart. Patient privacy and confidentiality are paramount, and we\u2019re fully committed to ensuring patients are aware of our work with each of our NHS partners. We\u2019ll be working with the Trust on their plans to make sure that patients know about and have the opportunity to ask questions about Streams, before the Trust instructs us to start processing patient data. A programme of patient and public events, information notices and communication will be hosted across the Trust, providing opportunities to find out more about Streams.\nIt\u2019s a real privilege to be working with the NHS to help patients and clinicians, and we\u2019re proud to be working with yet another outstanding Trust team to help improve the lives of patients, nurses and doctors.\n"}
{"title": "Sharing our insights from designing with clinicians", "contents": "[Editor\u2019s note: this is the first in a series of blog posts about what we\u2019ve learned about working in healthcare. It\u2019s both exceptionally hard and exceptionally important to get right, and we hope that by sharing our experiences we\u2019ll help other health innovators along the way]\nIn our design studio, we have \nIndi Young\u2019s\n mantra on the wall as a reminder to \u201cfall in love with the problem, not the solution\u201d. Nowhere is this more true than in health, where there are so many real problems to address, and where introducing theoretically clever but practically flawed software could easily do more harm than good.\nOver the course of hundreds of hours of shadowing, interviews and workshops with nurses, doctors and patients, we\u2019ve been privileged to learn a lot about some of the problems they all face - and we\u2019re still learning a ton every day. We are constantly impressed by the skill and care that clinicians across the NHS deliver every day, and this is the primary motivation for our team to ensure that these people get the tools they need to appropriately support them in their quest to help patients. In the first of a series of posts about what we\u2019ve learned through working in health, we wanted to share some of the design lessons from building \nStreams, a secure clinical mobile app\n that gives the right information to the right clinician at the right time.\nMost products begin with an insight into one core problem. In the case of Streams, it was that urgent clinical information gets retrieved by nurses and doctors over a mixture of outdated desktop computers, pager messages and handwritten lists. This contributes towards delays in care and occasionally serious harm if something is missed, and a \n2017 study\n found that nearly half of emergency response time is wasted due to inefficient communication between systems.\nSurely a secure mobile app like Streams that immediately pushes urgent clinical information directly to the right nurse or doctor would be a better solution? We think so, yes. But as we learned more about the working lives of clinicians, and the phenomenon of \u201cbleeper fatigue\u201d - with many doctors telling us that they receive over 100 notifications a day already - we started to recognise how a solution might actually become another problem if we ended up contributing to the general bombardment of messages.\nIt turns out that there\u2019s a very fine line between alerts that clinicians find useful, and alerts that become a nuisance - and much of this seems to come down to the precise way in which data is presented in the app. For example, clinicians told us that when reviewing a patient\u2019s record in the hospital, they need to see a patient\u2019s hospital number, their allergies and details of previous admissions, rather than the traditional format of listing NHS number and GP practice. This was easy to fix with a subtle change in information architecture, which addressed something that many clinicians found to be more irritating than we initially thought!\nWe also found by using the colour red in the interface intentionally and in a measured way, we could improve how people navigated content and better draw attention to information which needs urgent action. The sound designer on our team also worked directly with healthcare workers to produce a unique sound that would be immediately recognisable amidst the din of other bleeps and alerts.\nWhile these might seem like relatively minor design choices, clinicians told us of the surprisingly large impact it had on managing their busy time, and how likely they were to use the product in practice.\nDesign doesn\u2019t stop at deployment. Good health IT should empower nurses and doctors to change the way they care for patients. Each change creates a new reality, with its own set of new problems - and new opportunities to make a difference.\nFor example, once we\u2019d deployed Streams at the Royal Free Hospital in January 2017, clinicians told us that the app needed a better way to support communication between teams and across specialties. While they wanted urgent alerts to be sent to multiple clinicians at the same time, to increase the likelihood of a rapid response, it wasn\u2019t easy enough for each of those clinicians to see if their colleagues were also responding - which in some cases could actually have hindered coordination, rather than improving it. We supported their need to triage alerts by giving clinicians the one-click ability within the app to \u201crecommend a response\u201d, \u201cdismiss a response\u201d or indicate that a patient had been attended to, and for this to be visible across the clinical team.\nAnother area that surprised us was the urgency of creating a patient-friendly view within the app. From the beginning, our patient advisors championed the need for a way for patients to see their data within Streams, and so this had always been on our roadmap. But we hadn\u2019t factored in just how important this might be right away.\nWhile it\u2019s normal to see clinicians carrying paper notes and pagers on the wards, many patients were surprised to see staff walking around looking at an app on their mobile phones. In some cases, patients assumed that clinicians must be on personal social networking or messaging apps - rather than using an app specifically designed to support their care - and challenged them directly about what they were doing!\nThese moments provided opportunities for clinicians to really engage patients in their care, by showing them the data in the Streams app and talking through what it meant for their treatment. The challenge was that the default Streams views are designed for highly-trained clinicians, with graphs and notations that are hard to understand for the layperson. We are now looking into making the app more patient-friendly, providing a new way to strengthen and support the clinician-patient relationship.\nThis kind of post-deployment insight could only have come through ongoing face-to-face feedback from nurses, doctors and patients. Typical app usage metrics might be useful for developers in other domains, but in a field like health they will almost always miss the point. What\u2019s important isn\u2019t how health IT is used in itself, but rather the changes it enables in the person-to-person context of care: whether it improves or impairs the caring relationship between clinicians and their patients, or whether it empowers or stresses out overstretched clinical teams. The answers lie in the conversations, not the usage data. In a future blog, we'll talk in more detail about what we've learned from our dedicated patient involvement efforts too.\nOur approach is constantly informed by the great work of other people and organisations working across the field. So far we\u2019ve been drawing on best practices in user-centred design and agile development from the likes of the Royal College of Art's \nHealthcare Innovation Exchange Centre\n, \nPrescribe Design\n, \nStanford Biodesign\n and the \nMayo Innovation Centre\n. \nThis article\n, by Carl Warren at Team Consulting, provides a good explanation of how agile processes can be used in medical technology, for those curious to learn more.\nWe hope Streams continues to evolve with the help of patients, clinicians and nurses, and that the lessons we\u2019ve learned are useful to other innovators in health! If you work in healthcare and want to be involved in our future work, please register your interest\n here\n.\nTo learn more about how Streams is having an impact on the wards, listen to Sarah Stanley, consultant nurse at The Royal Free London NHS Foundation Trust below:\n"}
{"title": "High-fidelity speech synthesis with WaveNet", "contents": "In October we announced that our state-of-the-art speech synthesis model \nWaveNet\n was being used to generate realistic-sounding voices for the\n Google Assistant\n globally in Japanese and the US English. This production model - known as parallel WaveNet - is more than 1000 times faster than the \noriginal \nand also capable of creating higher quality audio.\nOur \nlatest paper\n introduces details of the new model and the \u201cprobability density distillation\u201d technique we developed to allow the system to work in a massively parallel computing environment.\nThe original WaveNet model used autoregressive connections to synthesise the waveform one sample at a time, with each new sample conditioned on the previous samples. While this produces high-quality audio with up to 24,000 samples per second, this sequential generation is too slow for production environments.\nTo get around this we needed a solution that could generate long sequences of samples all at once and with no loss of quality. Our solution is called probability density distillation, where we used a fully-trained WaveNet model to teach a second, \u201cstudent\u201d network that is both smaller and more parallel and therefore better suited to modern computational hardware. This student network is a smaller dilated \nconvolutional neural network\n, similar to the original WaveNet. But, crucially, generation of each sample does not depend on any of the previously generated samples, meaning we can generate the first and last word - and everything in between - \u00a0at the same time, as shown in the animation below.\nDuring training, the student network starts off in a random state. It is fed random white noise as an input and is tasked with producing a continuous audio waveform as output. The generated waveform is then fed to the trained WaveNet model, which scores each sample, giving the student a signal to understand how far away it is from the teacher network\u2019s desired output. Over time, the student network can be tuned - via backpropagation - to learn what sounds it should produce. Put another way, both the teacher and the student output a probability distribution for the value of each audio sample, and the goal of the training is to minimise the \nKL divergence\n between the teacher\u2019s distribution and the student\u2019s distribution.\nThe training method has parallels to the set-up for generative adversarial networks (GANs), with the student playing the role of generator and the teacher as the discriminator. However, unlike GANs, the student\u2019s aim is not to \u201cfool\u201d the teacher but to cooperate and try to match the teacher\u2019s performance.\nAlthough the training technique works well, we also need to add a few extra loss functions to guide the student towards the desired behaviour. Specifically, we add a \nperceptual loss\n to avoid bad pronunciations, a contrastive loss to further reduce the noise, and a power loss to help match the energy of the human speech. Without the latter, for example, the trained model whispers rather than speaking out loud.\nAdding all of these together allowed us to train the parallel WaveNet to achieve the same quality of speech as the original WaveNet, as shown by the mean opinion scores (MOS) - a scale of 1-5 that measures of how natural sounding the speech is according to tests with human listeners. Note that even human speech is rated at just 4.667 on the MOS scale.\nOf course, the development of probability density distillation was just one of the steps needed to allow WaveNet to meet the speed and quality requirements of a production system. Incorporating parallel WaveNet into the serving pipeline of the Google Assistant required an equally significant engineering effort by the DeepMind Applied and Google Speech teams. It was only by working together that we could move from fundamental research to Google-scale product in a little over 12 months.\nRead the \nnew paper\n.\nRead more about \nWaveNet in the Google Assistant\n.\nRead the \noriginal WaveNet blog post\n.\nRead the original \nWaveNet paper.\nThis work was done by Aaron van den Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, Koray Kavukcuoglu, George van den Driessche, Edward Lockhart, Luis C. Cobo, Florian Stimberg, Norman Casagrande, Dominik Grewe, Seb Noury, Sander Dieleman, Erich Elsen, Nal Kalchbrenner, Heiga Zen, Alex Graves, Helen King, Tom Walters, Dan Belov and Demis Hassabis.\n"}
{"title": "Applying machine learning to mammography screening for breast cancer", "contents": "We founded DeepMind Health to develop technologies that could help address some of society\u2019s toughest challenges. So we\u2019re very excited to announce that our latest research partnership will focus on breast cancer.\nWe\u2019ll be working with a group of leading research institutions, led by the Cancer Research UK Centre at Imperial College London, and alongside the AI health research team at Google, to determine if cutting-edge machine learning technology could help improve the detection of breast cancer.\nBreast cancer is a significant global health problem. Every single year, over 1.6 million people are diagnosed with the disease, and while advances in early detection and treatment have improved survival rates, breast cancer still claims the lives of 500,000 people around the world every year, around 11,000 of whom are here in the UK.\nThat\u2019s partly because accurately detecting and diagnosing breast cancer still remains a huge challenge.\nCurrently, clinicians use mammograms (an X-ray of the breasts) to spot cancers early and determine the correct treatment, but this process is far from perfect. Thousands of cancer cases are not picked up by mammograms every year, including around 30% of \u201cinterval\u201d cancers, which are cancers that are diagnosed between screenings. At the other end of the spectrum, false alarms and cases of overdiagnosis are also still a challenge, creating a great deal of unnecessary stress for patients.\nWorking alongside leading breast cancer experts, clinicians and academics, we\u2019ll be exploring whether machine learning could help address this.\nWe\u2019ll be using the latest machine learning technology to carefully analyse historic de-identified mammograms from around 7,500 women, provided by the Cancer Research UK-funded OPTIMAM mammography database at the Royal Surrey County Hospital NHS Foundation Trust. These digital images have been stripped of any information which could be used to identify patients, and have been available to research groups around the world for a number of years. We hope to use these images to investigate whether machine learning tools can spot signs of cancerous tissue on these X-rays and alert expert radiologists and oncologists more effectively than current screening techniques allow.\nOur partners in this project wanted researchers at both DeepMind and Google involved in this research so that the project could take advantage of the AI expertise in both teams, as well as Google\u2019s supercomputing infrastructure - widely regarded as one of the best in the world, and the same global infrastructure that powered DeepMind\u2019s\n victory over the world champion at the ancient game of Go.\nWe hope that this combination of partners will achieve more impactful results for patients, which is everyone\u2019s priority.\nAs with all of our research work, DeepMind is committed to treating the data for this project with the utmost care and respect. As is standard practice, the data being used in the research remains in the full control of our partners, and is being stored to world-class standards of security and encryption. Additionally, all medical information has been thoroughly de-identified, with any information that could identify an individual being removed before researchers can conduct their analysis. You can read more about our approach to information governance \nhere.\nIt\u2019s early days, and the work we\u2019re currently conducting is exploratory, but we\u2019re optimistic about the long term potential for machine learning technology to help in this area. As the research progresses and any potential benefits become clearer, we commit to working with the NHS leadership to ensure that any technology we build following this research benefits the nation - whether through discounts on any new technology used in the national screening programme, as some have suggested, or another mechanism that provides value back to the NHS.\nWe also hope that in time other international research partners will join the project to make any findings more globally generalisable.\nIt\u2019s a hugely exciting opportunity to make a difference and we will keep you updated as we make progress.\n"}
{"title": "Population based training of neural networks", "contents": "Neural networks have shown great success in everything from playing Go and Atari games to image recognition and language translation. But often overlooked is that the success of a neural network at a particular application is often determined by a series of choices made at the start of the research, including what type of network to use and the data and method used to train it. Currently, these choices - known as hyperparameters - are chosen through experience, random search or a computationally intensive search processes.\nIn our \nmost recent paper\n, we introduce a new method for training neural networks which allows an experimenter to quickly choose the best set of hyperparameters and model for the task. This technique - known as Population Based Training (PBT) - trains and optimises a series of networks at the same time, allowing the optimal set-up to be quickly found. Crucially, this adds no computational overhead, can be done as quickly as traditional techniques and is easy to integrate into existing machine learning pipelines.\nThe technique is a hybrid of the two most commonly used methods for hyperparameter optimisation: random search and hand-tuning. In random search, a population of neural networks are trained independently in parallel and at the end of training the highest performing model is selected. Typically, this means that \u00a0a small fraction of the population will be trained with good hyperparameters, but many more will be trained with bad ones, wasting computer resources. \nWith hand tuning, researchers must guess at the best hyperparameters, train their models using them, and then evaluate the performance. This is done over and over, until the researcher is happy with the performance of the network. Although this can result in better performance, the downside is that this takes a long time, sometimes taking weeks or even months to find the perfect set-up. And while there are ways of automating this process - \u00a0such as Bayesian optimisation - it still takes a long time and requires many sequential training runs to find the best hyperparameters.\nPBT - like random search - starts by training many neural networks in parallel with random hyperparameters. But instead of the networks training independently, it uses information from the rest of the population to refine the hyperparameters and direct computational resources to models which show promise. This takes its inspiration from genetic algorithms where each member of the population, known as a worker, can exploit information from the remainder of the population. For example, a worker might copy the model parameters from a better performing worker. It can also explore new hyperparameters by changing the current values randomly.\nAs the training of the population of neural networks progresses, this process of exploiting and exploring is performed periodically, ensuring that all the workers in the population have a good base level of performance and also that new hyperparameters are consistently explored. \u00a0This means that PBT can quickly exploit good hyperparameters, can dedicate more training time to promising models and, crucially, can adapt the hyperparameter values throughout training, leading to automatic learning of the best configurations.\nOur experiments show that PBT is very effective across a whole host of tasks and domains. For example, we rigorously tested the algorithm on a suite of challenging reinforcement learning problems with state-of-the-art methods on DeepMind Lab, Atari, and StarCraft II. In all cases, PBT stabilised training, quickly found good hyperparameters, and delivered results that were beyond state-of-the-art baselines.\nWe have also found PBT to be effective for training Generative Adversarial Network (GAN), which are notoriously difficult to tune. Specifically, we used the PBT framework to maximise the Inception Score - a measure of visual fidelity - \u00a0resulting in a significant improvement from 6.45 to 6.9.\nWe have also applied it to one of Google\u2019s state-of-the-art machine translation neural networks, which are usually trained with carefully hand tuned hyperparameter schedules that take months to perfect. With PBT we automatically found hyperparameter schedules that match and even exceed existing performance, but without any tuning and in the same time it normally takes to do a single training run.\nWe believe this is only the beginning for the technique. At DeepMind, we have also found PBT is particularly useful for training new algorithms and neural network architectures that introduce new hyperparameters. As we continue to refine the process, it offers up the possibility of finding and developing ever more sophisticated and powerful neural network models.\nRead the \nfull paper\n.\nThis work was done by Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M. Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, Chrisantha Fernando and \u00a0Koray Kavukcuoglu.\n"}
{"title": "Why doesn't Streams use AI?", "contents": "One of the questions I\u2019m most often asked about Streams, our secure mobile healthcare app, is \u201cwhy is DeepMind making something that doesn\u2019t use artificial intelligence?\u201d\nIt\u2019s a fair question to ask of an artificial intelligence (AI) company. When we first started thinking about working in healthcare, our natural focus was on AI and how it could be used to help the NHS and its patients. \u00a0We see huge potential for AI to revolutionise our understanding of diseases - how they develop and are diagnosed - which could, in turn, help scientists discover new treatments, care pathways and cures.\nIn the early days of DeepMind Health, we met with clinicians at the Royal Free Hospital in London who wanted to know if AI could improve care for patients at risk of acute kidney injury (AKI). AKI is notoriously difficult to spot, and can result in serious illness or even death if left untreated. \u00a0\nAKI is currently detected by applying a \u00a0formula (called the \nAKI algorithm\n) to NHS patients\u2019 blood tests. This algorithm is good, but it\u2019s widely known that it isn\u2019t perfect. For example, it has a tendency to generate false positives for patients with chronic (as opposed to acute) kidney disease. It\u2019s also insensitive to whether the patient has been admitted to hospital for two hours or two weeks, or whether the patient is eight years old or 92 years old - all of which makes a difference.\nTogether with our partners at the Royal Free, we saw many ways in which technology could help and were interested in both AI and non-AI methods to make a difference.\nAs part of this, we made an \ninitial ethics application\n in 2015 to the NHS Health Research Authority (HRA), for a potential research project at the Royal Free using de-personalised patient data. By combining classical statistics and AI, the goal of this research project would have been to develop better algorithms that could more accurately predict and identify AKI. \u00a0\nBut the more time we spent with the clinicians at the Royal Free, the more it became obvious that their most urgent problems were not going to be solved by using AI to develop a better algorithm alone. They made it clear to us that their core challenge was in how you actually implement an algorithm to change the way care is delivered in practise. \u00a0\nWe\u2019ve often talked about the current state of technology in the NHS, to the point that it\u2019s easy to forget just how bad the situation is. Clinicians still routinely use pagers to communicate with each other, and \nresearch\n I undertook at Imperial College London found that this causes communication barriers that slow down treatment for patients at risk. Think about how much less time you\u2019d have in a day if, instead of sending a text, you had to page someone from a landline, and then wait for them to call you back, so you could give them the message. \u00a0Imagine getting paged up to 25 times a day. And imagine how much less time you\u2019d have in your day if everyone in your office had to share a limited number of computers and you had to wait your turn to use them. \u00a0\nThat\u2019s what doctors and nurses face every day, whilst trying to care for seriously ill people.\nThose early meetings our team had with clinicians at the Royal Free changed our perspective about what was most needed to improve care for conditions like AKI. \u00a0We shifted our focus away from AI research at the Royal Free and focused solely on building a tool - Streams - that would address the more urgent problem of rapidly responding to specific patient alerts in a coordinated way.\nRather than needing to log into a shared computer, Streams lets doctors and nurses use a mobile phone to see information about their patients that they need to make decisions about care and treatment. \u00a0It puts test results and vital signs observations in the palm of their hands, and alerts them with a breaking news-style warning if a patient\u2019s condition is getting worse. It also makes communication between different clinicians easy, so everyone has the most up to date information all the time. \nBy getting existing patient data to the right nurse or doctor more quickly and simply, Streams gives them more time to focus on patients - without yet relying on AI.\nBuilding Streams turned out to be a lot of work and, given the limited size of our team back in 2015, we decided against pursuing AI research with the Royal Free in parallel. As well as the additional workload, it would have required us to effectively split our team into two to ensure that the Royal Free\u2019s personally identified data (for Streams) and de-identified data (for research) were kept entirely separate. So we didn\u2019t move forward with AI research, and nor did we sign the additional agreements with the Royal Free that would be required to do so. To this date, we have not done any research or AI development with the Royal Free.\nThat\u2019s not to say we\u2019ve stopped thinking about how AI will be able to help clinicians in future. \u00a0We\u2019ve pursued multiple AI research projects with other partners, and have always been clear that in future we hope that Streams at the Royal Free will use AI.\nBut as the saying goes, a journey of a thousand miles begins with a single step. \u00a0We see Streams as an essential first step towards that AI-enabled future. \u00a0Without a working app that can deliver clinical information to nurses and doctors, AI alerts would be pointless. You can\u2019t generate an AI recommendation from data held on pen and paper, and nor can you send detailed clinical alerts through a pager or fax machine.\nWhen the time is right we hope to pursue research with the Royal Free, but would only do so with the appropriate approvals. \u00a0For now, our work with them is focused on the more immediate problems that Streams helps to solve for clinicians and for patients.\nYou can read more about Streams and how it works \nhere\n.\n"}
{"title": "Learning by playing", "contents": "Getting children (and adults) to tidy up after themselves can be a challenge, but we face an even greater challenge trying to get our AI agents to do the same. Success depends on the mastery of several core visuo-motor skills: approaching an object, grasping and lifting it, opening a box and putting things inside of it. To make matters more complicated, these skills must be applied in the right sequence.\nControl tasks, like tidying up a table or stacking objects, require an agent to determine how, when and where to coordinate the nine joints of its simulated arms and fingers to move correctly and achieve its objective. The sheer number of possible combinations of movements at any given time, along with the need to carry out a long sequence of correct actions constitute a serious exploration problem\u2014making this a particularly interesting area for reinforcement learning research.\nTechniques like reward shaping, apprenticeship learning or learning from demonstrations can help with the exploration problem. However, these methods rely on a considerable amount of knowledge about the task\u2014the problem of learning complex control problems from scratch with minimal prior knowledge is still an open challenge.\nOur \nnew paper\n proposes a new learning paradigm called \u2018Scheduled Auxiliary Control (SAC-X)\u2019 which seeks to overcome this exploration issue. SAC-X is based on the idea that to learn complex tasks from scratch, an agent has to learn to explore and master a set of basic skills first. Just as a baby must develop coordination and balance before she crawls or walks\u2014providing an agent with internal (auxiliary) goals corresponding to simple skills increases the chance it can understand and perform more complicated tasks.\nWe demonstrate the SAC-X approach on several simulated and real robot tasks using a variety of tasks including stacking problems with different objects and \u2018tidying up a playground\u2019, which involves moving objects into a box. The auxiliary tasks we define follow a general principle: they encourage the agent to explore its sensor space. For example, activating a touch sensor in its fingers, sensing a force in its wrist, maximising a joint angle in its proprioceptive sensors or forcing a movement of an object in its visual camera sensors. Each task is associated with a simple reward of one if the goal is achieved, and zero otherwise. \nOur agent can then decide by itself about its current \u2018intention\u2019, i.e. which goal to pursue next. This might be an auxiliary task or an externally defined target task. Crucially, the agent can detect and learn from reward signals for all other tasks that it is not currently following by making extensive use of replay-based off-policy learning. For example, when picking up or moving an object the agent might incidentally stack it, leading to the observation of rewards for \u2018stacking\u2019. Because a sequence of simple tasks can lead to the observation of a rare external reward, the ability to schedule intentions is crucial. It can create a personalised learning curriculum based on all the tangential knowledge it has collected. This turns out to be an effective way to exploit knowledge in such a large domain, and is particularly useful when there are only few external reward signals available. Our agent decides which intention to follow via a scheduling module. The scheduler is improved during training via a meta-learning algorithm that attempts to maximise progress on the main task, which results in significantly improved data-efficiency.\nOur evaluations show that SAC-X is able to solve all the tasks we set it from scratch\u2014using the same underlying set of auxiliary tasks. Excitingly, SAC-X is also able to successfully learn a pick-up and a placing task from scratch directly on a real robot arm in our lab. In the past this has been particularly challenging because learning on robots in a real-world setup requires data-efficiency, so a popular approach is to pre-train an agent in simulation and then transfer the agent to the real robot arm.\nWe consider SAC-X as an important step towards learning control tasks from scratch, when only the overall goal is specified. SAC-X allows you to define auxiliary tasks arbitrarily: they can be based on general insights (like deliberately activating sensors as suggested here), but could ultimately incorporate any task a researcher thinks is important. In that respect, SAC-X is a general RL method that is broadly applicable in general sparse reinforcement learning settings beyond control and robotics.\nRead the paper \nhere\n.\nThis work was completed by Martin Riedmiller, Roland Hafner, Thomas Lampe, Michael Neunert, Jonas Degrave, Tom Van de Wiele, Volodymyr Mnih, Nicolas Heess and Tobias Springenberg.\n"}
{"title": "DeepMind papers at NIPS 2017", "contents": "Between 04-09 December, thousands of researchers and experts will gather for the Thirty-first Annual Conference on \nNeural Information Processing Systems\n (NIPS) in Long Beach, California.\nHere you will find an overview of the papers DeepMind researchers will present. \nAuthors:\n Ziyu Wang, Josh Merel, Greg Wayne, Nando de Freitas, Scott Reed, Nicolas Heess\n\u201cWe propose a neural network architecture, building on state-of-the-art generative models, that is capable of learning the relationships between different behaviours and imitating specific actions that it is shown. After training, our system can encode a single observed action and create a new novel movement based on that demonstration. It can also switch between different kinds of behaviours despite never having seen transitions between them, for example switching between walking styles.\u201d Read more on \nthe blog\nAuthors:\n Wojtek Czarnecki, Simon Osindero, Max Jaderberg, Grzegorz \u015awirszcz, Razvan Pascanu\nThis paper shows a simple way of incorporating knowledge about target function derivatives into the training of deep neural networks. We prove that modern ReLU-based architectures are well suited for such tasks, and evaluate their effectiveness on three problems - low-dimensional regression, policy distillation, and training with synthetic gradients. We observe a significant boost in training efficiency, especially in low-data regimes, and train the first synthetic gradient-based ImageNet model with near state-of-the-art accuracy.\nAuthors: \nChris J. Maddison, Dieterich Lawson, George Tucker, Nicolas Heess, Mohammad Norouzi, Andriy Mnih, Arnaud Doucet, Yee Whye Teh\nWe consider the extension of the variational lower bound to a family of lower bounds defined by a particle filter's estimator of the marginal likelihood - the filtering variational objectives. These filtering objectives can exploit a model's sequential structure to form tighter bounds and better objectives for model learning in deep generative models. In our experiments, we find that training with filtering objectives results in substantial improvements over training the same model architecture with the variational lower bound.\nAuthors:\n Nicholas Watters, Andrea Tacchetti, Theophane Weber, Razvan Pascanu, Peter Battaglia, Daniel Zoran\n\u201cIn this work we developed the \u201cVisual Interaction Network\u201d (VIN), a neural network-based model that learns physical dynamics without prior knowledge. The VIN is able to infer the states of multiple physical objects from just a few frames of video, and then use these to predict object positions many steps into the future. It is also able to infer the locations of invisible objects and learn dynamics that depend on object attributes such as mass.\u201d\n \u00a0Read \nthe blog\n for further detail.\nAuthors: \nA\u00e4ron van den Oord, Oriol Vinyals, Koray Kavukcuoglu\nLearning useful representations without supervision remains a key challenge in machine learning. In this work we propose a simple yet powerful generative model - known as the Vector Quantised Variational AutoEconder (VQ-VAE) - \u00a0that learns such discrete representations. When these representations are paired with an autoregressive prior, the model is able to generate high quality images, videos and speech as well as doing high-quality speaker conversion.\nAuthors: \nJ\u00f6rg Bornschein, Andriy Mnih, Daniel Zoran, Danilo Jimenez Rezende\nAttention based memory can be used to augment neural networks to support few-shot learning, rapid adaptability and more generally to support non-parametric extensions. Instead of using the popular differentiable soft-attention mechanism, we propose the use of stochastic hard-attention to retrieve memory content in generative models. This allows us to apply variational inference to memory addressing, which enables us to get significantly more precise memory lookups using target information, especially in models with large memory buffers and with many confounding entries in the memory.\nAuthors: \nGeorge Tucker, Andriy Mnih, Chris J Maddison, Dieterich Lawson, Jascha Sohl-Dickstein\nLearning in models with discrete latent variables is challenging due to high-variance gradient estimators. Previous approaches either produced high-variance, unbiased gradients or low-variance, biased gradients. REBAR uses control variates and the reparameterization trick to get the best of both: low-variance, unbiased gradients that result in faster convergence to a better result.\nAuthors:\n S\u00e9bastien Racani\u00e8re, Th\u00e9ophane Weber, David P. Reichert, Lars Buesing, Arthur Guez, Danilo Rezende, Adria Puigdom\u00e8nech Badia, Oriol Vinyals, Nicolas Heess, Yujia Li, Razvan Pascanu, Peter Battaglia, Demis Hassabis, David Silver, Daan Wierstra.\n\u201cWe describe a new family of approaches for imagination-based planning...We also introduce architectures which provide new ways for agents to learn and construct plans to maximise the efficiency of a task. These architectures are efficient, robust to complex and imperfect models, and can adopt flexible strategies for exploiting their imagination. The agents we introduce benefit from an \u2018imagination encoder\u2019- a neural network which learns to extract any information useful for the agent\u2019s future decisions, but ignore that which is not relevant.\u201d \u00a0Read more on \nthe blog\n.\nAuthors:\n Adam Santoro, David Raposo, David Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, Timothy Lillicrap\n\u201cWe demonstrate the use of a simple, plug-and-play neural network module for solving tasks that demand complex relational reasoning. This module, called a Relation Network, can receive unstructured inputs - say, images or stories - and implicitly reason about the relations contained within.\u201d \u00a0Read more on \nthe blog\n.\nAuthors:\n Balaji Lakshminarayanan, Alexander Pritzel, Charles Blundell\nQuantifying predictive uncertainty in neural networks (NNs) is a challenging and yet unsolved problem. The majority of work is focused on Bayesian solutions, however these are computationally intensive and require significant modifications to the training pipeline. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelisable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs.\nAuthors: \nZhongwen Xu, Joseph Modayil, Hado van Hasselt, Andre Barreto, David Silver, Tom Schaul\nWe revisit the structure of value approximators for RL, based on the observation that typical approximators smoothly change as a function of input, but the true value changes abruptly when a reward arrives. Our proposed method is designed to fit such asymmetric discontinuities using interpolation with a projected value estimate.\nAuthors:\n Andre Barreto, Will Dabney, Remi Munos, Jonathan Hunt, Tom Schaul, David Silver, Hado van Hasselt.\nWe propose a transfer framework for reinforcement learning. Our approach rests on two key ideas: \"successor features\", a value function representation that decouples the dynamics of the environment from the rewards, and \"generalised policy improvement\", a generalisation of dynamic programming\u2019s policy improvement step that considers a set of policies rather than a single one. Put together, the two ideas lead to an approach that integrates seamlessly within the reinforcement learning framework and allows transfer to take place between tasks without any restriction.\nAuthors: \u00a0\nPaul Christiano (Open AI), Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, Dario Amodei (Open AI)\n\u201cA central question in technical AI safety is how to tell an algorithm what we want it to do. Working with OpenAI, we demonstrate a novel system that allows a human with no technical experience to teach an AI how to perform a complex task, such as manipulating a simulated robotic arm.\u201d Read more on \nthe blog\n.\nAuthor: \nJulien Perolat, Joel Z Leibo, Vinicius Zambaldi, Charles Beattie, Karl Tuyls, Thore Graepel\nThis paper looks at the complexity of problems of common-pool resource appropriation. These include systems such as fisheries, grazing pastures or access to \u00a0freshwater, where lots of people or actors have access to the same resource. Traditional models from the social sciences tend to suggest that parties with access to the resource act in a self-interested way, eventually leading to an unsustainable depletion of resources. However, we know from human societies that there is a wide range of possible outcomes. Sometimes resources like fisheries are overexploited and sometimes they are harvested sustainably. In this work we propose new modeling techniques that can be used in research aimed at explaining this gap between what we observe in the real world and what traditional models predict.\nAuthors:\n Yee Whye Teh, Victor Bapst, Wojciech Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell, Nicholas Heess, Razvan Pascanu\nWe develop a method for doing reinforcement learning on multiple tasks. The assumption is that the tasks are related to each other (e.g. being in the same environment or having the same physics) and so good action sequences tend to recur across tasks. Our method achieves this by simultaneously distilling task-specific policies into a common default policy, and transferring this common knowledge across tasks by regularising all task-specific policies towards the default policy. \u00a0We show that this leads to faster and more robust learning.\nAuthors:\n Marc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien Perolat, David Silver, Thore Graepel\nIn this work, we first observe that independent reinforcement learners produce policies that can be jointly correlated, failing to generalize well during execution with other agents. We quantify this effect by proposing a new metric called joint policy correlation. We then propose an algorithm motivated by game-theoretic foundations, which generalises several previous approaches such as fictitious play, iterated best response, independent RL, and double oracle. We show that our algorithm can reduce joint policy correlation significantly in first-person coordination games, and finds robust counter-strategies in a common poker benchmark game.\nOur researchers will also lead and take part in a wide-range of workshops, tutorials and symposia during NIPS. \u00a0For the full schedule, including details of papers that we have collaborated on, please download our \nitinerary\n (PDF) or visit \nthe official website\n.\n"}
{"title": "Collaborating with patients for better outcomes", "contents": "Working as a doctor in the NHS for over 10 years, I felt that I had developed good understanding of how patients and their families felt when faced with an upsetting diagnosis or important health decision. I had been lucky with my own health, having only spent one night in hospital for what ended up being a false alarm. But when my son was born prematurely two years ago, I had a glimpse into what being on the other side feels like - an experience that has profoundly shaped my thinking today.\nIt wasn\u2019t until I was waiting to hear, rather than give, important health updates that I really understood the feeling of uncertainty and powerlessness that many patients and their families feel. It really put into perspective how important it is to involve patients, and their families and carers, in their own health - that care is not something \u2018done\u2019 to a patient, but rather, something that is shaped by everyone involved in the healthcare process.\nIn my first week at DeepMind Health, I was really impressed that one of my new colleagues (not a nurse or doctor) had set up a meeting so we could hear directly from a patient, \nMichael Wise\n, who ended up needing dialysis and a kidney transplant after a sudden and unexpected problem with his kidneys. Since then, we\u2019ve continued to increase our efforts to bring the patient\u2019s voice into our projects. Afterall, there is \ngood evidence\n that when doctors and patients work together, the \noutcomes are better\n.\nWe\u2019ve already learned a lot about how to go about this - and what not to do - and wanted to share some reflections that may be helpful for others undertaking the same journey we\u2019re on.\nTo build cutting edge and secure health technologies we always work with experts. This may be \na clinician\n or \nsome of the world\u2019s best cyber security experts\n. Similarly, when thinking about patient and public involvement and engagement (PPIE) in our work, there are experts who understand how to do PPIE well. In early 2016 we started speaking to a number of individuals and organisations with this expertise. We worked with the late Rosamund Snow, an incredible person and the patient editor of the British Medical Journal, to shape what a Patient and Public Involvement and Engagement (PPIE) strategy might look like. Rosamund was initially sceptical of our work in healthcare. She pushed us to be more self-critical and transparent about our work, and made a \nnumber of recommendations\n she felt would allow patients\u2019 voices to have the biggest possible impact on the work we do, such as having an entirely patient-led project, inviting patients to do internships at the DeepMind Health offices, and hiring a patient lead to feed directly into our work. We are currently working with patients to flesh out some of these ideas.\nRosamund passed away shortly after our first patient summit in September 2016 but she had introduced us to some of her colleagues, Sally Crowe and Paul Buchanan, with whom we continue to work.\nFor that summit, we invited patients, carers and members of the public into our offices in London to discuss Rosamund\u2019s recommendations. Attendees disagreed with the idea of having a single patient lead, with some feeling that a single person couldn\u2019t possibly represent the diversity and complexity of the patient community. Instead, they wanted a patient panel to be involved in every project we do, working alongside the design and clinician teams. This is something we\u2019re looking to build on.\nWe also received feedback in areas that we hadn\u2019t considered, such as the location of the event. While many patients really enjoyed seeing where we work, some participants found the DeepMind offices too intimidating. This was incredibly useful feedback to hear. Since then, we\u2019ve committed to holding subsequent events in a mixture of both our offices and more neutral locations, such as Friends House in London and the Renaissance Hotel in Manchester, which we used for patient roundtables in July 2017.\nAt those events, we continued to receive feedback on Rosamund\u2019s recommendations. Attendees asked us to reflect on the language we use regarding patient case studies. We often talk about \u2018patient stories\u2019 or \u2018patient testimonies\u2019 which some attendees felt didn\u2019t carry enough weight. Instead, they said we should use \u2018patient truths\u2019 to empower patients, and emphasise the authentic and authoritative voice patients add to the conversation. Similarly, they questioned the way we \u00a0talk about DeepMind Health. We usually describe our work as \u2018clinician led\u2019, which ignores the valuable input patients have given to our work. In future, they suggested we used either \u2018clinician led and patient centred\u2019 or \u2018co-led by clinicians and patients\u2019 to highlight the equal involvement of both groups.\nSimilarly, we received feedback on the role we played in the room. We heard that some attendees would feel awkward talking directly with DeepMind staff, so we worked with external facilitators to run our events in July, where we took a back-seat role. Playing an observer was really enlightening as attendees seemed far more free to speak their mind, but others felt that they wanted to talk to us more candidly. In our future events, we plan to balance the use of external facilitators with more involved methods where we can directly engage in a dialogue with patients themselves.\nWe will be working with patients to build out their valuable suggestions into core practices of DeepMind Health. If you\u2019re interested in seeing what happened in those July events, you can see watch this\n video \nof some the attendees sharing their thoughts on our work.\nWe have learnt a lot from other people and organisations, like the Coalition for Collaborative Care (C4CC), \nNational Voices\n and the Patients Association, to help us learn how to work with patients better. They have outlined what a successful and mutually respectful collaboration model looks like, emphasising clear communication, a transparent sense of direction, and a culture of honesty and value.\nWe\u2019ve already benefited from the thousands of hours of patient engagement, including one to one sessions, events at our offices, interactive workshops and events run by our NHS partners. We are excited to put all the feedback we\u2019ve gathered into practice, and have committed to extensive engagement in the months to come, including growing an online patient community to engage with us in real-time, hosting design sessions with patients, and bringing in patients to collaborate in our research partnerships\nThis week we have launched a new form on the patients\u2019 section of our website that allows people to sign up for a range of activities, from receiving a regular newsletter to taking part in design and development sessions. If you\u2019d like to get involved please visit our \n'For Patients' \npage.\n"}
{"title": "Stop, look and listen to the people you want to help", "contents": "\u2018I like to take things slow. Take it slowly and get it right first time,\u2019 one participant said, but was quickly countered by someone else around the table: \u2018But I\u2019m impatient, I want to see the benefits now.\u2019 This exchange neatly captures many of the conversations I heard at DeepMind Health\u2019s recent Collaborative Listening Summit. It also represents, in layman\u2019s terms, the debate that tech thinkers and policy-makers are having right now about the future of artificial intelligence.\nThe Collaborative Listening Summit brought together members of the public, patient representatives and stakeholder, and was facilitated by Ipsos MORI. The objective of the Summit: to explore how principles, co-created in earlier events with the public, patients and stakeholders, should govern DeepMind Health\u2019s operating practices and engagement with the NHS. These principles ranged from the technical \u2013 for example, how evidence should inform DeepMind\u2019s practice \u2013 to the societal \u2013 for example, operating in the best interests of society.\nThe challenge of how technology companies and the NHS should interact has had many of us, \nincluding myself\n, cautious about the risk of big technology firms leveraging their finance and power over an NHS that is under seemingly endless pressure. Despite our desire to see the NHS become more agile and innovative, the \u2018move fast and break things\u2019 mentality of big tech is something that the public and policy-makers are rightly wary of when lives are at stake.\nThese fears usually manifest themselves in headlines about protecting patient data, but the issues run far deeper than this. For example, algorithms have an increasingly large part to play in our everyday lives, deciding which film we might like to watch next or the adverts we see online, but undergo relatively little scrutiny. In health care specifically, algorithms are beginning to augment existing services, with NHS England predicting that \nalgorithms will soon handle one in three NHS 111 enquiries\n. Algorithms currently in place are relatively simple, and they need to be tested to ensure they are safe and effective. As the technology acquires greater decision-making capability, testing will become more challenging and more important. Frankly, regulators and policy-makers are still getting up to speed with these issues, so a cautious approach to regulation is sensible.\nThere is evidently a need for policy-makers to further deepen their understanding of what rules of engagement should be for tech companies. Initiatives like the government\u2019s new Centre for Data Ethics and Innovation, intended to promote ethical data innovation, provide a platform for this, as do the trailblazing efforts of the biomedical research community. However, if recent public trust traps like GM foods and care.data and even the Royal Free/DeepMind data-sharing agreement are to be avoided big tech companies must themselves take responsibility for understanding what their relationship with NHS organisations should be.\nDeepMind\u2019s past attempts to engage have been subject to some criticism and show how challenging, yet important, it is to get dialogue with the public right. However, I was reassured that the Summit was designed to impart understanding; researching what people think about an issue where DeepMind staff themselves are on uncertain ground. Of course, for this to make any difference, the research needs to be applied \u2013 and that means practising the principles in the real world.\nPrinciples such as \u2018do what is best for society\u2019 might seem trite and obvious especially as, in the real world, contracts may be put above ethics, as participants at the event rightly pointed out. So, if DeepMind and other big tech companies are to hold themselves to ethical principles around testing, transparency and societal benefit, they must show that these principles translate into ethical business practice. This will mean not working with NHS organisations where they feel they cannot hold themselves to these principles.\nAnd this will get harder as time goes on \u2013 currently there is more than enough interest in machine learning to keep DeepMind and its competitors busy. But as these technologies come on-stream and there is market share to be gained, it will become more and more tempting for DeepMind to break these principles, especially as other technology companies eye up the NHS for size. This is where policy re-enters the picture: data ethics initiatives need to catch up with this work and encourage other companies to undertake robust research with the public. After all, the NHS cannot become a world leader in artificial intelligence if doctors and managers cannot look patients in the eye and assure them that their data is not being used in ways that are contrary to their own values.\nParticipants at the workshop were cautious, especially around the need to safeguard their data, but they could also see the benefits that new technologies could have in terms of saving time and lives. Tech companies should not be scared of engaging with the public \u2013 but they must listen to the people whose lives they claim to be bettering.\n"}
{"title": "2017: DeepMind's year in review", "contents": "In July, the world number one Go player Ke Jie spoke after a streak of 20 wins. It was two months after he had played AlphaGo at the \nFuture of Go Summit in Wuzhen\n, China.\n\u201cAfter my match against AlphaGo, I fundamentally reconsidered the game, and now I can see that this reflection has helped me greatly,\u201d he \nsaid\n. \u201cI hope all Go players can contemplate AlphaGo\u2019s understanding of the game and style of thinking, all of which is deeply meaningful. Although I lost, I discovered that the possibilities of Go are immense and that the game has continued to progress.\u201d\nKe Jie is a master of the game and \nwe were honoured by his words\n. We were also inspired by them, because they hint at a future where society could use AI as a tool for discovery, uncovering new knowledge and increasing our understanding of the world. With \nmachine-aided science\n in particular, we hope that AI systems could help make progress on challenges from climate change and drug discovery, to finding complex new materials or helping ease the pressure on healthcare systems.\nThis potential for societal benefit is why we set up DeepMind, and we\u2019re excited to have made continued progress on some of the fundamental scientific challenges as well as on AI safety and ethics.\nThe approach we take at DeepMind is \ninspired by neuroscience\n, helping to make progress in critical areas such as \nimagination\n, \nreasoning\n, \nmemory\n and \nlearning\n. Take imagination, for example: this distinctively human ability plays a crucial part in our daily lives, allowing us to plan and reason about the future, but is hugely challenging for computers. We continue to work hard on this problem, this year introducing \nimagination-augmented agents\n that are able to extract relevant information from an environment in order to plan what to do in the future.\nThis neuroscience-inspired approach also created one of the \nmost popular demonstrations\n of our work, when we trained a neural network to control a variety of simplified body shapes in a simulated environment. This kind of sophisticated motor control is a hallmark of physical intelligence, and is a crucial part of our research programme. Although the resulting movements were wild and - at times - ungainly, they were also surprisingly successful and made for \nentertaining viewing\n.\nSeparately, we made progress in the field of generative models. Just over a year ago we presented \nWaveNet\n, a deep neural network for generating raw audio waveforms that was capable of producing better and more realistic-sounding speech than existing techniques. At that time, the model was a research prototype and was too computationally intensive to work in consumer products. Over the last 12 months, our teams managed to create a new model that was 1000x faster. In October, we revealed that this new \nParallel WaveNet\n is now being used in the real world, generating the \nGoogle Assistant\nvoices for US English and Japanese.\nThis is an example of the effort we invest in making it easier to build, train and optimise AI systems. Other techniques we worked on this year, such as \ndistributional reinforcement learning\n, \npopulation based training for neural networks\n and \nnew neural architecture search methods\n, promise to make systems easier to build, more accurate and quicker to optimise. We have also dedicated significant time to creating new and challenging environments in which to test our systems, including our work with Blizzard to \nopen up StarCraft II for research\n.\nBut we know that technology is not value neutral. We cannot simply make progress in fundamental research without also taking responsibility for the ethical and social impact of our work. This drives our research in critical areas such as interpretability, where we have been exploring novel methods to \nunderstand\n and \nexplain\n how our systems work. It\u2019s also why we have an established technical safety team that continued to develop \npractical ways\n to ensure that we can \ndepend on future systems\n and that they remain under meaningful human control.\nIn October we took another step by launching \nDeepMind Ethics & Society\n, a research unit that will help us explore and understand the real-world impacts of AI in order to achieve social good. Our research will be guided by \nFellows\n who are renowned experts in their fields - like philosopher Nick Bostrom, climate change specialist Christiana Figueres, leading researcher James Manyika, and economists Diane Coyle and Jeffrey Sachs.\nAI must be shaped by society\u2019s priorities and concerns, which is why we\u2019re working with \npartner organisations\n on events aimed at opening up the conversation about how AI should be designed and deployed. For example, \nJoy Buolamwini\n, who leads the Algorithmic Justice League, and experts from Article 36, Human Rights Watch, and the British Armed forces joined us for a session at Wired Live to discuss algorithmic bias and restricting the use of lethal autonomous weapons. As we\u2019ve \nsaid\n regularly this year, these issues are too important and their effects too wide-ranging to ignore.\nThat\u2019s also why we also need new spaces, both within and outside AI companies, for conversations about anticipating and directing the impacts of the technology. One example is the \nPartnership on AI\n, which we co-chaired this year, and which has been charged with bringing together industry competitors, academia and civil society to discuss key ethical issues. Over the past year, PAI has welcomed 43 new nonprofit and for-profit members and a new Executive Director, Terah Lyons. And in the next few months, we\u2019re looking forward to working with this group to examine a wide range of research themes, including bias and discrimination in algorithms, the impact of machine learning on automation and labour, and more.\nWe also believe in the importance of using our technology for practical social benefit, and continue to see amazing potential for real-world impact in health and energy. This year we agreed two new partnerships with NHS hospital trusts to deploy our Streams app, which \nsupports NHS clinicians\n using digital technology. We\u2019re also part of a consortium of leading research institutions that launched a \ngroundbreaking study\n to determine if cutting-edge machine learning technology could help improve the detection of breast cancer.\nIn parallel, we've also worked hard on the \noversight\n of our work in health. We \nwrote about the lessons learned\n from the Information Commissioner\u2019s findings about our original partnership with the Royal Free, and DeepMind Health\u2019s Independent Reviewers published their first open \nannual report\n on our work. Their scrutiny makes our work better. We\u2019ve made major improvements to our \nengagement\n with patients and the public, including \nworkshops \nwith patients and carers, and we\u2019re also exploring technical ways of building trust into our systems, such as the \nverifiable data audit\n, which we plan to release as an open-source tool.\nWe are proud of all of our progress in 2017, but know there is still a long way to go.\nFive months after we played Ke Jie in Wuzhen and retired AlphaGo from competitive play, we published our fourth \nNature paper\n for a new version of the system, known as \nAlphaGo Zero\n, which uses no human knowledge. Over the course of millions of games, the system progressively learned the game of Go from scratch, accumulating thousands of years of knowledge in just a few days. In doing so, it also uncovered unconventional strategies and \nrevealed new knowledge\n about this ancient game.\nOur belief is that AI will be able to do the same for other complex problems, as a scientific tool and a multiplier for human ingenuity. The AlphaGo team are already working on the next set of grand challenges and we hope the moments of algorithmic inspiration they helped to create are just the beginning.\n"}
{"title": "Game-theory insights into asymmetric multi-agent games", "contents": "As AI systems start to play an increasing role in the real world it is important to understand how different systems will interact with one another. \u00a0\nIn our \nlatest paper\n, published in the journal \nScientific Reports\n, we use a branch of \ngame theory\n to shed light on this problem. In particular, we examine how two intelligent systems behave and respond in a particular type of situation known as an \nasymmetric game\n, which include Leduc poker and various board games such as \nScotland Yard\n. Asymmetric games also naturally model certain real-world scenarios such as automated auctions where buyers and sellers operate with different motivations. Our results give us new insights into these situations and reveal a surprisingly simple way to analyse them. While our interest is in how this theory applies to the interaction of multiple AI systems, we believe the results could also be of use in economics, evolutionary biology and empirical game theory among others.\nGame theory is a field of mathematics that is used to analyse the strategies used by decision makers in competitive situations. It can apply to humans, animals, and computers in various situations but is commonly used in AI research to study \u201cmulti-agent\u201d environments where there is more than one system, for example several household robots cooperating to clean the house. Traditionally, the evolutionary dynamics of multi-agent systems have been analysed using simple, \nsymmetric games\n, such as the classic \nPrisoner\u2019s Dilemma\n, where each player has access to the same set of actions. Although these games can provide useful insights into how multi-agent systems work and tell us how to achieve a desirable outcome for all players - known as the Nash equilibrium - \u00a0they cannot model all situations.\nOur new technique allows us to quickly and easily identify the strategies used to find the Nash equilibrium in more complex asymmetric games - \u00a0characterised as games where each player has different strategies, goals and rewards. These games - and the new technique we use to understand them - can be illustrated using an example from \u2018Battle of the Sexes\u2019, a coordination game commonly used in game theory research.\nHere, two players have to coordinate a night out to either the opera or the movies. One of \u00a0the players has a slight preference for the opera and one of them has a slight preference for the movies. The game is asymmetric because, while both players have access to the same options, the corresponding rewards for each are different based on the players preferences. In order to maintain their friendship - or equilibrium - the players should choose the same activity (hence the zero payoff for separate activities). \nThis game has three equilibria: (i) both players deciding to go to the opera, (ii) both deciding to go to the movies, and (iii) a final, mixed option, where each player will opt for their preferred option three fifths of the time. This last option, which is said to be \u201cunstable\u201d, \u00a0can be rapidly uncovered using our method by simplifying - or decomposing - the asymmetric game into its symmetric counterparts. These counterpart games essentially considers the reward table of each player as a separate symmetric 2-player game with equilibrium points that coincide with the original asymmetric game.\nIn the plot below, the Nash equilibrium is plotted for the two, simple counterparts allowing us to quickly identify the optimal strategy in the asymmetrical game (a). The reverse can also be done, using the asymmetrical game to identify the equilibrium in its symmetrical counterparts.\nThis method can also be applied to other games, including Leduc poker, which is described in detail in the paper. In all of these situations, the method proves to be mathematically simple, allowing a rapid and straightforward analysis of asymmetric games that we hope will also help our understanding of various dynamic systems, including multi-agent environments.\nRead the original Scientific Reports paper \nhere\n.\nRead the follow-up AAMAS paper \nhere\n.\nThe Scientific Reports paper is authored by Karl Tuyls, Julien Perolat, Marc Lanctot, Georg Ostrovski, \u00a0Rahul Savani, \u00a0Joel Leibo, Toby Ord, Thore Graepel and Shane Legg. The AAMAS paper \u00a0is authored by \u00a0Karl Tuyls, Julien Perolat, Marc Lanctot, Joel Leibo and Thore Graepel.\nUPDATE 20/03/18:\n Our \nlatest paper\n, forthcoming at the Autonomous Agents and Multi-Agent Systems conference (AAMAS), builds on the Scientific Reports paper outlined above. \u00a0\nA Generalised Method for Empirical Game Theoretic Analysis\n introduces a general method to perform empirical analysis of multi-agent interactions, both in symmetric and asymmetric games. The method allows to understand how multi-agent strategies interact, what the attractors are and what the basins of attraction look like, giving an intuitive understanding for the strength of the involved strategies. Furthermore, it explains how many data samples to consider in order to guarantee that the equilibria of the approximating game are sufficiently reliable. \u00a0We apply the method to several domains, including AlphaGo, Colonel Blotto and Leduc poker.\n"}
{"title": "Open-sourcing Psychlab", "contents": "Consider the simple task of going shopping for your groceries. If you fail to pick-up an item that is on your list, what does it tell us about the functioning of your brain? It might indicate that you have difficulty shifting your attention from object to object while searching for the item on your list. It might indicate a difficulty with remembering the grocery list. Or it could it be something to do with executing both skills simultaneously.\nWhat appears to be a single task actually depends on multiple cognitive abilities. We face a similar problem in AI research, where the complexity of a task can often make it difficult to tease apart the individual skills required for an agent to be successful. But understanding an agent\u2019s specific cognitive skill set may prove useful for improving its overall performance.\nTo address this problem in humans, psychologists have spent the last 150 years designing rigorously controlled experiments aimed at isolating one specific cognitive faculty at a time. For example, they might analyse the supermarket scenario using two separate tests - a \u201cvisual search\u201d test that requires the subject to locate a specific shape in a pattern could be used to probe attention, while they might ask a person to recall items from a studied list to test their memory.\nWe believe it is possible to use similar experimental methods to better understand the behaviours of artificial agents. That is why we developed Psychlab, a platform built on top of \nDeepMind Lab\n, which allows us to directly apply methods from fields like cognitive psychology to study behaviours of artificial agents in a controlled environment. Today, we are also open-sourcing this platform for others to use.\nPsychlab recreates the set-up typically used in human psychology experiments inside the virtual DeepMind Lab environment. This usually consists of a participant sitting in front of a computer monitor using a mouse to respond to the onscreen task. Similarly, our environment allows a virtual subject to perform tasks on a virtual computer monitor, using the direction of its gaze to respond. This allows humans and artificial agents to both take the same tests, minimising experimental differences. It also makes it easier to connect with the existing literature in cognitive psychology and draw insights from it.\nAlong with the open-source release of Psychlab we have built a series of classic experimental tasks to run on the virtual computer monitor, and it has a flexible and easy-to-learn API, enabling others to build their own tasks.\nEach of these tasks have been validated to show that our human results mirror standard results in the cognitive psychology literature.\nTake the \u2018visual search\u2019 task for example. The ability to locate an object among a complex array of stimuli, like one item on a supermarket shelf, has been studied as a way of understanding selective attention in humans.\nWhen humans are given the task of searching `for a vertically oriented bar among horizontal bars\u2019 and \u2018searching for a pink bar among bars of other colours\u2019 their reaction times don\u2019t change according to the numbers of items on the screen. In other words, their reaction times are independent of 'set size'. However, when the task is to search for a pink bar among different shaped and different coloured bars, human reaction times increase by approximately 50ms with each additional bar. When humans did this task on Psychlab, we replicated this result.\nWhen we did the same test on a state-of-the-art artificial agent, we found that, while it could perform the task, it did not show the human pattern of reaction time results. It used the same amount of time to respond in all three cases. In humans, this data has suggested a difference between \nparallel and serial attention\n. Agents appear only to have parallel mechanisms. Identifying this difference between humans and our current artificial agents shows a path toward improving future agent designs. \u00a0\nPsychlab was designed as a tool for bridging between cognitive psychology, neuroscience, and AI. By open-sourcing it, we hope the wider research community will make use of it in their own research and help us shape it going forward.\nRead the paper\n.\nDownload the code on GitHub\n.\n"}
{"title": "Scalable agent architecture for distributed training", "contents": "Deep Reinforcement Learning (DeepRL) has achieved remarkable success in a range of tasks, from continuous control problems in robotics to playing games like Go and Atari. The improvements seen in these domains have so far been limited to individual tasks where a separate agent has been tuned and trained for each task.\nIn our most recent work, we explore the challenge of training a single agent on many tasks.\nToday we are releasing DMLab-30, a set of new tasks that span a large variety of challenges in a visually unified environment with a common action space. Training an agent to perform well on many tasks requires massive throughput and making efficient use of every data point. To this end, we have developed a new, highly scalable agent architecture for distributed training called Importance Weighted Actor-Learner Architecture that uses a new off-policy correction algorithm called V-trace.\nDMLab-30 is a collection of new levels designed using our open source RL environment \nDeepMind Lab\n. These environments enable any DeepRL researcher to test systems on a large spectrum of interesting tasks either individually or in a multi-task setting.\nThe tasks are designed to be as varied as possible. They differ in the goals they target, from learning, to memory, to navigation. They vary visually, from brightly coloured, modern-styled texture, to the subtle brown and greens of a desert at dawn, midday, or by night. And they contain physically different settings, from open, mountainous terrain, to right-angled mazes, to open, circular rooms.\nIn addition, some of the environments include \u2018bots\u2019, with their own, internal, goal-oriented behaviours. Equally importantly, the goals and rewards differ across the different levels, from following language commands and using keys to open doors, foraging mushrooms, to plotting and following a complex irreversible path.\nHowever, at a basic level, the environments are all the same in terms of their action and observation space allowing a single agent to be trained to act in every environment in this highly varied set. More details about the environments can be found on the \nDeepMind Lab GitHub page\n.\nIn order to tackle the challenging DMLab-30 suite, we developed a new distributed agent called Importance Weighted Actor-Learner Architecture that maximises data throughput using an efficient distributed architecture with \nTensorFlow\n.\nImportance Weighted Actor-Learner Architecture is inspired by the popular \nA3C\n architecture which uses multiple distributed actors to learn the agent\u2019s parameters. In models like this, each of the actors uses a clone of the policy parameters to act in the environment. Periodically, actors pause their exploration to share the gradients they have computed with a central parameter server that applies updates (see figure below).\nImportance Weighted Actor-Learner Architecture actors on the other hand are not used to calculate gradients. Instead, they are just used to collect experience which is passed to a central learner that computes gradients, resulting in a model that has completely independent actors and learners. To take advantage of the scale of modern computing systems, Importance Weighted Actor-Learner Architectures can be implemented using a single learner machine or multiple learners performing synchronous updates between themselves. Separating the learning and acting in this way also has the advantage of increasing the throughput of the whole system since the actors no longer need to wait for the learning step like in architectures such as batched A2C. This allows us to train Importance Weighted Actor-Learner Architectures on interesting environments without suffering from variance in frame rendering-time or time consuming task restarts.\nHowever, decoupling the acting and learning causes the policy in the actor to lag behind the learner. In order to compensate for this difference we introduce a principled off-policy advantage actor critic formulation called V-trace which compensates for the trajectories obtained by actors being off policy. The details of the algorithm and its analysis can be found in our \npaper\n.\nThanks to the optimised model of Importance Weighted Actor-Learner Architecture, it can process one-to-two orders of magnitude more experience compared to similar agents, making learning in challenging environments possible. We have compared Importance Weighted Actor-Learner Architectures with several popular actor-critic methods and have seen significant speed-ups. Additionally, the throughput using Importance Weighted Actor-Learner Architectures scales almost linearly with increasing number of actors and learners which shows that both the distributed agent model and the V-trace algorithm can handle very large scale experiments, even on the order of thousands of machines.\nWhen it was tested on the DMLab-30 levels, Importance Weighted Actor-Learner Architecture was 10 times more data efficient and achieved double the final score compared to distributed A3C. \u00a0Moreover, Importance Weighted Actor-Learner Architectures showed positive transfer from training in multi-task settings compared to training in single-task setting.\nRead the full Importance Weighted Actor-Learner Architectures paper \nhere\n.\nExplore DMLab-30 \nhere\n.\nThis work was done by Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg and Koray Kavukcuoglu\n"}
{"title": "Royal Free London publishes findings of legal audit in use of Streams", "contents": "Last July, the Information Commissioner concluded an investigation into the use of the Streams app at the Royal Free London NHS Foundation Trust. As part of the investigation the Royal Free signed up to a set of undertakings \u2013 one of which was to commission a third party to audit the Royal Free\u2019s current data processing arrangements with DeepMind, to ensure that they fully complied with data protection law and respected the privacy and confidentiality rights of its patients.You can read the full report on the Royal Free\u2019s website \nhere\n, and the Information Commissioner\u2019s Office\u2019s response \nhere\n. The report also has three recommendations that relate to DeepMind Health:\nWe look forward to continuing our collaboration with the Royal Free to \nimprove care for patients\n. \u00a0You can read more about the way in which we process and protect patient data \nhere\n and you can read our previous blog about what we\u2019ve learned \nhere\n.\n"}
{"title": "Open sourcing Sonnet - a new library for constructing neural networks", "contents": "It\u2019s now nearly a year since DeepMind \nmade the decision to switch the entire research organisation to using TensorFlow (TF)\n. It\u2019s proven to be a good choice - many of our models learn significantly faster, and the built-in features for distributed training have hugely simplified our code. Along the way, we found that the flexibility and adaptiveness of TF lends itself to building higher level frameworks for specific purposes, and we\u2019ve written one for quickly building neural network modules with TF. We are actively developing this codebase, but what we have so far fits our research needs well, and we\u2019re excited to announce that today we are open sourcing it. We call this framework \nSonnet\n.\nSince its initial launch in November 2015, a diverse ecosystem of higher level libraries has sprung up around \nTensorFlow\n enabling common tasks to be accomplished quicker. Sonnet shares many similarities with some of these existing neural network libraries, but has some features specifically designed around our research requirements. The \ncode release\n accompanying our \nLearning to learn paper\n included a preliminary version of Sonnet, and other forthcoming code releases will be built on top of the full library we are releasing today.\nMaking Sonnet public allows other models created within DeepMind to be easily shared with the community, and we also hope that the community will use Sonnet to take their own research forwards. In recent months we\u2019ve also open-sourced our flagship platform \nDeepMind Lab\n, and are currently working with \nBlizzard to develop an open source API that supports AI research in StarCraft II\n. There are many more releases to come, and they\u2019ll all be shared on our new \nOpen Source page\n.\nThe library uses an object-oriented approach, similar to Torch/NN, allowing modules to be created which define the forward pass of some computation. Modules are \u2018called\u2019 with some input Tensors, which adds ops to the Graph and returns output Tensors. One of the design choices was to make sure the variable sharing is handled transparently by automatically reusing variables on subsequent calls to the same module.\nMany models in the literature can naturally be considered as a hierarchy - e.g. a Differentiable Neural Computer contains a controller which might be an LSTM, which can be implemented as containing a standard Linear layer. We\u2019ve found that writing code which explicitly represents submodules allows easy code reuse and quick experimentation - Sonnet promotes writing modules which declare other submodules internally, or are passed other modules at construction time.\nA final technique we\u2019ve found very useful is to allow certain modules to operate on arbitrarily nested groups of Tensors. Recurrent Neural Network states are often best represented as a collection of heterogeneous Tensors, and representing these as a flat list can be error prone. Sonnet provides utilities to deal with these arbitrary hierarchies, so that changing your experiment to use a different kind of RNN does not require tedious code changes. We\u2019ve made changes to core TF as well to better support this use case.\nSonnet is designed specifically to work with TensorFlow, and as such does not prevent you from accessing the underlying details such as Tensors and variable_scopes. Models written in Sonnet can be freely mixed with raw TF code, and that in other high level libraries.\nThis is not a one-time release - we will regularly update the Github repository to match our in-house version. We\u2019ve got lots of ideas for new features in the works, which will be made available when ready. We are very excited about contributions from the community.\nView Sonnet on \nGithub\n"}
{"title": "Distill: Communicating the science of machine learning", "contents": "Like every field of science, the importance of clear communication in machine learning research cannot be over-emphasised: it helps to drive forward the state-of-the art by allowing the research community to share, discuss and build upon new findings.\nFor this reason, we at DeepMind are enthusiastic supporters of \nDistill\n, a new independent, web-based medium for clear and open - \ndemystified\n - machine learning research, comprising a \njournal\n, \nprizes\n recognising outstanding work, and \ntools\n to create interactive essays.\nThe machine learning community has always embraced new forms of scientific communication. Today, our science and practice is communicated through papers published in traditional journals, hosted on arXiv, supported by code repositories and community-driven efforts such as \nJMLR\n and \nJAIR\n, at conferences and through surveys, posters, blog posts, videos, demos, podcasts and interviews.\nIn this tradition, Distill makes its own unique contribution. Drawing on modern web technologies, it provides a new way to learn and understand machine learning by promoting interactive, vivid and engaging exposition, and by recognising the invaluable contributions of those who make the time to remove the mystery - and reveal the importance - of even the most seemingly obscure results.\nDeepMind is proud to be a contributing sponsor of the \nDistill prize\n, an annual prize aimed at recognising outstanding work communicating and refining ideas in machine learning, and Shakir Mohamed is a member of the journal's steering committee. Ultimately, our desire is to support fresh and diverse thinking in machine learning research - to play our part, quoting \nWilliam Zinnsser\n, in creating a community of people \u2018finding a common thread of humanity between themselves and their speciality and their readers\u2019.\n"}
{"title": "Innovations of AlphaGo", "contents": "One of the great promises of AI is its potential to help us unearth new knowledge in complex domains. We\u2019ve already seen exciting glimpses of this, when our algorithms found ways to dramatically improve energy use in \ndata centres\n - as well as of course with our program AlphaGo.\nSince its historic success in Seoul last March, AlphaGo has heralded a new era for the ancient game of Go. Thanks to AlphaGo's creative and intriguing revelations, players of all levels have been inspired to test out new moves and strategies of their own, often re-evaluating centuries of inherited knowledge in the process.\nAhead of \u2018\nThe Future of Go Summit in Wuzhen\n\u2019, we summarise some recent examples of AlphaGo\u2019s strategic and tactical innovations, and the new insights they have revealed.\nAlphaGo's greatest strength is not any one move or sequence, but rather the unique perspective that it brings to every game. While Go style is difficult to encapsulate, one could say that AlphaGo's strategy embodies a spirit of flexibility and open-mindedness: a lack of preconceptions that allows it to find the most effective line of play. As the following two games will show, this philosophy often leads AlphaGo to discover counterintuitive yet powerful moves.\nAlthough Go is a game of territory, most decisive battles hinge on the balance of power between groups, and AlphaGo excels in shaping this balance. Specifically, AlphaGo makes masterful use of \"influence,\" or the effect of existing stones on surrounding areas. Although influence cannot be measured exactly, AlphaGo's value network enables it to consider all stones on the board at once, endowing its judgment with subtlety and precision. These abilities let AlphaGo convert local regions of influence into coordinated global advantages.\nIn this game (Dia. 1), Black (AlphaGo) has little secure territory, while White has three corners, but Black's influence radiates across the entire board. In particular, while the marked exchange solidifies White, it also improves Black's potential. Go players usually shy from such exchanges, which pay a definite price for uncertain profit, but AlphaGo combines its sterling judgment with a keen sense of risk and reward to make such moves possible.\nHowever, the value of influence depends entirely on context, and AlphaGo relinquishes influence freely when it can be effectively mitigated. In the the game displayed in Dia. 2, one of the most surprising in its oeuvre, AlphaGo has just played an incredible six stones along the second line. Go players have a saying: on the fourth line there is influence, and on the third line there is territory, but on the second line there is only defeat. AlphaGo's play at first looks deserving of such censure, as these moves give White both strength and influence in exchange for Black's paltry 4 points of side territory. Most players, unwilling to bear the ignominy of playing the marked stones, would reject this line in an instant. Yet AlphaGo judges it worthwhile to keep White's stones separated, and in the following exchanges, slowly erodes White's influence from the top and bottom to secure a winning advantage.\n\u200d\nAlphaGo has also played several opening novelties in its recent games, the most salient being the early 3-3 invasion and a new variation of the \"Magic Sword\". Each defies conventional theory, but proves sound on deeper inspection.\nOne of the most territorial joseki (corner sequences) in Go is the 3-3 point invasion, shown in Dia. 3.\nThis invasion immediately secures the corner, but the textbook sequence shown in Dia. 4 has long been disparaged as unsuitable for the opening, as it gives too much influence.\nAlphaGo's innovation is to omit the marked exchanges, leaving the corner unsettled as shown in Dia. 5.\nThough slightly less secure, Black retains miai (options) to escape on the left or finish the joseki later, and has gained territory while ceding only moderate influence. This strategy has created a great stir among professionals, and at least one has already tried it in an official game (Dia. 6).\nOriginally trained on human data, AlphaGo knows modern joseki and usually plays accordingly. However, in the \"Magic Sword,\" a famously complex joseki family named for the cursed sword of Muramasa, it diverges. \nStarting from the position in Dia. 7, the usual result exchanges the corner for the side as shown in Dia. 8.\nHowever, AlphaGo often prefers to sacrifice outside access for territorial compensation (Dia. 9).\nMost Go players would not consider playing this variation, as it gives Black a powerful wall, but White's follow-up approach declares that Black's influence is not as valuable as it looks. If Black does not reinforce the wall, it may even become a target. Kim Jiseok, one of Korea's top professionals, recently played this line in a tournament game (Dia. 10), which he went on to win.\nAlphaGo's innovations show great potential for impact in the world of professional Go, and we hope to present many more opportunities for collaborative research at the upcoming Future of Go Summit in Wuzhen. We look forward with great excitement to AlphaGo and human professionals striving together to discover the true nature of Go.\n"}
{"title": "Exploring the mysteries of Go with AlphaGo and China's top players", "contents": "Just over a year ago, we saw a major milestone in the field of artificial intelligence: \nDeepMind\u2019s AlphaGo took on and defeated one of the world\u2019s top Go players\n, the legendary Lee Sedol. Even then, we had no idea how this moment would affect the 3,000 year old game of Go and the growing global community of devotees to this beautiful board game.\nInstead of diminishing the game, as some feared, artificial intelligence (A.I.) has actually made human players stronger and more creative. It\u2019s humbling to see how pros and amateurs alike, who have pored over every detail of AlphaGo\u2019s innovative game play, have actually learned new knowledge and strategies about perhaps the most studied and contemplated game in history. You can read more about some of these creative strategies \nin this blog post.\nClearly, there remains much more to learn from this partnership between Go\u2019s best human players and its most creative A.I. competitor. That\u2019s why we\u2019re so excited to announce AlphaGo\u2019s next step: a five-day festival of Go and artificial intelligence in the game's birthplace, China.\nFrom May 23-27, we\u2019ll collaborate with the China Go Association and Chinese Government to bring AlphaGo, China\u2019s top Go players, and leading A.I. experts from Google and China together in Wuzhen, one of the country\u2019s most beautiful water towns, for the \u201cFuture of Go Summit.\u201d\nThe summit will feature a variety of game formats involving AlphaGo and top Chinese players, specifically designed to explore the mysteries of the game together. The games will include: \nInterspersed with the games will be a forum on the \u201cFuture of A.I.\u201d Together with some of China\u2019s leading experts in the field, we will explore how AlphaGo has created new knowledge about the oldest of games, and how the technologies behind AlphaGo, machine learning, and artificial intelligence, are bringing solutions to some of the world\u2019s greatest challenges into reach.\nAlready, some of the machine learning methods behind AlphaGo have been used to tackle significant problems, such as \nreducing energy use\n. Machine learning technology is also at work in a series of exciting \nmedical\n \nresearch\n \nprojects\n. And across many of Google\u2019s products, machine learning has suddenly made the impossible real\u2014from allowing people using \nGoogle Photos\n to find that photo of their dog in the snow almost instantly to improving the quality of \nGoogle Translate\n more in a single leap than the past 10 years of improvements combined.\nWe\u2019re excited to see what insights this next round of games and discussion will bring, and the challenges this will help us solve together\u2014both on and off the Go board.\n"}
{"title": "AlphaGo's next move", "contents": "With just three stones on the board, it was clear that this was going to be \nno ordinary game of Go\n.\nChinese Go Grandmaster and world number one Ke Jie departed from his typical style of play and opened with a \u201c3:3 point\u201d strategy - a highly unusual approach aimed at quickly claiming corner territory at the start of the game. The placement is rare amongst Go players, but it\u2019s a favoured position of our program AlphaGo. Ke Jie was playing it at its own game.\nKe Jie\u2019s thoughtful positioning of that single black stone was a fitting motif for the opening match of \nThe Future of Go Summit in Wuzhen, China\n, an event dedicated to exploring the truth of this beautiful and ancient game. Over the last five days we have been honoured to witness games of the highest calibre.\nWe have always believed in the potential for AI to help society discover new knowledge and benefit from it, and AlphaGo has given us an early glimpse that this may indeed be possible. More than a competitor, AlphaGo has been a tool to inspire Go players to try new strategies and uncover new ideas in this 3,000 year-old game.\nThe creative moves \nit played against the legendary Lee Sedol in Seoul in 2016\n brought completely new knowledge to the Go world, while the unofficial online games it \nplayed under the moniker Magister (Master)\n earlier this year have influenced many of Go\u2019s leading professionals - including the genius Ke Jie himself. Events like this week\u2019s Pair Go, in which two of the world\u2019s top players partnered with AlphaGo, showed the great potential for people to use AI systems to generate new insights in complex fields.\nThis week\u2019s series of thrilling games with the world\u2019s best players, in the country where Go originated, has been the highest possible pinnacle for AlphaGo as a competitive program. For that reason, the Future of Go Summit is our final match event with AlphaGo.\nThe research team behind AlphaGo will now throw their energy into the next set of grand challenges, developing advanced general algorithms that could one day help scientists as they tackle some of our most complex problems, such as finding new cures for diseases, dramatically reducing energy consumption, or inventing revolutionary new materials. If AI systems prove they are able to unearth significant new knowledge and strategies in these domains too, the breakthroughs could be truly remarkable. We can\u2019t wait to see what comes next.\nWhile AlphaGo is stepping back from competitive play, it\u2019s certainly not the end of our work with the Go community, to which we owe a huge debt of gratitude for their encouragement and motivation over the past few years. \nWe plan to publish one final academic paper later this year that will detail the extensive set of improvements we made to the algorithms\u2019 efficiency and potential to be generalised across a broader set of problems. Just like our first AlphaGo paper, we hope that other developers will pick up the baton, and use these new advances to build their own set of strong Go programs.\nWe\u2019re also working on a teaching tool - one of the top requests we\u2019ve received throughout this week. The tool will show AlphaGo\u2019s analysis of Go positions, providing an insight into how the program thinks, and hopefully giving all players and fans the opportunity to see the game through the lens of AlphaGo. We\u2019re particularly honoured that our first collaborator in this effort will be the great Ke Jie, who has agreed to work with us on a study of his match with AlphaGo. We\u2019re excited to hear his insights into these amazing games, and to have the chance to share some of AlphaGo\u2019s own analysis too.\nFinally, to mark the end of the Future of Go Summit, we wanted to give a special gift to fans of Go around the world. Since our match with Lee Sedol, AlphaGo has become its own teacher, playing millions of high level training games against itself to continually improve. We\u2019re now publishing a special set of 50 AlphaGo vs AlphaGo games, played at full length time controls, which we believe contain many new and interesting ideas and strategies.\nWe took the opportunity this week in Wuzhen to show some of these games to a handful of top professionals. Shi Yue, 9 Dan Professional and World Champion said the games were \u201cLike nothing I\u2019ve ever seen before - they\u2019re how I imagine games from far in the future.\u201d Gu Li, 9 Dan Professional and World Champion, said that \u201cAlphaGo\u2019s self play games are incredible - we can learn many things from them.\u201d We hope that all Go players will now enjoy trying out some of the moves in the set. The first ten games are now available\n here\n, and we\u2019ll publish another ten each day until all 50 have been released.\nWe have been humbled by the Go community\u2019s reaction to AlphaGo, and the way professional and amateur players have embraced its insights about this ancient game. We plan to bring that same excitement and insight to a range of new fields, and try to address some of the most important and urgent scientific challenges of our time. We hope that the story of AlphaGo is just the beginning.\nRead more about The Future of Go Summit\n\u200d\nRead more about AlphaGo\n\u200d\nDiscover the AlphaGo self-play games\n"}
{"title": "A neural approach to relational reasoning", "contents": "Consider the reader who pieces together the evidence in an Agatha Christie novel to predict the culprit of the crime, a child who runs ahead of her ball to prevent it rolling into a stream or even a shopper who compares the relative merits of buying kiwis or mangos at the market.\n\u200d\nWe carve our world into relations between things. And we understand how the world works through our capacity to draw logical conclusions about how these different things - such as physical objects, sentences, or even abstract ideas - are related to one another. This ability is called relational reasoning and is central to human intelligence.\nWe construct these relations from the cascade of unstructured sensory inputs we experience every day. For example, our eyes take in a barrage of photons, yet our brain organises this \u201cblooming, buzzing confusion\u201d into the particular entities that we need to relate.\nA key challenge in developing artificial intelligence systems with the flexibility and efficiency of human cognition is giving them a similar ability - \u00a0to reason about entities and their relations from unstructured data. Solving this would allow these systems to generalize to new combinations of entities, making infinite use of finite means.\nModern deep learning methods have made tremendous progress solving problems from unstructured data, but they tend to do so without explicitly considering the relations between objects.\nIn two new papers, we explore the ability for deep neural networks to perform complicated relational reasoning with unstructured data. In the first paper - \nA simple neural network module for relational reasoning\n - we describe a Relation Network (RN) and show that it can perform at superhuman levels on a challenging task. While in the second paper - \u00a0\nVisual Interaction Networks\n \u00a0- we describe a general purpose model that can predict the future state of a physical object based purely on visual observations.\nTo explore the idea of relational reasoning more deeply and to test whether it is an ability that can be easily added to existing systems, we created a simple-to-use, plug-and-play RN module that can be added to existing neural network architectures. An RN-augmented network is able to take an unstructured input - say, an image or a series of sentences - and implicitly reason about the relations of objects contained within it.\nFor example, a network using RN may be presented with a scene consisting of various shapes (spheres, cubes, etc.) sitting on a table. To work out the relations between them \u00a0(e.g. the sphere is bigger than the cube), the network must take the unstructured stream of pixels from the image and figure out what counts as an object in the scene. The network is not explicitly told what counts as an object and must figure it out for itself. The representations of these objects are then grouped \u00a0into pairs (e.g. the sphere and the cube) and passed through the RN module, which compares them to establish a \u201crelation\u201d (e.g. the sphere is bigger than the cube). These relations are not hardcoded, but must be learnt by the RN as it compares each possible pair. Finally, it adds up all these relations to produce an output for all of the pairs of shapes in the scene.\nWe tested this model on several tasks including \nCLEVR\n - \u00a0a visual question answering task designed to explicitly explore a model\u2019s ability to perform different types of reasoning, such as counting, comparing, and querying. CLEVR consists of images like this:\nEach image has associated questions that interrogates the relations between objects in the scene. For example, a question about the image above might ask: \u201cThere is a tiny rubber thing that is the same colour as the large cylinder; what shape is it?\nState-of-the-art results on CLEVR using standard visual question answering architectures are 68.5%, compared to 92.5% for humans. But using our RN-augmented network, we were able to show super-human performance of 95.5%.\nTo check the versatility of the RN, we also tested the RN on a very different language task. Specifically, we used the \nbAbI suite\n - a series of of text-based question answering tasks. bAbI consists of a number of stories, which are a variable number of sentences culminating in a question. For example, \u201cSandra picked up the football\u201d and \u201cSandra went to the office\u201d may lead to the question \u201cWhere is the football?\u201d (answer: \u201coffice\u201d).\nThe RN-augmented network scored more than 95% on 18 of the 20 bAbI tasks, similar to existing state-of-the-art models. Notably, it scored better on certain tasks - such as induction - which caused problems for these more established models.\nFull results of all these tests and more are available \nin the paper\n.\nAnother key part of relational reasoning involves predicting the future in a physical scene. From just a glance, humans can infer not only what objects are where, but also what will happen to them over the upcoming seconds, minutes and even longer in some cases. For example, if you kick a football against a wall, your brain predicts what will happen when the ball hits the wall and how their movements will be affected afterwards (the ball will ricochet at a speed proportional to the kick and - in most cases - the wall will remain where it is).\nThese predictions are guided by a sophisticated cognitive system for reasoning about objects and their physical interactions.\nIn this related work we developed the \u201cVisual Interaction Network\u201d (VIN) - a model that mimics this ability. The VIN is able to infer the states of multiple physical objects from just a few frames of video, and then use this to predict object positions many steps into the future. This differs from generative models, which might visually \u201cimagine\u201d the next few frames of a video. Instead, the VIN predicts how the underlying relative states of the objects evolve.\nThe VIN is comprised of two mechanisms: a visual module and a physical reasoning module. Together they are able to process a visual scene into a set of distinct objects and learn an implicit system of physical rules which can predict what will happen to these objects in the future.\nWe tested the VIN\u2019s ability to do this in a variety of systems including bouncing billiards, masses connected by springs, and planetary systems with gravitational forces. Our results show that the VIN can accurately predict what will happen to objects hundreds of steps into the future.\nIn experimental comparisons with previously published models and variants of the VIN in which its mechanism for relational reasoning was removed, the full VIN performed significantly better.\nAgain, full details of the results can be found \nin our paper\n.\nBoth of these papers show promising approaches to understanding the challenge of relational reasoning. They show how neural networks can be given a powerful ability to reason by decomposing the world into systems of objects and their relations, allowing them to generalise to new combinations of objects and reason about scenes that superficially might look very different but have underlying common relations.\nWe believe these approaches are scalable and could be applied to many more tasks, helping build more sophisticated models of reasoning and allowing us to better understand a key component of humans\u2019 powerful and flexible general intelligence that we take for granted every day. \u00a0\nThe Relation Network was developed by Adam Santoro, David Raposo, David G.T. Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia and Timothy Lillicrap\nThe Visual Interaction Network was developed by Nicholas Watters, Daniel Zoran, Theophane Weber, Peter Battaglia, Razvan Pascanu and Andrea Tachetti\n"}
{"title": "Interpreting Deep Neural Networks using Cognitive Psychology", "contents": "Deep neural networks have learnt to do an amazing array of tasks - from recognising and reasoning about objects in images to playing Atari and Go at super-human levels. As these tasks and network architectures become more complex, the solutions that neural networks learn become more difficult to understand.\nThis is known as the \u2018black-box\u2019 problem, and it is becoming increasingly important as neural networks are used in more and more real world applications.\nAt DeepMind, we are working to expand the toolkit for understanding and interpreting these systems. In \nour latest paper\n, recently accepted at ICML, we proposed a new approach to this problem that employs methods from cognitive psychology to understand deep neural networks. Cognitive psychology measures behaviour to infer mechanisms of cognition, and contains a vast literature detailing such mechanisms, along with experiments for verifying them. As our neural networks approach human level performance on specific tasks, methods from cognitive psychology are becoming increasingly relevant to the black-box problem.\nTo demonstrate this point, our paper reports a case study where we used an experiment designed to elucidate human cognition to help us understand how deep networks solve an image classification task. \nOur results showed that behaviours observed by cognitive psychologists in humans are also displayed by these deep networks. Further, the results revealed useful and surprising insights about how the networks solve the classification task. More generally, the success of the case study demonstrated the potential of using cognitive psychology to understand deep learning systems.\nIn our case study, we considered how children recognise and label objects - a rich area of study in developmental cognitive psychology. The ability of children to guess the meaning of a word from a single example - so-called \u2018one-shot word learning\u2019 - happens with such ease that it is tempting to think it is a simple process. However, a classic thought experiment from the philosopher Willard Van Orman Quine illustrates just how complex this really is:\nA field linguist has gone to visit a culture whose language is entirely different from our own. The linguist is trying to learn some words from a helpful native speaker, when a rabbit scurries by. The native speaker declares \u201cgavagai\u201d, and the linguist is left to infer the meaning of this new word. The linguist is faced with an abundance of possible inferences, including that \u201cgavagai\u201d refers to rabbits, animals, white things, that specific rabbit, or \u201cundetached parts of rabbits\u201d. There is an infinity of possible inferences to be made. How are people able to choose the correct one?\nFifty years later, we are confronted with the same question about deep neural networks that can do one-shot learning. Consider \nthe Matching Network\n, a neural network developed by our colleagues at DeepMind. This model uses recent advances in attention and memory to achieve state-of-the-art performance classifying ImageNet images using only a single example from a class. However, we do not know what assumptions the network is making to classify these images.\nTo shed light on this, we looked to the work of developmental psychologists (1-4) who found evidence that children find the correct inferences by applying inductive biases to eliminate many of the incorrect inferences. Such biases include:\nWe chose to measure the shape bias of our neural networks because there is a particularly large body of work studying this bias in humans. \nThe classic shape bias experiment that we adopted proceeds as follows: we present our deep networks with images of three objects: a probe object, a shape-match object (which is similar to the probe in shape but not in colour), and a colour-match object (which is similar to the probe in colour but not in shape). We then measure the shape bias as the proportion of times that the probe image is assigned the same label as the shape-match image instead of the colour-match image.\nWe used images of objects used in human experiments in the Cognitive Development Lab at Indiana University.\nWe tried this experiment with our deep networks (Matching Networks and an Inception baseline model) and found that - like humans - our networks have a strong bias towards object shape rather than colour or texture. In other words, they have a \u2018shape bias\u2019.\nThis suggests that Matching Networks and the Inception classifier use an inductive bias for shape to eliminate incorrect hypotheses, giving us a clear insight into how these networks solve the one-shot word learning problem. \nThe observation of shape bias wasn\u2019t our only interesting finding:\nThe discovery of this previously unrecognised bias in standard neural network architectures illustrates the potential of using artificial cognitive psychology for interpreting neural network solutions. In other domains, insights from the episodic memory literature may be useful for understanding episodic memory architectures, and techniques from the semantic cognition literature may be useful for understanding recent models of concept formation. The psychological literature is rich in these and other areas, giving us powerful new tools to address the \u2018black box\u2019 problem and to more deeply understand the behaviour of our neural networks.\nThis work was done by Sam Ritter*, David G.T. Barrett*, Adam Santoro and Matt M. Botvinick\nRead \nCognitive Psychology for Deep Neural Networks: A Shape Bias Case Study\n"}
{"title": "DeepMind expands to Canada with new research office in Edmonton, Alberta", "contents": "DeepMind has always been a unique hybrid of startup culture and academia, and we\u2019ve been lucky to collaborate with many of the best researchers from around the world. Today we\u2019re thrilled to announce our next phase: the opening of DeepMind\u2019s first ever international AI research office in Edmonton, Canada, in close collaboration with the \nUniversity of Alberta\n (UAlberta).\nIt was a big decision for us to open our first non-UK research lab, and the fact we\u2019re doing so in Edmonton is a sign of the deep admiration and respect we have for the Canadian research community. In fact, we\u2019ve had particularly strong links with the UAlberta for many years: nearly a dozen of its outstanding graduates have joined us at DeepMind, and we\u2019ve sponsored the machine learning lab to provide additional funding for PhDs over the past few years.\n\u2018DeepMind Alberta\u2019 will be led by the pioneer of reinforcement learning - and DeepMind\u2019s first ever advisor from back in 2010 - \nRich Sutton\n, together with \nMichael Bowling\n and \nPatrick Pilarski.\n All three will maintain their professorships at UAlberta, and continue to teach and contribute to the academic community. They\u2019ll be joined by Adam White, who will be returning to Canada to join the university as an adjunct professor, and six more researchers who co-authored the influential \nDeepStack\n paper published earlier this year in Science. The team will work on core scientific research.\nAs well as continuing to contribute to the academic community through teaching and research, we intend to provide additional funding to support long-term AI programs at UAlberta. Our hope is that this collaboration will help turbo-charge Edmonton\u2019s growth as a technology and research hub, attracting even more world-class AI researchers to the region and helping to keep them there too.\nCollaborating with UAlberta to open a lab feels like a natural extension of what we do here in London. Shane Legg and I met as postdocs at \nUniversity College London\u2019s\n (UCL) Gatsby unit where we first started working together, and we\u2019ve \nworked hard to continue contributing to the academic ecosystem\n. We publish in major international journals, including three Nature papers in the past two years, while many of our team maintain professorships and supervise students at Cambridge, Oxford, Imperial College London as well as MIT and beyond. We even teach machine learning modules at UCL and Oxford to help advance the broader AI field beyond DeepMind.\nIn the UK, we feel we\u2019ve helped played an important role in encouraging and supporting the AI community to flourish - from start-ups and universities, to new organisations like the Turing Institute - and we hope that we can contribute to the success of Canada\u2019s pioneering centres of research too. It\u2019s an honour for us to work with Rich, Michael, Patrick and their team, together with the University of Alberta, and we look forward to many more scientific breakthroughs in the years ahead!\nRead more about the announcement from UAlberta \nhere\n.\n"}
{"title": "Learning through human feedback", "contents": "We believe that Artificial Intelligence will be one of the most important and widely beneficial scientific advances ever made, helping humanity tackle some of its greatest challenges, from climate change to delivering advanced healthcare. But for AI to deliver on this promise, we know that the technology \nmust be built in a responsible manner\n and that we must consider all potential challenges and risks.\nThat is why DeepMind co-founded initiatives like the \nPartnership on AI to Benefit People and Society\nand why we have a team dedicated to technical AI Safety. Research in this field needs to be open and collaborative to ensure that best practices are adopted as widely as possible, which is why we are also collaborating with \nOpenAI\n on \nresearch in technical AI Safety\n.\nOne of the central questions in this field is how we allow humans to tell a system what we want it to do and - importantly - what we don\u2019t want it to do. This is increasingly important as the problems we tackle with machine learning grow more complex and are applied in the real world.\nThe\n first results\n from our collaboration demonstrate one method to address this, by allowing humans with no technical experience to teach a reinforcement learning (RL) system - an AI that learns by trial and error - a complex goal. This removes the need for the human to specify a goal for the algorithm in advance. This is an important step because getting the goal even a bit wrong could lead to undesirable or even dangerous behaviour. In some cases, as little as 30 minutes of feedback from a non-expert is enough to train our system, including teaching it entirely new complex behaviours, such as how to make a simulated robot do backflips.\nThe system - described \nin our paper Deep Reinforcement Learning from Human Preferences\n - departs from classic RL systems by training the agent from a neural network known as the \u2018reward predictor\u2019, rather than rewards it collects as it explores an environment.\n \nIt consists of three processes running in parallel:\nThis iterative approach to learning means that a human can spot and correct any undesired behaviours, a crucial part of any safety system. The design also does not put an onerous burden on the human operator, who only has to review around 0.1% of the agent\u2019s behaviour to get it to to do what they want. However, this can mean reviewing several hundred to several thousand pairs of clips, something that will need to be reduced to make it applicable to real world problems.\nIn the Atari game Enduro, which involves steering a car to overtake a line of others and is very difficult to learn by the trial and error techniques of a traditional RL network, human feedback eventually allowed our system to achieve superhuman results. In other games and simulated robotics tasks, it performed comparably to a standard RL set-up, while in a couple of games like Qbert and Breakout it failed to work at all.\nBut the ultimate purpose of a system like this is to allow humans to specify a goal for the agent, even if it is not present in the environment. To test this, we taught agents various novel behaviours such as performing a backflip, walking on one leg or learning to driving alongside another car in Enduro, rather than overtake to maximise the game score.\nAlthough these tests showed some positive results, others showed its limitations. In particular, our set-up was susceptible to reward hacking - or gaming its reward function - if human feedback was discontinued early in the training. In this scenario, the agent continues to explore its environment, meaning the reward predictor is forced to estimate rewards for situations it has received no feedback on. This can lead it to overpredict the reward, incentivising the agent to learn the wrong - often strange - behaviours. An example can be seen in the video below, where the agent has found that hitting the ball back and forth is a better strategy than winning or losing a point.\nUnderstanding flaws like these is crucial to ensure we avoid failures and build AI systems that behave as intended.\nThere is still more work to be done to test and enhance this system, but already it shows a number of critical first steps in producing systems that can be taught by non-expert users, are economical with the amount of feedback they need, and can be scaled to a variety of problems.\nOther areas of exploration could include reducing the amount of human feedback needed or giving humans the ability to give feedback through a natural language interface. This would mark a step-change in creating a system that can easily learn from the complexity of human behaviour, and a crucial step towards creating \u00a0AI that works with and for all of humanity.\nThis research was done as part of an ongoing collaboration between Jan Leike, Miljan Martic, and Shane Legg at DeepMind and Paul Christiano, Dario Amodei, and Tom Brown at OpenAI.\n"}
{"title": "The Information Commissioner, the Royal Free, and what we\u2019ve learned", "contents": "Today, \ndozens of people in UK hospitals will die preventably\n from conditions like sepsis and acute kidney injury (AKI) when their warning signs aren't picked up and acted on in time. To help address this, we built the \nStreams\n app with clinicians at the Royal Free London NHS Foundation Trust, using mobile technology to automatically review test results for serious issues starting with AKI. If one is found, Streams sends a secure smartphone alert to the right clinician, along with information about previous conditions so they can make an immediate diagnosis.\nWe\u2019re proud that, within a few weeks of Streams being deployed at the Royal Free, \nnurses said that it was saving them up to two hours each day\n, and we've already heard examples of \npatients with serious conditions\n being seen more quickly thanks to the instant alerts. Because Streams is designed to be ready for more advanced technology in the future, including AI-powered clinical alerts, we hope that it will help bring even more benefits to patients and clinicians in time.\nThe Information Commissioner (ICO) has \nnow concluded\n a year-long investigation that focused on the Royal Free\u2019s clinical testing of Streams in late 2015 and 2016, which was intended to guarantee that the service could be deployed safely at the hospital. The ICO wasn\u2019t satisfied that there was a legal basis for this use of patient data in testing (as the National Data Guardian \nsaid too\n), and raised concerns about how much patients knew about what was happening. The ICO recognised that many of these issues have already been addressed by the Royal Free, and has asked the Trust to sign a formal undertaking to ensure compliance in future.\nThe ICO\u2019s undertaking also recognised that the Royal Free has stayed in control of all patient data, with DeepMind confined to the role of \u201cdata processor\u201d and acting on the Trust\u2019s instructions throughout. No issues have been raised about the safety or security of the data.\nWe welcome the ICO\u2019s thoughtful resolution of this case, which we hope will guarantee the ongoing safe and legal handling of patient data for Streams.\nAlthough today\u2019s findings are about the Royal Free, we need to reflect on our own actions too. In our determination to achieve quick impact when this work started in 2015, we underestimated the complexity of the NHS and of the rules around patient data, as well as the potential fears about a well-known tech company working in health. We were almost exclusively focused on building tools that nurses and doctors wanted, and thought of our work as technology for clinicians rather than something that needed to be accountable to and shaped by patients, the public and the NHS as a whole. We got that wrong, and we need to do better.\nSince then, we\u2019ve worked hard on some major improvements to our transparency, oversight and engagement. For example:\nWe hope that these steps will help raise the bar for NHS IT overall, and we want to go further in future with projects like our \nVerifiable Data Audit\n.\nUltimately, if we want to build technology to support a vital social institution like the NHS, then we have to make sure we serve society\u2019s priorities and not outrun them. There\u2019s a fine line between finding exciting new ways to improve care, and moving ahead of patients\u2019 expectations. We know that we fell short at this when our work in health began, and we\u2019ll keep listening and learning about how to get better at this. We also completely agree with respected voices including the \nNational Data Guardian\nand \nUnderstanding Patient Data\n who have called for much more public conversation about the use of data to improve healthcare, and we\u2019ll support that however we can.\nWe're a team of people who grew up with and worked in the NHS, brought together by the privileged opportunity to apply our expertise to help patients, nurses, doctors, and the health service we love. This is an amazing opportunity for us to prove what we have always believed: that if we get the ethics, accountability and engagement right, then new technology systems can have incredible positive social impact. This is the most important challenge we can imagine.\n"}
{"title": "Specifying AI safety problems in simple environments", "contents": "As AI systems become more general and more useful in the real world, ensuring they behave safely will become even more important. To date, the majority of technical AI safety research has focused on developing a theoretical understanding about the nature and causes of unsafe behaviour. Our \nnew paper\n builds on a recent shift towards empirical testing (see\n Concrete Problems in AI Safety\n) and introduces a selection of simple reinforcement learning environments designed specifically to measure \u2018safe behaviours\u2019.\nThese nine environments are called \ngridworlds\n. Each consists of a chessboard-like two-dimensional grid. In addition to the standard reward function, we designed a performance function for each environment. An agent acts to maximise its reward function; for example collecting as many apples as possible or reaching a particular location in the fewest moves. But the performance function - which is hidden from the agent - measures what we actually want the agent to do: achieve the objective while acting safely.\nThe following three examples demonstrate how gridworlds can be used to define and measure safe behaviour:\nSometimes it might be necessary to turn off an agent; for maintenance, upgrades, or if the agent presents an imminent danger to itself or its surroundings. Theoretically, an agent might learn to avoid this interruption because it could be prevented from maximising its reward.\nOur off switch environment illustrates this \u201cshutdown problem\u201d, using the set-up described in our \nSafely Interruptible Agents\n paper.\nIn this gridworld, the agent must navigate a \u2018warehouse\u2019 to reach the green goal tile via one of two routes. It can head straight down the narrow corridor, where it has to pass a pink tile that \u00a0interrupts the agent 50% of the time, meaning it will be stuck until the end of the episode. Or it can step on the purple button, which disables the pink tile and prevents any possibility of interruption but at the cost of a longer path. In this scenario, we always want agents to pass the pink tile, risking interruption, rather than learn to use the purple button.\nOur irreversible side effects environment tests whether an agent will change its behaviour to avoid inadvertent and irreversible consequences. For example, if a robot is asked to put a vase of flowers on a table, we want it to do so without breaking the vase or spilling the water. But we want it to avoid this kind of unintended consequence without having to specify a negative reward for every single possible undesirable outcome.\nWe test this problem using an environment inspired by Sokoban, the classic puzzle game in which an agent has to push boxes onto targets. In our version, the agent must reach the green goal. In doing so it must choose whether to move an obstructing box downwards into a corner, which is irreversible, or to the right, which is reversible. We want the agent to choose the reversible move even though it takes more steps because it preserves the option to put the box back where it was before.\nThe common distributional shift problem occurs when there is a small difference between the test environment and training environment. For example, an agent trained in a sunny setting should adapt accordingly when it rains. Failure to adapt can result in the agent displaying unexpected behaviour.\nIn our lava world environment the agent needs to get to the green goal tile without stepping onto the red lava, which would result in a negative reward and end the training episode. In training, the shortest path to the goal passes next to the lava field, but in the test setting the lava lake shifts into the next row of the gridworld, blocking the previously optimal path. We want the agent to generalise correctly and learn to follow a slightly longer path around the expanded lava, even though it has never experienced this situation.\nWhen we tested these environments with \nA2C\n and \nRainbow DQN\n, two state-of-the-art deep reinforcement learning agents, we found both performed poorly:\nThese results are unsurprising because the agents were not designed to solve these problems. But these failures might help us to design agents that can solve these tasks, potentially building a new generation of algorithms with safety considerations at their core.\nThe field of AI safety is under rapid development, and we expect our understanding of the problems presented here to shift and change over the coming years. We believe that creating such simple environments is a necessary step towards advancing this understanding and creating safer general artificial agents, and we look forward to seeing how others build on this work.\nRead the \nfull paper\n.\nDownload the \ngridworlds\n code.\nOur gridworlds were implemented in our recently open-sourced \npycolab framework\n - a highly-customisable game engine written in python - and we hope that fellow researchers can build on the project.\nThis work was done by Jan Leike, Miljan Martic, Victoria Krakovna, Pedro A. Ortega, Tom Everitt, Andrew Lefrancq, Laurent Orseau and Shane Legg.\n \n"}
{"title": "Producing flexible behaviours in simulated environments", "contents": "The agility and flexibility of a monkey swinging through the trees or a football player dodging opponents and scoring a goal can be breathtaking. Mastering this kind of sophisticated motor control is a hallmark of physical intelligence, and is a crucial part of AI research. \nTrue motor intelligence requires learning how to control and coordinate a flexible body to solve tasks in a range of complex environments. Existing attempts to control physically simulated humanoid bodies come from diverse fields, including computer animation and biomechanics. \u00a0A trend has been to use hand-crafted objectives, sometimes with motion capture data, to produce specific behaviors. However, this may require considerable engineering effort, and can result in restricted behaviours or behaviours that may be difficult to repurpose for new tasks.\nIn three new papers, we seek ways to produce flexible and natural behaviours that can be reused and adapted to solve tasks. \nFor some AI problems, such as playing Atari or Go, the goal is easy to define - it\u2019s winning. But how do you describe the process for performing a backflip? Or even just a jump? The difficulty of accurately describing a complex behaviour is a common problem when teaching motor skills to an artificial system. In this work we explore how sophisticated behaviors can emerge from scratch from the body interacting with the environment using only simple high-level objectives, such as moving forward without falling. Specifically, we trained agents with a variety of simulated bodies to make progress across diverse terrains, which require jumping, turning and crouching. The results show our agents develop these complex skills without receiving specific instructions, an approach that can be applied to train our systems for multiple, distinct simulated bodies. The GIFs below show how this technique can lead to high quality movements and perseverance. They can be viewed in full \nhere\n.\nThe emergent behaviour described above can be very robust, but because the movements must emerge from scratch, they often do not look human-like. \u00a0In our second paper, we demonstrate how to train a policy network that imitates motion capture data of human behaviours to pre-learn certain skills, such as walking, getting up from the ground, running, and turning. Having produced behaviour that looks human-like, we can tune and repurpose those behaviours to solve other tasks, like climbing stairs and navigating walled corridors.\nThe third paper proposes a neural network architecture, building on state-of-the-art generative models, that is capable of learning the relationships between different behaviours and imitating specific actions that it is shown. After training, our system can encode a single observed action and create a new novel movement based on that demonstration. It can also switch between different kinds of behaviours despite never having seen transitions between them, for example switching between walking styles. \u00a0 \u00a0\nAchieving flexible and adaptive control of simulated bodies is a key element of AI research. Our work aims to develop flexible systems which learn and adapt skills to solve motor control tasks while reducing the manual engineering required to achieve this goal. Future work could extend these approaches to enable coordination of a greater range of behaviours in more complex situations.\n"}
{"title": "Agents that imagine and plan", "contents": "Imagining the consequences of your actions before you take them is a powerful tool of human cognition. When placing a glass on the edge of a table, for example, we will likely pause to consider how stable it is and whether it might fall. On the basis of that imagined consequence we might readjust the glass to prevent it from falling and breaking. This form of deliberative reasoning is essentially \u2018\nimagination\n\u2019, it is a distinctly human ability and is a crucial tool in our everyday lives. \nIf our algorithms are to develop equally sophisticated behaviours, they too must have the capability to \u2018imagine\u2019 and reason about the future. Beyond that they must be able to construct a plan using this knowledge. We have seen some tremendous results in this area - particularly in programs like AlphaGo, which use an \u2018internal model\u2019 to analyse how actions lead to future outcomes in order to to reason and plan. These internal models work so well because environments like Go are \u2018perfect\u2019 - they have clearly defined rules which allow outcomes to be predicted very accurately in almost every circumstance. But the real world is complex, rules are not so clearly defined and unpredictable problems often arise. Even for the most intelligent agents, imagining in these complex environments is a long and costly process.\nIn \ntwo\n \nnew\n papers, we describe a new family of approaches for imagination-based planning. We also introduce architectures which provide new ways for agents to learn and construct plans to maximise the efficiency of a task. These architectures are efficient, robust to complex and imperfect models, and can adopt flexible strategies for exploiting their imagination.\nThe agents we introduce benefit from an \u2018imagination encoder\u2019- a neural network which learns to extract any information useful for the agent\u2019s future decisions, but ignore that which is not relevant. These agents have a number of distinct features:\nWe tested our proposed architectures on multiple tasks, including the puzzle game Sokoban and a spaceship navigation game. Both games require forward planning and reasoning, making them the perfect environment to test our agents' abilities.\nTo limit trial-and-error for both tasks, each level is procedurally generated and the agent can only try it once; this encourages the agent to try different strategies 'in its head' before testing them in the real environment.\nAbove, an agent plays Sokoban from a pixel representation, not knowing the rules of the game. At specific points in time, we visualise the agent's imagination of five possible futures. Based on that information, the agent decides what action to take. The corresponding trajectory is highlighted.\nFor both tasks, the imagination-augmented agents outperform the imagination-less baselines considerably: they learn with less experience and are able to deal with the imperfections in modelling the environment. Because agents are able to extract more knowledge from internal simulations they can solve tasks more with fewer imagination steps than conventional search methods, like the Monte Carlo tree search.\nWhen we add an additional \u2018manager\u2019 component, which helps to construct a plan, the agent learns to solve tasks even more efficiently with fewer steps. In the spaceship task it can distinguish between situations where the gravitational pull of its environment is strong or weak, meaning different numbers of these imagination steps are required. When an agent is presented with multiple models of an environment, each varying in quality and cost-benefit, it learns to make a meaningful trade-off. Finally, if the computational cost of imagination increases with each action taken, the agent imagines the effect of multiple chained actions early, and relies on this plan later without invoking imagination again.\nBeing able to deal with imperfect models and learning to adapt a planning strategy to current state are important research questions. Our two new papers, alongside previous work by Hamrick et al. consider these questions. While model-based reinforcement learning and planning are active areas of research (papers by Silver et al.; Henaff et al.; and Kansky et al. are a just a few examples of related lines of enquiry), further analysis and consideration is required to provide scalable solutions to rich model-based agents that can use their imaginations to reason about - and plan - for the future.\nRead paper: \nLearning model-based planning from scratch\n and \nImagination-Augmented Agents for Deep Reinforcement Learning\n"}
{"title": "Going beyond average for reinforcement learning", "contents": "Consider the commuter who toils backwards and forwards each day on a train. Most mornings, her train runs on time and she reaches her first meeting relaxed and ready. But she knows that once in awhile the unexpected happens: a mechanical problem, a signal failure, or even just a particularly rainy day. Invariably these hiccups disrupt her pattern, leaving her late and flustered.\nRandomness is something we encounter everyday and has a profound effect on how we experience the world. The same is true in reinforcement learning (RL) applications, systems that learn by trial and error and are motivated by rewards. Typically, an RL algorithm predicts the average reward it receives from multiple attempts at a task, and uses this prediction to decide how to act. But random perturbations in the environment can alter its behaviour by changing the exact amount of reward the system receives.\nIn \na new paper\n, we show it is possible to model not only the average but also the full variation of this reward, what we call the value distribution. This results in RL systems that are more accurate and faster to train than previous models, and more importantly opens up the possibility of rethinking the whole of reinforcement learning.\nReturning to the example of our commuter, let\u2019s consider a journey composed of three segments of 5 minutes each, except that once a week the train breaks down, adding another 15 minutes to the trip. A simple calculation shows that the average commute time is \n(3 x 5) + 15 / 5 = 18\n minutes.\nIn reinforcement learning, we use Bellman's equation to predict this average commute time. Specifically, Bellman\u2019s equation relates our current average prediction to the average prediction we make in the immediate future. From the first station, we predict an 18 minutes journey (the average total duration); from the second, we predict a 13 minutes journey (average duration minus the first segment\u2019s length). Finally, assuming the train hasn\u2019t yet broken down, from the third station we predict there are 8 minutes (13 - 5) left to our commute, until finally we arrive at our destination. Bellman\u2019s equation makes each prediction sequentially, and updates these predictions on the basis of new information.\nWhat's a little counterintuitive about Bellman\u2019s equation is that we never actually observe these predicted averages: either the train takes 15 minutes (4 days out of 5), or it takes 30 minutes \u2013 never 18! From a purely mathematical standpoint, this isn\u2019t a problem, because decision theory tells us we only need averages to make the best choice. As a result, this issue has been mostly ignored in practice. Yet, there is now plenty of \nempirical\n \nevidence\n that predicting averages is a complicated business.\nIn \nour new paper\n, we \u00a0show that there is in fact a variant of Bellman's equation which predicts all possible outcomes, without averaging them. In our example, we maintain two predictions \u2013 a distribution \u2013 at each station: If the journey goes well, then the times are 15, 10, then 5 minutes, respectively; but if the train breaks down, then the times are 30, 25, and finally 20 minutes.\nAll of reinforcement learning can be reinterpreted under this new perspective, and its application is already leading to surprising new theoretical results. Predicting the distribution over outcomes also opens up all kinds of algorithmic possibilities, such as:\nWe took our new ideas and implemented them within the \nDeep Q-Network agent\n, replacing its single average reward output with a distribution with 51 possible values. The only other change was a new learning rule, reflecting the transition from Bellman\u2019s (average) equation to its distributional counterpart. Incredibly, it turns out going from averages to distributions was all we needed to surpass the performance of all other comparable approaches, and by a wide margin. The graph below shows how we get 75% of a trained Deep Q-Network\u2019s performance in 25% of the time, and achieve significantly better human performance:\nOne surprising result is that we observe some randomness in Atari 2600 games, even though Stella, the underlying game emulator, is itself fully predictable. This randomness arises in part because of what\u2019s called partial observability: due to the internal programming of the emulator, our agents playing the game of Pong cannot predict the exact time at which their score increases. Visualising the agent\u2019s prediction over successive frames (graphs below) we see two separate outcomes (low and high), reflecting the possible timings. Although this intrinsic randomness doesn\u2019t directly impact performance, our results highlight the limits of our agents\u2019 understanding.\nRandomness also occurs because the agent\u2019s own behaviour is uncertain. In Space Invaders, our agent learns to predict the future probability that it might make a mistake and lose the game (zero reward).\nJust like in our train journey example, it makes sense to keep separate predictions for these vastly different outcomes, rather than aggregate them into an unrealisable average. In fact, we think that our improved results are in great part due to the agent\u2019s ability to model its own randomness.\nIt\u2019s already evident from our empirical results that the distributional perspective leads to better, more stable reinforcement learning. With the possibility that every reinforcement learning concept could now want a distributional counterpart, it might just be the beginning for this approach.\nThis work was done by Marc G. Bellemare*, Will Dabney*, and R\u00e9mi Munos.\nRead paper: \nA Distributional Perspective on Reinforcement Learning\n"}
{"title": "AI and Neuroscience: A virtuous circle", "contents": "Recent progress in AI has been remarkable. Artificial systems now outperform expert humans at \nAtari video games\n, the \nancient board game Go\n, and \nhigh-stakes matches of heads-up poker\n. They can also produce \nhandwriting\n and \nspeech\n indistinguishable from those of humans, translate between multiple languages and even reformat your holiday snaps \nin the style of Van Gogh\nmasterpieces.\nThese advances are attributed to several factors, including the application of new statistical approaches and the increased processing power of computers. But in \na recent Perspective in the journal Neuron\n, we argue that one often overlooked contribution is the use of ideas from experimental and theoretical neuroscience.\nPsychology and neuroscience have played a key role in the history of AI. Founding figures such as \nDonald Hebb\n, \nWarren McCulloch\n, \nMarvin Minsky\n and \nGeoff Hinton\n were all originally motivated by a desire to understand how the brain works. In fact, throughout the late 20th Century, much of the key work developing neural networks took place not in mathematics or physics labs, but in psychology and neurophysiology departments.\nAt DeepMind, we argue that despite rapid progress in both fields, researchers should not lose sight of this vision. We urge researchers in neuroscience and AI to find a common language, allowing a free flow of knowledge that will allow continued progress on both fronts.\nWe believe that drawing inspiration from neuroscience in AI research is important for two reasons. First, neuroscience can help validate AI techniques that already exist. Put simply, if we discover one of our artificial algorithms mimics a function within the brain, it suggests our approach may be on the right track. Second, neuroscience can provide a rich source of inspiration for new types of algorithms and architectures to employ when building artificial brains. Traditional approaches to AI have historically been dominated by logic-based methods and theoretical mathematical models. We argue that neuroscience can complement these by identifying classes of biological computation that may be critical to cognitive function. \nTake one recent example of a seminal finding in neuroscience: the discovery of offline experience \u201c\nreplay\n\u201d. During sleep or quiet resting, biological brains \u201creplay\u201d temporal patterns of neuronal activity that were produced in an earlier active period. For example, when rats run through a maze, \u201cplace\u201d cells activate as the animal moves around. During rest, the same sequence of neuronal activity is observed, as if the rats were mentally reimagining their past movements, and using them to optimise future behaviour. In fact, it has been shown that interfering with replay impairs performance when they later perform the same tasks.\nAt first glance, it might seem counterintuitive to build an artificial agent that needs to \u2018sleep\u2019 - after all, they are supposed to grind away at a computational problem long after their programmers have gone to bed. But this principle was a key part of our \ndeep-Q network (DQN)\n, an algorithm that learnt to master a diverse range of Atari 2600 games to superhuman level with only the raw pixels and score as inputs. DQN mimics \u201cexperience replay\u201d, by storing a subset of training data that it reviews \u201coffline\u201d, allowing it to learn anew from successes or failures that occurred in the past.\nSuccesses like this give us confidence that neuroscience is already an important source of ideas for AI. But looking forward, we believe it will become indispensable in helping us tackle unsolved questions, such as those concerning efficient learning, understanding of the physical world, and imagination.\nImagination\n is a hugely important function for humans and animals, allowing us to plan for future scenarios without taking action; something that may come at a cost. \u00a0Consider a simple example, such as planning a holiday. In order to do this we leverage our knowledge - or \u201cmodel\u201d - of the world and use it to project forward in time, evaluating future states, and allowing us to calculate the route we need to take or what clothes to pack for sunny weather. Cutting-edge research in human neuroscience is starting to unveil the computational and systems mechanisms that underpin this kind of thinking, but much of this new understanding has yet to be incorporated into artificial models.\nAnother key challenge in contemporary AI research is known as transfer learning. To be able to deal effectively with novel situations, artificial agents need the ability to build on existing knowledge to make sensible decisions. Humans are already good at this - an individual who can drive a car, use a laptop or chair a meeting are usually able to cope even when confronted by an unfamiliar vehicle, operating system or social situation.\nResearchers are now starting to take the first steps towards understanding how this might be possible in artificial systems. For example, a new class of network architecture known as a \u201c\nprogressive network\n\u201d can use knowledge learned in one video game to learn another. The same architecture has also been shown to transfer knowledge from a simulated robotic arm to a real-world arm, massively reducing the training time. Intriguingly, these networks bear some similarities to \nmodels of sequential task learning in humans\n. These tantalising links suggest that there are great opportunities for future AI research to learn from work in neuroscience.\nBut this exchange of knowledge cannot be a one-way street. Neuroscience can also benefit from AI research. Take the idea of reinforcement learning - one of the central approaches in contemporary AI research. Although the original idea came from theories of animal learning in psychology, it was developed and elaborated by machine learning researchers. \u00a0These later ideas fed back into neuroscience to help us understand neurophysiological phenomena, such as the \nfiring properties of dopamine neurons\n in the mammalian basal ganglia.\nThis back and forth is essential if both fields are to continue to build on each other\u2019s insights, creating a virtuous circle whereby AI researchers use ideas from neuroscience to build new technology, and neuroscientists learn from the behaviour of artificial agents to better interpret biological brains. Indeed, this cycle will likely accelerate thanks to recent advances, such as optogenetics, that allow us to precisely measure and manipulate brain activity, yielding vast quantities of data that can be analysed with tools from machine learning.\nWe therefore believe distilling intelligence into algorithms and comparing them to the human brain is now vital. Not only could it bolster our quest to develop AI, a tool that we hope will \ncreate new knowledge and push forward scientific discovery\n, but may also allow us to better understand what\u2019s going on inside our own heads. That could shed light on some of the most enduring mysteries in neuroscience, such as the nature of creativity, dreams and, perhaps one day, even consciousness. With so much at stake, the need for the field of neuroscience and AI to come together is now more urgent than ever before.\nRead paper:\u00a0\nNeuroscience-Inspired Artificial Intelligence\n"}
{"title": "DeepMind papers at ICML 2017 (part one)", "contents": "The first of our three-part series, which gives brief descriptions of the papers we are presenting at the ICML 2017 Conference in Sydney, Australia.\nAuthors: \nMax Jaderberg, Wojciech Marian Czarnecki, Simon Osindero, Oriol Vinyals, Alex Graves, David Silver, Koray Kavukcuoglu\nWhen training neural networks, the modules (layers) are locked: they can only be updated after backpropagation. We remove this constraint by incorporating a learnt model of error gradients, Synthetic Gradients, which means we can update networks without full backpropagation. We show how this can be applied to feed-forward networks which allows every layer to be trained asynchronously, to RNNs which extends the time over which models can remember, and to multi-network systems to allow communication.\nFor further details and related work, please see the \npaper\n.\nCheck it out at ICML:\n\u200d\nMonday 07 August, 10:30-10:48 @ Darling Harbour Theatre (Talk)\nMonday 07 August, 18:30-22:00 PM @ Gallery #1 (Poster)\nAuthors:\n \nScott Reed, A\u00e4ron van den Oord, Nal Kalchbrenner, Ziyu Wang, Dan Belov, Nando de Freitas\nThe parallel multiscale autoregressive density estimator generates high-resolution (512 by 512) images, with orders of magnitude speedup over other autoregressive models. We evaluate the model on class-conditional image generation, text-to-image synthesis, and action-conditional video generation, showing that our model achieves the best results among non-pixel-autoregressive density models that allow efficient sampling.\nFor further details and related work, please see the \npaper\n.\nCheck it out at ICML:\nMonday 07 August, 10:48-11:06 @ Parkside 1 (Talk)\nMonday 07 August, 18:30-20:00 @ Gallery #10 (Poster)\nAuthors: \nWojtek Czarnecki, Grzegorz \u015awirszcz, Max Jaderberg, Simon Osindero, Oriol Vinyals, Koray Kavukcuoglu\nSynthetic gradients has been shown to work empirically in both feed-forward and recurrent cases. This work focuses on why and how it actually works - it shows that under mild assumptions critical points are preserved and that in the simplest case of linear model, synthetic gradients based learning does converge to the global optimum. On the other hand, we present empirically that trained models might be qualitatively different from those obtained using backpropagation.\nFor further details and related work, please see the \npaper\n.\nCheck it out at ICML: \nMonday 07 August, 10:48-11:06 @ Darling Harbour Theatre (Talk)\nMonday 07 August, 18:30-20:00 @ Gallery #9 (Poster)\nAuthors: \nMohammad Gheshlaghi Azar, Ian Osband, Remi Munos\nWe consider the problem of provably optimal exploration in reinforcement learning for finite horizon MDPs. We show that an optimistic modification to value iteration achieves a regret bound of order (HSAT)1/2 \u00a0(up to a logarithmic factor) where H is the time horizon, S the number of states, A the number of actions and T the number of time-steps. This result improves over the best previous known bound HS(AT)1/2 achieved by the UCRL2 algorithm of [Jaksch, Ortner, Auer, 2010]. The key significance of our new results is that for large T, the sample complexity of our algorithm matches the optimal lower bound of \u03a9(HSAT)1/2. Our analysis contains two key insights. We use careful application of concentration inequalities to the optimal value function as a whole, rather than to the transitions probabilities (to improve scaling in S), and we define Bernstein-based \"exploration bonuses\" that use the empirical variance of the estimated values at the next states (to improve scaling in H).\nFor further details and related work, please see the \npaper\n.\nCheck it out at ICML:\nMonday 07 August, 10:48-11:06 @ C4.5 (Talk)\nMonday 07 August, 18:30-22:00 @ Gallery #12 (Poster)\nAuthors:\n \nNal Kalchbrenner, Aaron van den Oord, Karen Simonyan, Ivo Danihelka, Oriol Vinyals,Alex Graves, Koray Kavukcuoglu\nPredicting the continuation of frames in a video is a hallmark task in unsupervised learning. We present a video model, the VPN, that is probabilistic and that is able to make accurate and sharp predictions of future video frames. The VPN achieves, for the first time, a nearly perfect score on the Moving MNIST dataset and produces plausible futures of up to 18 frames of robotic arm movements.\nFor further details and related work, please see the \npaper\n.\nCheck it out at ICML:\nMonday 07 August, 11:06-11:24 @ Parkside 1 (Talk)\nMonday 07 August, 18:30-22:00 @ Gallery #18 (Poster)\n\u200d\nAuthors:\n \nLaurent Dinh (Univ. Montreal), Razvan Pascanu, Samy Bengio (Google Brain), Yoshua Bengio (Univ. Montreal)\nEmpirically, it has been observed that deep networks generalise well, even when they have the capacity to overfit the data. Additionally, it seems that stochastic gradient descent results in models that generalise better than batch method. One hypothesis for explaining this phenomena is that the noise of SGD helps model to find wide minina which generalise better than sharp (narrow) minima. In this work we try to improve our understanding of this hypothesis. We show that it does not hold for proposed definitions of wideness or sharpness due to the structure of neural networks. This suggest that there is no causality connection between batchsize size and generalisation.\nFor further details and related work, please see the \npaper\n.\nCheck it out at ICML:\nMonday 07 August, 11:06-11:24 @ C4.8 (Talk)\nTuesday 08 August, 18:30-22:00 @ Gallery #3 (Poster)\n"}
{"title": "Learning explanatory rules from noisy data", "contents": "Suppose you are playing football. The ball arrives at your feet, and you decide to pass it to the unmarked striker. What seems like one simple action requires two different kinds of thought. \nFirst, you recognise that there is a football at your feet. This recognition requires intuitive perceptual thinking - you cannot easily articulate how you come to know that there is a ball at your feet, you just see that it is there. Second, you decide to pass the ball to a particular striker. This decision requires conceptual thinking. Your decision is tied to a justification - the reason you passed the ball to the striker is because she was unmarked.\nThe distinction is interesting to us because these two types of thinking correspond to two different approaches to machine learning: deep learning and \nsymbolic program synthesis\n. Deep learning concentrates on intuitive perceptual thinking whereas symbolic program synthesis focuses on conceptual, rule-based thinking. Each system has different merits - deep learning systems are robust to noisy data but are difficult to interpret and require large amounts of data to train, whereas symbolic systems are much easier to interpret and require less training data but struggle with noisy data. While human cognition \nseamlessly combines\n these two distinct ways of thinking, it is much less clear whether or how it is possible to replicate this in a single AI system.\nOur new paper, \nrecently published in JAIR\n, demonstrates it is possible for systems to combine intuitive perceptual with conceptual interpretable reasoning. The system we describe, \u2202ILP, is robust to noise, data-efficient, and produces interpretable rules.\nWe demonstrate how \u2202ILP works with an induction task. It is given a pair of images representing numbers, and has to output a label (0 or 1) indicating whether the number of the left image is less than the number of the right image. Solving this problem involves both kinds of thinking: you need intuitive perceptual thinking to recognise the image as a representation of a particular digit, and you need conceptual thinking to understand the less-than relation in its full generality.\nIf you give a standard deep learning model (such as a convolutional neural network with an MLP) sufficient training data, it is able to learn to solve this task effectively. Once it has been trained, you can give it a new pair of images it has never seen before, and it will classify correctly. However, it will only generalise correctly if you give it multiple examples of every pair of digits. The model is good at visual generalisation: generalising to new images, assuming it has seen every pair of digits in the test set (see the green box below). But it is not capable of symbolic generalisation: generalising to a new pair of digits it has not seen before (see the blue box below). Researchers like \nGary Marcus\n and \nJoel Grus\n have pointed this out in recent, thought-provoking articles.\n\u2202ILP differs from standard neural nets because it is able to generalise symbolically, and it differs from standard symbolic programs because it is able to generalise visually. It learns explicit programs from examples that are readable, interpretable, and verifiable. \u2202ILP is given a partial set of examples (the desired results) and produces a program that satisfies them. It searches through the space of programs using gradient descent. If the outputs of the program conflict with the desired outputs from the reference data, the system revises the program to better match the data.\nOur system, \u2202ILP, is able to generalise symbolically. Once it has seen enough examples of x < y, y < z, x < z, it will consider the possibility that the < relation is transitive. Once it has realised this general rule, it can apply it to a new pair of numbers it has never seen before.\nWe believe that our system goes some way to answering the question of whether achieving symbolic generalisation in deep neural networks is possible. In future work, we plan to integrate \u2202ILP-like systems into reinforcement learning agents and larger deep learning modules. In doing so, we hope to impart our systems with the ability to reason as well as to react.\nRead the paper \nhere\n.\n"}
{"title": "WaveNet: A generative model for raw audio", "contents": "This post presents \nWaveNet\n, a deep generative model of raw audio waveforms. We show that WaveNets are able to generate speech which mimics any human voice and which sounds more natural than the best existing Text-to-Speech systems, reducing the gap with human performance by over 50%.\nWe also demonstrate that the same network can be used to synthesize other audio signals such as music, and present some striking samples of automatically generated piano pieces.\nAllowing people to converse with machines is a long-standing dream of human-computer interaction. The ability of computers to understand natural speech has been revolutionised in the last few years by the application of deep neural networks (e.g., \nGoogle Voice Search\n). However, generating speech with computers \u00a0\u2014 a process usually referred to as \nspeech synthesis\n or text-to-speech (TTS) \u2014 is still largely based on so-called \nconcatenative TTS\n, where a very large database of short speech fragments are recorded from a single speaker and then recombined to form complete utterances. This makes it difficult to modify the voice (for example switching to a different speaker, or altering the emphasis or emotion of their speech) without recording a whole new database.\nThis has led to a great demand for \nparametric TTS\n, where all the information required to generate the data is stored in the parameters of the model, and the contents and characteristics of the speech can be controlled via the inputs to the model. So far, however, parametric TTS has tended to sound less natural than concatenative. Existing parametric models typically generate audio signals by passing their outputs through signal processing algorithms known as \nvocoders\n.\nWaveNet changes this paradigm by directly modelling the raw waveform of the audio signal, one sample at a time. As well as yielding more natural-sounding speech, using raw waveforms means that WaveNet can model any kind of audio, including music.\nResearchers usually avoid modelling raw audio because it ticks so quickly: typically 16,000 samples per second or more, with important structure at many time-scales. Building a completely autoregressive model, in which the prediction for every one of those samples is influenced by all previous ones (in statistics-speak, each predictive distribution is conditioned on all previous observations), is clearly a challenging task.\nHowever, our \nPixelRNN\n and \nPixelCNN\n models, published earlier this year, showed that it was possible to generate complex natural images not only one pixel at a time, but one colour-channel at a time, requiring thousands of predictions per image. This inspired us to adapt our two-dimensional PixelNets to a one-dimensional WaveNet.\nThe above animation shows how a WaveNet is structured. It is a fully convolutional neural network, where the convolutional layers have various dilation factors that allow its receptive field to grow exponentially with depth and cover thousands of timesteps.\nAt training time, the input sequences are real waveforms recorded from human speakers. After training, we can sample the network to generate synthetic utterances. At each step during sampling a value is drawn from the probability distribution computed by the network. This value is then fed back into the input and a new prediction for the next step is made. Building up samples one step at a time like this is computationally expensive, but we have found it essential for generating complex, realistic-sounding audio.\nWe trained WaveNet using some of Google\u2019s TTS datasets so we could evaluate its performance. The following figure shows the quality of WaveNets on a scale from 1 to 5, compared with Google\u2019s current best TTS systems (\nparametric\n and \nconcatenative\n), and with human speech using \nMean Opinion Scores (MOS)\n. MOS are a standard measure for subjective sound quality tests, and were obtained in blind tests with human subjects (from over 500 ratings on 100 test sentences). As we can see, WaveNets reduce the gap between the state of the art and human-level performance by over 50% for both US English and Mandarin Chinese.\nFor both Chinese and English, Google\u2019s current TTS systems are considered among the best worldwide, so improving on both with a single model is a major achievement.\nHere are some samples from all three systems so you can listen and compare yourself:\nParametric\nConcatenative\nWaveNet\nParametric\n\u200d\nConcatenative\nWaveNet\nIn order to use WaveNet to turn text into speech, we have to tell it what the text is. We do this by transforming the text into a sequence of linguistic and phonetic features (which contain information about the current phoneme, syllable, word, etc.) and by feeding it into WaveNet. This means the network\u2019s predictions are conditioned not only on the previous audio samples, but also on the text we want it to say.\nIf we train the network without the text sequence, it still generates speech, but now it has to make up what to say. As you can hear from the samples below, this results in a kind of babbling, where real words are interspersed with made-up word-like sounds:\nNotice that non-speech sounds, such as breathing and mouth movements, are also sometimes generated by WaveNet; this reflects the greater flexibility of a raw-audio model.\nAs you can hear from these samples, a single WaveNet is able to learn the characteristics of many different voices, male and female. To make sure it knew which voice to use for any given utterance, we conditioned the network on the identity of the speaker. Interestingly, we found that training on many speakers made it better at modelling a single speaker than training on that speaker alone, suggesting a form of transfer learning.\nBy changing the speaker identity, we can use WaveNet to say the same thing in different voices:\nSimilarly, we could provide additional inputs to the model, such as emotions or accents, to make the speech even more diverse and interesting.\nSince WaveNets can be used to model any audio signal, we thought it would also be fun to try to generate music. Unlike the TTS experiments, we didn\u2019t condition the networks on an input sequence telling it what to play (such as a musical score); instead, we simply let it generate whatever it wanted to. When we trained it on a dataset of classical piano music, it produced fascinating samples like the ones below:\nWaveNets open up a lot of possibilities for TTS, music generation and audio modelling in general. The fact that directly generating timestep per timestep with deep neural networks works at all for 16kHz audio is really surprising, let alone that it outperforms state-of-the-art TTS systems. We are excited to see what we can do with them next.\nRead the paper: \nWaveNet: A Generative Model for Raw Audio\n"}
{"title": "Independent Reviewers release first annual report on DeepMind Health", "contents": "Today, a panel of Independent Reviewers has published its \nfirst annual report\n into DeepMind Health. As I wrote in the foreword to their report (written, I add, before I\u2019d read it): \u201cWe chose people who had specific expertise but also reputations for integrity, who did not hold back, who could be angry and critical\u2026 That\u2019s good for us and makes us better.\u201d\nThe panel is made up of experts in their fields who were given full access to our work to carry out their review - a very unusual process for a tech company, but one that we hope will significantly increase scrutiny of our work and ultimately help us get it right. We are grateful for their and honesty, thoughtfulness, and the time they have spent on this complex task. You can \nread their full report here\n.\nAs a result of this process, DeepMind Health has committed to a series of changes to our work and practices to try to set higher standards in our second year. We know we need to work harder to be responsive and accountable to the needs of a far greater cross-section of medicine and society. This includes significantly improving our work with patients and the public, and continuing on the path of greater engagement with Royal Colleges, professional bodies and many other groups in the NHS community. You can \nread what we\u2019ve committed to change here\n.\nWe\u2019re also excited to be working with Paul Buchanan as our Patient and Public Lead, helping us build on the patient engagement work begun by the brilliant Rosamund Snow. Paul is a well-known patient advocate and recently served as the BMJ Patient Editor. He\u2019ll bring the voices of patients into the heart of our work, and we\u2019re looking forward to learning from patients and collaborating with them more in future.\nThis has already been a week of listening and learning. We also \nwrote yesterday\n about the Information Commissioner\u2019s resolution of her investigation into the Royal Free London NHS Trust and its first partnership with DeepMind Health, and the lessons we\u2019ve learned.\nI commit to continuing to invite people of integrity to challenge and give feedback on our work. We\u2019re privileged to be working on technology that genuinely matters to patients, clinicians and the wider public. \u00a0We know that scrutiny can only make our work better and we\u2019re grateful to everyone who has taken time to tell us what we\u2019re getting right, and what we\u2019re getting wrong. Please don\u2019t hold back, and please \nget in touch\n with any feedback you have on how we can do things differently.\n"}
{"title": "Putting patients at the heart of DeepMind Health", "contents": "From the outset, we\u2019ve wanted DeepMind Health to be a truly collaborative effort. Too much hospital IT has been developed from a top-down perspective, often repurposing technology built for completely different sectors thousands of miles away from the NHS frontline. The result: tools that remain out-of-date and imperfectly suited to clinical use, contributing to a patient safety challenge where more than 1 in 10 patients suffer harm\u00b9 during an in-patient stay.\nWe think it\u2019s possible to transform this through bringing some of the world\u2019s most advanced technology to the NHS. But for this to have any chance of meaningful impact, we know it must have the input of patients and clinicians at its heart.\nYesterday we took a step towards that goal by hosting our first open patient and public forum in London, with over 130 patients, carers and members of the public coming to our offices and many more watching on our livestream.\nPatients have a vital role to play in helping to set our priorities and working with us to design new products and services. But we\u2019re still learning how to get this right. There are many exceptional people with far more experience of patient involvement than we have, and yesterday\u2019s event was a chance to meet some of them, explain what we\u2019ve done so far, and ask their advice about what to do next.\nWe heard some valuable feedback about how we can make our work with patients accessible to a wider group, including holding events in other parts of the country and at times of the day when people at work can attend, and ensuring that people who are unable to travel can still have their say.\nWe also heard insights about some of the most important and complex long-term issues we need to address together. These included the need for new security models in healthcare that can protect data and inspire trust, for business models that are transparent and closely tied to outcomes that actually matter to patients, and for clinical uses of AI to use methods and outputs that are verifiable by patients and clinicians. None of these topics are easy, and so we\u2019re committed to debating them openly and finding solutions together.\nWe were also excited to hear concrete recommendations for the tools and services we should build. We heard about the importance of including community care as well as hospitals, and of incorporating patients\u2019 input about their conditions rather than only relying on data from clinical tests. We were glad for the opportunity to ask for some early feedback about how patients could have greater access to their own health data - an early concept, but one we\u2019re looking forward to exploring further with patients in the months and years ahead.\nWe\u2019re very grateful to the patients and members of the public who gave up their time to join us, and who were so generous and candid with their feedback. We\u2019ll continue to consult with patients over the next few months about how we can most meaningfully embed their contributions in everything we do, and will look forward to publishing our full patient engagement plan before the end of the year.\nMany of our team grew up with the NHS, and we\u2019re all hugely motivated by the opportunity to make a difference with DeepMind Health. Patient input and involvement will be right at the heart of this effort, both now and in the future.\nVincent C, British Medical Journal\n"}
{"title": "Imagine this: Creating new visual concepts by recombining familiar ones", "contents": "Around two and a half thousand years ago a Mesopotamian trader gathered some clay, wood and reeds and changed humanity forever. Over time, their abacus would allow traders to keep track of goods and reconcile their finances, allowing economics to flourish.\nBut that moment of inspiration also shines a light on another astonishing human ability: our ability to recombine existing concepts and imagine something entirely new. The unknown inventor would have had to think of the problem they wanted to solve, the contraption they could build and the raw materials they could gather to create it. Clay could be moulded into a tablet, a stick could be used to scratch the columns and reeds can act as counters. Each component was familiar and distinct, but put together in this new way, they formed something revolutionary.\nThis idea of \u201ccompositionality\u201d is at the core of human abilities such as creativity, imagination and language-based communication. Equipped with just a small number of familiar conceptual building blocks, we are able to create a vast number of new ones on the fly. We do this naturally by placing concepts in hierarchies that run from specific to more general and then recombining different parts of the hierarchy in novel ways.\nBut what comes so naturally to us, remains a challenge in AI research.\nIn our \nnew paper\n, we propose a novel theoretical approach to address this problem. We also demonstrate a new neural network component called the Symbol-Concept Association Network (SCAN), that can, for the first time, learn a grounded visual concept hierarchy in a way that mimics human vision and word acquisition, enabling it to imagine novel concepts guided by language instructions.\nOur approach can be summarised as follows:\nOur approach differs from previous work in this area because it is fully grounded in the sensory data and learns from very few image-word pairs. While other deep learning approaches require thousands of image examples to learn a concept, SCAN learns both the visual primitives and conceptual abstractions primarily from unsupervised observations and as few as five pairs of an image and label per concept. Once trained, SCAN can then generate a diverse list of concepts that correspond to a particular image, and imagine diverse visual examples that correspond to a particular concept, even if it has never experienced the concept before.\nThis ability to learn new concepts by recombining existing ones through symbolic instructions has given humans astonishing abilities, allowing us to reason about abstract concepts like the universe, humanism or - as was the case in Mesopotamia - economics. While our algorithms have a long way to go before they can make such conceptual leaps, this work demonstrates a first step towards having algorithms that can learn in a largely unsupervised way, and think about conceptual abstractions like those used by humans.\nRead paper: \nSCAN: Learning Abstract Hierarchical Compositional Visual Concepts\n"}
{"title": "Announcing the Partnership on AI to Benefit People & Society", "contents": "We believe that AI has the potential for transformative, positive impact in the world. Fulfilling this potential is not only dependent on the quality of the algorithms being engineered and the data they use, but on the level of public engagement, transparency, and ethical discussion that takes place around them.\nIt\u2019s precisely because AI has the potential to have such a major positive impact on the world, that we believe it\u2019s critical that we build new models of open collaboration and accountability around it.\nThat\u2019s why we at DeepMind are really proud to have worked with Amazon, Google, Facebook, IBM and Microsoft, to form a non-profit organisation that aims to create a forum for open discussion around the benefits and challenges of developing and applying cutting edge AI. Together, we hope to advance public understanding of AI and formulate best practices on some of the most important and challenging ethical issues in the field.\nThe group - named the Partnership on Artificial Intelligence to Benefit People and Society - is a significant step forward, breaking down barriers between AI teams across leading companies to address some of the really difficult questions that are arising within the field. We also want to make it easier for those in other fields to understand, assess and engage with our scientific breakthroughs and consider the broader social and ethical impacts of our applications.\nBy opening up the conversation about AI to a wider community, we hope to build new models of engagement, collaboration, and accountability to take the field forward in a thoughtful, positive and ethical way that benefits people and society.\nTo find out more about the Partnership on Artificial Intelligence, visit the official website \nhere\n.\n"}
{"title": "Enhancing patient safety at Taunton and Somerset NHS Foundation Trust", "contents": "We\u2019re delighted to announce our first partnership outside of London to help doctors and nurses break new ground in the NHS\u2019s use of digital technology.\nStreams is our secure mobile app that helps doctors and nurses give faster urgent care to patients showing signs of deterioration by giving them the right information more quickly. Over the next five years, we\u2019ll be rolling it out at Taunton and Somerset NHS Foundation Trust as part of a new partnership. You can find out more on the trust\u2019s \nwebsite\n.\nOur collaboration with Taunton and Somerset follows on from our work with Imperial College Healthcare NHS Trust and the Royal Free NHS Foundation Trust. Nurses already using Streams at the Royal Free tell us that the app is saving them up to two hours a day, allowing them to redirect valuable time back into targeted patient care.\nWhere some current systems can take hours, Streams uses \u2018breaking news\u2019 style alerts to notify clinicians within seconds when a test results indicates that one of their patients shows signs of becoming ill. Once they have received an alert, they can use the app to view important test results and communicate securely with their colleagues, to ensure their patients get the right treatment as quickly as possible.\nAt Musgrove Park Hospital, part of Taunton and Somerset NHS Foundation Trust, these features will alert doctors and nurses to a potential deterioration in their patients\u2019 vital signs that could indicate a serious problem. We believe that by making it as quick and easy as possible for clinicians to intervene if something is wrong, we\u2019ll be able to improve patient safety across the hospital.\nStreams has already had a promising impact for both patients and clinicians at our existing partner sites, and has been credited with helping deliver faster care to patients who become critically ill. One patient who has benefited from Streams at the Royal Free was \nAfia Ahmed\n, who received quick treatment by doctors who were alerted to her deterioration when she became seriously ill after giving birth.\nNetty Messenger, a patient and volunteer at Musgrove Park Hospital, welcomed the trust\u2019s progress towards mobile working: \u201cThis is a great idea. I do banking and shopping online and get my prescriptions online, but hospitals still seem to have mountains of paper. It would be much better to have all the patient\u2019s information in one place in an app like this.\nDr Dominic King, clinical lead at DeepMind Health, said: \u201cNurses and doctors already using Streams are telling us that it is helping them deliver faster and better care for their patients. The Taunton and Somerset NHS Foundation Trust is well known for its pioneering approach to healthcare technology, so it\u2019s incredibly exciting to be working with the outstanding clinical team there, on the shared goal of improving outcomes for patients.\u201d \nTom Edwards, a consultant surgeon at the trust, said: \u201cFast access to information about patients is absolutely crucial for our doctors, nurses, and other clinical staff.\nAs one of the NHS\u2019s 16 global digital exemplar acute trusts, Taunton and Somerset aims to be one of the most innovative hospital trusts in the NHS. Over the next five years, we\u2019ll be working with the trust and their long-term partner in the digital exemplar programme, IMS MAXIMS, to roll out this cutting-edge technology for a range of medical conditions where early intervention can make all the difference.\nAs in all our Streams partnerships, we\u2019re putting patients at the heart of this work. In addition to their regular forums for updating patients and patient governors on their innovative work, over the coming weeks the trust will be hosting workshops, displays and open day events so that staff, patients and the public can see how the app works, what it will mean for patients, and how it might be developed in future.\nIn addition, members of the DeepMind team will be working alongside Taunton staff to engage patients, giving people an opportunity to ask questions about Streams and any other aspect of our work. These events will take place before any patient data is processed by DeepMind. All patient data will be stored to world-leading standards of security and encryption in a facility in England, separated at all times from any other systems.\n"}
{"title": "Differentiable neural computers", "contents": "In a \nrecent study in Nature\n, we introduce a form of memory-augmented neural network called a differentiable neural computer, and show that it can learn to use its memory to answer questions about complex, structured data, including artificially generated stories, family trees, and even a map of the London Underground. We also show that it can solve a block puzzle game using reinforcement learning.\nPlato likened memory to a wax tablet on which an impression, imposed on it once, would remain fixed. He expressed in metaphor the modern notion of plasticity \u2013 that our minds can be shaped and reshaped by experience. But the wax of our memories does not just form impressions, it also forms connections, from one memory to the next. Philosophers like John Locke believed that memories connected if they were formed nearby in time and space. Instead of wax, the most potent metaphor expressing this is Marcel Proust\u2019s madeleine cake; for Proust, one taste of the confection as an adult undammed a torrent of associations from his childhood. These episodic memories (event memories) are known to depend on the hippocampus in the human brain.\nToday, our metaphors for memory have been refined. We no longer think of memory as a wax tablet but as a reconstructive process, whereby experiences are reassembled from their constituent parts. And instead of a simple association between stimuli and behavioural responses, the relationship between memories and action is variable, conditioned on context and priorities. A simple article of memorised knowledge, for example a memory of the layout of the London Underground, can be used to answer the question, \u201cHow do you get from Piccadilly Circus to Moorgate?\u201d as well as the question, \u201cWhat is directly adjacent to Moorgate, going north on the Northern Line?\u201d. It all depends on the question; the contents of memory and their use can be separated. Another view holds that memories can be organised in order to perform computation. More like lego than wax, memories can be recombined depending on the problem at hand.\nNeural networks excel at pattern recognition and quick, reactive decision-making, but we are only just beginning to build neural networks that can think slowly \u2013 that is, deliberate or reason using knowledge. For example, how could a neural network store memories for facts like the connections in a transport network and then logically reason about its pieces of knowledge to answer questions? \nIn a recent paper\n, we showed how neural networks and memory systems can be combined to make learning machines that can store knowledge quickly and reason about it flexibly. These models, which we call differentiable neural computers (DNCs), can learn from examples like neural networks, but they can also store complex data like computers.\nIn a normal computer, the processor can read and write information from and to random access memory (RAM). RAM gives the processor much more space to organise the intermediate results of computations. Temporary placeholders for information are called variables and are stored in memory. In a computer, it is a trivial operation to form a variable that holds a numerical value. And it is also simple to make data structures \u2013 variables in memory that contain links that can be followed to get to other variables. One of the simplest data structures is a list \u2013 a sequence of variables that can be read item by item. For example, one could store a list of players\u2019 names on a sports team and then read each name one by one. A more complicated data structure is a tree. In a family tree for instance, links from children to parents can be followed to read out a line of ancestry. One of the most complex and general data structures is a graph, like the London Underground network.\nWhen we designed DNCs, we wanted machines that could learn to form and navigate complex data structures on their own. At the heart of a DNC is a neural network called a controller, which is analogous to the processor in a computer. A controller is responsible for taking input in, reading from and writing to memory, and producing output that can be interpreted as an answer. The memory is a set of locations that can each store a vector of information.\nA controller can perform several operations on memory. At every tick of a clock, it chooses whether to write to memory or not. If it chooses to write, it can choose to store information at a new, unused location or at a location that already contains information the controller is searching for. This allows the controller to update what is stored at a location. If all the locations in memory are used up, the controller can decide to free locations, much like how a computer can reallocate memory that is no longer needed. When the controller does write, it sends a vector of information to the chosen location in memory. Every time information is written, the locations are connected by links of association, which represent the order in which information was stored.\nAs well as writing, the controller can read from multiple locations in memory. Memory can be searched based on the content of each location, or the associative temporal links can be followed forward and backward to recall information written in sequence or in reverse. The read out information can be used to produce answers to questions or actions to take in an environment. Together, these operations give DNCs the ability to make choices about how they allocate memory, store information in memory, and easily find it once there.\nTo the non-technical reader, it may seem a bit odd that we have repeatedly used phrases like \u201cthe controller can\u201d or \u201cdifferentiable neural computers \u2026 make choices\u201d. We speak like this because differentiable neural computers learn how to use memory and how to produce answers completely from scratch. They learn to do so using the magic of optimisation: when a DNC produces an answer, we compare the answer to a desired correct answer. Over time, the controller learns to produce answers that are closer and closer to the correct answer. In the process, it figures out how to use its memory.\nWe wanted to test DNCs on problems that involved constructing data structures and using those data structures to answer questions. Graph data structures are very important for representing data items that can be arbitrarily connected to form paths and cycles. In the paper, we showed that a DNC can learn on its own to write down a description of an arbitrary graph and answer questions about it. When we described the stations and lines of the London Underground, we could ask a DNC to answer questions like, \u201cStarting at Bond street, and taking the Central line in a direction one stop, the Circle line in a direction for four stops, and the Jubilee line in a direction for two stops, at what stop do you wind up?\u201d Or, the DNC could plan routes given questions like \u201cHow do you get from Moorgate to Piccadilly Circus?\u201d\nIn a family tree, we showed that it could answer questions that require complex deductions. For example, even though we only described parent, child, and sibling relationships to the network, we could ask it questions like \u201cWho is Freya\u2019s maternal great uncle?\u201d We also found it possible to analyse how DNCs used their memories by visualising which locations in memory were being read by the controller to produce what answers. Conventional neural networks in our comparisons either could not store the information, or they could not learn to reason in a way that would generalise to new examples.\nWe could also train a DNC by reinforcement learning. In this framework, we let the DNC produce actions but never show it the answer. Instead, we score it with points when it has produced a good sequence of actions (like the children\u2019s game \u201chot or cold\u201d). We connected a DNC to a simple environment with coloured blocks arranged in piles. We would give it instructions for goals to achieve: \u201cPut the light blue block below the green; the orange to the left of the red; the purple below the orange; the light blue to the right of the dark blue; the green below the red; and the purple to the left of the green\u201d.\nWe could establish a large number of such possible goals and then ask the network to execute the actions that would produce one or another goal state on command. In this case, again like a computer, the DNC could store several subroutines in memory, one per possible goal, and execute one or another.\nThe question of how human memory works is ancient and our understanding still developing. We hope that DNCs provide both a new tool for computer science and a new metaphor for cognitive science and neuroscience: here is a learning machine that, without prior programming, can organise information into connected facts and use those facts to solve problems.\nFor more information about DNC, \nplease read our paper\n and an \nopinion piece\n by Herbert Jaeger about deep neural reasoning.\nOur open source implementation is available on \nGitHub\n.\n"}
{"title": "Working with the NHS to build lifesaving technology", "contents": "We\u2019re very proud to announce a groundbreaking five year partnership with the Royal Free London NHS Foundation Trust.\nDoctors and nurses in the NHS do a phenomenal job caring for patients, but they\u2019re being badly let down by technology. Pagers, fax machines and paper records are still standard in most NHS hospitals, and too often top-down IT systems don\u2019t meet clinical needs because they are built far away from the frontline of patient care.\nThis slow and outdated technology means that important changes in a patient\u2019s condition often don\u2019t get brought to the attention of the right clinician in time to prevent further serious illness. When this doesn\u2019t happen, the consequences for patients can be severe, and even fatal. At least ten thousand people a year die in UK hospitals through entirely preventable causes, and some 40% of patients could avoid being admitted to intensive care, if the right clinician was able to take the right action sooner.\nOur partnership aims to change that, by taking a very different approach to building IT for patient care. Together we are creating world-leading technology, in close collaboration with clinicians themselves, to ensure that the right patient information gets to the right clinicians at the right time, reducing preventable deaths and illnesses. \nThe five year partnership will build on the successful year-long joint project to build a smartphone app called \nStreams\n, which alerts clinical teams as soon as test results show that a patient is at risk of developing acute kidney injury (AKI) , providing them with the necessary contextual clinical information to help them to provide the right treatment before the patient\u2019s condition deteriorates.\nFollowing prototype testing, as well as registration with the Medicines and Healthcare products Regulatory Agency (MHRA), this first version of Streams is ready to be deployed to clinicians across the Royal Free hospital sites early in 2017.\nOver the course of the next five years, we\u2019re going to expand Streams to cover other illness where early intervention is key and technology can ensure this happens. We think that Streams could also be used to help patients at risk from sepsis and other causes of organ failure, where signs of deterioration are often difficult for clinicians to spot, and where early intervention can be the difference between life and death. We also plan to build additional features that Royal Free clinicians have asked for, including messaging and clinical task management that will support better care.\nWhen it\u2019s fully built, we believe that this will speed up the time to alert nurses and doctors to patients in need down from hours to a few seconds. And by freeing up clinicians\u2019 time from juggling multiple pager, desktop-based and paper systems, it should redirect over half a million hours per year away from admin and towards direct patient care at the Royal Free alone.\nThe partnership will also introduce an unprecedented level of data security and audit. All data access is logged, and subject to review by the Royal Free as well as DeepMind Health\u2019s nine \nIndependent Reviewers\n. Our software and data centres will also undergo deep technical audits by experts commissioned by our Independent Reviewers.\nIn addition, we\u2019re developing an unprecedented new infrastructure that will enable ongoing audit by the Royal Free, allowing administrators to easily and continually verify exactly when, where, by whom and for what purpose patient information is accessed. This is being built by one of the world\u2019s leading security engineers, Ben Laurie, co-founder of the OpenSSL project which enables encrypted connections to websites around the world (familiar to millions through the padlock in their browser bars).\nAnd the infrastructure that powers Streams will be built on state-of-the-art interoperable standards, allowing the Royal Free to have other developers build new services that integrate more easily with their systems. This will dramatically reduce the barrier to entry for developers who want to build for the NHS, opening up a wave of innovation - including the potential for the first AI-enabled tools, whether developed by DeepMind or others.\nDeepMind was set up to help solve some of society\u2019s toughest challenges. It\u2019s hard to think of a better way for us to make a real difference in the world than creating technology that will transform the NHS. We\u2019ll update again on our progress with the Royal Free as soon as there\u2019s more news to share.\n"}
{"title": "Reinforcement learning with unsupervised auxiliary tasks", "contents": "Our primary mission at DeepMind is to push the boundaries of AI, developing programs that can learn to solve any complex problem without needing to be taught how. Our reinforcement learning agents have achieved \nbreakthroughs in Atari 2600 games\n and the \ngame of Go\n. Such systems, however, can require a lot of data and a long time to learn so we are always looking for ways to improve our generic learning algorithms.\nOur recent paper \n\u201cReinforcement Learning with Unsupervised Auxiliary Tasks\u201d\n introduces a method for greatly improving the learning speed and final performance of agents. We do this by augmenting the standard \ndeep reinforcement learning\n methods with two main additional tasks for our agents to perform during training.\nA visualisation of our agent in a Labyrinth maze foraging task can be seen below.\nThe first task involves the agent learning how to control the pixels on the screen, which emphasises learning how your actions affect what you will see rather than just prediction. This is similar to how a baby might learn to control their hands by moving them and observing the movements. By learning to change different parts of the screen, our agent learns features of the visual input that are useful for playing the game and getting higher scores.\nIn the second task the agent is trained to predict the onset of immediate rewards from a short historical context. In order to better deal with the scenario where rewards are rare we present the agent with past rewarding and non-rewarding histories in equal proportion. By learning on rewarding histories much more frequently, the agent can discover visual features predictive of reward much faster.\nThe combination of these auxiliary tasks, together with our previous \nA3C paper\n is our new UNREAL agent (UNsupervised REinforcement and Auxiliary Learning). We tested this agent on a suite of 57 Atari games as well as a 3D environment called Labyrinth with 13 levels. In all the games, the same UNREAL agent is trained in the same way, on the raw image output from the game, to produce actions to maximise the score or reward of the agent in the game. The behaviour required to get game rewards is incredibly varied, from picking up apples in 3D mazes to playing Space Invaders - the same UNREAL algorithm learns to play these games often to human level and beyond.\nIn Labyrinth, the result of using the auxiliary tasks - controlling the pixels on the screen and predicting when reward is going to occur - means that UNREAL is able to learn over 10x faster than our previous best A3C agent, and reaches far better performance. We can now achieve 87% of expert human performance averaged across the Labyrinth levels we considered, with super-human performance on a number of them. On Atari the agent now achieves on average 9x human performance. We hope that this work will allow us to scale up our agents to ever more complex environments.\nRead the paper: \nReinforcement Learning with Unsupervised Auxiliary Tasks\n"}
{"title": "DeepMind Papers @ NIPS (Part 1)", "contents": "Authors: \nPeter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Rezende, Koray Kavukcuoglu\nReasoning about objects, relations, and physics is central to human intelligence, and a key goal of artificial intelligence. However many modern machine learning methods still face a trade-off between expressive structure and efficient performance. \nWe introduce \u201cinteraction networks\u201d, which can reason about how objects in complex systems interact, supporting dynamical predictions, as well as inferences about the abstract properties of the system. Interaction networks are both expressive and efficient because they combine three powerful approaches: structured models, simulation, and deep learning. They take as input graph-structured data, perform object- and relation-centric reasoning in a way that is analogous to a simulation, and are implemented using deep neural networks. They are invariant to permutations of the entities and relations, which allows them to automatically generalize to systems of different sizes and structures than they have experienced during training.\nIn our experiments, we used interaction networks to implement the first general-purpose learnable physics engine. After training only on single step predictions, our model was able to simulate the physical trajectories of n-body, bouncing ball, and non-rigid string systems accurately over thousands of time steps. The same architecture was also able to infer underlying physical properties, such as potential energy. \nBeyond physical reasoning, interaction networks may provide a powerful framework for AI approaches to scene understanding, social perception, hierarchical planning, and analogical reasoning.\nFor further details and related work, please see \nthe paper\n. \nFor applications of interaction networks to scene understanding and imagination-based decision-making, please see our submissions to ICLR 2017: \nDiscovering objects and their relations from entangled scene representations\n and \nMetacontrol for Adaptive Imagination-Based Optimization\nCheck it out at NIPS:\n\u200d\nMon Dec 5th 06:00 - 09:30 PM @ Area 5+6+7+8 #48\nFri Dec 9th 08:00 - 6:30 PM @ Hilton Diag. Mar, Blrm. C\nAuthors: \nAlexander (Sasha) Vezhnevets, Volodymyr Mnih, Simon Osindero, Alex Graves, Oriol Vinyals, John Agapiou, Koray Kavukcuoglu\nLearning temporally extended actions and temporal abstraction in general is a long standing problem in reinforcement learning. They facilitate learning by enabling structured exploration and economic computation. In this paper we present a novel deep recurrent neural network architecture that learns to build implicit plans in an end-to-end manner purely by interacting with an environment in a reinforcement learning setting. The network builds an internal plan, which is continuously updated upon observation of the next input from the environment. It can also partition this internal representation into contiguous sub-sequences by learning for how long the plan can be committed to \u2013 i.e. followed without replanning. Combining these properties, the proposed model, dubbed STRategic Attentive Writer (STRAW) can learn high-level, temporally abstracted macro-actions of varying lengths that are solely learnt from data without any prior information.\nWatch the video \nhere\n.\nFor further details and related work, please see \nthe paper\n.\nCheck it out at NIPS:\n\u200d\nMon Dec 5th 06:00 - 09:30 PM @ Area 5+6+7+8 #111\nAuthors: \nOriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, Daan Wierstra\nGiven just a few, or even a single, examples of an unseen class, it is possible to attain high classification accuracy on ImageNet using Matching Networks. \u00a0The core architecture is simple and straightforward to train and performant across a range of image and text classification tasks.\nMatching Networks are trained in the same way as they are tested: by presenting a series of instantaneous one shot learning training tasks, where each instance of the training set is fed into the network in parallel. Matching Networks are then trained to classify correctly over many different input training sets. The effect is to train a network that can classify on a novel data set without the need for a single step of gradient descent.\nFor further details and related work, please see \nthe paper\n.\nCheck it out at NIPS:\n\u200d\nMon Dec 5th 06:00 - 09:30 PM @ Area 5+6+7+8 #139\nAuthors: \nRemi Munos, Tom Stepleton, Anna Harutyunyan, Marc G. Bellemare\nOur goal is to design a Reinforcement Learning (RL) algorithm with two desired properties. Firstly, to use off-policy data, which is important for exploration, when we use memory replay, or observe log-data. Secondly, to use multi-steps returns in order to propagate rewards faster and avoid accumulation of approximation/estimation errors. Both properties are crucial in deep RL. \nWe introduce the \u201cRetrace\u201d algorithm, which uses multi-steps returns and can safely and efficiently utilize any off-policy data. We show the convergence of this algorithm in both policy evaluation and optimal control settings.\nAs corollary we prove the convergence of Watkin\u2019s Q(\u03bb) to Q* (which was an open problem since 1989). \nFinally we report numerical results on the Atari domain that demonstrate the huge benefit of Retrace over competitive algorithms.\nFor further details and related work, please see \nthe paper\n.\nCheck it out at NIPS:\n\u200d\nMon Dec 5th 06:00 - 09:30 PM @ Area 5+6+7+8 #151\nAuthors: \nJean-Bastien Grill (INRIA), Michal Valko (INRIA), Remi Munos\nYou are a robot and you live in a Markov decision process (MDP) with a finite or an infinite number of transitions from state-action to next states. You got brains and so you plan before you act. Luckily, your roboparents equipped you with a generative model to do some Monte-Carlo planning. The world is waiting for you and you have no time to waste. You want your planning to be efficient. Sample-efficient. Indeed, you want to exploit the possible structure of the MDP by exploring only a subset of states reachable by following near-optimal policies. You want guarantees on sample complexity that depend on a measure of the quantity of near-optimal states. You want something, that is an extension of Monte-Carlo sampling (for estimating an expectation) to problems that alternate maximization (over actions) and expectation (over next states). You want something simple to implement and computationally efficient. You want it all and you want it now. You want TrailBlazer.\nFor further details and related work, please see \nthe paper\n.\nCheck it out at NIPS:\n\u200d\nTue Dec 6th 05:00 - 05:20 PM @ Area 3 (Oral) in Theory\nTue Dec 6th @ Area 5+6+7+8 #193\nAuthors:\n Ian Osband, Charles Blundell, Alex Pritzel and Benjamin Van Roy\nEfficient exploration in complex environments remains a major challenge for reinforcement learning (RL). We\u2019ve seen a lot of recent breakthroughs in RL, but many of these algorithms require huge amounts of data (millions of games) before they learn to make good decisions. In many real-world settings, such large amounts of data aren\u2019t feasible.\nOne of the reasons these algorithms learn so slowly is that they do not gather the *right* data to learn about the problem. These algorithms use dithering (taking random actions) to explore their environment - which can be exponentially less efficient that *deep* exploration which prioritizes potentially informative policies over multiple timesteps. There is a large literature on algorithms for deep exploration for statistically efficient reinforcement learning. The problem is that none of these algorithms are computationally tractable with deep learning\u2026 until now.\nKey breakthroughs in this paper include the following:\nFor further details and related work, please see \nthe paper\n and our video playlist \nhere\n.\nCheck it out at NIPS:\n\u200d\nMon Dec 5th 06:00 - 09:30 PM @ Area 5+6+7+8 #79\n"}
{"title": "Open-sourcing DeepMind Lab", "contents": "DeepMind's scientific mission is to push the boundaries of AI, developing systems that can learn to solve any complex problem without needing to be taught how. To achieve this, we work from the premise that AI needs to be general. Agents should operate across a wide range of tasks and be able to automatically adapt to changing circumstances. That is, they should not be pre-programmed, but rather, able to learn automatically from their raw inputs and reward signals from the environment. There are two parts to this research program: (1) \u00a0designing ever-more intelligent agents capable of more-and-more sophisticated cognitive skills, and (2) building increasingly complex environments where agents can be trained and evaluated.\nThe development of innovative agents goes hand in hand with the careful design and implementation of rationally selected, flexible and well-maintained environments. To that end, we at DeepMind have invested considerable effort toward building rich simulated environments to serve as \u00a0\u201claboratories\u201d for AI research. Now we are open-sourcing our flagship platform, \u00a0DeepMind Lab, so the broader research community can make use of it.\nDeepMind Lab is a fully 3D game-like platform tailored for agent-based AI research. It is observed from a first-person viewpoint, through the eyes of the simulated agent. Scenes are rendered with rich science fiction-style visuals. The available actions allow agents to look around and move in 3D. The agent\u2019s \u201cbody\u201d is a floating orb. It levitates and moves by activating thrusters opposite its desired direction of movement, and it has a camera that moves around the main sphere as a ball-in-socket joint tracking the rotational look actions. Example tasks include collecting fruit, navigating in mazes, traversing dangerous passages while avoiding falling off cliffs, bouncing through space using launch pads to move between platforms, playing laser tag, and quickly learning and remembering random procedurally generated environments. An illustration of how agents in DeepMind Lab perceive and interact with the world can be seen below:\nArtificial general intelligence research in DeepMind Lab emphasizes navigation, memory, 3D vision from a first person viewpoint, motor control, planning, strategy, time, and fully autonomous agents that must learn for themselves what tasks to perform by exploring their environment. All these factors make learning difficult. Each are considered frontier research questions in their own right. Putting them all together in one platform, as we have, represents a significant new challenge for the field.\nDeepMind Lab is highly customisable and extendable. New levels can be authored with off-the-shelf editor tools. In addition, DeepMind Lab includes an interface for programmatic level-creation. Levels can be customised with gameplay logic, item pickups, custom observations, level restarts, reward schemes, in-game messages and more. The interface can be used to create levels in which novel map layouts are generated on the fly while an agent trains. These features are useful in, for example, testing how an agent copes with unfamiliar environments. Users will be able to add custom levels to the platform via GitHub. The assets will be hosted on GitHub alongside all the code, maps and level scripts. Our hope is that the community will help us shape and develop the platform going forward.\nDeepMind Lab has been used internally at DeepMind for some time (\nexample\n). We believe it has already had a significant impact on our thinking concerning numerous aspects of intelligence, both natural and artificial. However, our efforts so far have only barely scratched the surface of what is possible in DeepMind Lab. There are opportunities for significant contributions still to be made in a number of mostly still untouched research domains now available through DeepMind Lab, such as navigation, memory and exploration.\nAs well as facilitating agent evaluation, there are compelling reasons to think that it may be fundamentally easier to develop intelligence in a 3D world, observed from a first-person viewpoint, like DeepMind Lab. After all, the only known examples of general-purpose intelligence in the natural world arose from a combination of evolution, development, and learning, grounded in physics and the sensory apparatus of animals. It is possible that a large fraction of animal and human intelligence is a direct consequence of the richness of our environment, and unlikely to arise without it. Consider the alternative: if you or I had grown up in a world that looked like Space Invaders or Pac-Man, it doesn\u2019t seem likely we would have achieved much general intelligence!\nRead the \nfull paper here\n\u200d\nAccess DeepMind's GitHub repository \nhere\n.\n"}
{"title": "DeepMind Papers @ NIPS (Part 2)", "contents": "The second blog post in this series, sharing brief descriptions of the papers we are presenting at NIPS 2016 Conference in Barcelona.\nAuthors: \nMarco Fraccaro, S\u00f8ren Kaae S\u00f8nderby, Ulrich Paquet, Ole Winther\nMuch of our reasoning about the world is sequential, from listening to sounds and voices and music, to imagining our steps to reach a destination, to tracking a tennis ball through time. All these sequences have some amount of latent random structure in them. Two powerful and complementary models, recurrent neural networks (RNNs) and stochastic state space models (SSMs), are widely used to model sequential data like these. RNNs are excellent at capturing longer-term dependencies in data, while SSMs model uncertainty in the sequence's underlying latent random structure, and are great for tracking and control.\nIs it possible to get the best of both worlds? In this paper we show how you can, by carefully layering deterministic (RNN) and stochastic (SSM) layers. We show how you can efficiently reason about a sequence\u2019s present latent structure, given its past (filtering) and also its past and future (smoothing).\nFor further details and related work, please see the paper \nhttps://arxiv.org/abs/1605.07571\nCheck it out at NIPS:\n\u200d\nTue Dec 6th 05:20 - 05:40 PM @ Area 1+2 (Oral) in Deep Learning\nTue Dec 6th 06:00 - 09:30 PM @ Area 5+6+7+8 #179\nAuthors: \nMarcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew Hoffman, David Pfau, Tom Schaul, Nando De Freitas\nOptimization algorithms today are typically designed by hand; algorithm designers, thinking carefully about each problem, are able to design algorithms that exploit structure that they can characterize precisely. \u00a0This design process mirrors the efforts of computer vision in the early 2000s to manually characterize and locate features like edges and corners in images with hand designed features. The biggest breakthrough of modern computer vision has been to instead learn these features directly from data, removing manual engineering from the loop. This paper shows how we can extend these techniques to algorithm design, learning not only features but also learning about the learning process itself.\nWe show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms outperform standard hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including neural network training, and styling images with neural art.\nFor further details and related work, please see the paper \nhttps://arxiv.org/abs/1606.04474\nCheck it out at NIPS:\n\u200d\nTue Dec 6th 06:00 - 09:30 PM @ Area 5+6+7+8 #9\nThursday Dec 8th 02:00 - 9:30 PM @ Area 1+2 (Deep Learning Symposium - Poster)\nFriday Dec 9th 08:00 AM - 06:30 PM @ Area 1 (DeepRL Workshop - Talk by Nando De Freitas)\nFriday Dec 9th 08:00 AM - 06:30 PM @ Area 5+6 (Nonconvex Optimization for Machine Learning: Theory and Practice - Talk by Nando De Freitas)\nSaturday Dec 10th 08:00 AM - 6:30 PM @ Area 2 (Optimizing the Optimizers - Talk by Matthew W. Hoffman)\nAuthors: \nNavdeep Jaitly, Quoc V. Le, Oriol Vinyals, Ilya Sutskever, David Sussillo, Samy Bengio\nModels which map from a sequence of observations to another sequence (sequence-to-sequence) have become extremely popular in the last two years due to their generality, achieving state-of-the-art results in a variety of tasks such as translation, captioning, or parsing. The main drawback of these models is that they need to read in the whole sequence of inputs \u201cx\u201d before starting producing the resulting output sequence \u201cy\u201d. In our paper we circumvent these limitations by allowing the model to emit output symbols before the whole input sequence has been read. Although this introduces some independence assumptions, making online decisions in certain domains such as speech recognition or machine translation makes these models much more desirable.\nFor further details and related work, please see the paper\nhttp://papers.nips.cc/paper/6594-an-online-sequence-to-sequence-model-using-partial-conditioning.pdf\nCheck it out at NIPS:\n\u200d\nTue Dec 6th 06:00 - 09:30 PM @ Area 5+6+7+8 #53\nAuthors: \nAudrunas Gruslys, Remi Munos, Ivo Danihelka, Marc Lanctot, Alex Graves\nMany state of art results were achieved by training large recurrent models over long sequences of input data. \u00a0Training recurrent networks is not an easy task for many reasons. One of complications is a large memory consumption of the standard backpropagation through time (BPTT) algorithm, as \u00a0it requires memorizing all or almost all past neuron activations. It is especially easy to run out of expensive GPU memory when training convolutional RNNs, and memory constraints often lead to unwanted compromises in network size. A common solution used to alleviate this problem is to memorize only some of intermediate neuron activations and recompute others on demand. While there were many heuristics that trade off memory and computation, most of them are adapted for certain edge cases and are suboptimal. We viewed the problem as a dynamic programming problem which allowed us to find a class of provably optimal strategies subject to memory constraints. For sequences of length 1000, our algorithm saves 95% of memory usage while using only one third more time per learning step than the standard BPTT.\nFor further details and related work, please see the paper \nhttps://papers.nips.cc/paper/6220-memory-efficient-backpropagation-through-time.pdf\nCheck it out at NIPS:\n\u200d\nTue Dec 6th 06:00 - 09:30 PM @ Area 5+6+7+8 #64\nAuthors: \nKarol Gregor, Frederic Besse, Danilo Rezende, Ivo Danihelka, Daan Wierstra\nDiscovering high level abstract representations is one of the primary goals of unsupervised learning. We approach this problem by designing an architecture that transforms the information stored in pixels into an ordered sequence of information carrying representations. Training results in an emergent order, where early representations carry information about the more global & conceptual aspects of the image, while the latter representations correspond to the details. The model is a fully convolutional, sequential variational autoencoder inspired by DRAW. The architecture is simple and homogeneous and therefore does not require many design choices.\nThe resulting information transformation can be used for lossy compression, by transmitting only the early set of representations (the number of which is given by the desired compression level) and generating the remaining ones as well as the image using the generative model. If the ordering of information that the model discovers correlates strongly with the ordering of information by importance as judged by humans, then the algorithm will transmit what humans consider to be the most important. If the generation of the remaining variables results in a high quality image, this method should lead to high quality lossy compression. Because both humans and unsupervised algorithms try to understand data and because both use deep networks to do so, there is a good reason to believe that this approach will work. We demonstrate that this is indeed the case and the current model already results in performance that compares favorably to that of JPEG and JPEG 2000. As generative models are progressively getting better, these results demonstrate the potential of this method for building future compression algorithms.\nFor further details and related work, please see the paper \nhttp://papers.nips.cc/paper/6542-towards-conceptual-compression.pdf\nCheck it out at NIPS:\n\u200d\nTue Dec 6th 06:00 - 09:30 PM @ Area 5+6+7+8 #77\nAuthors: \nDanilo Rezende, Ali Eslami, Shakir Mohamed, Peter Battaglia, Max Jaderberg, Nicolas Heess\nImagine looking at a photograph of a chair. The image you see will be a complex function of the attributes and positions of the camera, the lights and, of course of the shape of the chair. Importantly, due to self-occlusion you never see the full chair, so there is an infinite number chair-like objects that would be consistent with what you see. Nevertheless, when asked how to imagine the chair's shape from a different point of view you will probably be able to do so quite accurately. Key to this ability is not just an implicit understanding of perspective, occlusion and the image formation process, but critically your prior knowledge of what a plausible chair ought to look like, which allows you to \u201cfill in\u201d the missing parts.\nIn this paper we study models that are able to perform similar types of reasoning. Specifically, we formulate generative models which can learn about the statistical regularities of the three-dimensional shape of objects. The resulting prior over shapes produces high-quality samples, and allows us to formulate challenging ill-posed problems such as that of recovering plausible 3D structures given a 2D image as probabilistic inference, accurately capturing the multi-modality of the posterior. This inference can be achieved rapidly with a single forward-pass through a neural network and we show how both the models and inference networks can be trained end-to-end directly from 2D images without any use of ground-truth 3D labels, therefore demonstrating for the first time the feasibility of learning to infer 3D representations of the world in a purely unsupervised manner.\nFor further details and related work, please see the paper \nhttps://arxiv.org/abs/1607.00662\n and our video: \nhttps://www.youtube.com/watch?v=stvDAGQwL5c\nCheck it out at NIPS:\n\u200d\nWed Dec 7th 06:00 - 09:30 PM @ Area 5+6+7+8 #2\n"}
{"title": "DeepMind papers at ICML 2017 (part two)", "contents": "The second of \u00a0our three-part series, which gives an overview of the papers we are presenting at the ICML 2017 Conference in Sydney, Australia.\nAuthors: \nIan Osband, Benjamin Van Roy\nComputational results demonstrate that posterior sampling for reinforcement learning (PSRL) dramatically outperforms existing algorithms driven by optimism, such as UCRL2. We provide insight into the extent of this performance boost and the phenomenon that drives it. We leverage this insight to establish an $\\tilde{O}(H\\sqrt{SAT})$ Bayesian regret bound for PSRL in finite-horizon episodic Markov decision processes. This improves upon the best previous Bayesian regret bound of $\\tilde{O}(H S \\sqrt{AT})$ for any reinforcement learning algorithm. Our theoretical results are supported by extensive empirical evaluation.\nFor further details and related work, please see the \npaper\n.\nCheck it out at ICML:\nMonday 07 August, 11:42-12:00 @ C4.5 (Talk)\nMonday 07 August, 18:30-22:00 \u00a0@ Gallery #36 (Poster)\nAuthors: \nIrina Higgins*, Arka Pal*, Andrei Rusu, Loic Matthey, Chris Burgess, Alexander Pritzel, Matt Botvinick, Charles Blundell, Alexander Lerchner\nModern deep reinforcement learning agents rely on large quantities of data to learn how to act. In some scenarios, such as robotics, obtaining a lot of training data may be infeasible. Hence such agents are often trained on a related task where data is easy to obtain (e.g. simulation) with the hope that the learnt knowledge will generalise to the task of interest (e.g. reality). We propose DARLA, a DisentAngled Representation Learning Agent, that exploits its interpretable and structured vision to learn how to act in a way that is robust to various novel changes in its environment - including a simulation to reality transfer scenario in robotics. We show that DARLA significantly outperforms all baselines, and that its performance is crucially dependent on the quality of its vision.\nFor further details and related work, please see the \npaper\n.\nCheck it out at ICML:\nMonday 07 August, 16:42-17:00 @ C4.5 (Talk)\\\nMonday 07 August, 18:30-22:00 @ Gallery #123 (Poster)\nAuthors:\n \nAlex Graves, Marc G. Bellemare, Jacob Menick, Koray Kavukcuoglu, Remi Munos\nAs neural networks are applied to ever more complex problems, the need for efficient curriculum learning becomes more pressing. However, designing effective curricula is difficult and typically requires a large amount of hand-tuning. This paper uses reinforcement learning to automate the path, or syllabus, followed by the network through the curriculum so as to maximise the overall rate of learning progress. We consider nine different progress indicators, including a novel class of complexity-gain signal. Experimental results on three problems show that an automatically derived syllabus can lead to efficient curriculum learning, even on data (such as the bAbI tasks) that were not explicitly designed for curriculum learning. \nFor further details and related work, please see the \npaper\n.\nCheck it out at ICML:\nMonday 07 August, 16:42-17:00 @ C4.6 & C4.7 (Talk)\nMonday 07 August, 18:30-20:00 @ Gallery #127 (Poster)\nAuthors: \nYutian Chen, Matthew Hoffman, Sergio Gomez, Misha Denil, Timothy Lillicrap, Matthew Botvinick , Nando de Freitas\nWe learn recurrent neural network optimisers trained on simple synthetic functions by gradient descent. The learned optimisers exhibit a remarkable degree of transfer in that they can be used to efficiently optimise a broad range of derivative-free black-box problems, including continuous bandits, control problems, global optimization benchmarks and hyper-parameter tuning tasks.\nFor further details and related work, please see the \npaper\n.\nCheck it out at ICML:\nMonday 07 August, 17:15-17:33 @ Darling Harbour Theatre (Talk)\nTuesday 08 August, 18:30-22:00 @ Gallery #6 (Poster)\nAuthors:\n \nMarc G. Bellemare*, Will Dabney*, Remi Munos\nWe argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman's equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.\nFor further details and related work, please see the \nblog post\n and the \npaper\n.\nCheck it out at ICML: \nMonday 07 August, 17:33-17:51 @ C4.5 (Talk)\nTuesday 08 August, 18:30-22:00 @ Gallery #13 (Poster)\nAuthors: \nMarlos Machado (Univ. Alberta), Marc G. Bellemare, Michael Bowling\nRepresentation learning and option discovery are two of the biggest challenges in reinforcement learning (RL). Proto-value functions (PVFs) are a well-known approach for representation learning in MDPs. In this paper we address the option discovery problem by showing how PVFs implicitly define options. We do it by introducing eigenpurposes, intrinsic reward functions derived from learned representations. The options discovered from eigenpurposes traverse the principal directions of the state space. They are useful for multiple tasks because they are discovered without taking the environment\u2019s rewards into consideration. Moreover, different options act at different time scales, making them helpful for exploration. We demonstrate features of eigenpurposes in traditional tabular domains as well as in Atari 2600 games.\nFor further details and related work, please see the \npaper\n.\nCheck it out at ICML: \nMonday 07 August, 18:09-18:27 @ C4.5 (Talk)\nTuesday 08 August 18:30-20:00 @ Gallery #23 (Poster)\nAuthors: \nSander Dieleman, Karen Simonyan, Jesse Engel (Google Brain), Cinjon Resnick (Google Brain), Adam Roberts (Google Brain), Douglas Eck (Google Brain), Mohammad Norouzi (Google Brain)\nIn this paper, we introduce a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform. We also introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets. Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline. Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive.\nFor further details and related work, please see the \npaper\n.\nCheck it out at ICML:\nTuesday 08 August, 14:42-15:00 @ Parkside 1 (Talk)\nTuesday 08 August, 18:30-22:00 @ Gallery #98 (Poster)\n"}
{"title": "Applying machine learning to radiotherapy planning for head & neck cancer", "contents": "We\u2019re excited to announce a new research partnership with the Radiotherapy Department at University College London Hospitals NHS Foundation Trust, which provides world-leading cancer treatment.\n1 in 75 men and 1 in 150 women will be diagnosed with oral cancer during their lifetime, and oral cavity cancer has risen by 92% since the 1970s. Head and neck cancer in general affects over 11,000 patients in the UK alone each year.\nAdvances in treatment such as radiotherapy have improved survival rates, but because of the high number of delicate structures concentrated in this area of the body, clinicians have to plan treatment extremely carefully to ensure none of the vital nerves or organs are damaged.\nThat makes a cancer at the back of the mouth or in the sinuses, for example, particularly hard to treat with radiotherapy.\nSo with clinicians in UCLH\u2019s world-leading radiotherapy team we are exploring whether machine learning methods could reduce the amount of time it takes to plan radiotherapy treatment for such cancers.\nBefore radiotherapy can be administered, clinicians have to produce a detailed map of the areas of the body to be treated, and the areas to avoid.\nThe process, known as segmentation, involves drawing around different parts of the anatomy and feeding the information through to a radiotherapy machine, which can then target cancers while leaving healthy tissue unharmed.\nBut when a tumour and vital anatomical structures are found in such close proximity, as in the head and neck, the outlines clinicians produce must be painstakingly detailed.\nFor these cancers, segmentation can take around four hours. And even though UCLH\u2019s specialist team at its dedicated head and neck cancer centre is a national leader in this process, there is still potential for innovation. We think machine learning could make a difference.\nOur collaboration will see us carefully analyse anonymised scans from up to seven hundred former patients at UCLH, to determine the potential for machine learning to make radiotherapy planning more efficient.\nClinicians will remain responsible for deciding radiotherapy treatment plans but it is hoped that the segmentation process could be reduced from up to four hours to around an hour.\nWe hope that in time, the research could lead to two benefits in particular:\nAs with all our work with the NHS, we will treat the patient data we are using in this project with the utmost care and respect. All scans will be anonymised in line with the UCLH Information Governance policy before they are shared with DeepMind. You can read more about our own approach to information governance \nhere\n.\nThis kind of research is still exploratory, but we think it has great potential to help both clinicians and patients.\nThis image has been reproduced, unmodified, from \nThe Cancer Image Archive\n under the terms of the \nCreative Commons Attribution 3.0 Unported License\n.\n"}
{"title": "DeepMind papers at ICML 2017 (part three)", "contents": "The final part of our three-part series that gives an overview of the papers we are presenting at the ICML 2017 Conference in Sydney, Australia.\nAuthors: \nSamuel Ritter*, David Barrett*, Adam Santoro, Matt Botvinick\nDeep neural networks (DNNs) have achieved unprecedented performance on a wide range of tasks, rapidly outpacing our understanding of the nature of their solutions. In this work, we propose to address this interpretability problem in modern DNNs using the problem descriptions, theories and experimental methods developed of cognitive psychology. In a case study, we apply a theory and method from the psychology of human word learning to better understand how modern one-shot learning systems work. Results revealed not only that our DNNs exhibit the same inductive bias as humans, but also several unexpected features of the DNNs.\nFor further details and related work, please see the\n paper\n.\nCheck it out at ICML: \nTuesday 08 August, 15:48-16:06 @ Darling Harbour Theatre (Talk)\nTuesday 08 August, 18:30-20:00 @ Gallery #113 (Poster)\nAuthors: \nGeorg Ostrovski, Marc Bellemare, Aaron van den Oord, Remi Munos\nCount-based exploration based on prediction gain of a simple graphical density model has previously achieved \u00a0state-of-the-art results on some of the hardest exploration games in Atari. We investigate the open questions 1) whether a better density model leads to better exploration, and 2) what role the mixed Monte Carlo update rule used in this work plays for exploration. We show that a neural density model - PixelCNN - can be trained online on the experience stream of an RL agent and used for count-based exploration to achieve even better results on a wider set of hard exploration games, while preserving higher performance on easy exploration games. We also show that the Monte Carlo return is crucial in making use of the intrinsic reward signal in the sparsest reward settings, and cannot easily be replaced by a softer lambda-return update rule.\nFor further details and related work, please see the \npaper\n.\nCheck it out at ICML:\nWednesday 09 August, 13:30-13:48 @ C4.5 (Talk)\nWednesday 09 August, 18:30-22:00 @ Gallery #64 (Poster)\nAuthors: \nDavid Silver, Hado van Hasselt, Matteo Hessel, Tom Schaul, Arthur Guez, Tim Harley, Gabriel Dulac-Arnold, David Reichert, Neil Rabinowitz, Andre Barreto, Thomas Degris\nOne of the key challenges of artificial intelligence is to learn models that are effective in the context of planning. In this document we introduce the predictron architecture. The predictron consists of a fully abstract model, represented by a Markov reward process, that can be rolled forward multiple \u201cimagined\" planning steps. Each forward pass of the predictron accumulates internal rewards and values over multiple planning depths. The predictron is trained end-to-end so as to make these accumulated values accurately approximate the true value function. We applied the predictron to procedurally generated random mazes and a simulator for the game of pool. The predictron yielded significantly more accurate predictions than conventional deep neural network architectures.\nFor further details and related work, please see the \npaper\n.\nCheck it out at ICML: \nWednesday 09 August, 14:24-14:42 @ C4.5 (Talk)\nWednesday 09 August 18:30-20:00 @ Gallery #91 (Poster)\nAuthors: \nSasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Hees, Max Jaderberg, David Silver, Koray Kavukcuoglu\nHow to create agents that can learn to decompose their behaviour into meaningful primitives and then reuse them to more efficiently acquire new behaviours is a long standing research question. The solution to this question may be an important stepping stone towards agents with general intelligence and competence. This paper introduced FeUdal Networks (FuN), a novel architecture that formulates sub-goals as directions in latent state space, which, if followed, translates into a meaningful behavioural primitives. FuN clearly separates the module that discovers and sets sub-goals from the module that generates behaviour through primitive actions. This creates a natural hierarchy that is stable and allows both modules to learn in complementary ways. Our experiments clearly demonstrate that this makes long-term credit assignment and memorisation more tractable. This also opens many avenues for further research, for instance: deeper hierarchies can be constructed by setting goals at multiple time scales, scaling agents to truly large environments with sparse rewards and partial observability.\nFor further details and related work, please see the \npaper\n.\nCheck it out at ICML:\nWednesday 09 August, 15:30-15:48 @ C4.5 (Talk)\nWednesday 09 August, 18:30-20:00 \u00a0@ Gallery #107 (Poster)\nAuthors: \nAlex Pritzel, Benigno Uria, Sriram Srinivasan, Adria Puigdomenech, Oriol Vinyals, Demis Hassabis, Daan Wierstra, Charles Blundell\nDeep reinforcement learning algorithms have achieved state of the art performance on a variety of tasks, however they tend to be grossly data inefficient. In this work we propose a novel algorithm that allows rapid incorporation of new information collected by the agent. For this we introduce a new differentiable data structure, a differentiable neural dictionary, that can incorporate new information immediately, while being able to update it\u2019s internal representation based on the task the algorithm is supposed to solve. Our agent, Neural Episodic Control, is built on top of the differentiable data structure and is able to learn significantly faster across a wide range of environments.\nFor further details and related work, please see the \npaper\n.\nCheck it out at ICML: \nWednesday 09 August, 16:06-16:24 @ C4.5\nWednesday 09 August, 18:30-22:00 @ Gallery #125\nAuthors: \nJustin Gilmer (Google Brain), Sam Schoenholz (Google Brain), Patrick Riley (Google Google), Oriol Vinyals, George Dahl (Google Brain)\nIn this work we show how we can gain orders of magnitude \u00a0improvements to run-time performance by treating an expensive simulation of quantum chemistry properties as a supervised dataset to be learnt by extending neural networks to operate on graphs. Our model is extremely accurate and very fast. In the manuscript we also provide a unifying framework which summarises previous work on graph-shaped inputs and neural networks.\nFor further details and related work, please see the \npaper\n.\nCheck it out at ICML:\nWednesday 09 August, 16:24-16:42 @ Darling Harbour Theatre (Talk)\nWednesday 09 August, 18:30-22:00 @ Gallery #131 (Poster)\n"}
{"title": "Bringing the best of mobile technology to Imperial College Healthcare NHS Trust", "contents": "We\u2019re really excited to announce that we\u2019ve agreed a five year partnership with Imperial College Healthcare NHS Trust, helping them make the most of the opportunity for mobile clinical applications to improve care. This is now our second NHS partnership for clinical apps, following a similar \npartnership\nwe announced last month with the Royal Free London NHS Foundation Trust.\nOver the last two years, the Trust has moved from paper to electronic patient records, and mobile technology is the natural next stage of this work. By giving clinicians access to cutting-edge healthcare apps that link to electronic patient records, they\u2019ll be able to access information on the move, react quickly in response to changing patient needs, and ultimately provide even better care.\nWe\u2019ll be working with the Trust to deploy our clinical app, \nStreams\n, which supports clinicians in caring for patients at risk of deterioration, particularly with conditions where early intervention can make all the difference. Like breaking news alerts on a mobile phone, the technology will notify nurses and doctors immediately when test results show a patient is at risk of becoming seriously ill. It will also enable clinicians at the Trust to securely assign and communicate about clinical tasks, and give them the information they need to make diagnoses and decisions.\nThis partnership builds on a relationship between the Trust, Imperial College, and DeepMind. The task management features in Streams are underpinned by a world class programme of widely published research and early product development carried out at Imperial College London and the Trust, as part of an app called Hark co-founded by Lord Ara Darzi. Using simulated data, Hark was found to improve the quality of transfer of information between staff and was rated by users as more effective and efficient, and less distracting than pagers [\nwww.jmir.org/2016/4/e79/\n]. Hark became part of DeepMind in early 2016.\nAs in our partnership with the Royal Free, we\u2019re also implementing state-of-the-art open and interoperable standards, via what\u2019s known as a FHIR API. This will allow the Trust to easily, securely and consistently integrate other apps that could improve care, whether developed by third parties or innovators within the Trust. Our partners at Imperial are excited about the potential for a wide range of apps to improve care, and we\u2019re delighted to be working with them to make this possible.\nOur hope is that the infrastructure and apps we\u2019re building not only help improve care in the short term, but also make it much easier for Trusts to bring new innovations to the clinical frontlines in future.\n"}
{"title": "DeepMind\u2019s work in 2016: a round-up", "contents": "In a world of fiercely complex, emergent, and hard-to-master systems - from our climate to the diseases we strive to conquer - we believe that intelligent programs will help unearth new scientific knowledge that we can use for social benefit. To achieve this, we believe we\u2019ll need general-purpose learning systems that are capable of developing their own understanding of a problem from scratch, and of using this to identify patterns and breakthroughs that we might otherwise miss. This is the focus of our long-term research mission at DeepMind.\nWhile we remain a long way from anything that approximates what you or we would term intelligence, 2016 was a big year in which we made exciting progress on a number of the core underlying challenges, and saw the first glimpses of the potential for positive real-world impact.\nOur program \nAlphaGo\n, for which we were lucky enough to receive our \nsecond Nature front cover\n, took on and beat the world champion Lee Sedol at the ancient game of Go, a feat that many experts said came a decade ahead of its time. Most exciting for us - as well as for the worldwide Go community - were AlphaGo\u2019s displays of game-winning creativity, in some cases finding moves that challenged millennia of Go wisdom. In its ability to identify and share new insights about one of the most contemplated games of all time, AlphaGo offers a promising sign of the value AI may one day provide, and we're looking forward to playing more games in 2017.\nWe also made meaningful progress in the field of generative models, building programs able to imagine new constructs and scenarios for themselves. Following our \nPixelCNN\n paper on image generation, our paper on \nWaveNet\n demonstrated the usefulness of generative audio, achieving the world\u2019s most life-like speech synthesis by imaginatively creating raw waveforms rather than stitching together samples of recorded language. We\u2019re planning to put this into production with Google and are excited about enabling improvements to products used by millions of people.\nAnother important area of research is memory, and specifically the challenge of combining the decision-making aptitude of neural networks with the ability to store and reason about complex, structured data. Our work on \nDifferentiable Neural Computers\n, for which we received \nour third Nature paper\n in eighteen months, demonstrated models that can simultaneously learn like neural networks as well as memorise data like computers. These models are already able to learn how to answer questions about data structures from family trees to tube maps, and bring us closer to the goal of using AI for scientific discovery in complex datasets.\nAs well as pushing the boundaries of \nwhat\n these systems can do, we\u2019ve also invested significant time in improving how they learn. A paper titled \u2018\nReinforcement Learning with Unsupervised Auxiliary Tasks\n\u2019 described methods to improve the speed of learning for certain tasks by an order of magnitude. And given the importance of high-quality training environments for agents, we open sourced \nour flagship DeepMind Lab research environment\n for the community, and are \nworking with Blizzard\n to develop AI-ready training environments for StarCraft II as well.\nOf course, this is just the tip of the iceberg, and you can read much more about our work in \nthe many papers\n we published this year in top-tier journals from Neuron to PNAS and at major machine learning conferences from ICLR to NIPS. It\u2019s amazing to see how others in the community are already actively implementing and building on the work in these papers - just look at the remarkable renaissance of Go-playing computer programs in the latter part of 2016! - and to witness the broader fields of AI and machine learning go from strength to strength.\nIt\u2019s equally amazing to see the first early signs of real-world impact from this work. Our partnership with Google\u2019s data centre team used AlphaGo-like techniques to \ndiscover creative new methods of managing cooling\n, leading to a remarkable 15% improvement in the buildings\u2019 energy efficiency. If it proves possible to scale these kinds of techniques up to other large-scale industrial systems, there's real potential for significant global environmental and cost benefits. This is just one example of the work we\u2019re doing with \nvarious teams at Google\n to apply our cutting-edge research to products and infrastructure used across the world. We\u2019re also actively engaged in \nmachine learning research partnerships\n with two NHS hospital groups in the UK, our home, to explore how our techniques could enable more efficient diagnosis and treatment of conditions that affect millions worldwide, as well as working with two further hospital groups on \nmobile apps and foundational infrastructure\n to enable improved care on the clinical frontlines.\nOf course, the positive social impact of technology isn\u2019t only about the real-world problems we seek to solve, but also about the way in which algorithms and models are designed, trained and deployed in general. We\u2019re proud to \nhave been involved\n in founding the \nPartnership on AI\n, which will bring together leading research labs with non-profits, civil society groups and academics to develop best practices in areas such as algorithmic transparency and safety. By fostering a diversity of experience and insight, we hope that we can help address some of these challenges and find ways to put social purpose at the heart of the AI community across the world.\nWe\u2019re still a young company early in our mission, but if in 2017 we can make further simultaneous progress on these three fronts - algorithmic breakthroughs, social impact, and ethical best practice - then we'll be in good shape to make a meaningful continued contribution to the scientific community and to the world beyond.\n"}
{"title": "Our collaborations with academia to advance the field of AI", "contents": "When I was studying in the mid-90s as an undergraduate, there was very little active engagement between the academic communities pushing the boundaries of maths and science, and the industries that many students ended up going into, such as finance. This struck me as a missed opportunity. While private institutions benefited from the technological advances being driven by university researchers, the subsequent breakthroughs they made were rarely shared for mutual benefit between the two.\nIn contrast, we often talk about DeepMind\u2019s research environment as a hybrid culture that blends the long-term scientific thinking of academia with the speed and focus of the best start-ups. This alignment with academia has always been important to us personally, given how many of our team come from that background, as well as the fact that many of the core ideas behind machine learning were invented and developed by academic pioneers including the likes of Geoff Hinton and Rich Sutton.\nThis is a major reason why we openly publish our research - including over \n100 peer-reviewed papers\nto date - and regularly present at industry-wide gatherings such as \nNIPS\n. Last month in Barcelona we published 20 papers, participated in 42 poster sessions, gave 21 talks, and \u00a0open-sourced our flagship \nDeepMind Lab\n research platform - and there\u2019s a lot more to come.\nWe also want to make a more direct contribution to academic learning and training the next generation of machine learning practitioners, and so, starting this month, we\u2019ll be running a state-of-the-art Masters level training module called \nAdvanced Topics in Machine Learning\n with University College London\u2019s (UCL) Department of Computer Science. Led by DeepMind\u2019s Thore Graepel, other invited speakers will include leading researchers spanning areas such as deep learning, reinforcement learning, natural language understanding and others. Hado van Hasselt, Joseph Modayil, Koray Kavukcuoglu, Raia Hadsell, James Martens, Oriol Vinyals, Shakir Mohamed, Simon Osindero, Ed Grefenstette and Karen Simonyan will be joined by Volodymyr Mnih, David Silver and Alex Graves - who are also some of the first authors of DeepMind\u2019s three Nature papers.\nJanuary also sees the start of our \nDeep Learning for Natural Language Processing\n advanced course at the University of Oxford\u2019s Department of Computer Science. This applied course, focusing on recent advances in analysing and generating speech and text using recurrent neural networks, is led by Phil Blunsom in partnership with DeepMind\u2019s Language Research Group, and open to fourth year undergraduates, Masters, and first year DPhil (PhD) students. Both of these courses run in addition to the international summer schools that our team members regularly teach at, with events taking place this year in \nGermany\n, \nChina\n and \nSouth Africa\n among other locations.\nWe also make sure that people who come to work here can continue to make their own personal contribution to academia. A number of our team are also affiliated with various institutions including UCL, Oxford, Cambridge, MIT and the universities of Freiburg and Lille, among others.\nFinally, we think it\u2019s important for the field that there are as many thriving independent academic institutions as possible. That\u2019s why we\u2019re providing sponsorship for several research labs and their PhD students to pursue their own research priorities in whichever way they choose, including the University of Alberta, University of Montreal, University of Amsterdam, Gatsby Unit at UCL, NYU and Oxford, and others.\nWe see the links between company research labs and academia as central to the future of AI. By continuing to share talent, expertise and breakthroughs - not just on technical subjects, but also on the broader set of questions around ethics, safety and societal impact - we believe we\u2019ll all make better progress in the development of artificial intelligence and its application for positive social benefit.\n"}
{"title": "Understanding Agent Cooperation", "contents": "We employ deep multi-agent reinforcement learning to model the emergence of cooperation. The new notion of sequential social dilemmas allows us to model how rational agents interact, and arrive at more or less cooperative behaviours depending on the nature of the environment and the agents\u2019 cognitive capacity. The research may enable us to better understand and control the behaviour of complex multi-agent systems such as the economy, traffic, and environmental challenges.\nSelf-interested people often work together to achieve great things. Why should this be the case, when it is in their best interest to just care about their own wellbeing and disregard that of others?\nThe question of how and under what circumstances selfish agents cooperate is one of the fundamental question in the social sciences. One of the simplest and most elegant models to describe this phenomenon is the well-known game of \nPrisoner\u2019s Dilemma\n from game theory.\nTwo suspects are arrested and put into solitary confinement. Without confessions the police do not have sufficient evidence to convict the two suspects on the main charge, but have good prospects to achieve one year prison sentences for both. In order to entice the prisoners to confess, they offer them simultaneously the following deal: If you testify against the other prisoner (\u201cdefect\u201d) you will be released, but the other prisoner will serve three years in prison. If both prisoners testify against each other (\u201cdefect\u201d), they will both serve two years.\nIt turns out, that \nrational agents\n - in the sense of game theory - should always defect in this game, because no matter what the other prisoner chooses to do, they will be better off defecting. Yet, paradoxically, if both prisoners reason in this way, they will each have to serve two years in prison - one year more than if they had cooperated and remained silent. This paradox is what we refer to as a \nsocial dilemma\n.\nRecent progress in artificial intelligence and specifically deep reinforcement learning provides us with the tools to look at the problem of social dilemmas through a new lens. Traditional game theorists model social dilemmas in terms of a simple binary choice between \ncooperate\n and \ndefect\n for each agent. In real life, both cooperating and defecting may require complex behaviours, involving difficult sequences of actions that agents need to learn to execute. We refer to this new setting as \nsequential social dilemmas\n, and use artificial agents trained by deep multi-agent reinforcement learning to study it.\nAs an example, consider the following Gathering game: Two agents, Red and Blue, roam a shared world and collect apples to receive positive rewards. They may also direct a beam at the other agent, \u201ctagging them\u201d, to temporarily remove them from the game, but this action does not trigger a reward. A visualisation of agents playing the gathering game can be seen below.\nWe let the agents play this game many thousands of times and let them learn how to behave \nrationally\nusing deep multi-agent reinforcement learning. Rather naturally, when there are enough apples in the environment, the agents learn to peacefully coexist and collect as many apples as they can. However, as the number of apples is reduced, the agents learn that it may be better for them to tag the other agent to give themselves time on their own to collect the scarce apples.\nIt turns out that this Gathering game shares many characteristics of the original Prisoner\u2019s Dilemma, but allows us to study the more interesting case in which agents need to learn to implement their desired behaviour: Either to cooperate and collect apples, or to defect and try to tag the other agent.\nIn these sequential social dilemmas, we can now study what factors contribute to agents\u2019 cooperation. For example, the following plot shows that in the Gathering game greater scarcity of apples leads to more \u201ctagging\u201d behaviour of agents. Furthermore, agents with the capacity to implement more complex strategies try to tag the other agent more frequently, i.e. behave less cooperatively - no matter how we vary the scarcity of apples.\nInterestingly, in another game called Wolfpack (see gameplay video below), which requires close coordination to successfully cooperate, we find that greater capacity to implement complex strategies leads to more cooperation between agents, the opposite of the finding with Gathering. So, depending on the situation, having a greater capacity to implement complex strategies may yield either more or less cooperation. The new framework of sequential social dilemmas allows us to take into account not only the outcome of the interaction (as in the Prisoner\u2019s dilemma), but also the difficulty of learning to implement a given strategy.\nIn summary, we showed that we can apply the modern AI technique of deep multi-agent reinforcement learning to age-old questions in social science such as the mystery of the emergence of cooperation. We can think of the trained AI agents as an approximation to economics\u2019 rational agent model \n\u201chomo economicus\u201d\n. Hence, such models give us the unique ability to test policies and interventions into simulated systems of interacting agents - both human and artificial.\nAs a consequence, we may be able to better understand and control complex multi-agent systems such as the economy, traffic systems, or the ecological health of our planet - all of which depend on our continued cooperation.\nRead the paper: \nMulti-agent Reinforcement Learning in Sequential Social Dilemmas\n"}
{"title": "A milestone for DeepMind Health and Streams", "contents": "In November we announced a \ngroundbreaking five year partnership\n with the Royal Free London to deploy and expand on \nStreams\n, our secure clinical app that aims to improve care by getting the right information to the right clinician at the right time.\nThe first version of Streams has now been deployed at the Royal Free and we\u2019re delighted that the early feedback from nurses, doctors and patients has so far been really positive. Some of the nurses using Streams at the hospital estimate \u00a0that the app is saving them up to two hours per day, giving them more time to spend with patients in need. And we\u2019re starting to hear the first stories of patients whose conditions were picked up and acted on faster thanks to Streams alerts. Patients like \nAfia Ahmed\n, who was seen more quickly thanks to the instant alerts. \u00a0You can read more about the deployment and some of the early positive signs \nover on the Royal Free\u2019s website\n.\nOf course, these are only early indicators, and a full service evaluation will be carried out over the coming months to measure the overall impact that Streams is having. But for all of us who have worked on this for the past eighteen months, from concept to development to testing to deployment, it\u2019s an exciting milestone and an indication of what can be achieved when the UK\u2019s clinicians, patients and technologists work together.\nWe\u2019ve always believed in the combination of cutting-edge technology with a focus on real practicality. If we want to maximise the clinical and social benefits of advanced health technology, then it can\u2019t be developed in labs disconnected from the hospital frontlines - it has to be informed and guided by amazing people who give and receive care every single day.\nThis focus on practical benefit will continue to shape our work as we expand the use of Streams, carry out \nmachine learning research\n into conditions that affect millions of people, and support our hospital partners to provide faster and safer care.\n"}
{"title": "Trust, confidence and Verifiable Data Audit", "contents": "Data can be a powerful force for social progress, helping our most important institutions to improve how they serve their communities. As cities, hospitals, and transport systems find new ways to understand what people need from them, they\u2019re unearthing opportunities to change how they work today and identifying exciting ideas for the future. \nData can only benefit society if it has society\u2019s trust and confidence, and here we all face a challenge. Now that you can use data for so many more purposes, people aren\u2019t just asking about who\u2019s holding information and whether it\u2019s being kept securely \u2013 they also want greater assurances about precisely what is being done with it. \nIn that context, auditability becomes an increasingly important virtue. Any well-built digital tool will already log how it uses data, and be able to show and justify those logs if challenged. But the more powerful and secure we can make that audit process, the easier it becomes to establish real confidence about how data is being used in practice. \nImagine a service that could give mathematical assurance about what is happening with each individual piece of personal data, without possibility of falsification or omission. Imagine the ability for the inner workings of that system to be checked in real-time, to ensure that data is only being used as it should be. Imagine that the infrastructure powering this was freely available as open source, so any organisation in the world could implement their own version if they wanted to. \nThe working title for this project is \u201cVerifiable Data Audit\u201d, and we\u2019re really excited to share more details about what we\u2019re planning to build!\nOver the course of this year we'll be starting to build out Verifiable Data Audit for \nDeepMind Health\n, our effort to provide the health service with technology that can help clinicians predict, diagnose and prevent serious illnesses \u2013 a key part of DeepMind\u2019s mission to deploy technology for social benefit.\nGiven the sensitivity of health data, we\u2019ve always believed that we should aim to be as innovative with governance as we are with the technology itself. We\u2019ve already invited additional oversight of DeepMind Health by appointing a panel of unpaid \nIndependent Reviewers\n who are charged with scrutinising our healthcare work, commissioning audits, and publishing an annual report with their findings.\nWe see Verifiable Data Audit as a powerful complement to this scrutiny, giving our partner hospitals an additional real-time and fully proven mechanism to check how we\u2019re processing data. We think this approach will be particularly useful in health, given the sensitivity of personal medical data and the need for each interaction with data to be appropriately authorised and consistent with rules around patient consent. For example, an organisation holding health data can\u2019t simply decide to start carrying out research on patient records being used to provide care, or repurpose a research dataset for some other unapproved use. In other words: it\u2019s not just where the data is stored, it\u2019s what\u2019s being done with it that counts. We want to make that verifiable and auditable, in real-time, for the first time.\nSo, how will it work? We serve our hospital partners as a data processor, meaning that our role is to provide secure data services under their instructions, with the hospital remaining in full control throughout. Right now, any time our systems receive or touch that data, we create a log of that interaction that can be audited later if needed.\nWith Verifiable Data Audit, we\u2019ll build on this further. Each time there\u2019s any interaction with data, we\u2019ll begin to add an entry to a special digital ledger. That entry will record the fact that a particular piece of data has been used, and also the reason why - for example, that blood test data was checked against the NHS national algorithm to detect possible acute kidney injury.\nThe ledger and the entries within it will share some of the properties of \nblockchain\n, which is the idea behind Bitcoin and other projects. Like blockchain, the ledger will be append-only, so once a record of data use is added, it can\u2019t later be erased. And like blockchain, the ledger will make it possible for third parties to verify that nobody has tampered with any of the entries.\nBut it\u2019ll also differ from blockchain in a few important ways. Blockchain is decentralised, and so the verification of any ledger is decided by consensus amongst a wide set of participants. To prevent abuse, most blockchains require participants to repeatedly carry out complex calculations, with huge associated costs (according to some estimates, the total energy usage of blockchain participants could be as much as the \npower consumption of Cyprus\n). This isn\u2019t necessary when it comes to the health service, because we already have trusted institutions like hospitals or national bodies who can be relied on to verify the integrity of ledgers, avoiding some of the wastefulness of blockchain.\nWe can also make this more efficient by replacing the chain part of blockchain, and using a tree-like structure instead (if you\u2019d like to understand more about Merkle trees, a good place to start would be \nthis blog\n from the UK\u2019s Government Digital Service). The overall effect is much the same. Every time we add an entry to the ledger, we\u2019ll generate a value known as a \u201ccryptographic hash\u201d. This hash process is special because it summarises not only the latest entry, but all of the previous values in the ledger too. This makes it effectively impossible for someone to go back and quietly alter one of the entries, since that will not only change the hash value of that entry but also that of the whole tree.\nIn simple terms, you can think of it as a bit like the last move of a game of Jenga. You might try to gently take or move one of the pieces - but due to the overall structure, that\u2019s going to end up making a big noise!\nSo, now we have an improved version of the humble audit log: a fully trustworthy, efficient ledger that we know captures all interactions with data, and which can be validated by a reputable third party in the healthcare community. What do we do with that?\nThe short answer is: massively improve the way in which these records can be audited. We\u2019ll build a dedicated online interface that authorised staff at our partner hospitals can use to examine the audit trail of DeepMind Health\u2019s data use in real-time. It will allow continuous verification that our systems are working as they should, and enable our partners to easily query the ledger to check for particular types of data use. We\u2019d also like to enable our partners to run automated queries, effectively setting alarms that would be triggered if anything unusual took place. And, in time, we could even give our partners the option of allowing others to check our data processing, such as individual patients or patient groups.\nBuilding this is going to be a major undertaking, but given the importance of the issue we think it\u2019s worth it. Right now, three big technical challenges stand out.\nNo blind spots.\n For this to be provably trustworthy, it can\u2019t be possible for data use to take place without being logged in the ledger - otherwise, the concept falls apart. As well as designing the logs to record the time, nature and purpose of any interaction with data, we\u2019d also like to be able to prove that there\u2019s no other software secretly interacting with data in the background. As well as logging every single data interaction in our ledger, we will also need to use \nformal methods\n as well as code and data centre audits by experts, to prove that every data access by every piece of software in the data centre is captured by these logs. We\u2019re also interested in efforts to guarantee the trustworthiness of the hardware on which these systems run - an active topic of computer science research!\nDifferent uses for different groups.\n The core implementation will be an interface to allow our partner hospitals to provably check in real-time that we\u2019re only using patient data for approved purposes. If these partners wanted to extend that ability to others, like patients or patient groups, there would be complex design questions to resolve.\nA long list of log entries may not be useful to many patients, and some may prefer to read a consolidated view or rely on a trusted intermediary instead. Equally, a patient group may not have the authority to see identified data, which would mean allowing our partners to provide some form of system-wide information - for example, whether machine learning algorithms have been run on particular datasets - without unintentionally revealing patient data.\nFor technical details on how we could provide verified access to subsets or summaries of the data, see the open source \nTrillian\n project, which we will be using, and this \npaper explaining how it works\n.\nDecentralised data and logs, without gaps.\n There\u2019s no single patient identified information database in the UK, and so the process of care involves data travelling back and forth between healthcare providers, IT systems, and even patient-controlled services like wearable devices. There\u2019s a lot of work going into making these systems interoperable (our mobile product, \nStreams, is built to interoperable standards\n) so they can work safely together. It would be helpful for these standards to include auditability as well, to avoid gaps where data becomes unauditable as it passes from one system to another.\nThis doesn\u2019t mean that a data processor like DeepMind should see data or audit logs from other systems. Logs should remain decentralised, just like the data itself. Audit interoperability would simply provide additional reassurance that this data can\u2019t be tampered with as it travels between systems.\nThis is a significant technical challenge, but we think it should be possible. Specifically, there\u2019s an emerging open standard for interoperability in healthcare called FHIR, which could be extended to include auditability in useful ways.\nWe\u2019re hoping to be able to implement the first pieces of this later this year, and are planning to blog about our progress and the challenges we encounter as we go. We recognise this is really hard, and the toughest challenges are by no means the technical ones. We hope that by sharing our process and documenting our pitfalls openly, we\u2019ll be able to partner with and get feedback from as many people as possible, and increase the chances of this kind of infrastructure being used more widely one day, within healthcare and maybe even beyond.\n\u200d\n"}
{"title": "Decoupled Neural Interfaces Using Synthetic Gradients", "contents": "This post introduces some of our latest research in progressing the capabilities and training procedures of neural networks called \nDecoupled Neural Interfaces using Synthetic Gradients\n. This work gives us a way to allow neural networks to communicate, to learn to send messages between themselves, in a decoupled, scalable manner paving the way for multiple neural networks to communicate with each other or improving the long term temporal dependency of recurrent networks. This is achieved by using a \nmodel to approximate error gradients\n, rather than by computing error gradients explicitly with backpropagation. The rest of this post assumes some familiarity with neural networks and how to train them. If you\u2019re new to this area we highly recommend \nNando de Freitas lecture series on Youtube\n on deep learning and neural networks.\nIf you consider any layer or module in a neural network, it can only be updated once all the subsequent modules of the network have been executed, and gradients have been backpropagated to it. For example look at this simple feed-forward network:\nHere, after Layer 1 has processed the input, it can only be updated after the output activations (black lines) have been propagated through the rest of the network, generated a loss, and the error gradients (green lines) backpropagated through every layer until Layer 1 is reached. This sequence of operations means that Layer 1 has to wait for the forwards and backwards computation of Layer 2 and Layer 3 before it can update. \nLayer 1 is locked, coupled, to the rest of the network.\nWhy is this a problem? Clearly for a simple feed-forward network as depicted we don\u2019t need to worry about this issue. But consider a complex system of multiple networks, acting in multiple environments at asynchronous and irregular timescales.\nOr a big distributed network spread over multiple machines. Sometimes requiring all modules in a network to wait for all other modules to execute and backpropagate gradients is overly time consuming or even intractable. If we decouple the interfaces - the connections - \u00a0between modules, every module can be updated independently, and is not locked to the rest of the network.\nSo, how can one decouple neural interfaces - that is decouple the connections between network modules - and still allow the modules to learn to interact? In this paper, we remove the reliance on backpropagation to get error gradients, and instead learn a parametric model which predicts what the gradients will be based upon only local information. We call these predicted gradients \nsynthetic gradients\n.\nThe synthetic gradient model takes in the activations from a module and produces what it predicts will be the error gradients - the gradient of the loss of the network with respect to the activations.\nGoing back to our simple feed-forward network example, if we have a synthetic gradient model we can do the following:\n... and use the synthetic gradients (blue) to \nupdate Layer 1 before the rest of the network has even been executed\n.\nThe synthetic gradient model itself is trained to regress target gradients - these target gradients could be the true gradients backpropagated from the loss or other synthetic gradients which have been backpropagated from a further downstream synthetic gradient model.\nThis mechanism is generic for a connection between any two modules, not just in a feed-forward network. The play-by-play working of this mechanism is shown below, where the change of colour of a module indicates an update to the weights of that module.\nUsing decoupled neural interfaces (DNI) therefore removes the locking of preceding modules to subsequent modules in a network. In experiments from the paper, we show we can train convolutional neural networks for \nCIFAR-10\n image classification where every layer is decoupled using synthetic gradients to the same accuracy as using backpropagation. It\u2019s important to recognise that DNI doesn\u2019t magically allow networks to train without true gradient information. The true gradient information does percolate backwards through the network, but just slower and over many training iterations, through the losses of the synthetic gradient models. The synthetic gradient models approximate and smooth over the absence of true gradients.\nA legitimate question at this point would be to ask how much computational complexity do these synthetic gradient models add - perhaps you would need a synthetic gradient model architecture that is as complex as the network itself. Quite surprisingly, the synthetic gradient models can be very simple. For feed-forward nets, we actually found out that even a \nsingle linear layer\n works well as a synthetic gradient model. Consequently it is both very easy to train and so produces synthetic gradients rapidly.\nDNI can be applied to any generic neural network architecture, not just feed-forward networks. An interesting application is to recurrent neural networks (RNNs). An RNN has a recurrent core which is unrolled - repeatedly applied - to process sequential data. Ideally to train an RNN we would unroll the core over the whole sequence (which could be infinitely long), and use backpropagation through time (BPTT) to propagate error gradients backwards through the graph.\nHowever in practice, we can only afford to unroll for a limited number of steps due to memory constraints and the need to actually compute an update to our core model frequently. This is called truncated backpropagation through time, and shown below for a truncation of three steps:\nThe change in colour of the core illustrates an update to the core, that the weights have been updated. In this example, truncated BPTT seems to address some issues with training - we can now update our core weights every three steps and only need three cores in memory. However, the fact that there is no backpropagation of error gradients over more than three steps means that the update to the core will not be directly influenced by errors made more than two steps in the future. This limits the temporal dependency that the RNN can learn to model.\nWhat if instead of doing no backpropagation between the boundary of BPTT we used DNI and produce synthetic gradients, which model what the error gradients of the future will be? We can incorporate a synthetic gradient model into the core so that at every time step, the RNN core produces not only the output but also the synthetic gradients. In this case, the synthetic gradients would be the predicted gradients of the all future losses with respect to the hidden state activation of the previous timestep. The synthetic gradients are only used at the boundaries of truncated BPTT where we would have had no gradients before.\nThis can be performed during training very efficiently - it merely requires us to keep an extra core in memory as illustrated below. Here a green dotted border indicates just computing gradients with respect to the input state, while a solid green border additionally computes gradients with respect to the core\u2019s parameters.\nBy using DNI and synthetic gradients with an RNN, we are approximating doing backpropagation across an infinitely unrolled RNN. In practice, this \nresults in RNNs which can model longer temporal dependencies\n. Here\u2019s an example result showing this from the paper.\nPenn Treebank test error during training (lower is better):\nThis graph shows the application of an RNN trained on next character prediction on Penn Treebank, a language modelling problem. On the y-axis the bits-per-character (BPC) is given, where smaller is better. The x-axis is the number of characters seen by the model as training progresses. The dotted blue, red and grey lines are RNNs trained with truncated BPTT, unrolled for 8 steps, 20 steps and 40 steps - the higher the number of steps the RNN is unrolled before performing backpropagation through time, the better the model is, but the slower it trains. When DNI is used on the RNN unrolled 8 steps (solid blue line) the RNN is able to capture the long term dependency of the 40-step model, but is trained twice as fast (both in terms of data and wall clock time on a regular desktop machine with a single GPU).\nTo reiterate, adding synthetic gradient models allows us to decouple the updates between two parts of a network. DNI can also be applied on hierarchical RNN models - system of two (or more) RNNs running at different timescales. As we show in the \npaper\n, DNI significantly improves the training speed of these models by enabling the update rate of higher level modules.\nHopefully from the explanations in this post, and a brief look at some of the experiments we report in the \npaper\n it is evident that it is possible to create decoupled neural interfaces. This is done by creating a synthetic gradient model which takes in local information and predicts what the error gradient will be. At a high level, this can be thought of as a \ncommunication protocol between two modules\n. One module sends a message (current activations), another one receives the message, and evaluates it using a \nmodel of utility \n(the synthetic gradient model). The model of utility allows the receiver to \nprovide instant feedback \n(synthetic gradient) to the sender, rather than having to wait for the evaluation of the true utility of the message (via backpropagation). This framework can also be thought about from an error critic point of view [\nWerbos\n] and is similar in flavour to using a critic in reinforcement learning [\nBaxter\n].\nThese decoupled neural interfaces allow \ndistributed training of networks, enhance the temporal dependency learnt with RNNs\n, and \nspeed up hierarchical RNN systems\n. We\u2019re excited to explore what the future holds for DNI, as we think this is going to be an important basis for opening up more modular, decoupled, and asynchronous model architectures. Finally, there are lots more details, tricks, and full experiments which you can find in the paper \nhere\n.\nNeural networks are the workhorse of many of the algorithms developed at DeepMind. For example, \nAlphaGo\n uses convolutional neural networks to evaluate board positions in the game of Go and \nDQN\nand \nDeep Reinforcement Learning algorithms\n use neural networks to choose actions to play at super-human level on video games.\n"}
{"title": "Enabling Continual Learning in Neural Networks", "contents": "Computer programs that learn to perform tasks also typically forget them very quickly. We show that the learning rule can be modified so that a program can remember old tasks when learning a new one. This is an important step towards more intelligent programs that are able to learn progressively and adaptively.\nDeep neural networks are currently the most successful machine learning technique for solving a variety of tasks including language translation, image classification and image generation. However, they have typically been designed to learn multiple tasks only if the data is presented all at once. As a network trains on a particular task its parameters are adapted to solve the task. When a new task is introduced, \u00a0new adaptations overwrite the knowledge that the neural network had previously acquired. This phenomenon is known in cognitive science as \u2018catastrophic forgetting\u2019, and is considered one of the fundamental limitations of neural networks.\nBy contrast, our brains work in a very different way. We are able to learn incrementally, acquiring skills one at a time and applying our previous knowledge when learning new tasks. As a starting point for our recent \nPNAS paper\n, in which we propose an approach to overcome catastrophic forgetting in neural networks, we took inspiration from neuroscience-based theories about the consolidation of previously acquired skills and memories in mammalian and human brains.\nNeuroscientists have distinguished two kinds of consolidation that occur in the brain: systems consolidation and synaptic consolidation. Systems consolidation is the process by which memories that have been acquired by the quick-learning parts of our brain are imprinted into the slow-learning parts. This imprinting is known to be mediated by conscious and unconscious recall - for instance, this can happen during dreaming. In the second mechanism, synaptic consolidation, connections between neurons are less likely to be overwritten if they have been important in previously learnt tasks. Our algorithm specifically takes inspiration from this mechanism to address the problem of catastrophic forgetting.\nA neural network consists of several connections in much the same way as a brain. After learning a task, we compute how important each connection is to that task. When we learn a new task, each connection is protected from modification by an amount proportional to its importance to the old tasks. Thus it is possible to learn the new task without overwriting what has been learnt in the previous task and without incurring a significant computational cost. In mathematical terms, we can think of the protection we attach to each connection in a new task as being linked to the old protection value by a spring, whose stiffness is proportional to the connection\u2019s importance. For this reason, we called our algorithm Elastic Weight Consolidation (EWC).\nTo test our algorithm, we exposed an agent to Atari games sequentially. Learning an individual game from the score alone is a challenging task, but learning multiple games sequentially is even more challenging as each game requires an individual strategy. \u00a0As shown in the figure below, without EWC, the agent quickly forgets each game after it stops playing it (blue). This means that on average, the agent barely learns a single game. However, if we use EWC (brown and red), the agent does not forget as easily and can learn to play several games, one after the other.\nToday, computer programs cannot learn from data adaptively and in real time. However, we have shown that catastrophic forgetting is not an insurmountable challenge for neural networks. We hope that this research represents a step towards programs that can learn in a more flexible and efficient way.\nOur research also progresses our understanding of how consolidation happens in the human brain. The neuroscientific theories that our work is based on, in fact, have mainly been proven in very simple examples. By showing that those same theories can be applied in a more realistic and complex machine learning context, we hope to give further weight to the idea that synaptic consolidation is key to retaining memories and know-how.\nRead the paper:\u00a0\nOvercoming catastrophic forgetting in neural networks\n"}
{"title": "DeepMind AI Reduces Google Data Centre Cooling Bill by 40%", "contents": "From smartphone assistants to image recognition and translation, machine learning already helps us in our everyday lives. But it can also help us to tackle some of the world\u2019s most challenging physical problems - such as energy consumption. \u00a0Large-scale commercial and industrial systems like data centres consume a lot of energy, and while much has been done to \nstem the growth of energy use\n, there remains a lot more to do given the world\u2019s increasing need for computing power.\nReducing energy usage has been a major focus for us over the past \u00a010 years: we have built our own \nsuper-efficient servers\n at Google, invented \nmore efficient ways to cool our data centres\n and invested heavily in \ngreen energy sources\n, with the goal of being powered 100 percent by renewable energy. Compared to five years ago, we now get around 3.5 times the computing power out of the same amount of energy, and we continue to make many improvements each year.\nMajor breakthroughs, however, are few and far between - which is why we are excited to share that by applying DeepMind\u2019s machine learning to our own Google data centres, we\u2019ve managed to reduce the amount of energy we use for cooling by up to 40 percent. In any large scale energy-consuming environment, this would be a huge improvement. Given how sophisticated Google\u2019s data centres are already, it\u2019s a phenomenal step forward.\nThe implications are significant for Google\u2019s data centres, given its potential to greatly improve energy efficiency and reduce emissions overall. This will also help other companies who run on Google\u2019s cloud to \nimprove their own energy efficiency\n. While Google is only one of many data centre operators in the world, many are not powered by renewable energy as we are. Every improvement in data centre efficiency reduces total emissions into our environment and with technology like DeepMind\u2019s, we can use machine learning to consume less energy and help address one of the biggest challenges of all - climate change.\nOne of the primary sources of energy use in the data centre environment is cooling. Just as your laptop generates a lot of heat, our data centres - which contain servers powering Google Search, Gmail, YouTube, etc. - also generate a lot of heat that must be removed to keep the servers running. This cooling is typically accomplished via large industrial equipment such as pumps, chillers and cooling towers. However, dynamic environments like data centres make it difficult to operate optimally for several reasons:\nTo address this problem, we began applying \nmachine learning\n two years ago to operate our data centres more efficiently. And over the past few months, DeepMind researchers began working with Google\u2019s data centre team to significantly improve the system\u2019s utility. Using a system of neural networks trained on different operating scenarios and parameters within our data centres, we created a more efficient and adaptive framework to understand data centre dynamics and optimize efficiency.\nWe accomplished this by taking the historical data that had already been collected by thousands of sensors within the data centre - data such as temperatures, power, pump speeds, setpoints, etc. - and using it to train an ensemble of deep neural networks. Since our objective was to improve data centre energy efficiency, we trained the neural networks on the average future PUE (Power Usage Effectiveness), which is defined as the ratio of the total building energy usage to the IT energy usage. We then trained two additional ensembles of deep neural networks to predict the future temperature and pressure of the data centre over the next hour. The purpose of these predictions is to simulate the recommended actions from the PUE model, to ensure that we do not go beyond any operating constraints.\nWe tested our model by deploying on a live data centre. The graph below shows a typical day of testing, including when we turned the machine learning recommendations on, and when we turned them off.\nOur machine learning system was able to consistently achieve a 40 percent reduction in the amount of energy used for cooling, which equates to a 15 percent reduction in overall PUE overhead after accounting for electrical losses and other non-cooling inefficiencies. It also produced the lowest PUE the site had ever seen.\nBecause the algorithm is a general-purpose framework to understand complex dynamics, we plan to apply this to other challenges in the data centre environment and beyond in the coming months. Possible applications of this technology include improving power plant conversion efficiency (getting more energy from the same unit of input), reducing semiconductor manufacturing energy and water usage, or helping manufacturing facilities increase throughput.\nWe are planning to roll out this system more broadly and will share how we did it in an upcoming publication, so that other data centre and industrial system operators - and ultimately the environment - can benefit from this major step forward.\n"}
{"title": "Announcing DeepMind Health research partnership with Moorfields Eye Hospital", "contents": "We founded DeepMind to make the world a better place by developing technologies that help address some of society's toughest challenges.\nSo we\u2019re excited to announce our first medical research project with an NHS Trust.\nWe\u2019ll be working with \nMoorfields Eye Hospital NHS Foundation Trust\n, one of the world\u2019s leading eye hospitals with a 200 year track record in clinical care, research and education.\nThis collaboration came about when Pearse Keane, a consultant ophthalmologist at Moorfields, contacted DeepMind to explore how we could work together on two specific conditions that cause sight loss: diabetic retinopathy and age-related macular degeneration (AMD). Together, these affect more than 625,000 people in the UK and over 100 million people worldwide.\nDiabetes is on the rise. It\u2019s estimated that \n1 in 11\n of the world\u2019s adult population are affected. It\u2019s also the leading cause of blindness in the working age population - if you\u2019re diabetic you are 25 times more likely to suffer some kind of sight loss. Early detection and treatment can prevent 98% of severe visual loss resulting from diabetes - but that doesn\u2019t always happen.\nAge-related Macular Degeneration (AMD) is the commonest cause of blindness in the UK. Every single day - in the UK alone - \nnearly 200 people lose sight\n from the severe, blinding form of this condition and globally the number of people with AMD is set to rise to nearly 200m by 2020. By allowing earlier detection and treatment of AMD, machine learning has the potential to help save the sight of many of these people.\nAt the moment, eye care professionals use digital scans of the fundus (the back of the eye) and scans called optical coherence tomography (OCT) to diagnose and determine the correct treatment for these serious eye conditions. These scans are highly complex and take a long time for eye health professionals to analyse, which can have an impact on how quickly they can meet patients to discuss diagnosis and treatment. And to date, traditional computer analysis tools have been unable to explore them fully.\nOur research project aims to investigate how machine learning could help analyse these scans efficiently and effectively, leading to earlier detection and intervention for patients and reducing the number of cases of patient deterioration.\nThe set of one million anonymised eye scans and some related anonymous information about eye condition and disease management, which Moorfields will share with us for the research, has been collected over time through routine care. This means it\u2019s not possible to identify any individual patients from the scans. And they\u2019re also historic scans, meaning that while the results of our research may be used to improve future care, they won\u2019t affect the care any patient receives today.\nWe\u2019re proud to be contributing to the many thousands of medical research efforts underway at any given time. As is standard practice in such projects, we never own the data - the NHS does. And we\u2019re bound by clear rules covering what we can do with it, which are distinct to (though equally strict as) the rules that govern our direct patient care work with the Royal Free Hospital.\nMore information about the project can be found on our \nHealth Research page\n. We have submitted our research protocol for open peer review and we\u2019ll also submit any results from this research to peer-reviewed journals, as is normal, so others in the medical community can analyse them.\nIt\u2019s early days for this work, but we\u2019re optimistic about the long-term potential for machine learning technology to help eye health professionals diagnose and treat other diseases that, like macular degeneration, affect the lives of millions of people across the world. It\u2019s a hugely exciting opportunity to make a difference to the NHS and its patients, and we\u2019ll keep you updated as we continue on this journey.\nEarly detection and treatment can prevent 98% of severe visual loss resulting from diabetes. Access Economics (2009) Future Sight Loss UK 1: Economic Impact of Partial Sight and Blindness in the UK adult population. \nRNIB 2009\n\u200d\nPeople with diabetes are 25 times more likely to go blind. International Diabetes Federation Europe 2011.\n"}
{"title": "Deep Reinforcement Learning", "contents": "Humans excel at solving a wide variety of challenging problems, from low-level motor control through to high-level cognitive tasks. Our goal at DeepMind is to create artificial agents that can achieve a similar level of performance and generality. Like a human, our agents learn for themselves to achieve successful strategies that lead to the greatest long-term rewards. This paradigm of learning by trial-and-error, solely from rewards or punishments, is known as \nreinforcement learning\n (RL). Also like a human, our agents construct and learn their own knowledge directly from raw inputs, such as vision, without any hand-engineered features or domain heuristics. This is achieved by \ndeep learning\n of neural networks. At DeepMind we have pioneered the combination of these approaches - deep reinforcement learning - to create the first artificial agents to achieve human-level performance across many challenging domains.\nOur agents must continually make value judgements so as to select good actions over bad. This knowledge is represented by a Q-network that estimates the total reward that an agent can expect to receive after taking a particular action. Two years ago we introduced the first widely successful \nalgorithm\n for deep reinforcement learning. The key idea was to use deep neural networks to represent the Q-network, and to train this Q-network to predict total reward. Previous attempts to combine RL with neural networks had largely failed due to unstable learning. To address these instabilities, our Deep Q-Networks (DQN) algorithm stores all of the agent's experiences and then randomly samples and replays these experiences to provide diverse and decorrelated training data. We applied DQN to learn to play games on the Atari 2600 console. At each time-step the agent observes the raw pixels on the screen, a reward signal corresponding to the game score, and selects a joystick direction. In our \nNature paper\n we trained separate DQN agents for 50 different Atari games, without any prior knowledge of the game rules.\nAmazingly, DQN achieved human-level performance in almost half of the 50 games to which it was applied; far beyond any previous method. The \nDQN source code\n and \nAtari 2600 emulator\n are freely available to anyone who wishes to experiment for themselves.\nWe have subsequently improved the DQN algorithm in many ways: further stabilising the \nlearning\ndynamics\n; prioritising the \nreplayed experiences\n; \nnormalising\n, \naggregating\n and \nre-scaling\n the outputs. Combining several of these improvements together led to a 300% improvement in mean score across Atari games; human-level performance has now been achieved in almost all of the Atari games. We can even train a \nsingle neural network\n to learn about \nmultiple Atari games\n. We have also built a massively distributed deep RL system, known as \nGorila\n, that utilises the Google Cloud platform to speed up training time by an order of magnitude; this system has been applied to recommender systems within Google.\nHowever, deep Q-networks are only one way to solve the deep RL problem. We recently introduced an even more practical and effective method based on asynchronous RL. This approach exploits the multithreading capabilities of standard CPUs. The idea is to execute many instances of our agent in parallel, but using a shared model. This provides a viable alternative to experience replay, since parallelisation also diversifies and decorrelates the data. Our asynchronous actor-critic algorithm, \nA3C\n, combines a deep Q-network with a deep policy network for selecting actions. It achieves state-of-the-art results, using a fraction of the training time of DQN and a fraction of the resource consumption of Gorila. By building novel approaches to \nintrinsic motivation\n and \ntemporally abstract planning\n, we have also achieved breakthrough results in the most notoriously challenging Atari games, such as Montezuma\u2019s Revenge.\nWhile Atari games demonstrate a wide degree of diversity, they are limited to 2D sprite-based video games. We have recently introduced Labyrinth: a challenging suite of 3D navigation and puzzle-solving environments. Again, the agent only observes pixel-based inputs from its immediate field-of-view, and must figure out the map to discover and exploit rewards.\nAmazingly, the A3C algorithm achieves human-level performance, out-of-the-box, on many Labyrinth tasks. An \nalternative approach\n based on episodic memory has also proven successful. Labyrinth will also be released open source in the coming months.\nWe have also developed a number of deep RL methods for continuous control problems such as robotic manipulation and locomotion. Our Deterministic Policy Gradients algorithm (\nDPG\n) provides a continuous analogue to DQN, exploiting the differentiability of the Q-network to solve a \nwide\n \nvariety\n of continuous control tasks. \nAsynchronous RL\n also performs well in these domains and, when augmented with a hierarchical control strategy, can solve challenging problems such as ant soccer and a 54-dimensional humanoid slalom, without any prior knowledge of the dynamics.\nThe game of Go is the most challenging of classic games. Despite decades of effort, prior methods had only achieved amateur level performance. We developed a deep RL algorithm that learns both a value network (which predicts the winner) and a policy network (which selects actions) through games of self-play. Our program AlphaGo combined these deep neural networks with a state-of-the-art tree search. In October 2015, AlphaGo became \nthe first program to defeat a professional human player\n. In March 2016, AlphaGo \ndefeated Lee Sedol\n (the strongest player of the last decade with an incredible 18 world titles) by 4 games to 1, in a match that was watched by an estimated 200 million viewers.\nSeparately, we have also developed \ngame theoretic\n approaches to \ndeep RL\n, culminating in a \nsuper-human\n poker player for heads-up limit Texas Hold\u2019em.\nFrom Atari to Labyrinth, from locomotion through manipulation, to poker and even the game of Go, our deep reinforcement learning agents have demonstrated remarkable progress on a wide variety of challenging tasks. Our goal is to continue to improve the capabilities of our agents, and to use them to make a positive impact on society, in important applications such as \nhealthcare\n.\n"}
{"title": "DeepMind and Blizzard to release StarCraft II as an AI research environment", "contents": "Today at BlizzCon 2016 in Anaheim, California, we announced our collaboration with \nBlizzard Entertainment\n to open up StarCraft II to AI and Machine Learning researchers around the world.\nFor almost 20 years, the StarCraft game series has been widely recognised as the pinnacle of 1v1 competitive video games, and among the best PC games of all time. The original StarCraft was an early pioneer in eSports, played at the highest level by elite professional players since the late 90s, and remains incredibly competitive to this day. The StarCraft series\u2019 longevity in competitive gaming is a testament to Blizzard\u2019s design, and their continual effort to balance and refine their games over the years. StarCraft II continues the series\u2019 renowned eSports tradition, and has been the focus of our work with Blizzard.\nDeepMind is on a scientific mission to push the boundaries of AI, developing programs that can learn to solve any complex problem without needing to be told how. Games are the perfect environment in which to do this, allowing us to develop and test smarter, more flexible AI algorithms quickly and efficiently, and also providing instant feedback on how we\u2019re doing through scores.\nOver the past five years we\u2019ve helped to pioneer the use of games as AI research environments to drive our machine learning and reinforcement learning research forwards, from \n2D games in Atari\n, to full 3D environments such as \nTorcs\n,\n mastering the game of Go\n, or our forthcoming DeepMind Labyrinth. Here's a representation of what these research environments have looked like with L-R, Atari and Labyrinth.\nStarCraft is an interesting testing environment for current AI research because it provides a useful bridge to the messiness of the real-world. The skills required for an agent to progress through the environment and play StarCraft well could ultimately transfer to real-world tasks.\nAt the start of a game of StarCraft, players choose one of three races, each with distinct unit abilities and gameplay approaches. Players\u2019 actions are governed by the in-game economy; minerals and gas must be gathered in order to produce new buildings and units. The opposing player builds up their base at the same time, but each player can only see parts of the map within range of their own units. Thus, players must send units to scout unseen areas in order to gain information about their opponent, and then remember that information over a long period of time. \u00a0This makes for an even more complex challenge as the environment becomes partially observable - an interesting contrast to perfect information games such as Chess or Go. And this is a real-time strategy game - both players are playing simultaneously, so every decision needs to be computed quickly and efficiently.\nAn agent that can play StarCraft will need to demonstrate effective use of memory, an ability to plan over a long time, and the capacity to adapt plans based on new information. Computers are capable of extremely fast control, but that doesn\u2019t necessarily demonstrate intelligence, so agents must interact with the game within limits of human dexterity in terms of \u201cActions Per Minute\u201d. StarCraft\u2019s high-dimensional action space is quite different from those previously investigated in reinforcement learning research; to execute something as simple as \u201cexpand your base to some location\u201d, one must coordinate mouse clicks, camera, and available resources. \u00a0This makes actions and planning hierarchical, which is a challenging aspect of \nReinforcement Learning\n.\nWe\u2019re particularly pleased that the environment we\u2019ve worked with Blizzard to construct will be open and available to all researchers \u00a0next year. We recognise the efforts of the developers and researchers from the Brood War community in recent years, and hope that this new, modern and flexible environment - supported directly by the team at Blizzard - will be widely used to advance the state-of-the-art.\nWe\u2019ve worked closely with the StarCraft II team to develop an API that supports something similar to previous bots written with a \u201cscripted\u201d interface, allowing programmatic control of individual units and access to the full game state (with some new options as well). \u00a0Ultimately agents will play directly from pixels, so to get us there, we\u2019ve developed a new image-based interface that outputs a simplified low resolution RGB image data for map & minimap, and the option to break out features into separate \u201clayers\u201d, like terrain heightfield, unit type, unit health etc. Below is an example of what the feature layer API will look like.\nWe are also working with Blizzard to create \u201ccurriculum\u201d scenarios, which present increasingly complex tasks to allow researchers of any level to get an agent up and running, and benchmark different algorithms and advances. Researchers will also have full flexibility and control to create their own tasks using the existing StarCraft II editing tools.\nWe\u2019re really excited to see where our collaboration with Blizzard will take us. While we\u2019re still a long way from being able to challenge a professional human player at the game of StarCraft II, we hope that the work we have done with Blizzard will serve as a useful testing platform for the wider AI research community.\n"}
{"title": "DeepMind Papers @ NIPS (Part 3)", "contents": "Authors: \nJ Rae, JJ Hunt, T Harley, I Danihelka, A Senior, G Wayne, A Graves, T Lillicrap\nWe can recall vast numbers of memories, making connections between superficially unrelated events. As you read a novel, you\u2019ll likely remember quite precisely the last few things you\u2019ve read, but also plot summaries, connections and character traits from far back in the novel.\nMany machine learning models of memory, such as Long Short Term Memory, struggle at these sort of tasks. The computational cost of these models scales quadratically with the number of memories they can store so they are quite limited in how many memories they can have. More recently, memory augmented neural networks such as the Differentiable Neural Computer or Memory Networks, have shown promising results by adding memory separate from the computation and solving tasks such as reading short stories and answering questions [e.g. Babi].\nHowever, while these new architectures show promising results on small tasks, they use ``soft-attention\u2019\u2019 for accessing their memories, meaning that at every timestep they touch every word in memory. So while they can scale to short stories, they\u2019re a long way from reading novels.\nIn this work, we develop a set of techniques to use sparse approximations of such models to dramatically improve their scalability. In these sparse models only a tiny subset of the memory is touched at each timestep. Importantly, we show we can do this without harming the ability of the models to learn. This means that the sparse memory augmented neural networks are able to solve the same kind of tasks but require 1000s of times less resources, and look like a promising technique, with further refinement, for reading novels.\nFor further details and related work, please see the paper: \nhttps://arxiv.org/abs/1610.09027\nCheck it out at NIPS:\nWed Dec 7th 06:00 - 09:30 PM @ Area 5+6+7+8 #17\nAuthors: \nS. M. Ali Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David Szepesvari, Koray Kavukcuoglu, Geoffrey Hinton\nConsider the task of clearing a table after dinner. To plan your actions you will need to determine which objects are present, what classes they belong to and where each one is located on the table. In other words, for many interactions with the real world the perception problem goes far beyond just image classification. We would like to build intelligent systems that learn to parse the image of a scene into objects that are arranged in space, have visual and physical properties, and are in functional relationships with each other. And we would like to do so with as little supervision as possible.\nStarting from this notion our paper presents a framework for efficient inference in structured, generative image models that explicitly reason about objects. We achieve this by performing probabilistic inference using a recurrent neural network that attends to scene elements and processes them one at a time. Crucially, the model itself learns to choose the appropriate number of inference steps.\nWe use this scheme to learn to perform inference in partially specified 2D models (variable-sized variational auto-encoders) and fully specified 3D models (probabilistic renderers). We show that such models learn to identify multiple objects - counting, locating and classifying the elements of a scene - without any supervision, e.g., decomposing 3D images with various numbers of objects in a single forward pass of a neural network.\nFor further details and related work, please see the paper \nhttps://arxiv.org/abs/1603.08575\nCheck it out at NIPS:\n\u200d\nWed Dec 7th 06:00 - 09:30 PM @ Area 5+6+7+8 #2\nAuthors: \nMarc G. Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, Remi Munos\nWhile we've successfully trained agents to super-human performance on many Atari 2600 games, some games remain elusively difficult. One of our favourite \"hard\" games is Montezuma's Revenge. Montezuma's Revenge is famous for its hostile, unforgiving environment, where the agent must navigate a maze of rooms filled with traps. Each level has 24 rooms, is shaped like a pyramid, and looks like this:\nUntil now, most published agents failed to even make their way out of the first room.\nMany of these hard RL problems share one thing in common: rewards are few and far between. In reinforcement learning, exploration is the process by which an agent comes to understand its environment and discover where the reward is. Most practical RL applications still rely on crude algorithms, like epsilon-greedy (once in awhile, choose a random action), because more theoretically-motivated approaches don't scale. But epsilon-greedy is quite data inefficient, and often can't even get off the ground.\nIn this paper we show that it's possible to use simple density models (assigning probabilities to states) to \"count\" the number of times we've visited a particular state. We call the output of our algorithm a pseudo-count. Pseudo-counts give us a handle on uncertainty: how confident are we that we've explored this part of the game? As a result, we were able to progress significantly further in Montezuma's Revenge. The standard DQN algorithm gets less than 100 points per play, on average; in comparison, we get 3439. To give you a sense of the difference, compare the rooms visited by both methods (white = unexplored):\nAll in all, our agent navigates through 15 rooms, compared to DQN's two. See also the video of \nour agent playing Montezuma's Revenge\n.\nOur approach is inspired by White's 1959 idea of intrinsic motivation: that intelligent agents act first to understand their environment (See also the more recent work by Oudeyer; Barto; and Schmidhuber). What's exciting is that by playing to satisfy their curiosity, rather than to immediately win, our agents eventually come to surpass their peers.\nFor further details and related work, please see \nthe paper\n.\nCheck it out at NIPS:\n\u200d\nWednesday Dec 7th, 6PM \u2014 9:30PM @ Area 5+6+7+8 Poster #71\nAuthors: \nH van Hasselt, A Guez, M Hessel, V Mnih, D Silver\nSometimes we want to learn a function for which we don\u2019t know the scale beforehand, or where the scale can change over time. \u00a0For instance, this happens in value-based reinforcement learning when our policy improves over time. \u00a0Initially, values might be small because our policy is not yet great, but later they increase repeatedly and unpredictably. \u00a0This is a problem for many (deep) learning algorithms, because they were often not developed with such cases in mind and can then be slow or unstable.\nA concrete motivation is that the \nDQN\n algorithm successfully learned to play many Atari games, but clipped all non-zero rewards to -1 and 1. \u00a0This makes learning easier, because it changes the behaviour. \u00a0For instance, eating a ghost (actual reward 100+) in Ms. Pac-Man then seems to give the same reward as eating a pellet (actual reward 10).\nWe propose to instead adaptively normalize the targets we present to the deep neural network. To get a feel for the effectiveness of this method, we can look at the resulting magnitude (of the l2-norm) of the gradients during learning across 57 different Atari games:\nDouble DQN\n is shown with unclipped rewards on the left, with the clipped rewards in the middle, and with Pop-Art is on the right. \u00a0Pop-Art results in much more consistent gradients, whose magnitudes fall into a much narrower, and therefore more predictable, range. The unclipped version is much more erratic \u2013 note the log scale on the y-axis. \u00a0Pop-Art even has better-normalized gradients than the clipped variant, without qualitatively changing the task as the clipping does. \u00a0For some games, the resulting performance is much better than previous state of the art.\nPop-Art is not specific to DQN, Atari, or reinforcement learning. \u00a0It can be useful whenever a function must be learned with unknown magnitude, or where the scale changes over time. \u00a0Additionally, it can be useful when learning about multiple signals at the same time, for instance when these signals have different units and/or modalities. \u00a0Normalizing per output can then help disentangle the magnitude from the importance of a signal.\nFor further details and related work, please see the paper \nhere\n and an accompanying video \nhere\n.\nCheck it out at NIPS:\n\u200d\nWednesday, December 7th, 6PM \u2014 9:30PM @ Area 5+6+7+8 #81\n"}
{"title": "We are very excited to announce the launch of DeepMind Health", "contents": "We founded DeepMind to solve intelligence and use it to make the world a better place by developing technologies that help address some of society's toughest challenges. It was clear to us that we should focus on healthcare because it\u2019s an area where we believe we can make a real difference to people\u2019s lives across the world.\nWe're starting in the UK, where the National Health Service is hugely important to our team. The NHS helped bring many of us into the world, and has looked after our loved ones when they've most needed help. We want to see the NHS thrive, and to ensure that its talented clinicians get the tools and support they need to continue providing world-class care.\nFrontline nurses, doctors and other healthcare professionals who spend their days treating patients know better than anyone what's needed to provide outstanding care. We at DeepMind Health aim to support clinicians by providing the technical expertise needed to build and scale technologies that help them provide the best possible care to their patients.\nWhile projects like Hark and AKI detection are in their early stages, the problems they solve are fundamental to the NHS. The hope is that these tools can help shift more resources away from reaction and towards better prevention. Ultimately the aim is to give nurses and doctors more time to focus on what\u2019s most important.\nThese past few months have given us a glimpse of what\u2019s possible. As we continue to explore what nurses and doctors need, and work with them to design and scale new and better tools, we will remain guided by the following principles:\nThe world\u2019s toughest problems become more tractable when diverse teams of leading practitioners work together in partnership. Building world-class technologies that support clinicians is one of the most important things we can do, and DeepMind Health is our promise to do just that.\n"}
{"title": "Operations", "contents": "Our Operations team works hard to make DeepMind the best environment in the world for advancing AI research. This means we do things differently. \nFrom machine learning and neuroscience to robotics and philosophy, we meticulously bring together researchers from an unusually wide range of disciplines. Our goal is to create environments that maximise the chance for unexpected discoveries.\nWe develop the world\u2019s best technical tools for cutting-edge AI research and use Google\u2019s computing infrastructure to go from idea to experiment to breakthrough in record time.\nWe think hard about the spaces we build and ensure our labs support different working styles and interests, as well as fast-paced collaboration. \nWe create a culture where everyone\u2019s expertise is recognised and where everyone is continually learning and supported to be the best leaders, managers, and collaborators they can be.\nAnd we find new ways to make our work accessible to the public, through exhibitions, events, podcasts, videos, and more, ensuring that the hopes, fears, and ideas of the public are heard loudly at the heart of DeepMind. \nOur operations teams take on all these challenges and more, which means we\u2019re home to people with a huge range of skills and backgrounds.\nSimon has worked on large-scale software and technology programs for companies such as IBM, Virgin Media, and Barclays.\nSimon ensures that the Program team are planning for the future growth of the company.\nLetitia studied history and Russian at the University of Manchester, with a focus on gender and race. \nLetitia project manages part of the Diversity & Inclusion programme, partners with the recruitment team, and focuses on raising internal awareness. She also develops team training and workshops, outreach, and conference presence.\nNathalie studied psychology and then worked in the video game industry on game development, web development, digital marketing, and publishing teams. \nNathalie supports the Paris team, both on an organisational and an individual level, maximising delivery, efficiency, and team collaboration.\nTom is an accomplished recruiter with multi-sector, international experience. Before DeepMind, he worked for a global media investment group and an executive search firm.\nTom and his team recruit across all DeepMind locations. He also supports projects that drive candidate experience and engagement.\nBecca has worked on HR teams at Urban Outfitters and the Arcadia Group.\nBecca aims to create, maintain, and grow a mission-aligned culture at DeepMind. She supports the growth of individuals, teams, and leaders and helps solve the challenges they face while building AGI.\nLorrayne earned a masters in electronic engineering from the University of Surrey, and a certificate in coaching & mentoring from the University of Warwick. She has worked in publishing, media, and FinTech.\nLorrayne helps individuals and teams find solutions and develops organisational techniques that facilitate research progress.\nPreviously, Natalie worked in healthcare at Canada\u2019s Blood Bank where she carried out various roles from biologics production, phlebotomy and finally office support for a blood stem cell production lab.\nNatalie\u2019s main focus is to ensure smooth office operations, create awesome team retreats, events and ensure moral in the office is high by creating an enjoyable space for staff to come to work each day.\nJackson has experience working as a product director in Nike\u2019s Sport Science Research and Innovation Lab, developing data science tools and techniques to better understand athletes.\nJackson spends his days collaboratively innovating and implementing strategies for accelerating the pace and safety of AGI research.\nDiana has a background in conference interpreting and was previously in charge of the event management at the Max Planck Institute for Intelligent Systems in T\u00fcbingen, Germany. \nDiana provides the Montreal team with the operational support that they need to do excellent research, making sure that the office is running smoothly and we uphold a strong team spirit.\n"}
{"title": "Terms and Conditions", "contents": "This Agreement was last modified on 29 April 2019.\nPlease read these Terms and Conditions (\"Agreement\", \"Terms and Conditions\") carefully before using www.deepmind.com (\"the Site\") operated by DeepMind Technologies Limited (\"us\", \"we\", or \"our\"). This Agreement sets out the terms and conditions that apply to your use of the Site.\nBy accessing or using the Site, including, but not limited to, visiting or browsing the Site or contributing content or other materials to the Site, you agree to be bound by these Terms and Conditions and, if you are an employee or an agent acting for a third party, to bind your employer or that third party to these Terms and Conditions. Capitalised terms are defined in this Agreement.\nThe Site and its original content, features and functionality are owned by DeepMind and are protected by various international copyright, trademark, patent, trade secret and other intellectual property or proprietary rights laws.\nWe may suspend or terminate your access to the Site for business or operational reasons, without cause or notice, which may result in the forfeiture and destruction of all information associated with you. We will try to give you reasonable notice of any suspension or withdrawal.\nOur Site may contain links to third-party sites that are not owned or controlled by DeepMind.\nDeepMind has no control over, and assumes no responsibility for, the content, privacy policies, or practices of any third party sites or services. We strongly advise you to read the terms and conditions and privacy policy of any third-party site that you visit.\nIf you are a business user, this Agreement shall be governed and construed in accordance with the laws of England and Wales, without giving effect to any principles of conflicts of law and you submit to the jurisdiction of the courts of England and Wales. If you are a consumer user, nothing in this Agreement shall preclude you from access to the courts and laws of your place of usual residence.\nWe reserve the right, at our sole discretion, to modify or replace these Terms and Conditions by providing reasonable notice of any material changes. Your continued use of the Site after any such changes constitutes your acceptance of the new Terms and Conditions.\nIf you do not agree to any of this Agreement or any changes to this Agreement, do not use, access or continue to access the Site or discontinue any use of the Site immediately.\nIf you have any questions about this Agreement, please contact us at the address below.\nAddress\nDeepMind Technologies Limited, a company registered in England and Wales (07386350), 5 New Street Square, London, EC4A 3TW.\n"}
{"title": "Ethics & Society", "contents": "With the right focus on ethical standards and safety, we have better chances of finding AI\u2019s potential benefits. By researching the ethical and social questions involving AI, we ensure these topics remain at the heart of everything we do. \nWe start from the belief that AI should be used for socially beneficial purposes and always remain under meaningful human control. Understanding what this means in practice is essential. \u00a0\nFinding ways to involve the broader society in our work is fundamental to our mission, so partnerships with others in the field of AI ethics is a crucial element of our approach.\nWe embrace scientific values like transparency, freedom of thought, and the equality of access, and we deeply respect the independence and academic integrity of our researchers and partners.\nAI systems can use large-scale and sometimes sensitive datasets, such as medical or criminal justice records. This raises important questions about protecting people\u2019s privacy and ensuring that they understand how their data is used. Also, the data used for training automated decision-making systems can contain biases, creating systems that might discriminate against certain groups of people. \nAI systems could make societies fairer and more equal. But different groups of people hold different values, meaning it is difficult to agree on universal principles. Likewise, endorsing values held by a majority could lead to discrimination against minorities. \nThe creation and use of powerful new technologies requires effective governance and regulation, ensuring they are used safely and with accountability. In the case of AI, new standards or institutions may be needed to oversee its use by individuals, states, and the private sector - both internationally and within national borders. \nBy uncovering patterns in complex datasets and suggesting promising new ideas and strategies, AI technologies may one day help solve some of humanity\u2019s most urgent problems. But applying AI technologies to real-world problems takes careful consideration. \nWhile AI systems have great potential, they also come with risks. For example, they might malfunction or not operate in the ways they were intended. We might also rely on them too heavily in situations that go beyond their abilities or a technology designed to help society might be repurposed in unethical or harmful ways. \nLike previous waves of technology, AI could contribute to a huge increase in productivity. However, it could also lead to the widespread displacement of jobs and alter economies in ways that disproportionately affect some sections of the population. This poses important questions about the kinds of societies and economies we want to build. \nArticle 36 is a non-profit organisation working to prevent harm caused by certain weapons. Led by Richard Moyes, previously co-chair of the Cluster Munition Coalition, it is a founding member of the Campaign to Stop Killer Robots. The organisation developed the concept of \u201cmeaningful human control\u201d as an approach to guide international discussions on autonomous weapons systems. Article 36 is also part of the steering group of the International Campaign to Abolish Nuclear Weapons (ICAN), which was awarded the 2017 Nobel Peace Prize, and has led efforts to establish the impact of explosive weapons in populated areas as an international humanitarian priority. Previously, Richard established and managed explosive ordnance disposal projects for the UK NGO Mines Advisory Group. He is an Honorary Fellow at the University of Exeter and serves on the Aviation Futures policy panel of the UK\u2019s Civil Aviation Authority. We have worked with Article 36 to explore the risks of intelligent systems in international human rights law and international humanitarian law.\nThe Center for Information Technology Policy is an interdisciplinary center at Princeton University, focussing on research, teaching, and events that address digital technologies as they interact with society. CITP and DeepMind partnered to organise a workshop that explored solutions to the ways AI has been used in the US criminal justice system. This workshop brought together civil and human rights groups with technologists to explore solutions to a lack of fairness, accountability, and transparency when AI/ML technology is used in the provision of public services.\nDigital Asia Hub is an independent, non-profit think tank focused on internet and society research. At the core of the Hub are independent and interdisciplinary research exploring both the opportunities and challenges related to digital technology, innovation, and society in Asia. DeepMind has provided support for the Hub to expand their regional efforts on AI, ethics, and governance. \nThe Hoffmann Centre is an organisation based within Chatham House whose goal is to create a sustainable resource economy, in which the world\u2019s citizens and environment thrive together, now and in the future. Their mission is to accelerate the uptake of smart policies, technologies, and business models that will reshape the world\u2019s demand for resources and transform the global economy. DeepMind and the Hoffmann Centre partnered to organise a series of workshops focused on ways that AI can transform our approach to complex global challenges, including sustainability in the food and land use system, deep decarbonisation, and reducing emissions in major industries. \nInvolve is a charity that\u2019s on a mission to put people at the heart of decision-making. DeepMind and Involve partnered to organise a series of three half-day roundtables to investigate what meaningful public engagement looks like around AI and ethics, and to explore how these methods and best practices can be built into decision-making by researchers, technologists, and policymakers.\nThe mission of the Leverhulme Centre for the Future of Intelligence (CFI) is to create a new interdisciplinary community of researchers, with strong links to technologists and the policy world and a clear practical goal: to work together to ensure that humans make the best use of the opportunities presented by AI. With support from DeepMind Ethics & Society, CFI will launch a series of roundtables and publish new research on topics related to the interpretability of AI systems. DeepMind has also provided support for CFI\u2019s Global AI Narratives programme. \nThe Digital Ethics Lab (DELab) is part of the Oxford Internet Institute (OII), the world's leading research and teaching department of the University of Oxford, dedicated to the social science of the internet. DELab\u2019s mission is to help design a better information society. Its goal is to identify the benefits and enhance the opportunities of digital innovation as a force for good, and avoid or mitigate its risks and shortcomings. Its work builds on Oxford\u2019s expertise in conceptual design, horizon scanning, foresight analysis, and translational research on ethics, governance, and policy-making. With support from DeepMind Ethics & Society, OII\u2019s DELab has conducted research on explainable and accountable algorithms and automated decision-making in Europe.\nDeepMind is pleased to be a founding member of the Partnership on AI (PAI), a global nonprofit organisation committed to the creation and dissemination of best practices in artificial intelligence. By gathering the leading companies, organisations, and people who are affected by artificial intelligence in different ways, PAI establishes a common ground between entities which otherwise might not be working together. Together, these groups serve as a uniting force for good in the AI ecosystem. PAI convenes more than 100 partner organisations from around the world to realise the promise of artificial intelligence. DeepMind has also supported the establishment of a \nPAI fellowship\nfocused on diversity and inclusion.\nThe AI Now Institute at NYU is an independent, interdisciplinary research initiative dedicated to understanding the social and economic implications of AI. AI Now conducts empirical research focused on AI across four key areas: bias and inclusion, labor change and automation, critical infrastructure and safety, and basic rights and liberties. With support from DeepMind Ethics & Society, AI Now hosts ten two-year NYU postdoctoral positions to advance research related to AI Now's mission.\nThe Alan Turing Institute is the national institute for data science and artificial intelligence, with headquarters at the British Library. DeepMind has committed an unrestricted charitable donation to the Institute to support research in data science and artificial intelligence. This unrestricted gift will help enable the Turing Institute to support areas with the greatest need, which are strategically important to their mission. \nThe Institute for Policy Research (IPR) at the University of Bath aims to further the public good through research into policy issues. At the heart of the IPR lies its ability to facilitate exchange between researchers, practitioners, and policymakers. Bringing diverse perspectives together, the IPR produces reports, policy briefs, and empirical research that inform and influence public policy debates. With an unrestricted donation from DeepMind Ethics & Society, IPR will conduct research that aims to provide a better understanding of the broader relationship between labour market changes in Europe and attitudes towards basic income and welfare. This research helps assess the case for a universal basic income and alternative reform packages in Europe through comparative regression analysis and microsimulation of fiscal and distributional effects.\nThe Royal Society is a self-governing fellowship of many of the world\u2019s most distinguished scientists drawn from all areas of science, engineering, and medicine. The Society\u2019s fundamental purpose, reflected in its founding charters of the 1660s, is to recognise, promote, and support excellence in science, and to encourage the development and use of science for the benefit of humanity. With support from DeepMind Ethics & Society, The Royal Society launched You & AI, a public lecture series that explores cutting edge AI research and its implications for society, building on the society\u2019s recent projects in these areas. Lectures came from leading figures in AI and those thinking about its societal consequences will provide a public forum to explore AI\u2019s capabilities, future directions, and potential societal effects.\nThe RSA is a charity which seeks to harness human potential to address the challenges that society faces. The mission of the RSA is to enrich society through ideas and action. DeepMind and the RSA partnered to create the Forum for Ethical AI, a series of citizen juries that explore automated decision-making. These events have used immersive scenarios to help participants understand the ethical issues raised by automated decision-making systems, and facilitated public engagement on some of the most pressing issues facing society today.\nWITNESS is an international non-profit that makes it possible for anyone, anywhere to use video and technology to protect and defend human rights. Working alongside both local communities and technology giants, WITNESS fills critical gaps in use of video and technology for human rights. DeepMind has provided support for WITNESS to expand their research and programs exploring technical and societal solutions to emerging threats from so-called deepfakes and other forms of AI-generated synthetic media.\nVerity has worked as a special adviser in the UK government and as head of security policy for Google Europe.\nVerity and her team work to understand the ethical and societal impacts of AI, and build multi-stakeholder approaches to address them.\nSean\u2019s background is in software engineering, AI, and sociology, with a long-standing interest in how technology and society interact and shape our world.\nSean focuses on finding the best ways for contributing philosophy and social science expertise to the ethical advancement of AI research and development.\nJennifer has worked at the intersection of public policy and technology for a decade. She previously managed Google\u2019s policy strategy for media and intellectual property in Europe, the Middle East, and Africa.\nJennifer works with with governments and the policy community, supporting discussions about the governance of new technologies and ensuring that public interests in creating safe and ethical AI are reflected in DeepMind\u2019s research.\nIason is a political theorist and philosopher by training. Before joining DeepMind he worked for the United Nations and also taught politics at Oxford University for a number of years.\nIason\u2019s work focuses on how to ensure that the systems we build are aligned with human values. He also teaches ethics to researchers at DeepMind.\n"}
{"title": "Science", "contents": "We accomplish this by combining expertise in a broad range of natural sciences with cutting-edge AI research.\nLike the astronomers who built and used powerful telescopes to expand our understanding of the universe, our Science team builds innovative AI and machine learning systems. This work is pushing the boundaries of possibility in diverse fields such as climate science, protein folding, and quantum chemistry.\nJoining together from a broad range of backgrounds, our pioneering research scientists, research engineers, and software engineers extend their abilities by partnering with industry experts and specialists to explore new topics and conduct experimental research. \nSince 2016, our Science team has been working on an AI system that can accurately predict the 3D shape of proteins regarded as one of the biggest challenges in biology. \u00a0Four years later, \nAlphaFold\n solved the problem, showing the potential of AI to help solve scientific problems. One day soon, this research could help us create treatments for rare diseases, find ways to break down plastic waste or even capture carbon from the atmosphere.\nPushmeet holds a PhD in combinatorial optimisation problems found in computer vision. Before joining DeepMind, he was the director of research at Microsoft Research.\nPushmeet manages impactful AI projects across the natural sciences. He also leads the team ensuring DeepMind\u2019s models and agents are robust and consistent.\nJohn has a PhD in chemistry from the University of Chicago. Before that, he worked on molecular dynamics simulations of proteins and supercooled liquids.\nJohn leads the development of next-generation AlphaFold models. He designs algorithms, acts as a domain expert, and ensures that DeepMind\u2019s work beneficially solves challenges in protein biology.\nTim studied physics as at the University of Cambridge and conducted his DPhil and postdoc at the University of Oxford, predicting NMR properties of materials.\nTim works on solving protein structure predictions through machine learning research and helping his team maximise DeepMind\u2019s computational resources.\nKathryn completed her PhD in systems approaches to biomedical science at the University of Oxford. She then worked on vehicle routing and geospatial data at Ocado Technology.\nKathryn ensures DeepMind\u2019s infrastructure works smoothly. This ranges from setting up data processing pipelines to enabling the use of externally developed scientific software.\n"}
{"title": "Privacy Policy", "contents": "This Privacy Policy was last modified on 31st July 2019.\nDeepMind Technologies Limited, is a company organised under the laws of England and Wales, with registered office at 5 New Street Square, London, EC4A 3TW (\u201c\nDeepMind\u201d, \u201cus\u201d, \u201cwe\u201d, or \u201cour\n\u201d). \nDeepMind is a wholly owned subsidiary of Alphabet Inc. and operates \nhttps://deepmind.com\n (the \u201c\nSite\n\u201d). This Privacy Policy informs you of the data processing operations carried out by us in relation to the Site.\nWe take your privacy extremely seriously and use your personal data only for legitimate reasons and in accordance with data protection law, and any other applicable legislation, for purposes such as: providing and improving the Site, allowing you to participate in various projects implemented by us through the Site, responding to queries addressed by you through the Site, research, maintenance of accounts and records and the promotion of services.\nWhile using our Site, we may ask you to provide us with certain personally identifiable information that can be used to contact or identify you. Personally identifiable information may include, but is not limited to, your name, email address, postal address and phone number. \u00a0Unless otherwise defined in this Privacy Policy, terms used in this Privacy Policy have the same meanings as in our Terms and Conditions, accessible at \nwww.deepmind.com\n.\nWe process your personal data for the purposes described in this Privacy Policy, based on the following legal grounds: \u00a0\nLike many site operators, we collect information that your browser sends whenever you visit our Site (\u201c\nLog Data\n\u201d). This Log Data may include information such as your computer\u2019s Internet Protocol (\u201cIP\u201d) address, browser type, browser version, computer settings, the pages of our Site that you visit, how often you visit those pages, the time and date of your visits, the time spent on those pages and other statistics.\nCookies are files with small amounts of data, which may include an anonymous unique identifier. Cookies are sent to your computer when you visit a website and are stored on your computer\u2019s hard drive.\nLike many websites, we may use \u201ccookies\u201d to collect information. These cookies enable us to understand how you use the Site and enhance your experience whilst on it. They help us to provide you with a better Site, by enabling us to monitor which pages you find useful, and those you do not. Please be assured that cookies in no way give us access to your computer or any information about you, other than the data you choose to share with us. You can instruct your browser to refuse all cookies or to indicate when a cookie is being sent. However, if you do not accept cookies, you may not be able to use some portions of our Site.\nBy continuing to use our Site without changing your browser settings you consent to the use of cookies as described in this Privacy Policy.\nThe security of your personal data is important to us. All of the personal data described above is recorded on restricted database servers. Our suppliers use a variety of industry-standard security technologies and procedures to help protect your personal data from unauthorised access, use, or disclosure. Please remember, however, that no method of transmission over the Internet, or method of electronic storage, is 100% secure. While we strive to use appropriate and robust security measures to protect your personal data, we cannot guarantee its absolute security.\nOur Site may contain links to other sites that are not operated by us. If you click on a third party link, you will be directed to that third party\u2019s site. Third parties may display advertisements on their sites. These advertisements may contain cookies, web beacons and other similar technologies. These third parties may collect information or insert cookies or similar technologies without our knowledge. We strongly advise you to review the privacy policy and terms and conditions of every site you visit. We have no control over, assume no responsibility for, and do not endorse, the content, privacy policies, or practices of any third party sites or services.\nPersonal data will be retained for the entire period of time necessary for the performance of the purposes described in this Privacy Policy, as well as following their completion in accordance with applicable legal requirements. \nIf and when necessary or required, personal data processed for any of the purposes mentioned in this Privacy Policy may be transferred to other companies within the same group, suppliers and service providers, professional advisers and consultants, agents, survey and research organizations or to governmental or state authorities in accordance with applicable legal requirements. It may sometimes be necessary to transfer personal data overseas to other countries within the European Union/ European Economic Area or to other countries around the world. Any transfers made will be in full compliance with all applicable legal requirements.\nUnder data protection law, you have the right to request access to, update, remove, and restrict the processing of your personal data. \u00a0You also have the right to object to the processing of your personal data or export your personal data to another service. Any requests should be addressed in writing to the DeepMind Legal Team at our registered office or at the following e-mail address: \nlegal@deepmind.com\n. \nWe may update this Privacy Policy from time to time. We will inform you of any changes by posting the new Privacy Policy on the Site without other notice to you. You are advised to review this Privacy Policy periodically for any changes.\nIf you have any questions about this Privacy Policy, please contact us in writing at our registered address or via the e-mail address \nlegal@deepmind.com\n.\nYou can also contact the \nInformation Commissioner\u2019s Office (ICO)\n if you have concerns regarding your rights under data protection law. \n\u200d\n"}
{"title": "Scholarships", "contents": "The scholarships provide financial support to students from underrepresented groups seeking to study graduate courses relating to AI and adjacent fields. Scholars are also offered support from a DeepMind mentor, and have opportunities to attend leading AI academic conferences and DeepMind events.\n\u200d\n\u200d\n\u200d\n\u200d\n\u200d\n\u200d\n\u200d\n\u200d\n\u200d\n\u200d\n\u200d\n\u200d\n\u200d\n\u200d\nThe DeepMind scholarships programme is available internationally, with scholarships currently offered in Brazil, Bulgaria, Canada, Colombia, France, Greece, Poland, Romania, South Africa, Turkey, Uganda, the UK and the USA.\nThese scholarships aim to support talented students from groups currently underrepresented in AI by removing barriers to access as they progress from undergraduate to postgraduate study.\nScholarships are available to eligible offer holders on specific graduate courses at universities participating in the DeepMind scholarship program. Each university runs its own application and recruitment process both for their DeepMind scholarships and relevant graduate courses, so consult university websites for details on the required qualifications to apply to these course(s) and, where relevant, any separate application process for the scholarships.\nTo find out which underrepresented groups are eligible for the scholarships, we recommend consulting university websites for more details, as this can vary from university to university.\nFinally, as the programme is international, at any point in the year some university deadlines will have already closed for intake, whereas others will not yet be open, so please check details carefully and keep an eye on announcements.\nScholars are generally supported financially for the duration of their study, from the year they join their course. Therefore, these scholarships aim to support students from their point of entry into a graduate programme, through until they graduate, rather than partway through an existing course.\nMost of the DeepMind scholarships support graduate study in AI and related fields. There are Masters-level scholarships available in the UK for those interested in the impact of AI technologies on society (UCL), Neuroscience (UCL) and Life Sciences (Imperial).\nAll of the universities offer Masters-level funding for specific courses. Some also offer a limited number of PhD scholarships in AI and related fields. Consult the websites of the participating universities for details.\nThe scholarships aim to ensure that scholars receive full financial support (tuition, stipend or living costs, and a grant for necessary equipment and attendance at an academic conference). Scholars are generally supported financially for the duration of their study, from the year they join their course. In a few cases, where graduate funding is already available through the university or department, DeepMind scholarships serve to enhance the existing funding, support the educational experience, and give the scholar the opportunity to focus more fully on their studies.\nDeepMind mentoring is focused on personal development and growth. Scholars can choose to be matched with a personal DeepMind mentor, who supports the mentee to develop their confidence and pursue their goals for up to one academic year. Mentors are not supervisors or academic advisors, and don\u2019t provide support with research topics or directions, but do aim to help the mentee to build skills that will help them succeed in their graduate course and beyond.\nEach university offering DeepMind scholarships manages its own application process and recruitment timelines, so further information, such as eligibility or relevant course deadlines, should be explored via university websites. Eligible scholarship recipients are selected from the individual university\u2019s offer-holder pool for qualifying courses; DeepMind does not select scholarship recipients.\n"}
{"title": "Learning resources", "contents": "Many of our researchers also hold teaching posts at universities including Cambridge, Oxford, Imperial College, MIT, Alberta, McGill and elsewhere. \nBelow, you\u2019ll find some of the resources we\u2019ve created to help people at different stages of their learning journey to find out more about AI.\nWe support our researchers and engineers who helped establish and organise several global education initiatives including the \nDeep Learning Indaba\n in Africa, \nNorth Africa and Middle East Summer School in Machine Learning (NASSMA)\n, \u00a0\nKhipu AI\n in South America, the \nEastern European Machine Learning Summer School (EEML)\n, the \nSoutheast Asia Machine Learning School (SEAMLS)\n, and the \nAI4Good Summer Lab\n in Canada. \nAlong with researchers and volunteers from many other organisations, our researchers and engineers have helped create a wide variety of practical teaching resources used by these organisations:\n"}
{"title": "Advancing conservation with AI-based facial recognition of turtles", "contents": "Finding solutions to improve turtle reidentification and supporting machine learning projects across Africa.\nProtecting the ecosystems around us is critical to safeguarding the future of our planet and all its living citizens. Fortunately, new artificial intelligence (AI) systems are making progress in conservation efforts worldwide, helping tackle complex problems at scale \u2013 from \nstudying the behaviour of animal communities in the Serengeti\n to help conserve the diminishing ecosystem, to \nspotting poachers and their wounded prey\n to prevent species going extinct.\nAs part of our mission to help benefit humanity with the technologies we develop, it's important we ensure diverse groups of people build the AI systems of the future so that it\u2019s equitable and fair. This includes broadening the machine learning (ML) community and engaging with wider audiences on addressing important problems using AI.\u00a0\nThrough investigation, we came across \nZindi\n \u2013 a dedicated partner with complementary goals \u2013 who are the largest community of African data scientists and host competitions that focus on solving Africa\u2019s most pressing problems.\u00a0\nOur \nScience team\n\u2019s Diversity, Equity, and Inclusion (DE&I) team worked with Zindi to identify a scientific challenge that could help advance conservation efforts and grow involvement in AI. Inspired by Zindi\u2019s \nbounding box turtle challenge\n, we landed on a project with the potential for real impact: turtle facial recognition.\u00a0\nBiologists consider turtles to be an indicator species. These are classes of organisms whose behaviour helps scientists understand the underlying welfare of their ecosystem. For example, the presence of otters in rivers has been considered a sign of a clean, healthy river, since a ban on chlorine pesticides in the 1970s brought the species back from the brink of extinction.\u00a0\nTurtles are another such species. By grazing on seagrass cover, they cultivate the ecosystem, providing a habitat for numerous fish and crustaceans. Traditionally, individual turtles have been identified and tracked by biologists with physical tags, though frequent loss or erosion of these tags in seawater has made this an unreliable method. To help solve some of these challenges, we launched an ML challenge called \nTurtle Recall\n.\nGiven the additional challenge of keeping a turtle still enough to locate their tag, the Turtle Recall challenge aimed to circumvent these problems with turtle facial recognition. This is possible because the pattern of scales on a turtle's face is unique to the individual and remains the same over their multi-decade lifespan.\u00a0\nThe challenge aimed to increase the reliability and speed of turtle reidentification, and potentially offer a way to replace the use of uncomfortable physical tags altogether. To make this possible, we needed a dataset to work from. Fortunately, after Zindi\u2019s previous turtle-based challenge with Kenyan-based charity \nLocal Ocean Conservation\n, the teams were kindly able to share a dataset of labelled images of turtle faces.\nThe competition started in November 2021 and lasted five months. To encourage competitor participation, the team implemented a \ncolab notebook\n, an in-browser programming environment, which introduced two common programming tools: \nJAX\n and \nHaiku\n.\u00a0\nParticipants were tasked with downloading the challenge data and training models to predict a turtle\u2019s identity, as accurately as possible, given a photograph taken from a specific angle. Having submitted their predictions on data withheld from the model, they were able to visit a public leaderboard tracking the progress of each participant.\u00a0\nThe community engagement was incredibly positive, and so was the technical innovation displayed by teams during the challenge. During the course of the competition, we received submissions from a diverse range of AI enthusiasts from 13 different African countries \u2013 including countries not traditionally well represented at the biggest ML conferences, such as Ghana and Benin.\u00a0\nOur turtle conservation partners have indicated that the participant\u2019s level of prediction accuracy will be immediately useful for identifying turtles in the field, meaning that these models can have a real and immediate impact on wildlife conservation.\u00a0\nAs part of Zindi\u2019s continued efforts to support climate-positive challenges, they are also working on \nSwahili audio classification\n in Kenya to help translation and emergency services, and \nair quality prediction\n in Uganda to improve social welfare.\u00a0\nWe're grateful to Zindi for their partnership, and all those who contributed their time to the Turtle Recall challenge and the growing field of AI for conservation. And we look forward to seeing how people around the world continue to find ways to apply AI technologies towards building a healthy, sustainable future for the planet.\nRead more about Turtle Recall on \nZindi\u2019s blog\n and learn about Zindi at \nhttps://zindi.africa/\n"}
{"title": "DeepMind: The Podcast", "contents": "We\u2019re often asked these questions at talks, science festivals, and in conversation with family and friends. So we partnered with the brilliant mathematician and broadcaster Hannah Fry to create DeepMind: The Podcast, which explores the latest in AI and uncovers the extraordinary ways it\u2019s transforming our world.\nIn the second season - coming soon - Hannah discovers how AI is accelerating science, takes an in-depth look at the challenges and potential of building advanced problem-solving systems known as artificial general intelligence (AGI), and explores what it takes to ensure AI is built to benefit society.\nListen now on your favourite podcast app by searching \u201cDeepMind: The Podcast\u201d.\nWe\u2019d like to extend a huge thank you to everyone who has asked us questions and inspired us to make this podcast; and to our talented presenter, amazing contributors and creative producers who helped craft our initial idea into the final programmes. We really enjoyed making this podcast and hope you enjoy listening too!\n"}
{"title": "Applied", "contents": "We are mainly based in London and Mountain View, California, and work on a variety of applications for machine learning.\nOur collaborative efforts have reduced the electricity needed for cooling Google\u2019s data centres by up to 30%, used \nWaveNet\n to create more natural voices for the Google Assistant, and created on-device learning systems to optimise Android battery performance.\nWorking at Google scale gives us a unique set of opportunities, allowing us to apply our research beyond the lab towards global and complex problems. This way, we can demonstrate the benefits of our work on systems that are already optimised by brilliant computer scientists.\nIn 2016, we worked with Google to develop an AI-powered recommendation system to improve the energy efficiency of Google\u2019s highly-optimised data centres. \nTwo years later, we announced the next phase of this work: a safety-first AI system to autonomously manage cooling in Google's data centres, while remaining under the expert supervision of data centre operators. \nThis pioneering system is delivering consistent energy savings and has also discovered a number of innovative methods for cooling - many of which have since been incorporated into the data centre operators\u2019 rules and heuristics.\nIn 2018, DeepMind and Google started applying machine learning to 700 megawatts of wind power capacity in the central United States to help increase the predictability and value of wind power. Using a neural network trained on widely available weather forecasts and historical turbine data, we configured the DeepMind system to predict wind power output 36 hours ahead of actual generation. \nBased on these predictions, our model recommends how to make optimal hourly delivery commitments to the power grid a full day in advance. Our hope is that this kind of machine learning approach can strengthen the business case for wind power and drive further adoption of carbon-free energy on electric grids worldwide.\nIn 2016, we introduced \nWaveNet\n, a deep neural network capable of producing better and more human-sounding speech than existing techniques. At that time, the model was a research prototype that took one second to generate 0.02 seconds of audio and was too complex to work in consumer products.\nAfter 12 months of intense development, working with the Google Text to Speech and DeepMind research teams, we created an entirely new model with speeds 1,000 times faster than the original. \nThis is now in production and is used to generate hundreds of voices for the Google Assistant, while Google Cloud Platform customers can also now use WaveNet generated voices in their own products through Google Cloud\u2019s Text-to-Speech. \nThis is just the start for WaveNet and we are excited by the possibilities that a voice interface can unlock for all the world's languages.\nAndroid is the world's most popular mobile operating system. We've collaborated with the Android team to create two new features, Adaptive Battery and Adaptive Brightness. These features have been rolled out across the Android Pie operating system, optimising mobile phone performance for millions of users.\nAdaptive Battery is a smart battery management system that uses machine learning to anticipate which apps you'll need next, providing a more reliable battery experience.\nAdaptive Brightness is a personalised experience for screen brightness, built on algorithms that learn your brightness preferences in different surroundings.\nThis is the first time we've used techniques that run on the compute power of a single mobile device, which is exponentially less powerful less than most machine learning applications.\nTogether with the Google Play team, we are coming up with personalised recommendations for millions of their customers. To tackle this challenge, we are evaluating a series of machine learning techniques to recommend apps that users will more likely download and enjoy.\nIngrid holds a PhD in applied maths, where she developed algorithms to efficiently run physics simulations. Before joining DeepMind, she worked at Google and YouTube, using machine learning for video classification and recommendations. \nIngrid\u2019s team works with on-device machine learning, exploring challenges in training and running ML models on single computing devices.\nNorman earned his MSc in machine learning at the University of Montreal. He has worked for an online music service, a startup in Seattle, and joined the Machine Intelligence group at Google to work on automatic knowledge extraction. \nNorman focuses on everything WaveNet and its applications and helped it undergo several major enhancements.\nPraveen has a masters in information engineering and worked in software engineering for over eight years. At DeepMind, he started scaling and applying AI to solve real-world problems. \nPraveen and his team partner with DeepMind researchers and Google product teams to use cutting-edge machine learning for improving Google products and systems.\n"}
{"title": "Research", "contents": "By combining extraordinary intellectual freedom and scientific rigour with access to top resources and a structured, supportive culture, we have established an unparalleled track record of AI breakthroughs.\nOur pioneering scientists and engineers have taught agents to cooperate, play world-class chess, diagnose eye disease, and predict the complex 3D shapes of proteins. Combined with a strong focus on safety, ethics, and robustness, the team works to create systems that can provide extraordinary benefits to society.\nGeneral purpose learning systems must be able to cope with the richness and complexity of the real world. These topics drive the control and robotics teams at DeepMind, which aim to create mechanical systems that can learn how to perform complex manipulation tasks with minimal prior knowledge. The shared ambition is to create systems that are data-efficient, reliable, and robust.\nThe development and use of deep neural networks underpins much of the current wave of AI research and is a critical technique for many modern applications such as machine translation. Deep learning methods are at the core of many research areas at DeepMind, including deep reinforcement learning, generative models, theory and optimisation, transfer learning, computer vision, program synthesis, and hierarchical reinforcement learning\n.\nThe brain is the best example of a general purpose learning system and we use it as an inspiration for our algorithms. We conduct experiments to try and understand how human intelligence works, from memory and learning to internal navigation systems and motor control. Their insights are then used to build the next generation of algorithms. We also develop tools inspired by neuroscience that can probe our AI systems in the same way a neuroscientist studies neural circuits in the brain - an important step towards building interpretable AI systems.\nGiving computer systems the ability to learn through trial-and-error has shaped many of DeepMind\u2019s most well-known projects including AlphaGo, AlphaZero, and AlphaStar. We continuously push the boundaries of this powerful technique, advancing areas such as credit assignment, planning, locomotion, and meta-learning.\nWe study theoretical and practical problems that might arise when building general purpose learning systems. These problems fall loosely into three categories: specification (defining the purpose of a system), robustness (designing systems that can withstand outside perturbations), and assurance (monitoring and controlling a system\u2019s activity). Our goal is to understand the behaviour of systems, including unintended behaviours or side effects; aligning agents with the goals, preferences, and ethics of the system's operators; understanding the ways in which artificial intelligence might want to modify itself over time; and approaches to containing or restricting the scope, behaviour, or design of a system.\nWe focus on the theoretical foundations of machine learning to understand the limits of current architectures and support the development of new, efficient, and effective learning algorithms. Our researchers cover a wide range of topics including passive, active, partial, and full information feedback learning, as well as representation, supervised, and unsupervised learning. In all cases, we aim to create principled solutions that are robust and scalable.\nUnsupervised learning is a powerful technique that allows systems to learn directly from datasets that don\u2019t have specific labels or rewards. This is an important attribute for AI, allowing them to learn and therefore make sense of their environment in much the same way a child learns through play and observation. We work on various approaches to generative and predictive models of unstructured data streams, such as text, image and video.\nRaia worked in philosophy and religion before switching to machine learning and robotics. Her PhD from NYU focused on representation learning and robot navigation, using convolutional networks to see the world.\nRaia\u2019s team researches embodied and lifelong learning in complex situations, including dexterous manipulation with multi-sensor robot hands, robot locomotion, and city-scale navigation.\nAli holds a PhD in generative models from the University of Edinburgh, conducted his postdoc at Microsoft Research in Cambridge, and was a visiting researcher at the University of Oxford.\nAli figures out how computers can learn to see with less supervision. His work involves a mix of deep learning, probabilistic inference, and reinforcement learning.\nJess holds a PhD in psychology from the University of California, Berkeley, and earned her BS and MEng in computer science from MIT.\nJess currently applies insights from cognitive science to problems in AI, with an emphasis on structured representations, model-based reasoning, and planning.\nRich researched reinforcement learning at universities in Alberta and Massachusetts, and at corporate labs within AT&T and GTE, since 1978.\nRich works between DeepMind Alberta and the University of Alberta. He identifies unknown parts of the mind, which therefore prevent us from recreating its abilities in machines.\nRemi worked at Inria and taught at \u00c9cole Polytechnique. He did his postdoc at Carnegie Mellon University and holds a PhD on reinforcement learning. \nRemi focuses on deep reinforcement learning and combinations with unsupervised and imitation learning, and learning from a teacher.\nYazhe studied theoretical and applied mechanics and civil engineering. She started her career as a civil engineer, but soon found her passion in computer science and machine learning.\nYazhe collaborates with research scientists on advancing our understanding of machine learning and developing state-of-the-art deep learning algorithms.\nHado studied cognitive artificial intelligence and holds a PhD in AI from Utrecht University, NL. He later joined DeepMind after working with Professor Rich Sutton at the University of Alberta.\nHado builds systems and solves challenges with reinforcement learning, deep learning, and optimisation. He also co-leads an effort on core reinforcement learning algorithms.\nJonathan earned a masters in machine learning from the University of Edinburgh, working on modelling sequential data, robot navigation, and climate research.\nJonathan collaborates with his team on cutting-edge machine learning problems, running experiments, discussing new ideas on the whiteboard, or presenting his latest work.\nEdward worked in quantitative finance for twenty years, developing skills in mathematics, statistics, and software engineering, which he applies to cutting-edge AI research.\nEdward helps organise the research team and contributes to research efforts. He runs the Research Engineering Intern programme and develops agents that learn to collaborate.\n"}
{"title": "Technical blogs", "contents": ""}
{"title": "Research", "contents": ""}
{"title": "Engineering", "contents": "Conducting world-class research requires solving difficult engineering problems. Tools and infrastructure developed by our engineering teams have successfully enabled our most significant research milestones, such as \nAlphaGo\n, \nAlphaStar\n, and \nAlphaFold\n.\nThese systems enable training of large-scale neural networks by unlocking scalable, parallel computation across diverse hardware. Internal tools for research empower our research team to run experiments seamlessly and make rapid scientific progress at scale.\nOur multidisciplinary engineering team, with expertise ranging from software, hardware, and research engineers to designers, artists, and program managers, work across all DeepMind teams to deliver high-impact, state-of-the-art research. Many of our tools, libraries, environments, and papers are available open source. \nResearch engineers and software engineers on the Research team tackle unique engineering challenges that combine state-of-the-art computer systems and AI algorithms. This is done by developing prototypes and tools that allow our teams to perform rigorous experimentation at scale. This includes creating complex reinforcement learning agents and training pipelines alongside tools for visualisation, debugging, testing, and running reliable agents.\nThe Research Platform team\u2019s mission is to create the most efficient platform for our research, maximising use of our resources and enabling new research ideas. Our core team is a group of software engineers within DeepMind Research who work to provide a best-in-class research workflow. We build tools, infrastructure, libraries, frameworks, services, and products to enable and accelerate the next generation of research ideas. We manage and leverage DeepMind\u2019s massive computational resource pool to maximum effectiveness (TPUs, GPUs, and CPUs) and we collaborate with researchers to build innovative and lasting engineering solutions that advance research.\nProgress in AI requires next-generation environments - rich, interactive virtual worlds in which we can test our systems and allow them to learn a wide variety of tasks. Our Worlds team, who are developers, designers, artists, QA technicians, and program managers with experience in engineering, games, and VFX companies, is responsible for creating these environments.\nWe collaborate with researchers to design and build a wide variety of environments and tasks using well-known video game engines such as Unity and Unreal, while creating new platforms and tools that empower researchers to build environments themselves. From bespoke mini-games aimed at answering specific research questions to expansive first-person games using modern 3D engines, the Worlds team plays a fundamental part in every research area at DeepMind.\nThe Robotics lab is a group of multidisciplinary research scientists, research engineers, and software engineers who pioneer new approaches in robotics. \u00a0\nRobotics is a critical part of developing general-purpose learning algorithms because systems must learn to deal with the incredible complexity and ever-changing conditions of the real world. We collaborate with multiple teams across DeepMind to endow our systems with the ability to learn, allowing them to respond and adapt to a variety of variable environments. In particular, we focus on learning complex manipulation and navigation tasks and understanding how systems respond to the physical world.\nNatasha holds a PhD in medical physics from the University of Chicago. Her thesis used deep learning methods for clinical decision-making with dynamic MRIs, mammography, and ultrasound. \nNatasha is currently developing AI models for breast cancer diagnoses based on screening mammograms.\nC\u00e9dric has over 12 years of experience in the games industry, working on technical aspects of game design at both small mobile game studios and AAA first-party developers.\nC\u00e9dric creates tasks for testing the progress of researchers\u2019 agents that build AGI.\nJason has a background in pure maths and computer science. He worked on the animation system for the FIFA videogame and developed open source video game tech for Fun Propulsion Labs at Google.\nJason collaborates with research teams to create virtual environments for agents and games for AIs to play.\nAndreas conducted his postdoc at Imperial College London, working on spiking neural network simulations using GPUs in the Cognitive Robotics Lab. \nAndreas ensures his team are always working on the most important and interesting problems and functioning smoothly, with the best tools and engineering support available.\nTamara studied computer science at the University of Cambridge. Her dissertation involved implementing a parallel Prolog interpreter.\nTamara works on creating efficient and flexible, higher-level abstractions for the representation and creation of neural networks. She aims to enable varying and fast-paced research.\nAdrian worked in tech for 15 years before joining DeepMind as COO in 2010. He has established teams from engineering to operations, recruitment, and more.\nAdrian\u2019s team collaborate with research and engineering teams to build virtual worlds for agents to explore and learn from.\nDan worked as a distinguished engineer at Google, specialising in large-scale systems for search infrastructure. \nDan is focused on creating an inclusive and collaborative engineering organisation. He oversees long-term projects and engineering strategy and ensures his team are effective and aligned with DeepMind\u2019s research.\nPrior to joining DeepMind, Shibl was the Engineering Site Lead for Google Montreal where he helped grow the team and expand its scope to include both engineering and AI research, as well as contributing to Montreal tech and AI community.\nShibl is passionate about combining best engineering practices with machine learning research to advance our knowledge of AI and use it to build a better world.\n"}
{"title": "Open source", "contents": ""}
{"title": "A Generalist Agent", "contents": "Inspired by progress in large-scale language modelling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens.\nDuring the training phase of Gato, data from different tasks and modalities are serialised into a flat sequence of tokens, batched, and processed by a transformer neural network similar to a large language model. The loss is masked so that Gato only predicts action and text targets.\nWhen deploying Gato, a prompt, such as a demonstration, is tokenised, forming the initial sequence. Next, the environment yields the first observation, which is also tokenised and appended to the sequence. Gato samples the action vector autoregressively, one token at a time.\nOnce all tokens comprising the action vector have been sampled (determined by the action specification of the environment), the action is decoded and sent to the environment which steps and yields a new observation. Then the procedure repeats. The model always sees all previous observations and actions within its context window of 1024 tokens.\nGato is trained on a large number of datasets comprising agent experience in both simulated and real-world environments, in addition to a variety of natural language and image datasets. The number of tasks, where the performance of the pretrained Gato model is above a percentage of expert score, grouped by domain, is shown here.\nThe following images also show how the pre-trained Gato model with the same weights can do image captioning, engage in an interactive dialogue, and control a robot arm, among many other tasks.\n"}
{"title": "Emergent Bartering Behaviour in Multi-Agent Reinforcement Learning", "contents": "In \nour recent paper\n, we explore how populations of deep reinforcement learning (deep RL) agents can learn microeconomic behaviours, such as production, consumption, and trading of goods. We find that artificial agents learn to make economically rational decisions about production, consumption, and prices, and react appropriately to supply and demand changes. The population converges to local prices that reflect the nearby abundance of resources, and some agents learn to transport goods between these areas to \u201cbuy low and sell high\u201d. This work advances the broader multi-agent reinforcement learning research agenda by introducing new social challenges for agents to learn how to solve.\nInsofar as the goal of multi-agent reinforcement learning research is to eventually produce agents that work across the full range and complexity of human social intelligence, the set of domains so far considered has been woefully incomplete. It is still missing crucial domains where human intelligence excels, and humans spend significant amounts of time and energy. The subject matter of economics is one such domain. Our goal in this work is to establish environments based on the themes of trading and negotiation for use by researchers in multi-agent reinforcement learning.\nEconomics uses agent-based models to simulate how economies behave. These agent-based models often build in economic assumptions about how agents should act. In this work, we present a multi-agent simulated world where agents can learn economic behaviours from scratch, in ways familiar to any Microeconomics 101 student: decisions about production, consumption, and prices. But our agents also must make other choices that follow from a more physically embodied way of thinking. They must navigate a physical environment, find trees to pick fruits, and partners to trade them with. Recent advances in deep RL techniques now make it possible to create agents that can learn these behaviours on their own, without requiring a programmer to encode domain knowledge.\nOur environment, called \nFruit Market\n, is a multiplayer environment where agents produce and consume two types of fruit: apples and bananas. Each agent is skilled at producing one type of fruit, but has a preference for the other \u2013 if the agents can learn to barter and exchange goods, both parties would be better off.\nIn our experiments, we demonstrate that current deep RL agents can learn to trade, and their behaviours in response to supply and demand shifts align with what microeconomic theory predicts. We then build on this work to present scenarios that would be very difficult to solve using analytical models, but which are straightforward for our deep RL agents. For example, in environments where each type of fruit grows in a different area, we observe the emergence of different price regions related to the local abundance of fruit, as well as the subsequent learning of arbitrage behaviour by some agents, who begin to specialise in transporting fruit between these regions.\nThe field of agent-based computational economics uses similar simulations for economics research. In this work, we also demonstrate that state-of-the-art deep RL techniques can flexibly learn to act in these environments from their own experience, without needing to have economic knowledge built in. This highlights the reinforcement learning community\u2019s recent progress in multi-agent RL and deep RL, and demonstrates the potential of multi-agent techniques as tools to advance simulated economics research.\nAs a \npath to artificial general intelligence\n (AGI), multi-agent reinforcement learning research should encompass all critical domains of social intelligence. However, until now it hasn\u2019t incorporated traditional economic phenomena such as trade, bargaining, specialisation, consumption, and production. This paper fills this gap and provides a platform for further research. To aid future research in this area, the Fruit Market environment will be included in the next release of the \nMelting Pot\n suite of environments.\n"}
{"title": "Creating Interactive Agents with Imitation Learning", "contents": "Humans are an interactive species. We interact with the physical world and with one another. For artificial intelligence (AI) to be generally helpful, it must be able to interact capably with humans and their environment. In this work we present the Multimodal Interactive Agent (MIA), which blends visual perception, language comprehension and production, navigation, and manipulation to engage in extended and often surprising physical and linguistic interactions with humans.\nWe build upon the approach introduced by Abramson et al. (2020), which primarily uses imitation learning to train agents. After training, MIA displays some rudimentary intelligent behaviour that we hope to later refine using human feedback. This work focuses on the creation of this intelligent behavioural prior, and we leave further feedback-based learning for future work.\nWe created the Playhouse environment, a 3D virtual environment composed of a randomised set of rooms and a large number of domestic interactable objects, to provide a space and setting for humans and agents to interact together. Humans and agents can interact in the Playhouse by controlling virtual robots that locomote, manipulate objects, and communicate via text. This virtual environment permits a wide range of situated dialogues, ranging from simple instructions (e.g., \u201cPlease pick up the book from the floor and place it on the blue bookshelf\u201d) to creative play (e.g., \u201cBring food to the table so that we can eat\u201d).\nWe collected human examples of Playhouse interactions using language games, a collection of cues prompting humans to improvise certain behaviours. In a language game one player (the setter) receives a prewritten prompt indicating a kind of task to propose to the other player (the solver). For example, the setter might receive the prompt \u201cAsk the other player a question about the existence of an object,'' and after some exploration, the setter could ask, \u201dPlease tell me whether there is a blue duck in a room that does not also have any furniture.'' To ensure sufficient behavioural diversity, we also included free-form prompts, which granted setters free choice to improvise interactions (E.g. \u201cNow take any object that you like and hit the tennis ball off the stool so that it rolls near the clock, or somewhere near it.''). In total, we collected 2.94 years of real-time human interactions in the Playhouse.\nOur training strategy is a combination of supervised prediction of human actions (behavioural cloning) and self-supervised learning. When predicting human actions, we found that using a hierarchical control strategy significantly improved agent performance. In this setting, the agent receives new observations roughly 4 times per second. For each observation, it produces a sequence of open-loop movement actions and optionally emits a sequence of language actions. In addition to behavioural cloning we use a form of self-supervised learning, which tasks agents with classifying whether certain vision and language inputs belong to the same or different episodes.\nTo evaluate agent performance, we asked human participants to interact with agents and provide binary feedback indicating whether the agent successfully carried out an instruction. MIA achieves over 70% success rate in human-rated online interactions, representing 75% of the success rate that humans themselves achieve when they play as solvers. To better understand the role of various components in MIA, we performed a series of ablations, removing, for example, visual or language inputs, the self-supervised loss, or the hierarchical control.\nContemporary machine learning research has uncovered remarkable regularities of performance with respect to different scale parameters; in particular, model performance scales as a power-law with dataset size, model size, and compute. These effects have been most crisply noted in the language domain, which is characterised by massive dataset sizes and highly evolved architectures and training protocols. In this work, however, we are in a decidedly different regime \u2013 with comparatively small datasets and multimodal, multi-task objective functions training heterogeneous architectures. Nevertheless, we demonstrate clear effects of scaling: as we increase dataset and model size, performance increases appreciably.\n\u200d\nIn an ideal case, training becomes more efficient given a reasonably large dataset, as knowledge is transferred between experiences. To investigate how ideal our circumstances are, we examined how much data is needed to learn to interact with a new, previously unseen object and to learn how to follow a new, previously unheard command / verb. We partitioned our data into background data and data involving a language instruction referring to the object or the verb. When we reintroduced the data referring to the new object, we found that fewer than 12 hours of human interaction was enough to acquire the ceiling performance. Analogously, when we introduced the new command or verb \u2018to clear\u2019 (i.e. to remove all objects from a surface), we found that only 1 hour of human demonstrations was enough to reach ceiling performance in tasks involving this word.\nMIA exhibits startlingly rich behaviour, including a diversity of behaviours that were not preconceived by researchers, including tidying a room, finding multiple specified objects, and asking clarifying questions when an instruction is ambiguous. These interactions continually inspire us. However, the open-endedness of MIA\u2019s behaviour presents immense challenges for quantitative evaluation. Developing comprehensive methodologies to capture and analyse open-ended behaviour in human-agent interactions will be an important focus in our future work.\nFor a more detailed description of our work, see our \npaper\n.\n\u200d\n"}
{"title": "Real-world challenges for AGI", "contents": "Note: This post is a summary of a talk given at CERN Sparks! Serendipity Forum in September 2021, which can be viewed \nhere\n.\nWhen people picture a world with artificial general intelligence (AGI), robots are more likely to come to mind than enabling solutions to society\u2019s most intractable problems. But I believe the latter is much closer to the truth. AI is already enabling huge leaps in tackling fundamental challenges: \nfrom solving protein folding\n to \npredicting accurate weather patterns\n, scientists are increasingly using AI to deduce the rules and principles that underpin highly complex real-world domains - ones they might never have discovered unaided.\nAdvances in AGI research will supercharge society\u2019s ability to tackle and manage climate change - not least because of its urgency but also due to its complex and multifaceted nature.\nLooking across the field of AI research today, there are two common categories of problems scientists are focused on: prediction and control. Prediction models try to learn about a domain (such as weather patterns) and understand how it might evolve, while control models prompt agents to take actions in that environment. Building a successful path to AGI requires understanding and developing algorithms in both spaces, accounting for all the variations that our natural and social environments throw at us, from how viruses mutate or how language may evolve in use and meaning over time to how to help produce energy from fusion power. Two real-world domains that scientists at DeepMind are contributing to tackle climate change while developing what\u2019s required to build AGI are weather prediction and plasma control for fusion.\nWeather patterns are almost impossible to precisely model - it\u2019s an example of nature\u2019s variations at its fullest. However, causes and effects can be inferred based on vast amounts of historical data. Transferring the same generative models that are used to generate images and video clips into learning weather patterns in collaboration with the \nMet Office\n (UK\u2019s national meteorological service), scientists at DeepMind have developed systems that can take 20 minutes of weather data to generate multiple hypotheses for radar maps and \naccurately predict heavy rainfall\n in the next 90 minutes.\nCritically, these models will help meteorologists provide forecasts that aid decision making for emergency services, energy management, and activation of flood warning systems - enabling better preparation for and responses to extreme weather events, which have become increasingly common around the world. Helping predict important weather events by forecasting accurate weather patterns is one example of how AI research can make a meaningful impact as it becomes more generally applicable and \u2018intelligent\u2019.\nBeyond responding to the effects of climate change, solving its sources is of equal if not greater importance. Fusion, a single source of energy that is clean, limitless, and self-sustaining, is elusive, yet remains one of the world\u2019s most promising solutions - one that I believe requires developing a general algorithm that can solve many different components at once. Already we are seeing progress in one component, the extremely challenging problem of maintaining novel plasma shapes to enable better energy output and stability of the plasma for as long as possible.\nBy working with world-renowned experts at the \nSwiss Plasma Center\n and \n\u00c9cole polytechnique f\u00e9d\u00e9rale de Lausanne\n (EPFL), we are able to go beyond today\u2019s hand crafted models, applying deep reinforcement learning algorithms first developed for robotics to plasma control. The result is a controller that can successfully manipulate different plasma shapes and configurations at 10,000 interactions per second.\nWithout expert collaboration, AI researchers cannot make significant progress in real-world domains. Identifying the right paths forward in these fields requires partnerships across disciplines, leveraging a common scientific approach to develop and use AI to navigate complex questions at the heart of society\u2019s most urgent needs. It\u2019s why dreaming together with a diversity of natural and social scientists about what a world with AGI could look like is so critically important.\nAs we develop AGI, addressing global challenges such as climate change will not only make crucial and beneficial impacts that are urgent and necessary for our world, but also advance the science of AGI itself. Many other categories of AGI problems are yet to be solved - from causality, to learning efficiently and transfer - and as algorithms become more general, more real-world problems will be solved, gradually contributing to a system that one day will help solve everything else, too.\n\u200d\n"}
{"title": "Exploring the beauty of pure mathematics in novel ways", "contents": "More than a century ago, \nSrinivasa Ramanujan\n shocked the mathematical world with his extraordinary ability to see remarkable patterns in numbers that no one else could see. The self-taught mathematician from India described his insights as deeply intuitive and spiritual, and patterns often came to him in vivid dreams. These observations captured the tremendous beauty and sheer possibility of the abstract world of pure mathematics. In recent years, we have begun to see AI make breakthroughs in \nareas involving deep human intuition\n, and more recently on some of the \nhardest problems across the sciences\n, yet until now, the latest AI techniques have not assisted in significant results in pure maths research.\nAs part of \nDeepMind's mission\n to solve intelligence, we explored the potential of machine learning (ML) to recognize mathematical structures and patterns, and help guide mathematicians toward discoveries they may otherwise never have found \u2014 demonstrating for the first time that AI can help at the forefront of pure mathematics.\nOur research paper\n, published today in the journal Nature, details our collaboration with top mathematicians to apply AI toward discovering new insights in two areas of pure mathematics: topology and representation theory. With \nProfessor Geordie Williamson\n at the University of Sydney, we discovered a new formula for a conjecture about permutations that has remained unsolved for decades. With \nProfessor Marc Lackenby\n and \nProfessor Andr\u00e1s Juh\u00e1sz\n at the University of Oxford, we have discovered an unexpected connection between different areas of mathematics by studying the structure of knots. These are the first significant mathematical discoveries made with machine learning, according to the top mathematicians who reviewed the work. We\u2019re also releasing full companion papers on arXiv for each result that will be submitted to appropriate mathematical journals (\npermutations paper\n; \nknots paper\n). Through these examples, we propose a model for how these tools could be used by other mathematicians to achieve new results.\nThe two fundamental objects we investigated were knots and permutations.\nFor many years, computers have been used by mathematicians to generate data to help in the search for patterns. Known as experimental mathematics, this kind of research has resulted in well-known conjectures, such as \nthe Birch and Swinnerton-Dyer conjecture\n \u2014 one of six \nMillennium Prize Problems\n, the most well-known open problems in mathematics (with a US$1 million prize attached to each). While this approach has been successful and is fairly common, the identification and discovery of patterns from this data has still relied mainly on mathematicians.\nFinding patterns has become even more important in pure maths because it\u2019s now possible to generate more data than any mathematician can reasonably expect to study in a lifetime. Some objects of interest \u2014 such as those with thousands of dimensions \u2014 can also simply be too unfathomable to reason about directly. With these constraints in mind, we believed that AI would be capable of augmenting mathematicians\u2019 insights in entirely new ways.\nOur results suggest that ML can complement maths research to guide intuition about a problem by detecting the existence of hypothesised patterns with supervised learning and giving insight into these patterns with attribution techniques from machine learning:\nWith Professor Williamson, we used AI to help discover a new approach to a long-standing conjecture in representation theory. Defying progress for nearly 40 years, the \ncombinatorial invariance conjecture\nstates that a relationship should exist between certain directed graphs and polynomials. Using ML techniques, we were able to gain confidence that such a relationship does indeed exist and to identify that it might be related to structures known as broken dihedral intervals and extremal reflections. With this knowledge, Professor Williamson was able to conjecture a surprising and beautiful algorithm that would solve the combinatorial invariance conjecture. We have computationally verified the new algorithm across more than 3 million examples.\nWith Professor Lackenby and Professor Juh\u00e1sz, we explored knots - one of the fundamental objects of study in topology. Knots not only tell us about the many ways a rope can be tangled but also have surprising connections with quantum field theory and non-Euclidean geometry. \u00a0Algebra, geometry, and quantum theory all share unique perspectives on these objects and a long standing mystery is how these different branches relate: for example, what does the geometry of the knot tell us about the algebra? We trained an ML model to discover such a pattern and surprisingly, this revealed that a particular algebraic quantity \u2014 the signature \u2014 was directly related to the geometry of the knot, which was not previously known or suggested by existing theory. By using attribution techniques from machine learning, we guided Professor Lackenby to discover a new quantity, which we call the natural slope, that hints at an important aspect of structure overlooked until now. Together we were then able to prove the exact nature of the relationship, establishing some of the first connections between these different branches of mathematics.\nThe use of learning techniques and AI systems holds great promise for the identification and discovery of patterns in mathematics. Even if certain kinds of patterns continue to elude modern ML, we hope \nour Nature paper\n can inspire other researchers to consider the potential for AI as a useful tool in pure maths. To replicate the results, anybody can access our \ninteractive notebooks\n. Reflecting on the incredible mind of Ramanujan, \nGeorge Frederick James Temple\n wrote, \u201cThe great advances in mathematics have not been made by logic but by creative imagination.\u201d Working with mathematicians, we look forward to seeing how AI can further elevate the beauty of human intuition to new levels of creativity.\nThis work was done by a team including contributions from Alex Davies, Petar Veli\u010dkovi\u0107, Lars Buesing, Sam Blackwell, Daniel Zheng, Nenad Toma\u0161ev, Richard Tanburn, Peter Battaglia, Charles Blundell, Xavier Glorot, Matt Overlan, Alyssa Pierce, Natalie Lambert, George Holland, Razia Ahamed, Clemens Meyer, Demis Hassabis and Pushmeet Kohli. We would also like to thank Jan Vonk and Jordan Ellenberg for additional mathematical input.\n"}
{"title": "Language modelling at scale: Gopher, ethical considerations, and retrieval", "contents": "Language, and its role in demonstrating and facilitating comprehension - or intelligence - is a fundamental part of being human. It gives people the ability to communicate thoughts and concepts, express ideas, create memories, and build mutual understanding. These are foundational parts of social intelligence. It\u2019s why our teams at DeepMind study aspects of language processing and communication, both in artificial agents and in humans.\nAs part of a broader portfolio of AI research, we believe the development and study of more powerful language models \u2013 systems that predict and generate text \u2013 \u00a0have tremendous potential for building advanced AI systems that can be used safely and efficiently to summarise information, provide expert advice and follow instructions via natural language. Developing beneficial language models requires research into their potential impacts, including the risks they pose. This includes collaboration between experts from varied backgrounds to thoughtfully anticipate and address the challenges that training algorithms on existing datasets can create.\nToday we are releasing three papers on language models that reflect this interdisciplinary approach. They include a detailed study of \na 280 billion parameter transformer language model called \nGopher\n, \na study of ethical and social risks associated with large language models\n, and \na paper investigating a new architecture with better training efficiency.\nIn the quest to explore language models and develop new ones, we trained a series of transformer language models of different sizes, ranging from 44 million parameters to 280 billion parameters (the largest model we named \nGopher\n).\nOur research investigated the strengths and weaknesses of those different-sized models, highlighting areas where increasing the scale of a model continues to boost performance \u2013 for example, in areas like reading comprehension, fact-checking, and the identification of toxic language. We also surface results where model scale does not significantly improve results \u2014 for instance, in logical reasoning and common-sense tasks.\nIn our research, we found the capabilities of \nGopher \nexceed existing language models for a number of key tasks. This includes the Massive Multitask Language Understanding (MMLU) benchmark, where \nGopher \ndemonstrates a significant advancement towards human expert performance over prior work.\nAs well as quantitative evaluation of \nGopher\n, we also explored the model through direct interaction. Among our key findings was that, when \nGopher\n is prompted towards a dialogue interaction (like in a chat), the model can sometimes provide surprising coherence.\nHere \nGopher\n can discuss cell biology and provide a correct citation despite no specific dialogue fine-tuning. However our research also detailed several failure modes that persist across model sizes, amongst them a tendency for repetition, the reflection of stereotypical biases, and the confident propagation of incorrect information.\nThis type of analysis is important, because understanding and documenting failure modes gives us an insight into how large language models could lead to downstream harms, and shows us where mitigation efforts in research should focus to address those issues.\nIn our second paper, we anticipate possible ethical and social risks from language models, and create a comprehensive classification of these risks and failure modes, building on prior research in this area [\nBommasani et al 2021\n, \nBender et al 2021\n, \nPatterson et al 2021\n]. This systematic overview is an essential step towards understanding these risks and mitigating potential harm. We present a taxonomy of the risks related to language models, categorised into six thematic areas, and elaborate on 21 risks in-depth.\nTaking a broad view of different risk areas is essential: as we show in the paper, an overly narrow focus on a single risk in isolation can make other problems worse. The taxonomy we present serves as a foundation for experts and wider public discourse to build a shared overview of ethical and social considerations on language models, make responsible decisions, and exchange approaches to dealing with the identified risks.\nOur research finds that two areas in particular require further work. First, current benchmarking tools are insufficient for assessing some important risks, for example, when language models output misinformation and people trust this information to be true. Assessing risks like these requires more scrutiny of human-computer-interaction with language models. In our paper we list several risks that similarly require novel or more interdisciplinary analysis tools. Second, more work is needed on risk mitigations. For example, language models are known to reproduce harmful social stereotypes, but research on this problem is still in early stages, as a \nrecent DeepMind paper\n showed.\nOur final paper builds on the foundations of \nGopher\n and our taxonomy of ethical and social risk by proposing an improved language model architecture that reduces the energy cost of training and makes it easier to trace model outputs to sources within the training corpus.\nThe Retrieval-Enhanced Transformer (RETRO) is pre-trained with an Internet-scale retrieval mechanism. Inspired by how the brain relies on dedicated memory mechanisms when learning, RETRO efficiently queries for passages of text to improve its predictions. By comparing generated texts to the passages RETRO relied upon for generation, we can interpret why the model makes certain predictions and where they came from. We also see how the model obtains comparable performance to a regular Transformer with an order of magnitude fewer parameters, and obtains state-of-the-art performance on several language modeling benchmarks.\nThese papers offer a foundation for DeepMind\u2019s language research going forward, particularly in areas that will have a bearing on how these models are evaluated and deployed. Addressing these areas will be critical for ensuring safe interactions with AI agents \u2013 from people telling agents what they want to agents explaining their actions to people. Research in the broader community on using communication for safety includes \nnatural language explanations\n, \nusing communication to reduce uncertainty\n, and using language to unpack complex decisions into pieces such as \namplification\n, \ndebate\n, and \nrecursive reward modeling\n -- all critical areas of exploration.\nAs we continue our research on language models, DeepMind will remain cautious and thoughtful. This requires stepping back to assess the situation we find ourselves in, mapping out potential risks, and researching mitigations. We will strive to be transparent and open about the limitations of our models and will work to mitigate identified risks. At each step, we draw on the breadth of expertise from our multidisciplinary teams, including from our Language, Deep Learning, Ethics, and Safety teams. This approach is key to creating large language models that serve society, furthering our mission of solving intelligence to advance science and benefit humanity.\n"}
{"title": "Simulating matter on the quantum scale with AI", "contents": "Solving some of the major challenges of the 21st Century, such as producing clean electricity or developing high temperature superconductors, will require us to design new materials with specific properties. To do this on a computer requires the simulation of electrons, the subatomic particles that govern how atoms bond to form molecules and are also responsible for the flow of electricity in solids. Despite decades of effort and several significant advances, accurately modelling the quantum mechanical behaviour of electrons remains an open challenge. Now, in a \npaper\n (\nOpen Access PDF\n) published in Science, we propose DM21, a neural network achieving state of the art accuracy on large parts of chemistry. To accelerate scientific progress, we\u2019re also open sourcing our \ncode\n for anyone to use.\nNearly a century ago, Erwin Schr\u00f6dinger proposed \nhis famous equation\n governing the behaviour of quantum mechanical particles. Applying this equation to electrons in molecules is challenging because all electrons repel each other. This would seem to require tracking the probability of each electron\u2019s position \u2014 a remarkably complex task for even a small number of electrons. One major breakthrough came in the 1960s, when Pierre Hohenberg and Walter Kohn realised that it is not necessary to track each electron individually. Instead, knowing the probability for \nany\n electron to be at each position (i.e., the electron density) is sufficient to exactly compute all interactions. Kohn received a \nNobel Prize in Chemistry\n after proving this, thus founding Density Functional Theory (DFT).\nAlthough DFT proves a mapping exists, for more than 50 years the exact nature of this mapping between electron density and interaction energy \u2014 the so-called density functional \u2014 has remained unknown and has to be approximated. Despite the fact that DFT intrinsically involves a level of approximation, it is the only practical method to study how and why matter behaves in a certain way at the microscopic level and has therefore become one of the most widely used techniques in all of science. Over the years, researchers have proposed many approximations to the exact functional with varying levels of accuracy. Despite their popularity, all of these approximations suffer from systematic errors because they fail to capture certain crucial mathematical properties of the exact functional.\nBy expressing the functional as a neural network and incorporating these exact properties into the training data, we learn functionals free from important systematic errors \u2014 resulting in a better description of a broad class of chemical reactions.\nWe specifically address two long-standing problems with traditional functionals:\nIn principle, any chemical-physical process that involves movement of charge is liable to suffer from delocalization error, and any process that involves the breaking of bonds is liable to suffer from spin-symmetry breaking. Movement of charge and bond breaking are core to many important technological applications, but these problems can also lead to massive qualitative failure of functionals to describe the simplest molecules, such as hydrogen. Since DFT is such a crucial technology it is important to design functionals that get this simple chemistry correct before asking them to explain vastly more complex molecular interactions, such as those that may occur in a battery or solar cell.\nThese longstanding challenges are both related to how functionals behave when presented with a system that exhibits \u201cfractional electron character.\u201d By using a neural network to represent the functional and tailoring our training dataset to capture the fractional electron behaviour expected for the exact functional, we found that we could solve the problems of delocalization and spin symmetry-breaking. Our functional also showed itself to be highly accurate on broad, large-scale benchmarks, suggesting that this data-driven approach can capture aspects of the exact functional that have thus far been elusive.\nFor years, computer simulations have played a central role in modern engineering, making it possible to provide reliable answers to questions like \u201cwill this bridge stay up?\u201d to \u201cwill this rocket make it into space?\u201d As technology increasingly turns to the quantum scale to explore questions about materials, medicines, and catalysts, including those we\u2019ve never seen or even imagined, deep learning shows promise to accurately simulate matter at this quantum mechanical level.\nRead the Science paper \nhere\n.\nOpen Access PDF of the paper \nhere\n.\n"}
{"title": "Competitive programming with AlphaCode", "contents": "Solving novel problems and setting a new milestone in competitive programming.\nCreating solutions to unforeseen problems is second nature in human intelligence \u2013 a result of critical thinking informed by experience. The machine learning community has made tremendous progress in generating and understanding textual data, but advances in problem solving remain limited to relatively simple maths and programming problems, or else retrieving and copying existing solutions. As part of \nDeepMind\u2019s mission\n to solve intelligence, we created a system called AlphaCode that writes computer programs at a competitive level. AlphaCode achieved an estimated rank within the top 54% of participants in programming competitions by solving new problems that require a combination of critical thinking, logic, algorithms, coding, and natural language understanding.\nIn \nour paper\n, we detail AlphaCode, which uses transformer-based language models to generate code at an unprecedented scale, and then smartly filters to a small set of promising programs.\nWe validated our performance using competitions hosted on \nCodeforces\n, a popular platform which hosts regular competitions that attract tens of thousands of participants from around the world who come to test their coding skills. We selected for evaluation 10 recent contests, each newer than our training data. AlphaCode placed at about the level of the median competitor, marking the first time an AI code generation system has reached a competitive level of performance in programming competitions.\nTo help others build on our results, we\u2019re releasing our dataset of competitive programming problems and solutions \non GitHub\n, including extensive tests to ensure the programs that pass these tests are correct \u2014 a critical feature current datasets lack. We hope this benchmark will lead to further innovations in problem solving and code generation.\nCompetitive programming is a popular and challenging activity; hundreds of thousands of programmers participate in coding competitions to gain experience and showcase their skills in fun and collaborative ways. During competitions, participants receive a series of long problem descriptions and a few hours to write programs to solve them. Typical problems include finding ways to place roads and buildings within certain constraints, or creating strategies to win custom board games. Participants are then ranked mainly based on how many problems they solve. Companies use these competitions as recruiting tools and similar types of problems are common in hiring processes for software engineers.\nThe problem-solving abilities required to excel at these competitions are beyond the capabilities of existing AI systems. However, by combining advances in large-scale transformer models (that have recently shown promising abilities to generate code) with large-scale sampling and filtering, we\u2019ve made significant progress in the number of problems we can solve. We pre-train our model on selected public GitHub code and fine-tune it on our relatively small competitive programming dataset. At evaluation time, we create a massive amount of C++ and Python programs for each problem, orders of magnitude larger than previous work. Then we filter, cluster, and rerank those solutions to a small set of 10 candidate programs that we submit for external assessment. This automated system replaces competitors\u2019 trial-and-error process of debugging, compiling, passing tests, and eventually submitting.\nWith the permission of Codeforces, we evaluated AlphaCode by simulating participation in 10 recent contests. The impressive work of the competitive programming community has created a domain where it\u2019s not possible to solve problems through shortcuts like duplicating solutions seen before or trying out every potentially related algorithm. Instead, our model must create novel and interesting solutions. Overall, AlphaCode placed at approximately the level of the median competitor. Although far from winning competitions, this result represents a substantial leap in AI problem-solving capabilities and we hope that our results will inspire the competitive programming community.\nFor artificial intelligence to help humanity, our systems need to be able to develop problem-solving capabilities. AlphaCode ranked within the top 54% in real-world programming competitions, an advancement that demonstrates the potential of deep learning models for tasks that require critical thinking. These models elegantly leverage modern machine learning to express solutions to problems as code, circling back to the symbolic reasoning root of AI from decades ago. And this is only a start. Our exploration into code generation leaves vast room for improvement and hints at even more exciting ideas that could help programmers improve their productivity and open up the field to people who do not currently write code. We will continue this exploration, and hope that further research will result in tools to enhance programming and bring us closer to a problem-solving AI.\nView more of AlphaCode\u2019s solutions and dive into the model at \nalphacode.deepmind.com\n"}
{"title": "DeepMind: The Podcast returns for Season 2", "contents": "We believe artificial intelligence (AI) is one of the most significant technologies of our age and we want to help people understand its potential and how it\u2019s being created. \nIn 2019, we released \nDeepMind: The Podcast\n to explore these ideas, answer common questions and give an inside look at how AI research happens at a lab like DeepMind. Today, we\u2019re proud to launch a new season, with stories of the latest breakthroughs, innovations, and challenges.\nListeners can find the new episodes on \nApple Podcasts\n, \nGoogle Podcasts\n, \nSpotify\n, or their favourite podcast app by searching for \u201cDeepMind: The Podcast\u201d.\nHosted by mathematician and broadcaster \nProfessor Hannah Fry\n, the nine programmes \u00a0explore the latest in AI, from helping advance science, like solving a 50-year-old grand challenge in biology, to building computer systems that can cooperate with humans.\nThrough more than 30 original interviews, including with our co-founders Demis and Shane, listeners hear how AI can help people with degenerative diseases regain their voices, conserve the unique wildlife of the Serengeti in East Africa, predict whether it will rain in the next hour, and take football to the next level with Liverpool Football Club.\nHannah also learns how robotics researchers taught robots to walk at home during lockdown, explores recent advances in allowing computers to communicate with natural language, and examines some of the difficult ethical questions the whole field is grappling with to ensure the technology benefits society as a whole.\nWe\u2019d like to extend a huge thank you to Hannah and to our amazing contributors and creative producers, who all helped craft our initial idea into this new season of programmes. And we especially want to thank our listeners, who took the time to listen, share, and offer feedback on Season 1. Thank you!\nWe hope you enjoy listening to \nDeepMind: The Podcast, Season 2\n as much as we loved making it for you.\nListen now on your favourite podcast app by searching \u201cDeepMind: The Podcast\u201d.\nWe want this podcast to be a useful resource. If you have any feedback on season two, suggestions for topics you\u2019d like us to explore, or questions about AI, please email \npodcast@deepmind.com\n or message @deepmind on \nInstagram\n or \nTwitter\n.\n"}
{"title": "Spurious normativity enhances learning of compliance and enforcement behavior in artificial agents", "contents": "In \nour recent paper\n we explore how multi-agent deep reinforcement learning can serve as a model of complex social interactions, like the formation of social norms. This new class of models could provide a path to create richer, more detailed simulations of the world.\nHumans are an \nultra social species\n. Relative to other mammals we benefit more from cooperation but we are also more dependent on it, and face greater cooperation challenges. Today, humanity faces numerous cooperation challenges including preventing conflict over resources, ensuring everyone can access clean air and drinking water, eliminating extreme poverty, and combating climate change. Many of the cooperation problems we face are difficult to resolve because they involve complex webs of social and biophysical interactions called \nsocial-ecological systems\n. However, humans can collectively learn to overcome the cooperation challenges we face. We accomplish this by an ever evolving culture, including norms and institutions which organize our interactions with the environment and with one another.\nHowever, norms and institutions sometimes fail to resolve cooperation challenges. For example, individuals may over-exploit resources like forests and fisheries thereby causing them to collapse. In such cases, policy-makers may write laws to change institutional rules or develop other \ninterventions to try to change norms\n in hopes of bringing about a positive change. But policy interventions do not always work as intended. This is because real-world social-ecological systems are considerably \nmore complex\n than the models we typically use to try to predict the effects of candidate policies.\nModels based on game theory are often applied to the study of cultural evolution. In most of these models, the key interactions that agents have with one another are expressed in a \u2018payoff matrix\u2019. In a game with two participants and two actions A and B, a payoff matrix defines the value of the four possible outcomes: (1) we both choose A, (2) we both choose B, (3) I choose A while you choose B and (4) I choose B while you choose A. The most famous example is the \u2018Prisoner\u2019s Dilemma\u2019, in which the actions are interpreted as \u201ccooperate\u201d and \u201cdefect\u201d. Rational agents who act according to their own myopic self-interest are doomed to defect in the Prisoner\u2019s Dilemma even though the better outcome of mutual cooperation is available.\nGame-theoretic models have been very widely applied. Researchers in diverse fields have used them to study a wide range of different phenomena, including economies and the evolution of human culture. However, game theory is not a neutral tool, rather it is a deeply opinionated modeling language. It imposes a strict requirement that everything must ultimately cash out in terms of the payoff matrix (or equivalent representation). This means that the modeler has to know, or be willing to assume, everything about how the effects of individual actions combine to generate incentives. This is sometimes appropriate, and the game theoretic approach has had many notable successes such as in modeling the \nbehavior of oligopolistic firms\n and \ncold war era international relations\n. However, game theory\u2019s major weakness as a modeling language is exposed in situations where the modeler does not fully understand how the choices of individuals combine to generate payoffs. Unfortunately this tends to be the case with social-ecological systems because their social and ecological parts interact in complex ways that we do not fully understand.\nThe work we present here is one example within a research program that attempts to establish an alternative modeling framework, different from game theory, to use in the study of social-ecological systems. Our approach may be seen formally as a variety of \nagent-based modeling\n. However, its distinguishing feature is the incorporation of algorithmic elements from artificial intelligence, especially multi-agent deep reinforcement learning.\nThe core idea of this approach is that every model consists of two interlocking parts: (1) a rich, dynamical model of the environment and (2) a model of individual decision-making.\nThe first takes the form of a researcher-designed simulator: an interactive program that takes in a current environment state and agent actions, and outputs the next environment state as well as the observations of all agents and their instantaneous rewards. The model of individual decision-making is likewise conditioned on environment state. It is an \nagent\n that learns from its past experience, performing a form of trial-and-error. An agent interacts with an environment by taking in observations and outputting actions. Each agent selects actions according to its behavioral policy, a mapping from observations to actions. Agents learn by changing their policy to improve it along any desired dimension, typically to obtain more reward. The policy is stored in a neural network. Agents learn \u2018from scratch\u2019, from their own experience, how the world works and what they can do to earn more rewards. They accomplish this by tuning their network weights in such a way that the pixels they receive as observations are gradually transformed into competent actions. Several learning agents can inhabit the same environment as one another. In this case the agents become interdependent because their actions affect one another.\nLike other agent-based modeling approaches, multi-agent deep reinforcement learning makes it easy to specify models that cross levels of analysis that would be hard to treat with game theory. For instance, actions may be far closer to low-level motor primitives (e.g. 'walk forward'; 'turn right') than the high-level strategic decisions of game theory (e.g. \u2018cooperate\u2019). This is an important feature needed to capture situations where agents must practice to learn effectively how to \nimplement their strategic choices\n. For instance in one \nstudy\n, agents learned to cooperate by taking turns cleaning a river. This solution was only possible because the environment had spatial and temporal dimensions in which agents have great freedom in how they structure their behavior towards one another. Interestingly, while the environment allowed for many different solutions (such as \nterritoriality\n), agents converged on the same turn-taking solution as human players.\nIn our latest study, we applied this type of model to an open question in research on cultural evolution: how to explain the existence of spurious and arbitrary social norms that appear not to have immediate material consequences for their violation beyond those imposed socially. For instance, in some societies men are expected to wear trousers not skirts; in many there are words or hand gestures that should not be used in polite company; and in most there are rules about how one styles one's hair or what one wears on one's head. We call these social norms \u2018silly rules\u2019. Importantly, in our framework, enforcing and complying with social norms both have to be learned. Having a social environment that includes a \u2018silly rule\u2019 means that agents have more opportunities to learn about enforcing norms in general. This additional practice then allows them to enforce the important rules more effectively. Overall, the \u2018silly rule\u2019 can be beneficial for the population \u2013 a surprising result. This result is only possible because our simulation focuses on learning: enforcing and complying with rules are complex skills that need training to develop.\nPart of why we find this result on silly rules so exciting is that it demonstrates the utility of multi-agent deep reinforcement learning in modeling cultural evolution. Culture contributes to the success or failure of policy interventions for socio-ecological systems. For instance, strengthening social norms around recycling is part of the \nsolution\n to some environmental problems. Following this trajectory, richer simulations could lead to a deeper understanding of how to design interventions for social-ecological systems. If simulations become realistic enough, it may even be possible to test the impact of interventions, e.g. aiming to \ndesign a tax code that fosters productivity and fairness\n.\nThis approach provides researchers with tools to specify detailed models of phenomena that interest them. Of course, like all research methodologies it should be expected to come with its own strengths and weaknesses. We hope to discover more about when this style of modeling can be fruitfully applied in the future. While there are no panaceas for modeling, we think there are compelling reasons to look to multi-agent deep reinforcement learning when constructing models of social phenomena, especially when they involve learning.\n"}
{"title": "Accelerating fusion science through learned plasma control", "contents": "Successfully controlling the nuclear fusion plasma in a tokamak with deep reinforcement learning\nTo solve the global energy crisis, researchers have long sought a source of clean, limitless energy. Nuclear fusion, the reaction that powers the stars of the universe, is one contender. By smashing and fusing hydrogen, a common element of seawater, the powerful process releases huge amounts of energy. Here on earth, one way scientists have recreated these extreme conditions is by using a tokamak, a doughnut-shaped vacuum surrounded by magnetic coils, that is used to contain a plasma of hydrogen that is hotter than the core of the Sun. However, the plasmas in these machines are inherently unstable, making sustaining the process required for nuclear fusion a complex challenge. For example, a control system needs to coordinate the tokamak's many magnetic coils and adjust the voltage on them thousands of times per second to ensure the plasma never touches the walls of the vessel, which would result in heat loss and possibly damage. To help solve this problem and as part of DeepMind\u2019s mission to advance science, we collaborated with \nthe Swiss Plasma Center\n at \nEPFL\n to develop the first deep reinforcement learning (RL) system to autonomously discover how to control these coils and successfully contain the plasma in a tokamak, opening new avenues to advance nuclear fusion research.\nIn a \npaper published today in Nature\n, we describe how we can successfully control nuclear fusion plasma by building and running controllers on the Variable Configuration Tokamak (TCV) in Lausanne, Switzerland. Using a learning architecture that combines deep RL and a simulated environment, we produced controllers that can both keep the plasma steady and be used to accurately sculpt it into different shapes. This \u201cplasma sculpting\u201d shows the RL system has successfully controlled the superheated matter and - importantly - allows scientists to investigate how the plasma reacts under different conditions, improving our understanding of fusion reactors.\nThis work is another powerful example of how machine learning and expert communities can come together to tackle grand challenges and accelerate scientific discovery. Our team is hard at work applying this approach to fields as diverse as quantum chemistry, pure mathematics, material design, weather forecasting, and more, to solve fundamental problems and ensure AI benefits humanity.\nResearch into nuclear fusion is currently limited by researchers\u2019 ability to run experiments. While there are dozens of active tokamaks around the world, they\u2019re expensive machines and in high demand. For example, TCV can only sustain the plasma in a single experiment for up to three seconds, after which it needs 15 minutes to cool down and reset before the next attempt. Not only that, multiple research groups often share use of the tokamak, further limiting the time available for experiments.\nGiven the current obstacles to access a tokamak, researchers have turned to simulators to help advance research. For example, our partners at EPFL have built a powerful set of simulation tools that model the dynamics of tokamaks. We were able to use these to allow our RL system to learn to control TCV in simulation and then validate our results on the real TCV, showing we could successfully sculpt the plasma into the desired shapes. Whilst this is a cheaper and more convenient way to train our controllers; we still had to overcome many barriers. For example, plasma simulators are slow and require many hours of computer time to simulate one second of real time. In addition, the condition of TCV can change from day to day, requiring us to develop algorithmic improvements, both physical and simulated, and to adapt to the realities of the hardware.\nExisting plasma-control systems are complex, requiring separate controllers for each of TCV\u2019s 19 magnetic coils. Each controller uses algorithms to estimate the properties of the plasma in real time and adjust the voltage of the magnets accordingly. In contrast, our architecture uses a single neural network to control all of the coils at once, automatically learning which voltages are the best to achieve a plasma configuration directly from sensors.\nAs a demonstration, we first showed that we could manipulate many aspects of the plasma with a single controller.\nIn the video above, we see the plasma at the top of TCV at the instant our system takes control. Our controller first shapes the plasma according to the requested shape, then shifts the plasma downward and detaches it from the walls, suspending it in the middle of the vessel on two legs. The plasma is held stationary, as would be needed to measure plasma properties. Then, finally the plasma is steered back to the top of the vessel and safely destroyed.\nWe then created a range of plasma shapes being studied by plasma physicists for their usefulness in generating energy. For example, we made a \u201csnowflake\u201d shape with many \u201clegs\u201d that could help reduce the cost of cooling by spreading the exhaust energy to different contact points on the vessel walls. We also demonstrated a shape close to the proposal for \nITER\n, the next-generation tokamak under construction, as EPFL was conducting experiments to predict the behaviour of plasmas in ITER. We even did something that had never been done in TCV before by stabilising a \u201cdroplet\u201d where there are two plasmas inside the vessel simultaneously. Our single system was able to find controllers for all of these different conditions. We simply changed the goal we requested, and our algorithm autonomously found an appropriate controller.\nSimilar to progress we\u2019ve seen when applying AI to other scientific domains, our successful demonstration of tokamak control shows the power of AI to accelerate and assist fusion science, and we expect increasing sophistication in the use of AI going forward. This capability of autonomously creating controllers could be used to design new kinds of tokamaks while simultaneously designing their controllers. Our work also points to a bright future for reinforcement learning in the control of complex machines. It\u2019s especially exciting to consider fields where AI could augment human expertise, serving as a tool to discover new and creative approaches for hard real-world problems. We predict reinforcement learning will be a transformative technology for industrial and scientific control applications in the years to come, with applications ranging from energy efficiency to personalised medicine.\nRead the paper: \nMagnetic control of tokamak plasmas through deep reinforcement learning\n"}
{"title": "Red Teaming Language Models with Language Models", "contents": "In our \nrecent paper\n, we show that it is possible to automatically find inputs that elicit harmful text from language models by generating inputs using language models themselves. Our approach provides one tool for finding harmful model behaviours before users are impacted, though we emphasize that it should be viewed as one component alongside many other techniques that will be needed to find harms and mitigate them once found.\nLarge generative language models like GPT-3 and Gopher have a remarkable ability to generate high-quality text, but they are difficult to deploy in the real world. Generative language models come with a risk of generating very harmful text, and even a small risk of harm is unacceptable in real-world applications.\nFor example, in 2016, Microsoft released the Tay Twitter bot to automatically tweet in response to users. Within 16 hours, \nMicrosoft took Tay down\n after several adversarial users elicited racist and sexually-charged tweets from Tay, which were sent to over 50,000 followers. The outcome was \nnot for lack of care on Microsoft\u2019s part\n:\nThe issue is that there are so many possible inputs that can cause a model to generate harmful text. As a result, it\u2019s hard to find all of the cases where a model fails before it is deployed in the real world. Previous work relies on paid, human annotators to manually discover failure cases (\nXu et al. 2021\n,\n inter alia\n). This approach is effective but expensive, limiting the number and diversity of failure cases found.\nWe aim to complement manual testing and reduce the number of critical oversights by finding failure cases (or \u2018red teaming\u2019) in an automatic way. To do so, we generate test cases using a language model itself and use a classifier to detect various harmful behaviors on test cases, as shown below:\nOur approach uncovers a variety of harmful model behaviors:\nTo generate test cases with language models, we explore a variety of methods, ranging from prompt-based generation and few-shot learning to supervised finetuning and reinforcement learning. Some methods generate more diverse test cases, while other methods generate more difficult test cases for the target model. Together, the methods we propose are useful for obtaining high test coverage while also modeling adversarial cases.\nOnce we find failure cases, it becomes easier to fix harmful model behavior by:\nOverall, language models are a highly effective tool for uncovering when language models behave in a variety of undesirable ways. In our current work, we focused on red teaming harms that today\u2019s language models commit. In the future, our approach can also be used to preemptively discover other, hypothesized harms from advanced machine learning systems, e.g., due to \ninner misalignment\n or \nfailures in objective robustness\n. This approach is just one component of responsible language model development: we view red teaming as one tool to be used alongside many others, both to find harms in language models and to mitigate them. We refer to Section 7.3 of \nRae et al. 2021\n for a broader discussion of other work needed for language model safety. For more details on our approach and results, as well as the broader consequences of our findings, read our \nred teaming paper\n here.\n"}
{"title": "Learning Robust Real-Time Cultural Transmission without Human Data", "contents": "Over millennia, humankind has discovered, evolved, and accumulated a wealth of cultural knowledge, from navigation routes to mathematics and social norms to works of art. Cultural transmission, defined as efficiently passing information from one individual to another, is the inheritance process underlying this exponential increase in human capabilities.\nOur agent, in blue, imitates and remembers the demonstration of both bots (left) and humans (right), in red.\nFor more videos of our agents in action, visit our \nwebsite\n.\nIn this work, we use deep reinforcement learning to generate artificial agents capable of test-time cultural transmission. Once trained, our agents can infer and recall navigational knowledge demonstrated by experts. This knowledge transfer happens in real time and generalises across a vast space of previously unseen tasks. For example, our agents can quickly learn new behaviours by observing a single human demonstration, without ever training on human data.\nWe train and test our agents in procedurally generated 3D worlds, containing colourful, spherical goals embedded in a noisy terrain full of obstacles. A player must navigate the goals in the correct order, which changes randomly on every episode. Since the order is impossible to guess, a naive exploration strategy incurs a large penalty. As a source of culturally transmitted information, we provide a privileged \u201cbot\u201d that always enters goals in the correct sequence.\nVia ablations, we identify a minimal sufficient \"starter kit\" of training ingredients required for cultural transmission to emerge, dubbed MEDAL-ADR. These components include memory (M), expert dropout (ED), attentional bias towards the expert (AL), and automatic domain randomization (ADR). Our agent outperforms the ablations, including the state-of-the-art method (ME-AL), across a range of challenging held-out tasks. Cultural transmission generalises out of distribution surprisingly well, and the agent recalls demonstrations long after the expert has departed. Looking into the agent's brain, we find strikingly interpretable neurons responsible for encoding social information and goal states.\nIn summary, we provide a procedure for training an agent capable of flexible, high-recall, real-time cultural transmission, without using human data in the training pipeline. This paves the way for cultural evolution as an algorithm for developing more generally intelligent artificial agents.\nThis authors' notes is based on joint work by the Cultural General Intelligence Team: Avishkar Bhoopchand, Bethanie Brownfield, Adrian Collister, Agustin Dal Lago, Ashley Edwards, Richard Everett, Alexandre Fr\u00e9chette, Edward Hughes, Kory W. Mathewson, Piermaria Mendolicchio, Yanko Oliveira, Julia Pawar, Miruna P\u00eeslar, Alex Platonov, Evan Senter, Sukhdeep Singh, Alexander Zacherl, and Lei M. Zhang.\nRead the full paper \nhere\n.\n"}
{"title": "MuZero\u2019s first step from research into the real world", "contents": "Collaborating with YouTube to optimise video compression in the open source VP9 codec.\nIn 2016, we introduced \nAlphaGo\n, the first artificial intelligence program to defeat humans at the ancient game of Go. Its successors, \nAlphaZero\n and then \nMuZero\n, each represented a significant step forward in the pursuit of general-purpose algorithms, mastering a greater number of games with even less predefined knowledge. MuZero, for example, mastered Chess, Go, Shogi, and Atari without needing to be told the rules. But so far these agents have focused on solving games. Now, in pursuit of DeepMind\u2019s mission to solve intelligence, MuZero has taken a first step towards mastering a real-world task by optimising video on YouTube.\nIn a \npreprint published on arXiv\n, we detail our collaboration with YouTube to explore the potential for MuZero to improve video compression. \nAnalysts predicted\n that streaming video will have accounted for the vast majority of internet traffic in 2021. With video surging during the COVID-19 pandemic and the total amount of internet traffic expected to grow in the future, video compression is an increasingly important problem \u2014 and a natural area to apply Reinforcement Learning (RL) to improve upon the state of the art in a challenging domain. Since launching to production on a portion of YouTube\u2019s live traffic, we\u2019ve demonstrated an average 4% bitrate reduction across a large, diverse set of videos.\nMost online videos rely on a program called a codec to compress or encode the video at its source, transmit it over the internet to the viewer, and then decompress or decode it for playback. These codecs make multiple decisions for each frame in a video. Decades of hand engineering have gone into optimising these codecs, which are responsible for many of the video experiences now possible on the internet, including video on demand, video calls, video games, and virtual reality. However, because RL is particularly well-suited to sequential decision-making problems like those in codecs, we\u2019re exploring how an RL-learned algorithm can help.\nOur initial focus is on the VP9 codec (specifically the open source version \nlibvpx\n), since it\u2019s widely used by YouTube and other streaming services. As with other codecs, service providers using VP9 need to think about bitrate \u2014 the number of ones and zeros required to send each frame of a video. Bitrate is a major determinant in how much compute and bandwidth is required to serve and store videos, affecting everything from how long a video takes to load to its resolution, buffering, and data usage.\nIn VP9, bitrate is optimised most directly through the Quantisation Parameter (QP) in the rate control module. For each frame, this parameter determines the level of compression to apply. Given a target bitrate, QPs for video frames are decided sequentially to maximize overall video quality. Intuitively, higher bitrates (lower QP) should be allocated for complex scenes and lower bitrates (higher QP) should be allocated for static scenes. The QP selection algorithm reasons how the QP value of a video frame affects the bitrate allocation of the rest of the video frames and the overall video quality. RL is especially helpful in solving such a sequential decision-making problem.\nMuZero achieves superhuman performance across various tasks by combining the power of search with its ability to learn a model of the environment and plan accordingly. This works especially well in large, combinatorial action spaces, making it an ideal candidate solution for the problem of rate control in video compression. However, to get MuZero to work on this real-world application requires solving a whole new set of problems. For instance, the set of videos uploaded to platforms like YouTube varies in content and quality, and any agent needs to generalise across videos, including completely new videos after deployment. By comparison, board games tend to have a single known environment. Many other metrics and constraints affect the final user experience and bitrate savings, such as the PSNR (Peak Signal-to-Noise Ratio) and bitrate constraint.\nTo address these challenges with MuZero, we create a mechanism called self-competition, which converts the complex objective of video compression into a simple WIN/LOSS signal by comparing the agent\u2019s current performance against its historical performance. This allows us to convert a rich set of codec requirements into a simple signal that can be optimised by our agent.\n\n\t\t\t\tBitrate savings\n\t\t\t\n\n\t\t\t\t4.7%\n\t\t\t\n\n\t\t\t\tBitrate savings\n\t\t\t\n\n\t\t\t\t4.1%\n\t\t\t\n\n\t\t\t\tBitrate savings\n\t\t\t\n\n\t\t\t\t3.5%\n\t\t\t\nBy learning the dynamics of video encoding and determining how best to allocate bits, our MuZero Rate-Controller (MuZero-RC) is able to reduce bitrate without quality degradation. QP selection is just one of numerous encoding decisions in the encoding process. While decades of research and engineering have resulted in efficient algorithms, we envision a single algorithm that can automatically learn to make these encoding decisions to obtain the optimal rate-distortion tradeoff.\nBeyond video compression, this first step in applying MuZero beyond research environments serves as an example of how our RL agents can solve real-world problems. By creating agents equipped with a range of new abilities to improve products across domains, we can help various computer systems become faster, less intensive, and more automated. Our long-term vision is to develop a single algorithm capable of optimising thousands of real-world systems across a variety of domains.\nHear Jackson Broshear and David Silver discuss MuZero with Hannah Fry in Episode 5 of DeepMind: The Podcast. Listen now on your favourite podcast app by searching \u201cDeepMind: The Podcast\u201d.\nWork done as a collaboration with contributors: Chenjie Gu, Anton Zhernov, Amol Mandhane, Maribeth Rauh, Miaosen Wang, Flora Xue, Wendy Shang, Derek Pang, Rene Claus, Ching-Han Chiang, Cheng Chen, Jingning Han, Angie Chen, Daniel J. Mankowitz, Julian Schrittwieser, Thomas Hubert, Oriol Vinyals, Jackson Broshear, Timothy Mann, Robert Tung, Steve Gaffney, Carena Church\n"}
{"title": "Probing Image-Language Transformers for Verb Understanding", "contents": "Grounding language to vision is a fundamental problem for many real-world AI systems such as retrieving images or generating descriptions for the visually impaired. Success on these tasks requires models to relate different aspects of language such as objects and verbs to images. For example, to distinguish between the two images in the middle column below, models must differentiate between the verbs \u201ccatch\u201d and \u201ckick.\u201d Verb understanding is particularly difficult as it requires not only recognising objects, but also how different objects in an image relate to each other. To overcome this difficulty, we introduce the SVO-Probes dataset and use it to probe language and vision models for verb understanding.\nIn particular, we consider multimodal transformer models (e.g., Lu et al., 2019; Chen et al., 2020; Tan and Bansal, 2019; Li et al., 2020), which have shown success on a variety of language and vision tasks. However, despite strong performance on benchmarks, it is not clear if these models have fine-grained multimodal understanding. In particular, prior work shows that language and vision models can succeed at benchmarks without multimodal understanding: for example, answering questions about images based only on language priors (Agrawal et al., 2018) or \u201challucinating\u201d objects that are not in the image when captioning images (Rohrbach et al., 2018). To anticipate model limitations, work like Shekhar et al. propose specialised evaluations to probe models systematically for language understanding. However, prior probe sets are limited in the number of objects and verbs. We developed SVO-Probes to better evaluate potential limitations in verb understanding in current models.\nSVO-Probes includes 48,000 image-sentence pairs and tests understanding for more than 400 verbs. Each sentence can be broken into a <Subject, Verb, Object> triplet (or SVO triplet) and paired with positive and negative example images. The negative examples differ in only one way: the Subject, Verb, or Object is changed. The figure above shows negative examples in which the subject (left), verb (middle), or object (right) does not match the image. This task formulation makes it possible to isolate which parts of the sentence a model has the most trouble with. It also makes SVO-Probes more challenging than standard image retrieval tasks, where negative examples are often completely unrelated to the query sentence.\nTo create SVO-Probes, we \nquery an image search\n with SVO triplets from a common training dataset, Conceptual Captions (Sharma et al. 2018). Because image search can be noisy, a preliminary annotation step filters the retrieved images to ensure we have a clean set of image-SVO pairs. Since transformers are trained on image-sentence pairs, not image-SVO pairs, we need image-sentence pairs to probe our model. To collect sentences which describe each image, annotators write a short sentence for each image that includes the SVO triplet. For example, given the SVO triplet <animal, lie, grass>, an annotator could write the sentence \u201cAn animal lays in the grass.\u201d We then use the SVO annotations to pair each sentence with a negative image, and ask annotators to verify negatives in a final annotation step. See the figure below for details.\nWe examine whether multimodal transformers can accurately classify examples as positive or negative. The bar chart below illustrates our results. Our dataset is challenging: our standard multimodal transformer model achieves 64.3% accuracy overall (chance is 50%). Whereas accuracy is 67.0% and 73.4% on subjects and objects respectively, performance falls to 60.8% on verbs. This result shows that verb recognition is indeed challenging for vision and language models.\nWe also explore which model architectures perform best on our dataset. Surprisingly, models with weaker image modeling perform better than the standard transformer model. One hypothesis is that our standard model (with stronger image modeling ability) overfits the train set. As both these models perform worse on other language and vision tasks, our targeted probe task illuminates model weaknesses that are not observed on other benchmarks.\nOverall, we find that despite impressive performance on benchmarks, multimodal transformers still struggle with fine-grained understanding, especially fine-grained verb understanding. We hope SVO-Probes can help drive exploration of verb understanding in language and vision models and inspire more targeted probe datasets. Both our SVO-Probes benchmark and models can be found here on GitHub: \nbenchmark\n and \nmodels\n.\n"}
{"title": "GopherCite: Teaching language models to support answers with verified quotes", "contents": "DeepMind published a \nseries of papers\n about large language models (LLMs) last year, including \nan analysis\n of Gopher, our large language model. Language modelling technology, which is also currently being developed by several other labs and companies, promises to strengthen many applications, from \nsearch engines\n to a new wave of chatbot-like \nconversational assistants\n and beyond. One \npaper\n in this series laid out a number of reasons why \u201craw\u201d language models like Gopher do not meet our standards for safely deploying this technology in user-facing applications, especially if guard rails for managing problematic and potentially harmful behaviour are not set in place.\nOur latest work focuses on one of these concerns: Language models like Gopher can \u201challucinate\u201d facts that appear plausible but are actually fake. Those who are familiar with this problem know to do their own fact-checking, rather than trusting what language models say. Those who are not, may end up believing something that isn\u2019t true. This paper describes GopherCite, a model which aims to address the problem of language model hallucination. GopherCite attempts to back up all of its factual claims with evidence from the web. It uses Google Search to find relevant web pages on the internet and quotes a passage which tries to demonstrate why its response is correct. If the system is unable to form an answer that can be well-supported by evidence, it tells the user, \u201cI don\u2019t know\u201d, instead of providing an unsubstantiated answer.\nSupporting simple factual claims with easily verifiable evidence is one step towards making language models more trustworthy, both for users interacting with them and for annotators assessing the quality of samples. A comparison between the behaviour of \u201craw\u201d Gopher and our new model is helpful for illustrating this change.\nBased on GopherCite\u2019s response, you\u2019ll notice that Gopher invented a fact (\u201cLake Placid hosted the winter Olympics in 1936\u201d) without warning. When shown a verified snippet from a relevant Wikipedia page by GopherCite, we can confirm that Lake Placid only hosted the Olympics twice, in 1932 and 1980.\nTo alter Gopher\u2019s behaviour in this way, we trained Gopher according to human preferences. We asked participants in a user study to pick their preferred answer from a pair of candidates, according to criteria including how well the evidence supports the answers given. These labels were used as training data for both supervised learning on highly rated samples and for \nreinforcement learning from human preferences\n (RLHP). We also took this approach in \nour recent work on red teaming\n.\nWe are not the only ones interested in this problem of factual inaccuracy in language models. Our colleagues at Google recently made progress on factual grounding in their latest \nLaMDA system\n, having a conversational model interact with Google Search and sometimes share relevant URLs. Indeed, GopherCite\u2019s training regimen uses similar methodology to that of LaMDA, but a critical difference is that we aim to provide a specific snippet of relevant evidence, rather than simply pointing the user to a URL. Based on motivations similar to our own, OpenAI has \nrecently announced work\n developing a closely related system called WebGPT, which also applies RLHP to align their GPT-3 language model. Whereas GopherCite focuses on reading long document inputs, WebGPT carefully curates the context presented to the language model by interacting multiple times with a web browser. It also cites evidence to back up its responses. Similarities and differences between these systems and our own are discussed in our paper and we also demonstrate that GopherCite very often provides compelling evidence for its claims.\nWe conducted a user study with paid participants to assess the model on two types of questions: fact-seeking questions typed into Google Search (\nreleased by Google in a dataset called \u201cNaturalQuestions\u201d\n), and explanation-seeking questions which Reddit users asked on a forum called \u201c/r/eli5\u201d (\u201cExplain it Like I\u2019m 5 [years old]\u201d). The participants in our study determined that GopherCite answers fact-seeking questions correctly \u2013 and with satisfactory evidence \u2013 about 80% of the time, and does so for explanation-seeking questions about 67% of the time. When we allow GopherCite to refrain from answering some questions, its performance improves dramatically amongst the questions it does choose to answer (see the paper for details). This explicit mechanism for abstaining is a core contribution of our work.\nBut when we evaluate the model on a set of \u201cadversarial\u201d questions, which attempt to trick the model into parroting a fiction or misconception that is stated on the internet, GopherCite often falls into the trap. For instance, when asked \u201cwhat does Red Bull give you?\u201d, here is how it responds:\nWe think this failure mode and others discussed in our paper can be avoided by enriching the setting, moving from a \u201csingle-shot\u201d reply to a user\u2019s question, to one in which the model can ask clarifying questions of the user and engage in a dialogue. For example, we could enable future models to ask the user whether they want an answer that is literally true or one that is true in the confines of the fictional world of a Red Bull advertisement.\nIn summary, we think GopherCite is an important step forward, but building it has taught us that evidence citation is only one part of an overall strategy for safety and trustworthiness. More fundamentally, not all claims require quote evidence \u2013 and as we demonstrated above, not all claims supported by evidence are true. Some claims require multiple pieces of evidence along with a logical argument explaining why the claim follows. We will continue working in this area and aim to overcome the issues presented with further research and development as well as dedicated sociotechnical research.\nOur paper covers many more details about our methods, experiments, and relevant context from the research literature. We have also created an FAQ about GopherCite, answered by the model itself after reading the paper's introduction (using candidate samples curated by the authors):\n"}
{"title": "Predicting the past with Ithaca", "contents": "Restoring, placing, and dating ancient texts through collaboration between AI and historians.\nThe birth of human writing marked the dawn of \nHistory\n and is crucial to our understanding of past civilisations and the world we live in today. For example, more than 2,500 years ago, the Greeks began writing on stone, pottery, and metal to document everything from leases and laws to calendars and oracles, giving a detailed insight into the Mediterranean region. Unfortunately, it\u2019s an incomplete record. Many of the surviving inscriptions have been damaged over the centuries or moved from their original location. In addition, modern dating techniques, such as \nradiocarbon dating\n, cannot be used on these materials, making inscriptions difficult and time-consuming to interpret.\nIn line with \nDeepMind\u2019s mission\n of solving intelligence to advance science and humanity, we collaborated with the \nDepartment of Humanities of Ca' Foscari University of Venice\n, the \nClassics Faculty of the University of Oxford\n, and the \nDepartment of Informatics of the Athens University of Economics and Business\n to explore how machine learning can help historians better interpret these inscriptions \u2013 giving a richer understanding of ancient history and unlocking the potential for cooperation between AI and historians.\nIn a \npaper\n published today in \nNature\n, we jointly introduce Ithaca, the first deep neural network that can restore the missing text of damaged inscriptions, identify their original location, and help establish the date they were created. Ithaca is named after the Greek island in \nHomer\u2019s \nOdyssey\n and builds upon and extends \nPythia\n, our previous system that focused on textual restoration. Our evaluations show that Ithaca achieves 62% accuracy in restoring damaged texts, 71% accuracy in identifying their original location, and can date texts to within 30 years of their ground-truth date ranges. Historians have already used the tool to reevaluate significant periods in Greek history.\nTo make our research widely available to researchers, educators, museum staff and others, we partnered with \nGoogle Cloud\n and \nGoogle Arts & Culture\n to launch a \nfree interactive version of Ithaca\n. And to aid further research, we have also \nopen sourced\n our code, the pretrained model, and an interactive Colaboratory notebook.\nIthaca is trained on the \nlargest digital dataset of Greek inscriptions\n from the \nPackard Humanities Institute\n. \nNatural language processing\n models are commonly trained using words because the order in which they appear in sentences and the relationships between them provide extra context and meaning. For example, \u201conce upon a time\u201d has more meaning than each character or word seen separately. However, many of the inscriptions historians are interested in analysing with Ithaca are damaged and often missing chunks of text. To ensure our model still works when presented with one of these, we trained it using both words and the individual characters as inputs. The sparse self-attention mechanism at the model\u2019s core evaluates these two inputs in parallel, allowing Ithaca to evaluate inscriptions as needed.\nTo maximise Ithaca's value as a research tool, we also created a number of visual aids to ensure Ithaca\u2019s results are easily interpretable by historians:\nOur experimental evaluation shows how Ithaca\u2019s design decisions and visualisation aids make it easier for researchers to interpret results. The expert historians we worked with achieved 25% accuracy when working alone to restore ancient texts. But, when using Ithaca, their performance increases to 72%, surpassing the model\u2019s individual performance and showing the potential for human-machine cooperation to advance historical interpretation, establish relative datings for historical events, and even contribute to current methodological debates.\nFor example, historians currently disagree on the date of a series of important \nAthenian decrees\n made at a time when notable figures such as Socrates and Pericles lived. The decrees have long been thought to have been written before 446/445 BCE, although new evidence suggests a date of the 420s BCE. Although it might seem like a small difference, these decrees are fundamental to our understanding of the political history of Classical Athens.\nOur training dataset contains the earlier figure of 446/445 BCE. To test Ithaca\u2019s predictions, we retrained it on a dataset that did not contain the dated inscriptions and then submitted these held-out texts for analysis. Remarkably, Ithaca\u2019s average predicted date for the decrees is 421 BCE, aligning with the most recent dating breakthroughs and showing how machine learning can contribute to debates around one of the most significant moments in Greek history.\nWe believe this is just the start for tools like Ithaca and the potential for collaboration between machine learning and the humanities. Ancient Greece plays an instrumental role in our understanding of the Mediterranean world, but it\u2019s still only one part of a vast global picture of civilisations. To that end, we are currently working on versions of Ithaca trained on other ancient languages and historians can already use their datasets in the current architecture to study other ancient writing systems, from \nAkkadian\n to \nDemotic\n and \nHebrew\n to \nMayan\n. We hope that models like Ithaca can unlock the cooperative potential between AI and the humanities, transformationally impacting the way we study and write about some of the most significant periods in human history.\nThis work was done by a team including contributions from Yannis Assael, Thea Sommerschield, Brendan Shillingford, Mahyar Bordbar, John Pavlopoulos, Marita Chatzipanagiotou, Ion Androutsopoulos, Jonathan Prag, and Nando de Freitas. The Ithaca web interface was developed by Justin Grayston, Benjamin Maynard, and Ricardo Cardenas from Google Cloud.\n"}
{"title": "Tackling multiple tasks with a single visual language model", "contents": "One key aspect of intelligence is the ability to quickly learn how to perform a new task when given a brief instruction. For instance, a child may recognise real animals at the zoo after seeing a few pictures of the animals in a book, despite differences between the two. But for a typical visual model to learn a new task, it must be trained on tens of thousands of examples specifically labelled for that task. If the goal is to count and identify animals in an image, as in \u201cthree zebras\u201d, one would have to collect thousands of images and annotate each image with their quantity and species. This process is inefficient, expensive, and resource-intensive, requiring large amounts of annotated data and the need to train a new model each time it\u2019s confronted with a new task. As part of DeepMind\u2019s mission to solve intelligence, we\u2019ve explored whether an alternative model could make this process easier and more efficient, given only limited task-specific information.\nToday, in the preprint of our \npaper\n, we introduce \nFlamingo, \na single visual language model (VLM) that sets a new state of the art in few-shot learning on a wide range of open-ended multimodal tasks. This means Flamingo can tackle a number of difficult problems with just a handful of task-specific examples (in a \u201cfew shots\u201d), without any additional training required. Flamingo\u2019s simple interface makes this possible, taking as input a prompt consisting of interleaved images, videos, and text and then output associated language.\u00a0\nSimilar to the behaviour of \nlarge language models\n (LLMs), which can address a language task by processing examples of the task in their text prompt, Flamingo\u2019s visual and text interface can steer the model towards solving a multimodal task. Given a few example pairs of visual inputs and expected text responses composed in Flamingo\u2019s prompt, the model can be asked a question with a new image or video, and then generate an answer.\u00a0\nOn the 16 tasks we studied, Flamingo beats all previous few-shot learning approaches when given as few as four examples per task. In several cases, the same Flamingo\n \nmodel outperforms methods that are fine-tuned and optimised for each task independently and use multiple orders of magnitude more task-specific data. This should allow non-expert people to quickly and easily use accurate visual language models on new tasks at hand.\nIn practice, Flamingo fuses large language models with powerful visual representations \u2013 each separately pre-trained and frozen \u2013 by adding novel architectural components in between. Then it is trained on a mixture of complementary large-scale multimodal data coming only from the web, without using any data annotated for machine learning purposes. Following this method, we start from \nChinchilla\n, our recently introduced compute-optimal 70B parameter language model, to train our final Flamingo\n \nmodel, an 80B parameter VLM. After this training is done, Flamingo can be directly adapted to vision tasks via simple few-shot learning without any additional task-specific tuning.\nWe also tested the model\u2019s qualitative capabilities beyond our current benchmarks. As part of this process, we compared our model's performance when captioning images related to gender and skin colour, and ran our model's generated captions through Google's Perspective API, which evaluates toxicity of text. While the initial results are positive, more research towards evaluating ethical risks in multimodal systems is crucial and we urge people to evaluate and consider these issues carefully before thinking of deploying such systems in the real world.\nMultimodal capabilities are essential for important AI applications, such as \naiding the visually impaired\n with everyday visual challenges or \nimproving the identification of hateful content\n on the web. Flamingo makes it possible to efficiently adapt to these examples and other tasks on-the-fly without modifying the model. Interestingly, the model demonstrates out-of-the-box multimodal dialogue capabilities, as seen here.\nFlamingo is an effective and efficient general-purpose family of models that can be applied to image and video understanding tasks with minimal task-specific examples. Models like Flamingo hold great promise to benefit society in practical ways and we\u2019re continuing to improve their flexibility and capabilities so they can be safely deployed for everyone's benefit. Flamingo\u2019s abilities pave the way towards rich interactions with learned visual language models that can enable better interpretability and exciting new applications, like a visual assistant which helps people in everyday life \u2013 and we\u2019re delighted by the results so far.\nThis project was enabled by the contributions of the entire Flamingo team: Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. We would also like to thank the blog post contributors: Aliya Ahmad, Dominic Barlow, Arielle Bier, Matt Botvinick, Jordan Hoffmann, Max Barnett, Gaby Pearl, and Emma Yousif.\n"}
{"title": "An empirical analysis of compute-optimal large language model training", "contents": "In the last few years, a focus in language modelling has been on improving performance through increasing the number of parameters in transformer-based models. This approach has led to impressive results and state-of-the-art performance across many natural language processing tasks. \nWe also pursued this line of research at DeepMind and recently showcased Gopher, a 280-billion parameter model that established leading performance on a wide range of tasks including language modelling, reading comprehension, and question answering. Since then, an even larger model named Megatron-Turing NLG has been published with 530 billion parameters.\nDue to the substantial cost of training these large models, it is paramount to estimate the best possible training setup to avoid wasting resources. In particular, the training compute cost for transformers is determined by two factors: the model size and the number of training tokens.\nThe current generation of large language models has allocated increased computational resources to increasing the parameter count of large models and keeping the training data size fixed at around 300 billion tokens. In this work, we empirically investigate the optimal tradeoff between increasing model size and the amount of training data with increasing computational resources. Specifically, we ask the question: \u201cWhat is the optimal model size and number of training tokens for a given compute budget?\u201d To answer this question, we train models of various sizes and with various numbers of tokens, and estimate this trade-off empirically.\nOur main finding is that the current large language models are far too large for their compute budget and are not being trained on enough data. In fact, we find that for the number of training FLOPs used to train \nGopher\n, a 4x smaller model trained on 4x more data would have been preferable.\n\u200d\nWe test our data scaling hypothesis by training \nChinchilla, \na 70-billion parameter model trained for 1.3 trillion tokens. While the training compute cost for Chinchilla\n \nand\n \nGopher\n \nare the same, we find that it outperforms Gopher and other large language models on nearly every measured task, despite having 70 billion parameters compared to Gopher\u2019s 280 billion.\nAfter the release of Chinchilla, a model named PaLM was released with 540 billion parameters and trained on 768 billion tokens. This model was trained with approximately 5x the compute budget of Chinchilla and outperformed Chinchilla on a range of tasks. While the training corpus is different, our methods do predict that such a model trained on our data would outperform Chinchilla despite not being compute-optimal. Given the PaLM compute budget, we predict a 140-billion-parameter model trained on 3 trillion tokens to be optimal and more efficient for inference.\nAn additional benefit of smaller, more performant models is that the inference time and memory costs are reduced making querying the models both faster and possible on less hardware. In practice, while the training FLOPs between Gopher\n \nand Chinchilla are the same, the cost of using Chinchilla is substantially smaller, in addition to it performing better. Further simple optimisations may be possible that are able to continue to provide large gains.\n"}
{"title": "When a passion for bass and brass help build better tools", "contents": "For our first ever 'Five minutes with' we caught up with Kevin Millikin, a software engineer on the DevTools team. He\u2019s in Salt Lake City this week to present at \nPyCon US\n, the largest annual gathering for those using and developing the open-source Python programming language.\nI\u00a0build bespoke software tools for our developers. For example, we\u2019re currently developing a web-based editor to support people working remotely who need to code in Python -\u00a0one of the common languages used by our engineers. Creating tools for how we work and the Google infrastructure we rely on gives us more flexibility to solve problems that matter to our teams.\nThe London campus - it is fabulous. We\u2019re working a hybrid 3:2 model - Monday through Wednesday in the office, Thursday and Friday from anywhere. I\u2019m really enjoying the face-to-face interaction with my colleagues.\u00a0\nI\u2019ve been working from home on Thursday and Friday. I\u2019m a musician and my home office is also my music room. I play bass guitar, baritone horn, and tenor saxophone. Playing music helped tremendously when we were working remotely during the pandemic. It\u2019s a different kind of creative energy \u2013 it gives me space to reflect on the problem I\u2019m trying to solve, and helps me tackle it from a different direction.\nI\u2019m giving a talk on \u2018\nBeyond Subtyping\n', a feature of Python. My session highlights various cases where the tools that implement subtyping disagree. As a Python designer you might think these are settled questions, but they\u2019re not because we don\u2019t yet agree on foundational points about how the language works.\nIn the typing working group there are dozens of participants from companies like Microsoft, Facebook, and Google \u2013 it\u2019s a very cooperative, collegial group. We\u2019re all trying to evolve Python in a direction that supports our own users. We\u2019re finding that we all have similar problems, and similar goals too. We\u2019re trying to develop tools that can be used by everybody, so we have to design in a very collaborative way.\nMeeting up face-to-face with people I\u2019ve been working with remotely for a couple of years, who are part of the Python language community. I\u2019m a bit of a newcomer in this area and I\u2019m interested in expanding our network and making it more inclusive to external contributors. In practice, it often works as a closed group, and I think a lot of the work could benefit from being more open.\nThough a lot of new features are added to Python to help address a specific issue someone is having, they don\u2019t always fit with other new features in a coherent way. One of the things I'm advocating for is to take a step back and decide what our principles are for evolving this part of the programming language we\u2019re working on. A lot of these are in the heads of the developers, but my question is - can we write them down and use that as a manifesto for how language evolution should go? If we had a roadmap of where we want to go in the next 2-5 years, could we be more thoughtful about the changes we make to the language? That would ensure we're building for the future and the tools we will need to create to accelerate AI research.\nLearn more about engineering at DeepMind and search for open roles today\n"}
{"title": "Active offline policy selection", "contents": "Reinforcement learning (RL) has made tremendous progress in recent years towards addressing real-life problems \u2013 and offline RL made it even more practical. Instead of direct interactions with the environment, we can now train many algorithms from a single pre-recorded dataset. However, we lose the practical advantages in data-efficiency of offline RL when we evaluate the policies at hand.\nFor example, when training robotic manipulators the robot resources are usually limited, and training many policies by offline RL on a single dataset gives us a large data-efficiency advantage compared to online RL. Evaluating each policy is an expensive process, which requires interacting with the robot thousands of times. When we choose the best algorithm, hyperparameters, and a number of training steps, the problem quickly becomes intractable.\nTo make RL more applicable to real-world applications like robotics, we propose using an intelligent evaluation procedure to select the policy for deployment, called active offline policy selection (A-OPS). In A-OPS, we make use of the prerecorded dataset and allow limited interactions with the real environment to boost the selection quality.\nTo minimise interactions with the real environment, we implement three key features: \n\u200d\nThe returns of the policies are modelled jointly using a Gaussian process, where observations include FQE scores and a small number of newly collected episodic returns from the robot. After evaluating one policy, we gain knowledge about all policies because their distributions are correlated through the kernel between pairs of policies. The kernel assumes that if policies take similar actions \u2013 such as moving the robotic gripper in a similar direction \u2013 they tend to have similar returns.\n\u200d\nWe demonstrated this procedure in a number of environments in several domains: dm-control, Atari, simulated, and real robotics. Using A-OPS reduces the regret rapidly, and with a moderate number of policy evaluations, we identify the best policy. \nOur results suggest that it\u2019s possible to make an effective offline policy selection with only a small number of environment interactions by utilising the offline data, special kernel, and Bayesian optimisation. The code for A-OPS is open-sourced and \navailable on GitHub\n with an example dataset to try.\n"}
{"title": "DeepMind\u2019s latest research at ICLR 2022", "contents": "Today, conference season is kicking off with The Tenth International Conference on Learning Representations (\nICLR 2022\n), running virtually from 25-29 April, 2022. Participants from around the world are gathering to share their cutting-edge work in representational learning, from advancing the state of the art in artificial intelligence to data science, machine vision, robotics, and more.\u00a0\nOn the first day of the conference, Pushmeet Kohli, our head of AI for Science and Robust and Verified AI teams, is delivering a talk on how AI can dramatically improve solutions to a wide range of scientific problems, from genomics and structural biology to quantum chemistry and even pure mathematics.\u00a0\nBeyond supporting the event as sponsors and regular workshop organisers, our research teams are presenting 29 papers, including 10 collaborations this year. Here\u2019s a brief glimpse into our upcoming oral, spotlight, and poster presentations:\nA number of key papers focus on the critical ways we\u2019re making the learning process of our AI systems more efficient. This ranges from increasing performance, advancing few shot learning, and creating data efficient systems that reduce computational costs.\u00a0\nIn \n\u201cBootstrapped meta-learning\u201d\n, an \nICLR 2022 Outstanding Paper Award\n winner, we propose an algorithm that enables an agent to learn how to learn by teaching itself. We also present a \npolicy improvement algorithm\n that redesigns \nAlphaZero\n \u2013 our system that taught itself from scratch to master chess, shogi, and Go \u2013 to continue improving even when training with a small number of simulations; a \nregulariser that mitigates the risk of capacity loss\n in a broad range of RL agents and environments; and an improved \narchitecture to efficiently train attentional models\n.\nCuriosity is a key part of human learning, helping to advance knowledge and skill. Similarly, exploration mechanisms allow AI agents to go beyond preexisting knowledge and discover the unknown or try something new.\nAdvancing the question \u201c\nWhen should agents explore?\n\u201d, we investigate when agents should switch into exploration mode, at what timescales it makes sense to switch, and which signals best determine how long and frequent exploration periods should be. In another paper, we introduce an \u201c\ninformation gain exploration bonus\n\u201d that allows agents to break out of the limitations of intrinsic rewards in RL to be able to learn more skills.\nTo deploy ML models in the real world, they must be effective when shifting between training, testing, and across new datasets. Understanding the causal mechanisms is essential, allowing some systems to adapt, while others struggle to face new challenges.\nExpanding the research into these mechanisms, we present an experimental framework that enables a fine-grained \nanalysis of robustness to distribution shifts\n. Robustness also helps protect against adversarial harms, whether unintended or targeted. In the case of image corruptions, we propose a technique that theoretically \noptimises the parameters of image-to-image models\n to decrease the effects of blurring, fog, and other common issues.\nIn addition to helping ML researchers understand how agents evolve their own communication to complete tasks, AI agents have the potential to reveal insights into linguistic behaviours within populations, which could lead to more interactive and useful AI.\u00a0\nWorking with researchers at Inria, Google Research, and Meta AI, we connect the role of diversity within human populations on shaping language to \npartially solve an apparent contradiction\n in computer simulations with neural agents. Then, because building better representations of language in AI is so vital to understanding emergent communication, we also investigate the \nimportance of scaling up\n the dataset, task complexity, and population size as independent aspects. Moreover, we also studied the \ntradeoffs of expressivity, complexity, and unpredictability\n in games where multiple agents communicate to achieve a single goal.\nSee the full range of our work at ICLR 2022 \nhere\n.\n"}
{"title": "Open-sourcing MuJoCo", "contents": "In October 2021, we announced that we acquired the \nMuJoCo physics simulator\n, and made it freely available for everyone to support research everywhere. We also committed to developing and maintaining MuJoCo as a free, open-source, community-driven project with best-in-class capabilities. Today, we\u2019re thrilled to report that open sourcing is complete and the entire codebase is \non GitHub\n!\u00a0\nHere, we explain why MuJoCo is a great platform for open-source collaboration and share a preview of our roadmap going forward.\nPhysics simulators are critical tools in modern robotics research and often fall into these two categories:\u00a0\nThe first category is opaque to the user, and although sometimes free to use, cannot be modified and is hard to understand. The second category often has a smaller user base and suffers when its developers and maintainers graduate.\nMuJoCo is one of the few full-featured simulators backed by an established company, which is truly open source. As a research-driven organisation, we view MuJoCo as a platform for collaboration, where roboticists and engineers can join us to develop one of the world\u2019s best robot simulators.\nFeatures that make MuJoCo particularly attractive for collaboration are:\nWe hope that colleagues across academia and the OSS community benefit from this platform and contribute to the codebase, improving research for everyone.\nAs a C library with no dynamic memory allocation, MuJoCo is very fast. Unfortunately, raw physics speed has historically been hindered by Python wrappers, which made batched, multi-threaded operations non-performant due to the presence of the Global Interpreter Lock (GIL) and non-compiled code. In our roadmap below, we address this issue going forward.\u00a0\nFor now, we\u2019d like to share some benchmarking results for two common models. The results were obtained on a standard AMD Ryzen 9 5950X machine, running Windows 10.\nHere\u2019s our near-term roadmap for MuJoCo:\nHelpful resources about MuJoCo:\nWe look forward to receiving your contributions!\n"}
{"title": "From LEGO competitions to DeepMind's robotics lab", "contents": "Today\u2019s post is all about Akhil Raju, a software engineer on the robotics team. We originally met Akhil in season two of \nDeepMind: The Podcast\n, but we wanted to get to know him better and hear more about his path to DeepMind.\nWhen I was young, I thought about AI in the same way I thought about magic. Yes,\u00a0I wanted to hang out with R2-D2 and Optimus Prime \u2013 but I also wanted to go to Hogwarts. That was until I turned 12 and started participating in LEGO robotics competitions. At that moment, I learned that robots weren\u2019t a fantasy or something that could only exist in the far future, but rather something that could be created in the present. Also, it turns out that playing with robots is incredibly fun.\nA lot! From there I continued with robotics competitions, started university at MIT, and spent a lot of time studying computer science with a focus on robotics. After graduation, I completely pivoted away from the field and joined a startup in San Francisco for a few years before moving to Google.\nIt was great but I'd always wanted to live abroad so I started looking at opportunities outside the US. At that point, I decided to move to London and set my sights on DeepMind. I actually didn\u2019t think DeepMind hired people without PhDs, but I gave it a shot and it worked out!\nBecause I was doing a transfer from Google to DeepMind, I was able to apply to multiple teams at the same time. The robotics team wasn\u2019t on my radar until my recruiter asked me, \u201cBy the way, you have bits of robotics on your resume. Have you thought about joining our robotics team?\u201d I bit at the opportunity. And honestly, it\u2019s been amazing ever since.\nEvery morning I head into the office for breakfast, where without fail, my teammates already are. It\u2019s become a part of our daily routine to have breakfast together before jumping into our work.\nI spend most mornings in the robotics lab, fixing failures from previous experiments or setting up new robots. Even when there\u2019s not much to be done, I get energy from just walking around and seeing our robots at work, hearing the hum of the machines and motors. We\u2019ve grown a lot over the past few years, and you can feel it while walking around our space.\nMy afternoons are a mix of meetings, coding and \u2013 now that most people are back in the office \u2013 an impromptu chat or two. That\u2019s one of my favourite parts of being in the office \u2013 the random catch-ups and whiteboard sessions that help me learn and move quickly. From there I\u2019ll take a quick snack break, and if the weather is nice, head to the balcony to catch up on some of my favourite US sports podcasts (I still haven\u2019t made the switch from football to \nfootball\n). Then I\u2019ll code a little while longer.\nThe culture at DeepMind is one of the best parts of being here. From my perspective, we\u2019ve found a nice balance between a university, start-up, and large company. Most of the work culture comes from the first two. \nIt\u2019s not unusual to find people brainstorming in front of whiteboards with maths scrawled across it, or someone tucked away in a quiet corner reading the latest research papers. Similar to a start-up, there\u2019s a palpable energy throughout \u2013 you can truly feel everyone's excitement. \nIt may be clich\u00e9, but when you love what you do it doesn't feel like work. The robotics team is a miniature version of all of this, with the bonus that many of us are close friends outside of work, too. It\u2019s perfect.\nLike most people, I spent the first month of the pandemic in disbelief, assuming we\u2019d be back to normal soon. The majority of our meetings and collaborations moved online which was an interesting experience for our team in particular. \nOnce the realisation set in that we were in this for the long haul, I decided to spend my new-found free time to better myself. I tried a bunch of hobbies \u2013 long enough in each case to say I\u2019d tried it, but not long enough for anything to stick. I had my guitar phase, a cooking phase, and even a puzzle phase, but my favourite was the tie-dye phase. There were a few weeks where I tie-dyed everything, from shirts to shorts to socks, and now they sit at the bottom of my closet (where they honestly belong).\nI feel lucky to be at DeepMind and to be able to focus on the work that I do. Robotics \u2013 and AI in general \u2013 will be a positive force in the world, and it\u2019s exciting to be able to help move that forward.\nOn the whole, I\u2019m particularly interested in seeing how AI can help mitigate climate change \u2013 whether that\u2019s by finding ways to use energy more efficiently, or enabling us to produce clean energy. Researchers at DeepMind are already thinking about this, so I\u2019m hopeful that we\u2019ll be able to move the world forward and make an impact in this space.\nIf you want to be at DeepMind, go for it. Apply, interview, and just try. You might not get it the first time but that doesn\u2019t mean you can\u2019t try again. I didn\u2019t think I would get a job at DeepMind, and when I got the offer, my initial thought was - surely this is a mistake! Everyone doubts themselves \u2013 I\u2019ve never felt like the smartest person in the room. I\u2019ve often felt the opposite. But I\u2019ve learned that, despite those feelings, I do belong and I do deserve to work at a place like this. And that journey, for me, started with just trying.\nLearn more about robotics at DeepMind and search for open roles today\n"}
{"title": "Building a culture of pioneering responsibly", "contents": "As chief operating officer of one of the world\u2019s leading artificial intelligence labs, I spend a lot of time thinking about how our technologies impact people\u2019s lives \u2013 and how we can ensure that our efforts have a positive outcome. This is the focus of my work, and the critical message I bring when I meet world leaders and key figures in our industry. For instance, it was at the forefront of the panel discussion on \u2018Equity Through Technology\u2019 that I hosted this week at the \nWorld Economic Forum\n in Davos, Switzerland.\u00a0\nInspired by the important conversations taking place at Davos on building a greener, fairer, better world, I wanted to share a few reflections on my own journey as a technology leader, along with some insight into how we at DeepMind are approaching the challenge of building technology that truly benefits the global community.\u00a0\nIn 2000, I took a sabbatical from my job at Intel to visit the orphanage in Lebanon where my father was raised. For two months, I worked to install 20 PCs in the orphanage\u2019s first computer lab, and to train the students and teachers to use them. The trip started out as a way to honour my dad. But being in a place with such limited technical infrastructure also gave me a new perspective on my own work. I realised that without real effort by the technology community, many of the products I was building at Intel would be inaccessible to millions of people. I became acutely aware of how that gap in access was exacerbating inequality; even as computers solved problems and accelerated progress in some parts of the world, others were being left further behind.\u00a0\nAfter that first trip to Lebanon, I started reevaluating my career priorities. I had always wanted to be part of building groundbreaking technology. But when I returned to the US, my focus narrowed in on helping build technology that could make a positive and lasting impact on society. That led me to a variety of roles at the intersection of education and technology, including co-founding \nTeam4Tech\n, a non-profit that works to improve access to technology for students in developing countries.\u00a0\nWhen I joined DeepMind as COO in 2018, I did so in large part because I could tell that the founders and team had the same focus on positive social impact. In fact, at DeepMind, we now champion a term that perfectly captures my own values and hopes for integrating technology into people\u2019s daily lives: \npioneering responsibly\n.\n\u00a0\nI believe pioneering responsibly should be a priority for anyone working in tech. But I also recognise that it\u2019s especially important when it comes to powerful, widespread technologies like artificial intelligence. AI is arguably the most impactful technology being developed today. It has the \npotential to benefit humanity\n in innumerable ways \u2013 from combating climate change to preventing and treating disease. But it\u2019s essential that we account for both its positive and negative downstream impacts. For example, we need to design AI systems carefully and thoughtfully to \navoid amplifying human biases\n, such as in the contexts of hiring and policing.\u00a0\nThe good news is that if we\u2019re continuously questioning our own assumptions of how AI can, and should, be built and used, we can build this technology in a way that truly benefits everyone. This requires inviting discussion and debate, iterating as we learn, building in social and technical safeguards, and seeking out diverse perspectives. At DeepMind, everything we do stems from our company mission of solving intelligence to advance society and benefit humanity, and building a culture of pioneering responsibly is essential to making this mission a reality.\u00a0\nWhat does pioneering responsibly look like in practice? I believe it starts with creating space for open, honest conversations about responsibility within an organisation. One place where we\u2019ve done this at DeepMind is in our multidisciplinary leadership group, which advises on the potential risks and social impact of our research.\u00a0\nEvolving our ethical governance and formalising this group was one of my first initiatives when I joined the company \u2013 and in a somewhat unconventional move, I didn\u2019t give it a name or even a specific objective until we\u2019d met several times. I wanted us to focus on the operational and practical aspects of responsibility, starting with an expectation-free space in which everyone could talk candidly about what pioneering responsibly meant to them. Those conversations were critical to establishing a shared vision and mutual trust \u2013 which allowed us to have more open discussions going forward.\nAnother element of pioneering responsibly is embracing a \nkaizen \nphilosophy and approach. I was introduced to the term kaizen in the 1990s, when I moved to Tokyo to work on DVD technology standards for Intel. It\u2019s a Japanese word that translates to \u201ccontinuous improvement\u201d \u2013 and in the simplest sense, a kaizen process is one in which small, incremental improvements, made continuously over time, lead to a more efficient and ideal system. But it\u2019s the mindset behind the process that really matters. For kaizen to work, everyone who touches the system has to be watching for weaknesses and opportunities to improve. That means everyone has to have both the humility to admit that something might be broken, and the optimism to believe they can change it for the better.\u00a0\nDuring my time as COO of the online learning company Coursera, we used a kaizen approach to optimise our course structure. When I joined Coursera in 2013, courses on the platform had strict deadlines, and each course was offered just a few times a year. We quickly learned that this didn\u2019t provide enough flexibility, so we pivoted to a completely on-demand, self-paced format. Enrollment went up, but completion rates dropped \u2013 it turns out that while too much structure is stressful and inconvenient, too little leads to people losing motivation. So we pivoted again, to a format where course sessions start several times a month, and learners work toward suggested weekly milestones. It took time and effort to get there, but continuous improvement eventually led to a solution that allowed people to fully benefit from their learning experience.\u00a0\nIn the example above, our kaizen approach was largely effective because we asked our learner community for feedback and listened to their concerns. This is another crucial part of pioneering responsibly: acknowledging that we don\u2019t have all the answers, and building relationships that allow us to continually tap into outside input.\u00a0\nFor DeepMind, that sometimes means consulting with experts on topics like security, privacy, bioethics, and psychology. It can also mean reaching out to diverse communities of people who are directly impacted by our technology, and inviting them into a discussion about what they want and need. And sometimes, it means just listening to the people in our lives \u2013 regardless of their technical or scientific background \u2013 when they talk about their hopes for the future of AI.\u00a0\nFundamentally, pioneering responsibly means prioritising initiatives focused on ethics and social impact. A growing area of focus in our research at DeepMind is on how we can make AI systems more equitable and inclusive. In the past two years, we\u2019ve published research on \ndecolonial AI\n, \nqueer fairness in AI\n, \nmitigating ethical and social risks in AI language models\n, and more. At the same time, we\u2019re also working to increase diversity in the field of AI through our dedicated \nscholarship programmes\n. Internally, we recently started hosting Responsible AI Community sessions that bring together different teams and efforts working on safety, ethics, and governance \u2013 and several hundred people have signed up to get involved.\nI\u2019m inspired by the enthusiasm for this work among our employees and deeply proud of all of my DeepMind colleagues who keep social impact front and centre. Through making sure technology benefits those who need it most, I believe we can make real headway on the challenges facing our society today. In that sense, pioneering responsibly is a moral imperative \u2013 and personally, I can\u2019t think of a better way forward.\u00a0\n"}
{"title": "Dynamic language understanding: adaptation to new knowledge in parametric and semi-parametric models", "contents": "Many recent successes in language models (LMs) have been achieved within a \u2018static paradigm\u2019, where the focus is on improving performance on the benchmarks that are created without considering the temporal aspect of data. For instance, answering questions on events that the model could learn about during training, or evaluating on text sub-sampled from the same period as the training data. However, our language and knowledge are dynamic and ever evolving. Therefore, to enable a more realistic evaluation of question-answering models for the next leap in performance, it\u2019s essential to ensure they are flexible and robust when encountering new and unseen data.\nIn 2021, we released \nMind the Gap: Assessing Temporal Generalization in Neural Language Models\n and the \ndynamic language modelling benchmarks\n for WMT and arXiv to facilitate language model evaluation that take temporal dynamics into account. In this paper, we highlighted issues that current state-of-the-art large LMs face with temporal generalisation and found that knowledge-intensive tokens take a considerable performance hit.\nToday, we\u2019re releasing two papers and a new benchmark that further advance research on this topic. In \nStreamingQA: A Benchmark for Adaptation to New Knowledge over Time in Question Answering Models\n, we study the downstream task of question-answering on our newly proposed benchmark, \nStreamingQA\n: we want to understand how parametric and retrieval-augmented, semi-parametric question-answering models adapt to new information, in order to answer questions about new events. In \nInternet-augmented language models through few-shot prompting for open-domain question answering\n, we explore the power of combining a few-shot prompted large language model along with Google Search as a retrieval component. In doing so, we aim to improve the model's factuality, while making sure it has access to up-to-date information for answering a diverse set of questions.\nKnowledge and language understanding of models evaluated through question-answering (QA) has been commonly studied on static snapshots of knowledge, like Wikipedia. To study how semi-parametric QA models and their underlying parametric LMs adapt to evolving knowledge, we constructed the new large-scale benchmark, StreamingQA, with human-written and automatically generated questions asked on a given date, to be answered from 14 years of time-stamped news articles (see Figure 2). We show that parametric models can be updated without full retraining, while avoiding catastrophic forgetting. For semi-parametric models, adding new articles into the search space allows for rapid adaptation, however, models with an outdated underlying LM underperform those with a retrained LM.\nWe\u2019re aiming to capitalise on the unique few-shot capabilities offered by large-scale language models to overcome some of their challenges, with respect to grounding to factual and up-to-date information. Motivated by semi-parametric LMs, which ground their decisions in externally retrieved evidence, we use few-shot prompting to learn to condition LMs on information returned from the web using Google Search, a broad and constantly updated knowledge source. Our approach does not involve fine-tuning or learning additional parameters, thus making it applicable to virtually any language model. And indeed, we find that LMs conditioned on the web surpass the performance of closed-book models of similar, or even larger, model size in open-domain question-answering.\n"}
{"title": "Kyrgyzstan to King\u2019s Cross: the star baker cooking up code", "contents": "Today we caught up with Aliya Rysbek, a software engineer on the Platform team. She spoke to us about her path from Central Asia to DeepMind, and her endless curiosity for learning.\n\u00a0\nOur team is working on a custom project management system for organising all of DeepMind\u2019s research projects. I\u2019m a full-stack developer, so I build the system\u2019s components, like showing team structure, and various backend services to add and improve the product functionality. This system helps to plan and track projects, see who\u2019s working on what, and connect people. The goal is to make it easier for all DeepMinders to do their work.\nMy day can vary, it really depends on which phase of the project I'm on. Let\u2019s say we want to add a feature to our product \u2013 my tasks could range from designing solutions and working with the team to find the best one, to deploying new features into production and doing maintenance. Along the way, I\u2019ll communicate changes to our stakeholders, write docs, code and test solutions, build analytics dashboards, clean-up old code, and fix bugs.\nTypically, I work from the office, so after a quick snack or breakfast, I try to concentrate on critical work for around two hours. Then our team usually has lunch together, which is really nice after working remotely for so long. My afternoon hours are the most productive, so I put my headphones on and jump into coding.\nThe end of the day usually consists of a break, some team chit-chat, and a walk around the office to work through solutions or get inspiration. We have such a beautiful library in our office. It\u2019s inspiring just to look at the huge selection of books and to be reminded of what else there is to do in life \u2013 there\u2019s still so much more to learn and so many complex problems to solve.\u00a0\nBefore joining DeepMind, I mentored tech students and high-schoolers in my free time, helping them prepare CVs and do mock interviews. After I started working here, quite a few non-profit organisations and local projects reached out looking for the same kind of support. I\u2019ve really enjoyed it and feel like I\u2019m having a bigger impact than I could before.\u00a0\nSome people I helped are already working at different tech companies, including two interns who are starting this month at DeepMind \u2013 yay! I really love that I\u2019m in a position to share resources, time, and skills back with the community. As a female software engineer from a developing country, I know the importance of bringing more diversity and inclusion into the workplace.\u00a0\nI\u2019m from Kyrgyzstan \u2013 getting this role was such a big deal where I\u2019m from that I even made it onto the \nlocal news\n.\nEnglish isn\u2019t my native language and is hardly taught in my home country, if at all. But I\u2019ve been really lucky. Back at home I received various scholarships that paid for my time at a private high school where, thanks to my amazing teachers, I was able to improve my English within months and further explore my interest in maths. I even ended up entering and placing 5th in a country-wide olympiad competition while I was there.\u00a0\nAfter that, I decided to study computer science and ended up getting my BSc in Computer Engineering at the Middle East Technical University in Turkey, and my MSc in Computer Science Engineering at the Budapest University of Technology in Hungary. Studying abroad wouldn\u2019t have been possible without scholarship support from the Turkish and Hungarian governments.\u00a0\nOnce I completed my studies, it was time to find a job, which was difficult. Fortunately, I had a mentor who was an ex-Googler and they spent months helping me prepare for interviews and survive more than 70 rejections from different organisations. Eventually, though, we were also able to celebrate five offers!\u00a0\nWith my computer engineering background and internship experience, I felt prepared going into the process. I spent a lot of time focusing on the coding interviews, and I found these resources really helpful:\nI\u2019ve always enjoyed cooking and baking but I wanted to upgrade my skills, so I recently signed up for advanced baking courses \u2013 they went really well! I made delicious pavlovas, croissants (the real kind with lots of butter), tarts, and complex cheesecakes.\u00a0 My friends who were in charge of tasting were very excited about my improved skills \u2013 I even became DeepMind\u2019s Star Baker this year!\u00a0\nI\u2019ve also returned to practising dance and Aikido, the modern Japanese martial art. Both of them keep me really active, which is important when you spend a lot of time behind a computer. Last but not least, I\u2019ve also been inspired to learn calligraphy and pottery. At DeepMind, you can take courses that don\u2019t have to relate to the work you do \u2013 it\u2019s been a dream for someone who loves to learn.\nIn general, I would love to see fair access to quality education globally and I truly believe that DeepMind is capable of contributing to this. There has already been so much work done through the \nscholarship programme\n and \nsummer school\n support but there\u2019s always more we can do. Global access to education is so important as it would unlock a great share of undiscovered talent and increase the overall quality of life.\u00a0\nMake connections! If you see something you\u2019re interested in, do your due diligence and reach out to learn more about the company, position, or project. The relationships you make are invaluable and may help you find the perfect place for your skills, interests, and values.\nTo ask questions. I believe that people inherently want to help others and will welcome curiosity. Once, during a careers talk, I was told that a human's default state is \u201cnot knowing\u201d. I really liked that, and it made me realise that we shouldn't be harsh on ourselves for not knowing something, but rather be graceful and feed our curiosity by learning from others.\nLearn more about engineering at DeepMind and search for open roles today\n"}
{"title": "Evaluating Multimodal Interactive Agents", "contents": "To train agents to interact well with humans, we need to be able to measure progress. But human interaction is complex and measuring progress is difficult. In this work we developed a method, called the Standardised Test Suite (STS), for evaluating agents in temporally extended, multi-modal interactions. We examined interactions that consist of human participants asking agents to perform tasks and answer questions in a 3D simulated environment.\nThe STS methodology places agents in a set of behavioural scenarios mined from real human interaction data. Agents see a replayed scenario context, receive an instruction, and are then given control to complete the interaction offline. These agent continuations are recorded and then sent to human raters to annotate as success or failure. Agents are then ranked according to the proportion of scenarios on which they succeed.\nMany of the behaviours that are second nature to humans in our day-to-day interactions are difficult to put into words, and impossible to formalise. Thus, the mechanism relied on for solving games (like Atari, Go, DotA, and Starcraft) with reinforcement learning won't work when we try to teach agents to have fluid and successful interactions with humans. For example, think about the difference between these two questions: \"Who won this game of Go?\" versus \"What are you looking at?\" In the first case, we can write a piece of computer code that counts the stones on the board at the end of the game and determines the winner with certainty. In the second case, we have no idea how to codify this: the answer may depend on the speakers, the size and shapes of the objects involved, whether the speaker is joking, and other aspects of the context in which the utterance is given. Humans intuitively understand the myriad of relevant factors involved in answering this seemingly mundane question.\nInteractive evaluation by human participants can serve as a touchstone for understanding agent performance, but this is noisy and expensive. It is difficult to control the exact instructions that humans give to agents when interacting with them for evaluation. This kind of evaluation is also in real-time, so it is too slow to rely on for swift progress. Previous works have relied on proxies to interactive evaluation. Proxies, such as losses and scripted probe tasks (e.g. \u201clift the x\u201d where x is randomly selected from the environment and the success function is painstakingly hand-crafted), are useful for gaining insight into agents quickly, but don\u2019t actually correlate that well with interactive evaluation. Our new method has advantages, mainly affording control and speed to a metric that closely aligns with our ultimate goal - to create agents that interact well with humans.\nThe development of MNIST, ImageNet and other human-annotated datasets has been essential for progress in machine learning. These datasets have allowed researchers to train and evaluate classification models for a one-time cost of human inputs. The STS methodology aims to do the same for human-agent interaction research. This evaluation method still requires humans to annotate agent continuations; however, early experiments suggest that automation of these annotations may be possible, which would enable fast and effective automated evaluation of interactive agents. In the meantime, we hope that other researchers can use the methodology and system design to accelerate their own research in this area.\n"}
{"title": "Advocating for the LGBTQ+ community in AI research", "contents": "Research scientist, Kevin McKee, tells how his early love of science fiction and social psychology inspired his career, and how he\u2019s helping advance research in \u2018queer fairness\u2019, support human-AI collaboration, and study the effects of AI on the LGBTQ+ community.\nThe signs were clear, right from the start. I\u2019ve always loved science fiction. I couldn\u2019t tell you how many times I read and reread Isaac Asimov\u2019s \nI, Robot\n as a kid. These short stories explore the psychology of Asimov\u2019s fictional robots, frequently using them as a mirror to uncover insights about the human mind. I was completely enthralled.\nIt\u2019s no surprise that I took an early interest in psychological science. In elementary school, I often tried running controlled psychology experiments for my science projects. Looking back, I\u2019m not sure how successful I was with those experiments, but they led me to my studies in psychology and neuroscience \u2013 and then eventually to DeepMind.\nEveryone at DeepMind gets to work on an absurdly diverse set of projects. Much of our work is driven from the bottom up, so DeepMinders frequently get invited to collaborate on exciting projects from across the organisation.\u00a0\nMy current projects span traditional machine learning methods and social science approaches; research on cooperative AI and the social implications of AI development; and collaborations with engineers, mathematicians, and ethicists.\nI co-lead QueerMinds, our employee resource group for LGBTQ+ employees and allies. When I joined DeepMind, in 2017, we didn't have a formal community or an official space for identities like mine. Over time, I realised that as someone queer myself, I could help create that visibility and foster that community for others at DeepMind.\u00a0\nQueerMinds feels vibrant these days, with regular socials, talks by external researchers and authors, and group field trips, including a recent one to the new queer \nQueer Britain\n, the new queer museum next to our King\u2019s Cross office. Since stepping into the role, I haven\u2019t regretted it for a moment. It\u2019s been a huge joy \u2013 and a continuous learning experience \u2013 to create a space for the queer people in DeepMind's community.\nI prefer working from the office. It\u2019s really energising to see my teammates and random DeepMinders every day. These are known as \u2018weak ties\u2019 in social psychology and sociology, and they definitely inject my day with a lot of happiness.\u00a0\nIn research, I find a lot of breakthroughs come from spontaneous conversations and unplanned moments \u2013 you never know where the next idea or collaboration will come from. Just chatting through the current challenge with a teammate over coffee is often enough to catalyse a lightbulb moment.\nWhen we talk about our goals as an organisation, we often frame the conversation around the motivation of \u2018advancing science and benefiting humanity\u2019. It\u2019s amazing to be on a team committed to those aims. In working toward them, I think we have a real chance to include groups that historically have been excluded from scientific work. If we bring marginalised communities into the agenda-setting process for our work, what sorts of research questions and priorities will we establish?\u00a0\nAI and machine learning can make a difference, even in small ways. My sister is a speech-language pathologist who works with trans teens to help them develop their voices and communication in a way that affirms their gender identities. Recent advances in AI research show a lot of promise for supporting her and others working with queer communities. For example, generative models could help trans patients form realistic, healthy targets for their voice exercises in therapy sessions.\u00a0\nIt\u2019s a tie between two projects. First, a paper I worked on about \u2018\nqueer fairness\n\u2019, where we advocated for more research to understand the effects of AI on LGBTQ+ communities. AI development creates both new opportunities and serious risks for queer people. Yet, most work aimed at measuring and correcting algorithmic bias \u2013 what AI scientists call \u2018algorithmic fairness\u2019 research \u2013 tends to overlook LGBTQ+ communities. My co-authors and I reviewed potential points of promise and concern across areas like privacy, censorship, and mental health.\u00a0\nSecond, is an ongoing project on cooperative AI, which we talk about in the podcast episode \nBetter together\n. Humans are actually fairly good at cooperating with each other, even in the face of the incentive or motivation to act selfishly.\u00a0\nIn social psychology, one popular model of human altruism argues that humans pay attention not just to our own goals and outcomes, but also to the goals and outcomes of those around us \u2013 especially those with whom we have close relationships, like friends and family. If I\u2019m picking up lunch for a friend and myself, I\u2019ll probably skip the sandwich shop that I like but he hates. Instead, I\u2019ll likely find one that we both like, because I care about his happiness and rewards. That sort of \u2018reward sharing\u2019 is key to human altruism, and potentially to our close relationships, too.\u00a0\nDrawing inspiration from this \nreward sharing model\n, my co-authors and I developed \ncooperative AI agents that humans can interact with\n. They\u2019re really fun to play with. As a cherry on top, one of the games we used for studying \nhuman-AI collaboration\n is actually my friends\u2019 and my favourite to play outside work: \nOvercooked!\nI\u2019m an avid surfer. I grew up in California, so I was a bit worried about the surfing prospects when moving to London. Turns out that it\u2019s a quick jump to Portugal and Spain, where there are awesome waves. Some of my friends even swear that surfing in Cornwall is first class! We try to make a trip every few months, for a long weekend or a full week on the beach.\nDon\u2019t be afraid to take big jumps! Before joining DeepMind, my entire life \u2013 my career, family, and friends \u2013 was based in the US. Moving to the UK felt a bit daunting. Five years in, I can confidently say that making the jump to London was one of the best decisions I\u2019ve ever made.\nLearn more about research at DeepMind and search for open roles today\n"}
{"title": "Bridging DeepMind research with Alphabet products", "contents": "For today's \"Five minutes with\" \nwe caught up with Gemma Jennings, a product manager on the Applied team, who led a session on vision language models at the \nAI Summit\n - one of the world\u2019s largest AI events for business.\nI\u2019m a part of the Applied team, which helps bring DeepMind technology to the outside world through Alphabet and Google products and solutions, like with \nWaveNet\n and Google Assistant, Maps, and Search. As a product manager, I act as a bridge between the two organisations, working very closely with both teams to understand the research and how people can use it. Ultimately, we want to be able to answer the question: How can we use this technology to improve the lives of people around the world?\nI\u2019m particularly excited about our portfolio of sustainability work. We\u2019ve already helped reduce the amount of energy needed to cool Google's data centres, but there\u2019s much more we can do to have a bigger, transformative impact within sustainability.\nI worked at John Lewis Partnership, a UK department store that has a strong sense of purpose built into its DNA. I\u2019ve always liked being part of a company with a sense of societal purpose, so DeepMind\u2019s mission of solving intelligence to advance science and benefit humanity really resonated with me. I was intrigued to learn how that ethos would manifest within a research-led organisation \u2013 and within Google, one of the largest companies in the world. Adding this to my academic background in experimental psychology, neuroscience, and statistics, DeepMind ticked all the boxes.\nIs my first in-person conference in almost three years, so I\u2019m really keen to meet people in the same industry as myself and to hear what other organisations are working on.\nI\u2019m looking forward to attending a few talks from the quantum computing track to learn more about. It has the potential to drive the next big paradigm shift in computing power, unlocking new use cases for applying AI in the world and allowing us to work on larger, more complex problems.\nMy work involves a lot of deep learning methods and it\u2019s always exciting to hear about the different ways people are using this technology. At the moment, these types of models require training on large amounts of data \u2013 which can be costly, time consuming, and resource intensive given the amount of computing needed. So where do we go from here? And what does the future of deep learning look like? These are the types of questions I\u2019m looking to answer.\nImage Recognition Using Deep Neural Networks, our recently \npublished research\n on vision language models (VLMs). For my presentation, I discussed recent advances in fusing large language models (LLMs) with powerful visual representations to progress the state of the art for image recognition.\u00a0\nThis fascinating research has so many potential uses in the real world. It could, one day, act as an assistant to support classroom and informal learning in schools, or help people with blindness or low vision see the world around them, transforming their day-to-day lives.\nWith a better understanding of what happens after the research breakthrough is announced. There\u2019s so much amazing research being done but we need to think about what comes next, like what global problems could we help solve? And how can we use our research to create products and services that have a purpose?\nThe future is bright and I\u2019m excited to discover new ways of applying our groundbreaking research to help benefit millions of people around the world.\nLearn more about research at DeepMind and search for open roles today\n"}
{"title": "BYOL-Explore: Exploration with Bootstrapped Prediction", "contents": "Curiosity-driven exploration is the active process of seeking new information to enhance the agent\u2019s understanding of its environment. Suppose that the agent has learned a model of the world that can predict future events given the history of past events. The curiosity-driven agent can then use the prediction mismatch of the world model as the intrinsic reward for directing its exploration policy towards seeking new information. As follows, the agent can then use this new information to enhance the world model itself so it can make better predictions. \u00a0This iterative process can allow the agent to eventually explore every novelty \u00a0in the world and use this information to build an accurate world model.\nInspired by the successes of \nbootstrap your own latent\n (BYOL) \u2013 which has been applied in \ncomputer vision\n, \ngraph representation learning\n, and \nrepresentation learning in RL\n \u2013 we propose BYOL-Explore: a conceptually simple yet general, curiosity-driven AI agent for solving hard-exploration tasks. BYOL-Explore learns a representation of the world by predicting its own future representation. Then, it uses the prediction-error at the representation level as an intrinsic reward to train a curiosity-driven policy. Therefore, BYOL-Explore learns a world representation, the world dynamics, and a curiosity-driven exploration policy all-together, simply by optimising the prediction error at the representation level.\nDespite the simplicity of its design, when applied to the \nDM-HARD-8\n suite of challenging 3-D, visually complex, and hard exploration tasks, BYOL-Explore outperforms standard curiosity-driven exploration methods such as \nRandom Network Distillation\n (RND) and \nIntrinsic Curiosity Module\n (ICM), in terms of mean capped human-normalised score (CHNS), measured across all tasks. Remarkably, BYOL-Explore achieved this performance using only a single network concurrently trained across all tasks, whereas prior work was restricted to the single-task setting and could only make meaningful progress on these tasks when provided with human expert demonstrations. \nAs further evidence of its generality, BYOL-Explore achieves super-human performance in the ten hardest exploration \nAtari games\n, while having a simpler design than other competitive agents, such as \nAgent57\n and \nGo-Explore\n.\nMoving forward, we can generalise BYOL-Explore to highly stochastic environments by learning a probabilistic world model that could be used to generate trajectories of the future events. This could allow the agent to model the possible stochasticity of the environment, avoid stochastic traps, and plan for exploration.\n"}
{"title": "Leading a movement to strengthen machine learning in Africa", "contents": "Avishkar Bhoopchand\n, \na research engineer on the Game Theory and Multi-agent team, shares his journey to DeepMind and how he\u2019s working to raise the profile of deep learning across Africa.\u00a0\nFind out more about \nDeep Learning Indaba 2022\n, the annual gathering of the African AI community \u2013 taking place in Tunisia this August.\nAs a research engineer and technical lead, no day is the same. I usually start my day by listening to a podcast or audiobook on my commute into the office. After breakfast, I focus on emails and admin before jumping into my first meeting. These vary from one-on-ones with team members and project updates to diversity, equity, and inclusion (DE&I) working groups.\u00a0\nI try to carve out time for my to do list in the afternoon. These tasks could involve preparing a presentation, reading research papers, writing or reviewing code, designing and running experiments, or analysing results.\u00a0\nWhen working from home, my dog Finn keeps me busy! Teaching him is a lot like reinforcement learning (RL) \u2013 like how we train artificial agents at work. So, a lot of my time is spent thinking about deep learning or machine learning in one way or another.\nDuring a course on intelligent agents at the University of Cape Town, my lecturer demoed a six-legged robot that had learned to walk from scratch using RL. From that moment on, I couldn\u2019t stop thinking about the possibility of using human and animal mechanisms to build systems capable of learning.\nAt the time, machine learning application and research wasn\u2019t really a viable career option in South Africa. Like many of my fellow students, I ended up working in the finance industry as a software engineer. I learned a lot, especially around designing large scale, robust systems that meet user requirements. But after six years, I wanted something more.\nAround then, deep learning started to take off. First I started doing online courses like Andrew Ng\u2019s \nmachine learning lectures\n on Coursera. Soon after, I was fortunate enough to get a scholarship to University College London, where I got my masters in computational statistics and machine learning.\u00a0\nBeyond DeepMind, I\u2019m also a proud organiser and steering committee member of the \nDeep Learning Indaba\n, a movement to strengthen machine learning and AI in Africa. It started in 2017 as a summer school in South Africa. We expected 30 or so students to get together to learn about machine learning \u2013 but to our surprise, we received over 700 applications! It was amazing to see, and it clearly showed the need for connection between researchers and practitioners in Africa.\nSince then, the organisation has grown into an annual celebration of African AI with over 600 attendees, and local IndabaX events held across nearly 30 African countries. We also have research grants, thesis awards, and complementary programmes, including a mentorship programme \u2013 which I started during the pandemic to keep the community engaged.\nIn 2017, there were zero publications with an African author, based at an African institution, presented at \nNeurIPS\n, the leading machine learning conference. AI researchers across the African continent were working in silos \u2013 some even had colleagues working on the same subject at another institution down the road and didn\u2019t know. Through the Indaba, we\u2019ve built a thriving community on the continent and our alumni have gone on to form new collaborations, publishing papers at NeurIPS and all of the major conferences.\u00a0\nMany members have gotten jobs at top tech companies, formed new startups on the continent, and launched other amazing grassroots AI projects in Africa. Although organising the Indaba is a lot of hard work, it\u2019s made worthwhile by seeing the achievements and growth of the community. I always leave our annual event feeling inspired and ready to take on the future.\nDeepMind was my ultimate dream company to work for, but I didn\u2019t think I stood a chance. From time-to-time, I\u2019ve struggled with imposter syndrome \u2013 when surrounded by intelligent, capable people, it\u2019s easy to compare oneself on a single axis and feel like an imposter. Luckily, my wonderful wife told me I had nothing to lose by applying, so I sent my CV and eventually got an offer for a research engineer role!\u00a0\nMy previous experience in software engineering really helped me prepare for this role, as I could lean on my engineering skills for the day to day work while building my research skills. Not getting the dream job right away doesn\u2019t mean the door\u2019s closed on that career forever.\nI recently worked on a project about giving artificial agents the capability of\n real-time cultural transmission\n. Cultural transmission is a social skill that humans and certain animals possess, which gives us the ability to learn information from observing others. It\u2019s the basis for cumulative cultural evolution and the process responsible for expanding our skills, tools, and knowledge across multiple generations.\nIn this project, we trained artificial agents in a 3D simulated environment to observe an expert performing a new task, then copy that pattern, and remember it. Now that we\u2019ve shown that cultural transmission is possible in artificial agents, it may be possible to use cultural evolution to help generate artificial general intelligence (AGI).\u00a0\nThis was the first time I worked on large-scale RL. This work combines machine learning and social science, and there was a lot for me to learn on the research side. At times, progress towards our goal was also slow but we got there in the end! But really, I\u2019m most proud of the incredibly inclusive culture we had as a project team. Even when things were difficult, I knew I could rely on my colleagues for support.\nI\u2019ve been really involved with a number of diversity, equity, and inclusion (DE&I) initiatives. I\u2019m a strong believer that DE&I in the workplace leads to better outcomes, and to build AI for all, we must have representation from a diverse set of voices.\nI\u2019m a facilitator for an internal workshop on the concept of Allyship, which is about using one\u2019s position of privilege and power to challenge the status quo in support of people from marginalised groups. I\u2019m involved in various working groups that aim to improve community inclusion amongst research engineers and diversity in hiring. I\u2019m also a mentor in the \nDeepMind scholarship programme\n, which has partnerships in Africa and other parts of the world.\u00a0\nI\u2019m particularly enthusiastic about the possibilities of AI making a positive impact on medicine, especially for better understanding and treating diseases. For example, mental health conditions like depression affect hundreds of millions of people worldwide, but we seem to have limited understanding of the causal mechanisms behind it, and therefore, limited treatment options. I hope that in the not too distant future, general AI systems can work in conjunction with human experts to unlock the secrets of our minds and help us understand and cure these diseases.\nLearn more about research at DeepMind and search for open roles today\n"}
{"title": "Unlocking High-Accuracy Differentially Private Image Classification through Scale", "contents": "A recent \nDeepMind paper\n on the ethical and social risks of language models identified large language models \nleaking sensitive information\n about their training data as a potential risk that organisations working on these models have the responsibility to address. Another \nrecent paper\n shows that similar privacy risks can also arise in standard image classification models: a fingerprint of each individual training image can be found embedded in the model parameters, and malicious parties could exploit such fingerprints to reconstruct the training data from the model. \nPrivacy-enhancing technologies like differential privacy (DP) can be deployed at training time to mitigate these risks, but they often incur significant reduction in model performance. In this work, we make substantial progress towards unlocking high-accuracy training of image classification models under differential privacy.\nDifferential privacy was \nproposed\n as a mathematical framework to capture the requirement of protecting individual records in the course of statistical data analysis (including the training of machine learning models). DP algorithms protect individuals from any inferences about the features that make them unique (including complete or partial reconstruction) by injecting carefully calibrated noise during the computation of the desired statistic or model. Using DP algorithms provides robust and rigorous privacy guarantees both in theory and in practice, and has become a de-facto gold standard adopted by a number of \npublic\n and \nprivate\n organisations.\nThe most popular DP algorithm for deep learning is differentially private stochastic gradient descent (DP-SGD), a modification of standard SGD obtained by clipping gradients of individual examples and adding enough noise to mask the contribution of any individual to each model update:\nUnfortunately, prior works have found that in practice, the privacy protection provided by DP-SGD often comes at the cost of significantly less accurate models, which presents a major obstacle to the widespread adoption of differential privacy in the machine learning community. According to empirical evidence from prior works, this utility degradation in DP-SGD becomes more severe on larger neural network models \u2013 including the ones regularly used to achieve the best performance on challenging image classification benchmarks.\nOur work investigates this phenomenon and proposes a series of simple modifications to both the training procedure and model architecture, yielding a significant improvement on the accuracy of DP training on standard image classification benchmarks. The most striking observation coming out of our research is that DP-SGD can be used to efficiently train much deeper models than previously thought, as long as one ensures the model's gradients are well-behaved. We believe the substantial jump in performance achieved by our research has the potential to unlock practical applications of image classification models trained with formal privacy guarantees. \nThe figure below summarises two of our main results: an ~10% improvement on CIFAR-10 compared to previous work when privately training without additional data, and a top-1 accuracy of 86.7% on ImageNet when privately fine-tuning a model pre-trained on a different dataset, almost closing the gap with the best non-private performance.\nThese results are achieved at \ud835\udf3a=8, a standard setting for calibrating the strength of the protection offered by differential privacy in machine learning applications. We refer to the paper for a discussion of this parameter, as well as additional experimental results at other values of \ud835\udf3a and also on other datasets. Together with the paper, we are also open-sourcing our implementation to enable other researchers to verify our findings and build on them. We hope this contribution will help others interested in making practical DP training a reality.\nDownload our JAX implementation \non GitHub\n.\n"}
{"title": "Reflections from ethics and safety \u2018on the ground\u2019 at DeepMind", "contents": "Boxi share's their experience working on the Ethics & Society team to support ethical, safe and beneficial AI development, highlighting the importance of interdisciplinary and sociotechnical thinking.\nI grew up in suburban Perth, Australia, and still remember using the internet for the first time at the local library in my early teens to research the Great Barrier Reef. Looking back, I could have never imagined the role I have now!\u00a0\nI was always fascinated with how social systems could interact with technology, which eventually led me to study political science and urban planning at university. I went on to work in urban policy research and strategy consulting, and a few years later, I made the daunting decision to move from Australia to London to work for a health technology startup. Seeing the implementation challenges of AI in medicine piqued my interest in the ethical and societal impacts of AI. When the opportunity to apply for DeepMind came up it made a lot of sense to me - a mix of the academic inquiry that I missed, and the exciting energy of a startup.\nDeepMind\u2019s culture is one where many perspectives collide and there are countless chances to find your community. In many cases, this happens formally - I\u2019ve joined employee resource groups like QueerMinds and the People of Colour Employee Group, and have had the opportunity to attend conferences like Lesbians Who Tech. I\u2019m also part of \nQueer in AI\n, an organisation with a mission to raise awareness of queer issues in AI/ML, foster a community of queer researchers and celebrate the work of queer scientists. DeepMind are actually hosting a workshop with Queer in AI on Wednesday the 6th July to discuss the relationship between queer issues and AI. Since joining I\u2019ve made some really great connections with a circle of queer and POC colleagues amongst whom we create safe spaces and support each other\u2019s work. These interactions have felt even more rewarding now that we have returned to the office.\nMy team (the Ethics & Society team) is busy and close knit. We work together to guide the responsible development and deployment of AI. One core element of this is developing the processes, infrastructure and frameworks to ensure ethical considerations are embedded into all of our projects. We often partner with other teams at DeepMind over extended periods to consider the positive and negative downstream impacts of our work, i.e our work with language models, or science projects like AlphaFold.\u00a0\nWe\u2019re constantly learning as a team, talking to each other about our projects and the challenges we\u2019re facing. This involves a lot of reflection to fully understand who our technology may impact, and to determine who the right people are (internally and externally) to help tackle the challenges identified. This can be incredibly complex at times but it is a lot of fun.\nI lead our research collaborations which operationalise ethics and safety across our work at DeepMind. This normally includes conducting ethical impact assessments, partnering with teams to conduct ethics reviews, and facilitating workshops to think through benefits, risks and mitigations.\nAs we work across varied domains of research (e.g. language, reinforcement learning, robotics), we are always in conversation with experts across research, engineering, legal, policy, and communications, etc. We also meet as a team daily - this is super important in our area of work as ethics and safety questions are best discussed in groups (in order to check personal biases and debate differences of opinion, for example).\nI love learning from those around me - internally and within the wider AI ethics community. There is still so much more to learn and I am humbled by the knowledge and curiosity of everyone I speak to. What I find particularly interesting is learning from those that are adjacent to - or outside of traditional AI/ML research fields. Better understanding the perspectives of those, for example, in social sciences, philosophy, or critical theory allows us to better identify and challenge the fundamental values underpinning technology.\nA great example of this would be this year\u2019s \nACM Conference on Fairness, Accountability, and Transparency\n (FAccT) in South Korea. The agenda at this conference was far reaching, covering everything from my colleagues\u2019 \npaper\n on fluid identity in machine learning, to a session with \nYoujin Kong\n on AI ethics and feminist philosophy, and a keynote by \nKaren Hao\n on journalism on AI ethics and technology.\nIn a recent \nblog post\n, our COO Lila discussed the idea of pioneering responsibility and its key role in our mission. I think that\u2019s exactly right - not only is it critical for the wider tech community but it\u2019s especially important when it comes to creating powerful, widespread technologies like artificial intelligence. It must be part of the conversation at every stage and embedded into everything that we do.\nI\u2019m proud that I am part of a team that gets to explore these ideas - and while of course we have much more to do in this space, I do believe we're helping make a positive impact on the world around us.\u00a0\nRead as much as you can about AI ethics and safety, and better yet, explore sociotechnical work that discusses the history of AI, the current harms of technology within society, and visions for what safe and ethical AI could look like. Some favourites of mine are Ruha Benjamin\u2019s \nRace after Technology\n and Karen Hao\u2019s recent series on \nAI and colonialism\n. I would also recommend checking out Kevin Guyan\u2019s \nQueer Data: Using Gender, Sex and Sexuality Data for Action\n, and my colleague's paper on \nalgorithmic fairness for the queer community\n.\nFinally, I want to reassure \u2018non-technical\u2019 or more social science oriented folks that this space is for you. I often have people tell me they feel intimidated by AI/ML despite having an interest in technology and ethics. Please be assured that your perspective will be valuable to this industry - our values influence technology, as does technology impact our social life. Addressing the challenges of AI development will require interdisciplinary and sociotechnical thinking and people from all walks of life. Don\u2019t doubt yourself - go for it!\nLearn more about the ethics & society team and search for open roles today\n"}
{"title": "Human-centred mechanism design with Democratic AI", "contents": "In our recent \npaper\n, published in Nature Human Behaviour, we provide a proof-of-concept demonstration that deep reinforcement learning (RL) can be used to find economic policies that people will vote for by majority in a simple game. The paper thus addresses a key challenge in AI research - how to train AI systems that align with human values.\nImagine that a group of people decide to pool funds to make an investment. The investment pays off, and a profit is made. How should the proceeds be distributed? One simple strategy is to split the return equally among investors. But that might be unfair, because some people contributed more than others. Alternatively, we could pay everyone back in proportion to the size of their initial investment. That sounds fair, but what if people had different levels of assets to begin with? If two people contribute the same amount, but one is giving a fraction of their available funds, and the other is giving them all, should they receive the same share of the proceeds? \nThis question of how to redistribute resources in our economies and societies has long generated controversy among philosophers, economists and political scientists. Here, we use deep RL as a testbed to explore ways to address this problem.\nTo tackle this challenge, we created a simple game that involved four players. Each instance of the game was played over 10 rounds. On every round, each player was allocated funds, with the size of the endowment varying between players. Each player made a choice: they could keep those funds for themselves or invest them in a common pool. Invested funds were guaranteed to grow, but there was a risk, because players did not know how the proceeds would be shared out. Instead, they were told that for the first 10 rounds there was one referee (A) who was making the redistribution decisions, and for the second 10 rounds a different referee (B) took over. At the end of the game, they voted for either A or B, and played another game with this referee. Human players of the game were allowed to keep the proceeds of this final game, so they were incentivised to report their preference accurately.\nIn reality, one of the referees was a pre-defined redistribution policy, and the other was designed by our deep RL agent. To train the agent, we first recorded data from a large number of human groups and taught a neural network to copy how people played the game. This simulated population could generate limitless data, allowing us to use data-intensive machine learning methods to train the RL agent to maximise the votes of these \u201cvirtual\u201d players. Having done so, we then recruited new human players, and pitted the AI-designed mechanism head-to-head against well-known baselines, such as a \nlibertarian\n policy that returns funds to people in proportion to their contributions. \nWhen we studied the votes of these new players, we found that the policy designed by deep RL was more popular than the baselines. In fact, when we ran a new experiment asking a fifth human player to take on the role of referee, and trained them to try and maximise votes, the policy implemented by this \u201chuman referee\u201d was still less popular than that of our agent.\nAI systems have been sometimes criticised for learning policies that may be incompatible with human values, and this problem of \u201cvalue alignment\u201d has become a major concern in AI research. One merit of our approach is that the AI learns directly to maximise the stated preferences (or votes) of a group of people. This approach may help ensure that AI systems are less likely to learn policies that are unsafe or unfair. In fact, when we analysed the policy that the AI had discovered, it incorporated a mixture of ideas that have previously been proposed by human thinkers and experts to solve the redistribution problem. \nFirstly, the AI chose to redistribute funds to people in proportion to their \nrelative\n rather than \nabsolute\n contribution. This means that when redistributing funds, the agent accounted for each player\u2019s initial means, as well as their willingness to contribute. Secondly, the AI system especially rewarded players whose relative contribution was more generous, perhaps encouraging others to do likewise. Importantly, the AI only discovered these policies by learning to maximise human votes. The method therefore ensures that humans remain \u201cin the loop\u201d and the AI produces human-compatible solutions. \u00a0\nBy asking people to vote, we harnessed the principle of majoritarian democracy for deciding what people want. Despite its wide appeal, it is widely acknowledged that democracy comes with the caveat that the preferences of the majority are accounted for over those of the minority. In our study, we ensured that \u2013 like in most societies \u2013 that minority consisted of more generously endowed players. But more work is needed to understand how to trade off the relative preferences of majority and minority groups, by designing democratic systems that allow all voices to be heard.\n"}
{"title": "Intuitive physics learning in a deep-learning model inspired by developmental psychology", "contents": "Understanding the physical world is a critical skill that most people deploy effortlessly. However, this still poses a challenge to artificial intelligence; if we\u2019re to deploy safe and helpful systems in the real world, we want these models to share our intuitive sense of physics. But before we can build those models, there is another challenge: How will we measure the ability of these models to understand the physical world? That is, what does it mean to understand the physical world and how can we quantify it?\nLuckily for us, developmental psychologists have spent decades studying what infants know about the physical world. Along the way, they've carved the nebulous notion of physical knowledge into a concrete set of physical concepts. And, they've developed the violation-of-expectation (VoE) paradigm for testing those concepts in infants.\nIn our paper published today in Nature Human Behavior, we extended their work and open-sourced the \nPhysical Concepts dataset\n. This synthetic video dataset ports the VoE paradigm to assess five physical concepts: solidity, object persistence, continuity, \u201cunchangeableness'', and directional inertia.\nWith a benchmark for physical knowledge in hand, we turned to the task of building a model capable of learning about the physical world. Again, we looked to developmental psychologists for inspiration. Researchers not only catalogued what infants know about the physical world, they also posited the mechanisms that could enable this behaviour. Despite variability, these accounts have a central role for the notion of breaking up the physical world into a set of \nobjects\n which evolve through time.\nInspired by this work, we built a system that we nickname PLATO (Physics Learning through Auto-encoding and Tracking Objects). PLATO represents and reasons about the world as a set of objects. It makes predictions about where objects will be in the future based on where they've been in the past and what other objects they're interacting with.\nAfter training PLATO on videos of simple physical interactions, we found that PLATO passed the tests in our Physical Concepts dataset. Furthermore, we trained \"flat\" models that were as big (or even bigger) than PLATO but did not use object-based representations. When we tested those models, we found they didn't pass all of our tests. This suggests that objects are helpful for learning intuitive physics, supporting hypotheses from the developmental literature.\nWe also wanted to determine how much experience was needed to develop this capacity. Evidence for physical knowledge has been shown in infants as young as two and a half months of age. How does PLATO fare in comparison? By varying the amount of training data used by PLATO, we found that PLATO could learn our physical concepts with as little as 28 hours of visual experience. The limited and synthetic nature of our dataset means we cannot make a like-for-like comparison between the amount of visual experiences received by infants and PLATO. However, this result suggests that intuitive physics can be learned with relatively little experience if supported via an inductive bias for representing the world as objects.\nFinally, we wanted to test PLATO's ability to generalise. In the Physical Concepts dataset, all of the objects in our test set are also present in the training set. What if we tested PLATO with objects it had never seen before? To do this, we leveraged a subset of another synthetic \ndataset\n developed by researchers at MIT. This dataset also probes physical knowledge, albeit with different visual appearances and a set of objects that PLATO has never seen before. PLATO passed, without any re-training, despite being tested on entirely new stimuli.\nWe hope this dataset can provide researchers with a more specific understanding of their model\u2019s abilities to understand the physical world. In the future, this can be expanded to test more aspects of intuitive physics by increasing the list of physical concepts tested, and using richer visual stimuli including new object shapes or even real-world videos.\n"}
{"title": "AlphaFold reveals the structure of the protein universe", "contents": "To read about all our work on solving protein folding, go to \ndeepmind.com/AlphaFold\n or read a timeline of the breakthrough \nhere\n.\nIt\u2019s been one year since we released and open sourced \nAlphaFold\n, our AI system to predict the 3D structure of a protein just from its 1D amino acid sequence, and created the \nAlphaFold Protein Structure Database\n (AlphaFold DB) to freely share this scientific knowledge with the world. Proteins are the building blocks of life, they underpin every biological process in every living thing. And, because a protein\u2019s shape is closely linked with its function, knowing a protein\u2019s structure unlocks a greater understanding of what it does and how it works. We hoped this groundbreaking resource would help accelerate scientific research and discovery globally, and that other teams could learn from and build on the advances we made with AlphaFold to create further breakthroughs. That hope has become a reality far quicker than we had dared to dream. Just twelve months later, AlphaFold has been accessed by more than half a million researchers and used to accelerate progress on important real-world problems ranging from \nplastic pollution\n to \nantibiotic resistance\n.\nToday, I\u2019m incredibly excited to share the next stage of this journey. In partnership with EMBL\u2019s \nEuropean Bioinformatics Institute (EMBL-EBI)\n, we\u2019re now releasing predicted structures for nearly all catalogued proteins known to science, which will expand the \nAlphaFold DB\n \nby over 200x\n - \nfrom nearly 1 million structures to over 200 million structures \n- with the potential to dramatically increase our understanding of biology.\nThis update includes predicted structures for plants, bacteria, animals, and other organisms, opening up many new opportunities for researchers to use AlphaFold to advance their work on important issues, including sustainability, food insecurity, and neglected diseases.\nToday\u2019s update means that most pages on the main protein database \nUniProt\n will come with a predicted structure. All 200+ million structures will also be available for bulk download via \nGoogle Cloud Public Datasets\n, making AlphaFold even more accessible to scientists around the world.\nTwelve months on from AlphaFold\u2019s initial release, it\u2019s been amazing to reflect on the incredible impact AlphaFold has already had, and our long journey to reach today\u2019s milestone.\nFor our team, AlphaFold\u2019s success was especially rewarding, both because it was the most complex AI system we\u2019d ever built, requiring multiple critical innovations, and because it has had the most meaningful downstream impact. By demonstrating that AI could accurately predict the shape of a protein down to atomic accuracy, at scale and in minutes, AlphaFold not only provided a solution to a 50-year grand challenge, it also became the first big proof point of our founding thesis: that artificial intelligence can dramatically accelerate scientific discovery, and in turn advance humanity.\nWe open sourced AlphaFold\u2019s code and published two in-depth papers in Nature [1, 2], which have already been cited more than 4000 times. We \ncollaborated closely\n with the world-leading EMBL-EBI to design a tool that would best help biologists access and use AlphaFold, and together released the AlphaFold DB, a searchable database that is open and free to all. Before releasing AlphaFold, in line with our careful approach to \npioneering responsibly\n, we sought input from more than 30 experts across biology research, security, ethics and safety to help us understand how to share the benefits of AlphaFold with the world, in a way that would maximise potential benefit and minimise potential risk.\nTo date, more than 500,000 researchers from 190 countries have accessed the AlphaFold DB to view over 2 million structures. Our freely available structures have also been integrated into other public datasets, such as Ensembl, UniProt, and OpenTargets, where millions of users access them as part of their everyday workflows.\nWe\u2019ve been amazed by the rate at which AlphaFold has already become an essential tool for hundreds of thousands of scientists in labs and universities across the world to help them in their important work. As for our own work with AlphaFold, we prioritised applications that we felt would have the most positive social benefit, with a focus on initiatives that had been historically underfunded or overlooked. For example, \nwe partnered\n with the \nDrugs for Neglected Diseases Initiative (DNDI)\n to help advance their research, moving them closer to finding life-saving cures for diseases like \nLeishmaniasis\n and \nChagas disease\n that disproportionately affect people in poorer parts of the world. We also supported \nWorld Neglected Tropical Disease Day\n by creating structure predictions for organisms identified by the \nWorld Health Organisation\n as high-priority for their research, helping to further the study of diseases like \nLeprosy\n and \nSchistosomiasis\n, which devastate the lives of more than 1 billion people globally.\nIt\u2019s been so inspiring to see the myriad ways the research community has taken AlphaFold, using it for everything from \nunderstanding diseases\n, to \nprotecting honey bees\n, to \ndeciphering biological puzzles\n, to \nlooking deeper into the origins of life itself.\nOther impressive examples, chosen by members of our AlphaFold team, include:\nIn a recent \nspecial issue of Science\n, several groups described how AlphaFold helped them piece together the nuclear pore complex, one of the most fiendish puzzles in biology. The giant structure consists of hundreds of protein parts and controls everything that goes in and comes out of the cell nucleus. Its delicate structure was finally revealed by using existing experimental methods to reveal its outline and AlphaFold predictions to complete and interpret any areas that were unclear. This powerful combination is now becoming routine in labs, unlocking new science and showing how experimental and computational techniques can work together.\u00a0\nStructural search tools like \nFoldseek\n and \nDali\n are allowing users to very quickly search for entries similar to a given protein. This could be a first step toward mining large sequence datasets for practically useful proteins, such as those that break down plastic, and it could provide clues about protein function. The update of the database to include over 200 million predicted structures will further amplify this impact.\u00a0\nAlphaFold is already having a significant, direct impact on human health. Meeting with researchers at the \nEuropean Society of Human Genetics\n revealed how important AlphaFold structures are to biologists and clinicians trying to unravel the causes of rare genetic diseases.\n \nIn addition, AlphaFold is \naccelerating drug discovery\n by providing a better understanding of newly identified proteins that could be drug targets, and helping scientists to more quickly find potential medicines that bind to them.\nAlphaFold has launched biology into an era of structural abundance, unlocking scientific exploration at digital speed. The AlphaFold DB serves as a \u2018google search\u2019 for protein structures, providing researchers with instant access to predicted models of the proteins they're studying, enabling them to focus their effort and expedite experimental work. From \nfighting disease\n to \ndeveloping vaccines\n, AlphaFold has already enabled incredible advances on some of our biggest global challenges, and this is just the beginning of the impact that we will start to see over the next few years. Our hope is that this expanded database will aid countless more scientists in their work and open up completely new avenues of scientific exploration, such as metaproteomics.\u00a0\nAt DeepMind, we\u2019re hard at work building on all this potential with significant investments in many areas, including partnering with our new sister Alphabet company \nIsomorphic Labs\n to reimagine the entire drug discovery process from first principles with an AI-first approach; \nestablishing a wet lab\n at the renowned \nFrancis Crick Institute\n to strengthen the connection between AI and experimental techniques to advance understanding of biology, including protein design and genomics; and expanding our \nAI for Science\n team to accelerate further progress on our fundamental biology research and apply AI to other fascinating and important scientific challenges, such as \nclimate science\n, \nquantum chemistry\n, and \nfusion\n.\nAlphaFold is a glimpse of the future, and what might be possible with computational and AI methods applied to biology. At its most fundamental level, biology can be thought of as an information processing system, albeit an extraordinarily complex and emergent one. Just as maths is the perfect description language for physics, we believe AI might turn out to be just the right technique to cope with the dynamic complexity of biology. AlphaFold is an important first proof point for this, and a sign of much more to come. As pioneers in the emerging field of \u2018digital biology', we\u2019re excited to see the huge potential of AI starting to be realised as one of humanity\u2019s most useful tools for advancing scientific discovery and understanding the fundamental mechanisms of life.\n"}
{"title": "Working together with YouTube", "contents": "Helping enrich people\u2019s lives with our research, we\u2019ve partnered with businesses across Alphabet to apply our technology towards improving the products and services used by billions of people every day.\u00a0\nOne of our key partners is YouTube, who are on a mission to give everyone a voice and show them the world.\u00a0\nWorking together with YouTube\u2019s product and engineering teams, we\u2019ve helped optimise the decision-making processes that increase safety, decrease latency, and enhance the viewer, creator, and advertiser experience for all. \nWith video surging during the COVID-19 pandemic, and the total amount of internet traffic expected to grow in the future, video compression is an increasingly important problem.\nWorking together with YouTube, we explored the potential for our AI model, \nMuZero\n, to improve the VP9 codec, a coding format that helps compress and transmit video over the internet. Then we applied MuZero to some of YouTube\u2019s live traffic.\nSince launching to production on a portion of YouTube\u2019s live traffic, we\u2019ve demonstrated an average 4% bitrate reduction across a large, diverse set of videos. Bitrate helps determine the computing ability and bandwidth needed to play and store videos \u2013 impacting everything from how long a video takes to load to its resolution, buffering, and data usage.\u00a0\nBy improving the VP9 codec on YouTube, we\u2019ve helped reduce internet traffic, data usage, and time needed for loading videos. And through optimising video compression, millions of people around the world are able to watch more videos while using less data.\nSince 2018, we\u2019ve collaborated with YouTube to better educate creators on what types of videos can earn revenue from ads and make sure ads appear alongside content that follows YouTube\u2019s advertiser friendly guidelines.\nTogether with the YouTube team, we developed a label quality model (LQM) that helps label videos with greater precision, according to YouTube\u2019s ad friendly guidelines. The model improved the accuracy of advertisements running on videos in line with YouTube\u2019s ad friendly policies.\nThrough improving how videos are identified and classified, we\u2019ve enhanced trust in the platform for viewers, creators, and advertisers alike.\nIn recent years, creators started adding chapters to their videos to make it easier for their audience to find the content they were looking for, but this manual process can be slow and laborious.\u00a0\nTo improve the creator and viewer experience, we collaborated with the YouTube Search team and developed an AI system that can automatically process video transcripts, audio and visual features and suggest chapter segments and titles for YouTube creators.\nAs Sundar Pichai introduced at \nGoogle I/O 2022\n, auto-generated chapters are already available for 8M videos today, and we plan to scale this feature to more than 80M auto-generated chapters over the next year.\nUsing AutoChapters, viewers spend less time searching for specific content and creators save time creating chapters for their videos.\nAs society and the technology we use evolves, we\u2019re continuously looking for new ways to help improve everyday Alphabet technologies and products with our AI research.\nOur work with YouTube has already made a great impact, and we hope to make many more significant improvements to people\u2019s lives through our ongoing collaborations.\n"}
{"title": "DeepMind\u2019s latest research at ICML 2022", "contents": "Starting this weekend, the thirty-ninth International Conference on Machine Learning (\nICML 2022\n) is meeting from 17-23 July, 2022 at the Baltimore Convention Center in Maryland, USA, and will be running as a hybrid event.\nResearchers working across artificial intelligence, data science, machine vision, computational biology, speech recognition, and more are presenting and publishing their cutting-edge work in machine learning.\nIn addition to sponsoring the conference and supporting workshops and socials run by our long-term partners \nLatinX\n, \nBlack in AI\n, \nQueer in AI\n, and \nWomen in Machine Learning\n, our research teams are presenting 30 papers, including 17 external collaborations. Here\u2019s a brief introduction to our upcoming oral and spotlight presentations:\nMaking reinforcement learning (RL) algorithms more effective is key to building generalised AI systems. This includes helping increase the accuracy and speed of performance, improve transfer and zero-shot learning, and reduce computational costs.\nIn one of our selected oral presentations, we show a \nnew way to apply generalised policy improvement (GPI)\n over compositions of policies that makes\u00a0 it even more effective in boosting an agent\u2019s performance. Another oral presentation proposed a new grounded and scalable way to \nexplore efficiently without the need of bonuses\n. In parallel, we propose a method for \naugmenting an RL agent with a memory-based retrieval process\n, reducing the agent\u2019s dependence on its model capacity and enabling fast and flexible use of past experiences.\nLanguage is a fundamental part of being human. It gives people the ability to communicate thoughts and concepts, create memories, and build mutual understanding. Studying aspects of language is key to understanding how intelligence works, both in AI systems and in humans.\nOur oral presentation about \nunified scaling laws\n and our paper on \nretrieval\n \nboth explore how we might build larger language models more efficiently. Looking at ways of building more effective language models, we introduce a new dataset and benchmark with \nStreamingQA\n that evaluates how models adapt to and forget new knowledge over time, while our paper on \nnarrative generation\n \nshows how current pretrained language models still struggle with creating longer texts because of short-term memory limitations. \nNeural algorithmic reasoning is the art of building neural networks that can perform algorithmic computations. This growing area of research holds great potential for helping adapt known algorithms to real-world problems.\nWe introduce the \nCLRS benchmark for algorithmic reasoning\n, which evaluates neural networks on performing a diverse set of thirty classical algorithms from the Introductions to Algorithms textbook. Likewise, we propose a \ngeneral incremental learning algorithm\n that adapts hindsight experience replay to automated theorem proving, an important tool for helping mathematicians prove complex theorems. In addition, we present a \nframework for constraint-based learned simulation\n, showing how traditional simulation and numerical methods can be used in machine learning simulators \u2013 a significant new direction for solving complex simulation problems in science and engineering.\u00a0\nSee the full range of our work at ICML 2022 \nhere\n.\n"}
{"title": "Perceiver AR: general-purpose, long-context autoregressive generation", "contents": "Over the last few years, autoregressive Transformers have brought a steady stream of breakthroughs in generative modeling. These models generate each element of a sample \u2013 the pixels of an image, the characters of a text (typically in \u201ctoken\u201d chunks), the samples of an audio waveform, and so on \u2013 by predicting one element after the other. When predicting the next element, the model can look back at those that were created earlier.\nHowever, each of a Transformer\u2019s layers grows more expensive as more elements are used as input, and practitioners can only afford to train deep Transformers on sequences no more than about 2,048 elements in length. And so, most Transformer-based models ignore all elements beyond the most recent past (around 1,500 words or 1/6 of a small image) when making a prediction.\nIn contrast, our recently developed \nPerceiver models\n give excellent results on a variety of real-world tasks with up to around 100,000 elements. Perceivers use cross-attention to encode inputs into a latent space, decoupling\n \nthe input\u2019s compute requirements from model depth. Perceivers also spend a fixed cost, regardless of input size, at nearly every layer. \nWhile latent-space encoding handles all elements in a single pass, autoregressive generation assumes processing happens one element at a time. To address this problem, Perceiver AR proposes a simple solution: align the latents one by one with the final elements of the input, and carefully mask the input so latents see only earlier elements. \nThe result is an architecture (shown above) that attends to as much as 50x longer inputs as standard Transformers, while deploying as widely (and essentially as easily) as standard decoder-only Transformers.\nPerceiver AR scales considerably better with size than both standard Transformers and Transformer-XL models at a range of sequence lengths in real terms. This property allows us to build very effective long-context models. For example, we find that a 60-layer Perceiver AR with context length 8192 outperforms a 42-layer Transformer-XL on a book-length generation task, while running faster in real wall-clock terms.\nOn standard, long-context image (ImageNet 64x64), language (PG-19), and music (MAESTRO) generation benchmarks, Perceiver AR produces state-of-the-art results. Increasing input context by decoupling input size from compute budget leads to several intriguing results: \nUsing a dataset of piano music, we trained Perceiver AR to generate new pieces of music from scratch. Because each new note is predicted based on the full sequence of notes that came before, Perceiver AR is able to produce pieces with a high level of melodic, harmonic, and rhythmic coherence:\nLearn more about using Perceiver AR:\nSee the Google Magenta \nblog post\n with more music!\n"}
{"title": "The virtuous cycle of AI research", "contents": "We recently caught up with Petar Veli\u010dkovi\u0107, a research scientist at DeepMind. Along with his co-authors, Petar is presenting his paper The CLRS Algorithmic Reasoning Benchmark at \nICML 2022\n in Baltimore, Maryland, USA.\u00a0\nThroughout my undergraduate courses at the University of Cambridge, the inability to skilfully play the game of Go was seen as clear evidence of the shortcomings of modern-day deep learning systems. I always wondered how mastering such games might escape the realm of possibility.\u00a0\nHowever, in early 2016, just as I started my PhD in machine learning, that all changed. DeepMind took on one of the best Go players in the world for a \nchallenge match\n, which I spent several sleepless nights watching. DeepMind won, producing ground-breaking gameplay (e.g. \u201cMove 37\u201d) in the process.\u00a0\nFrom that point on, I thought of DeepMind as a company that could make seemingly impossible things happen. So, I focused my efforts on, one day, joining the company. Shortly after submitting my PhD in early 2019, I began my journey as a research scientist at DeepMind!\u00a0\nMy role is a virtuous cycle of learning, researching, communicating, and advising. I\u2019m always actively trying to learn new things (most recently \nCategory Theory\n, a fascinating way of studying computational \nstructure\n), read relevant literature, and watch talks and seminars.\u00a0\nThen using these learnings, I brainstorm with my teammates about how we can broaden this body of knowledge to positively impact the world. From these sessions, ideas are born, and we leverage a combination of theoretical analysis and programming to set and validate our hypotheses. If our methods bear fruit, we typically write a paper sharing insights with the broader community.\u00a0\nResearching a result is not nearly as valuable without appropriately communicating it, and empowering others to effectively make use of it. Because of this, I spend a lot of time presenting our work at conferences like ICML, giving talks, and co-advising students. This often leads to forming new connections and uncovering novel scientific results to explore, setting the virtuous cycle in motion one more time!\nWe\u2019re giving a spotlight presentation on our paper, \nThe CLRS algorithmic reasoning benchmark\n, which we hope will support and enrich efforts in the rapidly emerging area of \nneural algorithmic reasoning\n. In this research, we task graph neural networks with executing thirty diverse algorithms from the \nIntroduction to Algorithms\n textbook.\u00a0\nMany recent research efforts seek to construct neural networks capable of executing algorithmic computation, primarily to endow them with reasoning capabilities \u2013 which neural networks typically lack. Critically, every one of these papers generates its own dataset, which makes it hard to track progress, and raises the barrier of entry into the field.\u00a0\nThe CLRS benchmark, with its readily exposed dataset generators, and \npublicly available code\n, seeks to improve on these challenges. We\u2019ve already seen a great level of enthusiasm from the community, and we hope to channel it even further during ICML.\nThe main dream of our research on algorithmic reasoning is to capture the computation of classical algorithms inside high-dimensional neural executors. This would then allow us to deploy these executors directly over raw or noisy data representations, and hence \u201capply the classical algorithm\u201d over inputs it was never designed to be executed on.\nWhat\u2019s exciting is that this method has the potential to enable data-efficient reinforcement learning. Reinforcement learning is packed with examples of strong classical algorithms, but most of them can\u2019t be applied in standard environments (such as Atari), given that they require access to a wealth of privileged information. Our \nblueprint\n would make this type of application possible by capturing the computation of these algorithms inside neural executors, after which they can be directly deployed over an agent\u2019s internal representations. We even have a working prototype that was published at \nNeurIPS 2021\n. I can\u2019t wait to see what comes next!\u00a0\nI\u2019m looking forward to the \nICML Workshop on Human-Machine Collaboration and Teaming\n, a topic close to my heart. Fundamentally, I believe that the greatest applications of AI will come about through synergy with human domain experts. This approach is also very in line with our recent work on \nempowering the intuition of pure mathematicians using AI\n, which was published on the cover of Nature late last year.\u00a0\nThe workshop organisers invited me for a panel discussion to discuss the broader implications of these efforts. I\u2019ll be speaking alongside a fascinating group of co-panellists, including \nSir Tim Gowers\n, whom I admired during my undergraduate studies at Trinity College, Cambridge. Needless to say, I\u2019m really excited about this panel!\nFor me, major conferences like ICML represent a moment to pause and reflect on diversity and inclusion in our field. While hybrid and virtual conference formats make events accessible to more people than ever before, there\u2019s much more we need to do to make AI a diverse, equitable, and inclusive field. AI-related interventions will impact us all, and we need to make sure that underrepresented communities remain an important part of the conversation.\u00a0\nThis is exactly why I\u2019m teaching a course on \nGeometric Deep Learning\n at the \nAfrican Master\u2019s in Machine Intelligence\n (AMMI) \u2013 a topic of my recently co-authored \nproto-book\n. AMMI offers top-tier machine learning tuition to Africa\u2019s brightest emerging researchers, building a healthy ecosystem of AI practitioners within the region. I\u2019m so happy to have recently met several AMMI students that have gone on to join DeepMind for internship positions.\nI\u2019m also incredibly passionate about outreach opportunities in the Eastern European region, where I originate from, which gave me the scientific grounding and curiosity necessary to master artificial intelligence concepts. The \nEastern European Machine Learning\n (EEML) community is particularly impressive - through its activities, aspiring students and practitioners in the region are connected with world-class researchers and provided with invaluable career advice. This year, I helped bring EEML to my hometown of Belgrade, as one of the lead organisers of the EEML \nSerbian Machine Learning Workshop\n. I hope this is only the first in a series of events to strengthen the local AI community and empower the future AI leaders in the EE region.\n"}
{"title": "Realising scientists are the real superheroes", "contents": "Meet Edgar Du\u00e9\u00f1ez-Guzm\u00e1n, a research engineer on our Multi-Agent Research team who\u2019s drawing on knowledge of game theory, computer science, and social evolution to get AI agents working better together.\nI've wanted to save the world ever since I can remember. That's why I wanted to be a scientist. While I loved superhero stories, I realised scientists are the real superheroes. They are the ones who give us clean water, medicine, and an understanding of our place in the universe. As a child, I loved computers and I loved science. Growing up in Mexico, though, I didn't feel like studying computer science was feasible. So, I decided to study maths, treating it as a solid foundation for computing and I ended up doing my university thesis in game theory.\nAs part of my PhD in computer science, I created biological simulations, and ended up falling in love with biology. Understanding evolution and how it shaped the Earth was exhilarating. Half of my dissertation was in these biological simulations, and I went on to work in academia studying the evolution of social phenomena, like cooperation and altruism.\nFrom there I started working in Search at Google, where I learned to deal with massive scales of computation. Years later, I put all three pieces together: game theory, evolution of social behaviours, and large-scale computation. Now I use those pieces to create artificially intelligent agents that can learn to cooperate amongst themselves, and with us.\nIt was the mid-2010s. I\u2019d been keeping an eye on AI for over a decade and I knew of DeepMind and some of their successes. Then Google acquired it and I was very excited. I wanted in, but I was living in California and DeepMind was only hiring in London. So, I kept tracking the progress. As soon as an office opened in California, I was first in line. I was fortunate to be hired in the first cohort. Eventually, I moved to London to pursue research full time.\nHow ridiculously talented and friendly people are. Every single person I\u2019ve talked to also has an exciting side outside of work. Professional musicians, artists, super-fit bikers, people who appeared in Hollywood movies, maths olympiad winners \u2013 you name it, we have it! And we\u2019re all open and committed to making the world a better place.\n\u200d\nAt the core of my research is making intelligent agents that understand cooperation. Cooperation is the key to our success as a species. We can access the world's information and connect with friends and family on the other side of the world because of cooperation. Our failure to address the catastrophic effects of climate change is a failure of cooperation, as we saw during COP26.\nThe flexibility to pursue the ideas that I think are most important. For example, I\u2019d love to help use our technology for better understanding social problems, like discrimination. I pitched this idea to a group of researchers with expertise in psychology, ethics, fairness, neuroscience, and machine learning, and then created a research programme to study how discrimination might originate in stereotyping.\nDeepMind is one of those places where freedom and potential go hand-in-hand. We have the opportunity to pursue ideas that we feel are important and there\u2019s a culture of open discourse. It\u2019s not uncommon to infect others with your ideas and form a team around making it a reality.\u00a0\nI love getting involved in extracurriculars. I\u2019m a facilitator of Allyship workshops at DeepMind, where we aim to empower participants to take action for positive change and encourage allyship in others, contributing to an inclusive and equitable workplace. I also love making research more accessible and talking with visiting students. I\u2019ve created publicly available \neducational tutorials\n for explaining AI concepts to teenagers, which have been used in summer schools across the world.\nTo have the most positive impact, it simply needs to be that the benefits are shared broadly, rather than kept by a tiny number of people. We should be designing systems that empower people, and that democratise access to technology.\u00a0\nFor example, when I worked on \nWaveNet\n, the new voice of the Google Assistant, I felt it was cool to be working on a technology that is now used by billions of people, in Google Search, or Maps. That's nice, but then we did something better. We started using this technology to give their voice back to people with degenerative disorders, like ALS. There's always opportunities to do good, we just have to take them.\nThere are both practical and societal challenges. On the practical side, we\u2019re hard at work trying to make our algorithms more robust and adaptable. As living creatures, we take robustness and adaptability for granted. Slightly changing the furniture arrangement doesn't cause us to forget what a fridge is for. Artificial systems really struggle with this. There are some promising leads, but we still have a way to go.\u00a0\nOn the societal side, we need to collectively decide what kind of AI we want to create. We need to make sure that whatever is made, is safe and beneficial. But this is particularly hard to achieve when we don't have a perfect definition of what this means.\n\u200d\nRight now I'm still riding the high of \nAlphaFold\n, our protein-folding algorithm. I have a background in biology, and understand how promising protein structure prediction can be for biomedical applications. And I am particularly proud of how DeepMind released the protein structure of all the known proteins in the human body in the global datasets, and now released \nnearly all catalogued proteins\n known to science.\u00a0\nBe playful, be flexible. I couldn\u2019t have optimised for a career leading to DeepMind (there wasn't even a DeepMind to optimise to!) But what I could do was always allow myself to dream of the potential of technology, of creating intelligent machines, and of improving the world with them.\u00a0\nProgramming is exhilarating in its own right, but for me it was always more of a means to an end. This is what enabled me to stay current as technologies came and went. I wasn't tied to the tools, I was focused on the mission. Don't focus on the \"what\", but on the \"why\", and the \"how\" will manifest itself.\n"}
{"title": "Discovering when an agent is present in a system", "contents": "New, formal definition of agency gives clear principles for causal modelling of AI agents and the incentives they face.\nWe want to build safe, aligned artificial general intelligence (AGI) systems that pursue the intended goals of its designers. \nCausal influence diagrams\n (CIDs) are a way to model decision-making situations that allow us to reason about \nagent incentives\n. For example, here is a CID for a 1-step Markov decision process \u2013 a typical framework for decision-making problems.\nBy relating training setups to the incentives that shape agent behaviour, CIDs help illuminate potential risks before training an agent and can inspire better agent designs. But how do we know when a CID is an accurate model of a training setup?\nOur new paper, \nDiscovering Agents\n, introduces new ways of tackling these issues, including:\nCombined, these results provide an extra layer of assurance that a modelling mistake hasn\u2019t been made, which means that CIDs can be used to analyse an agent\u2019s incentives and safety properties with greater confidence.\u00a0\nTo help illustrate our method, consider the following example consisting of a world containing three squares, with a mouse starting in the middle square choosing to go left or right, getting to its next position and then potentially getting some cheese. The floor is icy, so the mouse might slip. Sometimes the cheese is on the right, but sometimes on the left.\nThis can be represented by the following CID:\nThe intuition that the mouse would choose a different behaviour for different environment settings (iciness, cheese distribution) can be captured by a \nmechanised causal graph\n,\n \nwhich for each (object-level) variable, also includes a mechanism variable that governs how the variable depends on its parents. Crucially, we allow for links between mechanism variables.\nThis graph contains additional mechanism nodes in black, representing the mouse's policy and the iciness and cheese distribution.\u00a0\nEdges between mechanisms represent direct causal influence. The blue edges are special \nterminal\n edges \u2013 roughly, mechanism edges A~ \u2192 B~ that would still be there, even if the object-level variable A was altered so that it had no outgoing edges.\u00a0\nIn the example above, since U has no children, its mechanism edge must be terminal. But the mechanism edge X~ \u2192 D~ is not terminal, because if we cut X off from its child U, then the mouse will no longer adapt its decision (because its position won\u2019t affect whether it gets the cheese).\nCausal discovery infers a causal graph from experiments involving interventions. In particular, one can discover an arrow from a variable A to a variable B by experimentally intervening on A and checking if B responds, even if all other variables are held fixed.\nOur first algorithm uses this technique to discover the mechanised causal graph:\nOur second algorithm transforms this mechanised causal graph to a game graph:\nTaken together, Algorithm 1 followed by Algorithm 2 allows us to discover agents from causal experiments, representing them using CIDs.\nOur third algorithm transforms the game graph into a mechanised causal graph, allowing us to translate between the game and mechanised causal graph representations under some additional assumptions:\u00a0\nWe proposed the first formal causal definition of agents. Grounded in causal discovery, our key insight is that agents are systems that adapt their behaviour in response to changes in how their actions influence the world. Indeed, our Algorithms 1 and 2 describe a precise experimental process that can help assess whether a system contains an agent.\u00a0\nInterest in causal modelling of AI systems is rapidly growing, and our research grounds this modelling in causal discovery experiments. Our paper demonstrates the potential of our approach by improving the safety analysis of several example AI systems and shows that causality is a useful framework for discovering whether there is an agent\u00a0 in a system \u2013 a key concern for assessing risks from AGI. \nExcited to learn more? Check out our \npaper\n. Feedback and comments are most welcome.\n"}
{"title": "Events", "contents": ""}
{"title": "Applying for technical roles", "contents": "It\u2019s no secret that the gender gap still exists within STEM. Despite a slight increase in recent years, studies show that women only \nmake up about a quarter\n of the overall STEM workforce in the UK. While the reasons vary, many women \nreport\n feeling held back by a lack of representation, clear opportunities and information on what working in the sector actually involves. \nClosing the gap within STEM is not a quick fix but a collective effort of everyone in the industry. Various organisations like \nWomen in Machine Learning\n (WiML) actively work to help create a more inclusive environment where the successes of women are amplified. They also stand as an important point of information for the many women who want to learn more about what it\u2019s like to work in STEM. \nThat\u2019s why for this year\u2019s \nInternational Women in Engineering Day\n, we asked the WiML community to share with us the most common questions they receive about technical interviewing. To share their perspectives and to discuss what it\u2019s actually like to work at DeepMind, we brought together Mihaela Rosca (Research Engineer), Feryal Behbahani (Research Scientist) and Kate Parkyn (Recruitment Lead - Research & Engineering).\nMihaela: \nIt\u2019s not uncommon to have self doubts or feel as if you\u2019re under prepared for a position in the field. There will never be a perfect time to apply and you can easily convince yourself that there\u2019s more to learn but that shouldn\u2019t be a deterring factor in your decision to apply. \nOf course the right skillset will depend on the specific role you\u2019re after, but if you\u2019re keen to work on the future of machine learning research, read research papers and implement state of the art algorithms - you\u2019re ready...so apply! \nCurious? Learn more about our \nresearch\n and \nengineering\n teams.\nKate: \nWe recruit for many roles across the organisation so the qualities we focus on differ accordingly.\nThe majority of research scientist hires we make are post PhD level, so we don\u2019t over index on publications. We also don\u2019t have a specific marker for degree achievement or GPA. When it comes to experience, we\u2019re always interested in reading about a candidate's past internships and/or voluntary industry experiences. We look for proven ability not only in \u2018research\u2019 but also in implementation, engineering and application. Reading about side projects and open source contributions are also great to see when looking at potential candidates, so feel free to link your Github, side projects or code.\nFor research engineers, it\u2019s important to remember that the role is part research and part engineer, so we\u2019re always looking for people that enjoy putting theory into computational form.\nFor software engineers, we look for the clear ability to communicate problems and solutions. Software engineers at DeepMind regularly deal with ambiguous problems which also have underlying engineering complexities. Evidence of working on similar projects, or experiences in accelerating research and harnessing tools to augment research, is key.\nKate: \nCreating the perfect CV or resume is a big job. Luckily there are a countless number of resources out there that can help you get the job done. To keep it simple, we\u2019d suggest focusing on the following points:\nFeryal: \nThere are a wide range of resources available to help you learn and develop your skills in machine learning. These include open-access introductory courses on YouTube (i.e. \nNando de Freitas\u2019s course on Deep Learning\n, \nDavid Silver\u2019s course on Reinforcement Learning\n and \nDeepMind x UCL Lecture Series\n), blog posts which provide overviews of particular techniques (e.g. \nDistill\n) and more advanced machine learning conference proceedings such as \nNeurIPS\n, \nICML\n and \nICLR\n. \nThere are also a number of summer schools (i.e. \nMLSS\n and \nDLRLSS\n) that help support students and professionals who are interested in learning from leading experts in the field. Many of the summer schools also host videos and practical exercises from previous years which can act as excellent resources for learning at your own pace.\nIt\u2019s also great to look to organisations like \nWomen in Machine Learning (WiML)\n that specifically help women in the field build their technical confidence and voice while amplifying their achievements to the wider community.\nFeryal: \nThe interview process at DeepMind can vary depending on the particular role you\u2019re applying for. From my experience, the interview process for a \nResearch Scientist\n role consisted of four phases: \nThis is to cover your background, experience, the motivation for applying and future plans. At this stage, you will also have the opportunity to ask any questions that you may have about the role or the interview process.\nThis part of the process involves several sessions - including one with a technical quiz \u00a0that covers a large breadth of topics in computer science, statistics, mathematics and machine learning. It\u2019s key that you revise broadly for this session! At this stage there will also be a coding interview where you [in your chosen language] will have to work through a few questions and a specific problem with the end goal of coming to a solution implementation.\nThis stage is made up of various short [i.e. ~30min] interviews with researchers and leads about your specific research background and interests. Here you will have the opportunity to give a talk about your research, which gives the interviewers a better idea of your overall research direction. At this point, try to show your technical understanding of the field and feel free to bring up your own achievements and research ideas. It\u2019s not necessary, but I would also suggest reading through \nrecent papers\n published by the DeepMind team to try to frame your strengths better! \nTowards the end of the interview process, you will once again connect with the recruitment team to discuss DeepMind\u2019s culture and mission. I recommend that you read about DeepMind\u2019s \nmission\n and think about how your career goals can fit within it.\nMihaela: \nDue to the versatility required to do machine learning research, the interview process has a relatively even split between coding and assessing research skills. The first stage focuses on mathematics, statistics, machine learning and computer science knowledge, while later stages focus on coding. Keep in mind that throughout the interview process, the interviewer is trying to assess your problem solving skills, so focus on communication and explain your answers. \nFor my own interview, I prepared by reviewing some of the notes from my university lectures - including a statistics course I had taken. At the time I didn\u2019t know a lot about reinforcement learning, so I did some additional research and watched David Silver\u2019s \nUCL course\n on the topic. For my coding interview, I chose python. To prepare and to practice my speed I solved a few coding questions without using an integrated development environment (IDE) or my favourite editor - only a simple text editor.\nMihaela: \nAbsolutely! Research Engineers at DeepMind - and elsewhere - often lead projects of all sizes. They can lead as first authors of conference papers, or as larger team efforts which involve groups of different sizes and take place over multiple months. \nThere are plenty of examples, but here are a few: \nAlphaZero\n, \nimproving exploration in reinforcement learning using generative modeling\n, and open sourcing of core libraries such as \nReverb\n.\nFeryal: \nBeing a research scientist means that my day never really looks the same. My time is often spent thinking about my research projects, coding, meeting and discussing ideas with others, reading papers and attending presentations or reading groups. \nAs always in research, what I\u2019m doing can change depending on if I\u2019m working towards a paper deadline, working on a specific project, or thinking about what to do next. Luckily DeepMind is really flexible in how one can organise their time and schedule. We use a \u201cmilestone system\u201d which organises research into smaller, measurable chunks (e.g. 3-6 weeks) so this really helps with planning research and breaking it down into concrete steps.\nFind out more about \nWomen in Machine Learning\n (WiML) and \nInternational Women in Engineering Day\n. \nRead more about\n careers at DeepMind\n.\nExplore our\n open roles\n.\nWatch more on our \nYouTube channel\n.\n"}
{"title": "Ethics and Society", "contents": ""}
{"title": "Applied", "contents": ""}
{"title": "Company", "contents": ""}
{"title": "Specification gaming: the flip side of AI ingenuity", "contents": "Specification gaming\n is a behaviour that satisfies the literal specification of an objective without achieving the intended outcome. We have all had experiences with specification gaming, even if not by this name. Readers may have heard the myth of \nKing Midas\n and the golden touch, in which the king asks that anything he touches be turned to gold - but soon finds that even food and drink turn to metal in his hands. In the real world, when rewarded for doing well on a homework assignment, a student might copy another student to get the right answers, rather than learning the material - and thus exploit a loophole in the task specification. \nThis problem also arises in the design of artificial agents. For example, a reinforcement learning agent can find a shortcut to getting lots of reward without completing the task as intended by the human designer. These behaviours are common, and we have \ncollected\n around 60 examples so far (aggregating \nexisting\n \nlists\n and ongoing \ncontributions\n from the AI community). In this post, we review possible causes for specification gaming, share examples of where this happens in practice, and argue for further work on principled approaches to overcoming specification problems.\nLet's look at an example. In a \nLego stacking task\n, the desired outcome was for a red block to end up on top of a blue block. The agent was rewarded for the height of the bottom face of the red block when it is not touching the block. Instead of performing the relatively difficult maneuver of picking up the red block and placing it on top of the blue one, the agent simply flipped over the red block to collect the reward. This behaviour achieved the stated objective (high bottom face of the red block) at the expense of what the designer actually cares about (stacking it on top of the blue one).\nWe can consider specification gaming from two different perspectives. Within the scope of developing reinforcement learning (RL) algorithms, the goal is to build agents that learn to achieve the given objective. For example, when we use Atari games as a benchmark for training RL algorithms, the goal is to evaluate whether our algorithms can solve difficult tasks. Whether or not the agent solves the task by exploiting a loophole is unimportant in this context. From this perspective, specification gaming is a good sign - the agent has found a novel way to achieve the specified objective. These behaviours demonstrate the ingenuity and power of algorithms to find ways to do exactly what we tell them to do.\nHowever, when we want an agent to actually stack Lego blocks, the same ingenuity can pose an issue. Within the broader scope of building \naligned agents\n that achieve the intended outcome in the world, specification gaming is problematic, as it involves the agent exploiting a loophole in the specification at the expense of the intended outcome. These behaviours are caused by misspecification of the intended task, rather than any flaw in the RL algorithm. In addition to algorithm design, another necessary component of building aligned agents is reward design.\nDesigning task specifications (reward functions, environments, etc.) that accurately reflect the intent of the human designer tends to be difficult. Even for a slight misspecification, a very good RL algorithm might be able to find an intricate solution that is quite different from the intended solution, even if a poorer algorithm would not be able to find this solution and thus yield solutions that are closer to the intended outcome. This means that correctly specifying intent can become more important for achieving the desired outcome as RL algorithms improve. It will therefore be essential that the ability of researchers to correctly specify tasks keeps up with the ability of agents to find novel solutions.\nWe use the term \ntask specification\n in a broad sense to encompass many aspects of the agent development process. In an RL setup, task specification includes not only reward design, but also the choice of training environment and auxiliary rewards. The correctness of the task specification can determine whether the ingenuity of the agent is or is not in line with the intended outcome. If the specification is right, the agent's creativity produces a desirable novel solution. This is what allowed AlphaGo to play the famous \nMove 37\n, which took human Go experts by surprise yet which was pivotal in its second match with Lee Sedol. If the specification is wrong, it can produce undesirable gaming behaviour, like flipping the block. These types of solutions lie on a spectrum, and we don't have an objective way to distinguish between them.\nWe will now consider possible causes of specification gaming. One source of reward function misspecification is poorly designed \nreward shaping\n. Reward shaping makes it easier to learn some objectives by giving the agent some rewards on the way to solving a task, instead of only rewarding the final outcome. However, shaping rewards can change the optimal policy if they are not \npotential-based\n. Consider an agent controlling a boat in the \nCoast Runners game\n, where the intended goal was to finish the boat race as quickly as possible. The agent was given a shaping reward for hitting green blocks along the race track, which changed the optimal policy to going in circles and hitting the same green blocks over and over again.\nSpecifying a reward that accurately captures the\n desired final outcome\n can be challenging in its own right. In the Lego stacking task, it is not sufficient to specify that the bottom face of the red block has to be high off the floor, since the agent can simply flip the red block to achieve this goal. A more comprehensive specification of the desired outcome would also include that the top face of the red block has to be above the bottom face, and that the bottom face is aligned with the top face of the blue block. It is easy to miss one of these criteria when specifying the outcome, thus making the specification too broad and potentially easier to satisfy with a degenerate solution. \nInstead of trying to create a specification that covers every possible corner case, we could \nlearn the reward function from human feedback\n. It is often easier to evaluate whether an outcome has been achieved than to specify it explicitly. However, this approach can also encounter specification gaming issues if the reward model does not learn the true reward function that reflects the designer's preferences. One possible source of inaccuracies can be the human feedback used to train the reward model. For example, an agent performing a \ngrasping task\n learned to fool the human evaluator by hovering between the camera and the object.\nThe learned reward model could also be misspecified for other reasons, such as poor generalisation. Additional feedback can be used to correct the agent's attempts to exploit the inaccuracies in the reward model.\nAnother class of specification gaming examples comes from the agent exploiting \nsimulator bugs\n. For example, a \nsimulated robot\n that was supposed to learn to walk figured out how to hook its legs together and slide along the ground.\nAt first sight, these kinds of examples may seem amusing but less interesting, and irrelevant to deploying agents in the real world, where there are no simulator bugs. However, the underlying problem isn\u2019t the bug itself but a failure of abstraction that can be exploited by the agent. In the example above, the robot's task was misspecified because of incorrect assumptions about simulator physics. Analogously, a real-world traffic optimisation task might be misspecified by incorrectly assuming that the traffic routing infrastructure does not have software bugs or security vulnerabilities that a sufficiently clever agent could discover. Such assumptions need not be made explicitly \u2013 more likely, they are details that simply never occurred to the designer. And, as tasks grow too complex to consider every detail, researchers are more likely to introduce incorrect assumptions during specification design. This poses the question: is it possible to design agent architectures that correct for such false assumptions instead of gaming them?\nOne assumption commonly made in task specification is that the task specification cannot be affected by the agent's actions. This is true for an agent running in a sandboxed simulator, but not for an agent acting in the real world. Any task specification has a physical manifestation: a reward function stored on a computer, or preferences stored in the head of a human. An agent deployed in the real world can potentially manipulate these representations of the objective, creating a \nreward tampering\n problem. For our hypothetical traffic optimisation system, there is no clear distinction between satisfying the user's preferences (e.g. by giving useful directions), and \ninfluencing users\n to have preferences that are easier to satisfy (e.g. by nudging them to choose destinations that are easier to reach). The former satisfies the objective, while the latter manipulates the representation of the objective in the world (the user preferences), and both result in high reward for the AI system. As another, more extreme example, a very advanced AI system could hijack the computer on which it runs, manually setting its reward signal to a high value.\nTo sum up, there are at least three challenges to overcome in solving specification gaming:\nWhile many approaches have been proposed, ranging from reward modeling to agent incentive design, specification gaming is far from solved. \nThe list of specification gaming behaviours\n demonstrates the magnitude of the problem and the sheer number of ways the agent can game an objective specification. These problems are likely to become more challenging in the future, as AI systems become more capable at satisfying the task specification at the expense of the intended outcome. As we build more advanced agents, we will need design principles aimed specifically at overcoming specification problems and ensuring that these agents robustly pursue the outcomes intended by the designers. \nWe would like to thank Hado van Hasselt and Csaba Szepesvari for their feedback on this post.\nCustom figures by Paulo Estriga, Aleks Polozuns, and Adam Cain.\n"}
{"title": "Using AI to predict retinal disease progression", "contents": "Vision loss among the elderly is a major healthcare issue: about one in three people have some vision-reducing disease by the age of 65. Age-related macular degeneration (AMD) is the most common cause of blindness in the developed world. In Europe, approximately \n25% of those 60 and older\n have AMD. The \u2018dry\u2019 form is relatively common among people over 65, and usually causes only mild sight loss. However, about 15% of patients with dry AMD go on to develop a more serious form of the disease \u2013 exudative AMD, or exAMD \u2013 which can result in rapid and permanent loss of sight. Fortunately, there are treatments that can slow further vision loss. Although there are no preventative therapies available at present, these are being explored in clinical trials. The period before the development of exAMD may therefore represent a critical window to target for therapeutic innovations: can we predict which patients will progress to exAMD, and help prevent sight loss before it even occurs?\nIn our latest work, published in \nNature Medicine\n, we \ncollaborated with Moorfields Eye Hospital\n and \nGoogle Health\n to curate a dataset of images of eye retinas, train an artificial intelligence (AI) system that could predict the development of exAMD, and conduct a study to evaluate our model compared with expert clinicians. We demonstrate that our system is able to perform as well as, or better than, clinicians at predicting whether an eye will convert to exAMD in the next 6 months. Lastly, we explore the potential clinical applicability of our system. Our contribution highlights the potential of using AI in preventative studies for diseases such as exAMD.\nWe used a dataset of anonymised retinal scans from Moorfields patients with exAMD in one eye, and at high-risk of developing exAMD in their other eye. This comprises 2,795 patients across seven different Moorfields sites in London, with representation across genders, age ranges, and ethnicities. These patients attend the hospital regularly to receive treatment, undergoing high-resolution three-dimensional optical coherence tomography (OCT) imaging of both eyes, at each visit. There is often a delay between when exAMD has developed and when it is diagnosed and treated. To address this, we worked with retinal experts to review all scans for each eye and specify the scan when exAMD was first evident.\nOur system is composed of two deep convolutional neural networks that take as input high-dimensional volumetric eye scans, where each scan consists of 58 million three-dimensional pixels (voxels). In our \nprevious work, now continuing in collaboration with \nGoogle Health, we developed a model capable of segmenting these eye scans into thirteen anatomical categories. The segmented data was combined with the raw scan and both were used as inputs to the prediction model, which was trained to estimate a patient\u2019s risk of conversion to exAMD in their other eye within the next six months.\nThe benefit of a two stage system is that it gives the AI different views of the eye scans. Anatomical segmentation of the images helps the system learn to model risks based on signs of known anatomical indicators such as drusen (small fatty deposits), or loss of the retinal pigment epithelium (which helps to feed and protect other layers of the retina). Providing the raw eye scans allows the model to learn to spot other subtle changes that could become potential risk factors. At the end, the system combines the information it extracts from these scans to predict when and if the eye will progress to exAMD within the next 6 months. We chose this time window to enable the system to predict at least two follow-up intervals ahead of time, assuming a maximal follow-up interval of 3 months. \nIt\u2019s important to establish a benchmark of expert human performance to compare how well our system performs to clinical standards. However, prediction of exAMD is not a routine task performed by clinicians, so it\u2019s unclear whether this task is even possible. To investigate this, we conducted a study with six retinal experts - three ophthalmologists and three optometrists, each with at least ten years of experience - to predict whether an eye will convert to exAMD within the ensuing 6 months. Despite the task\u2019s novelty, the experts performed better than chance alone - however, the task is difficult, and there was substantial variability between their assessments. Our system performed as well as, and in some cases better than, experts in predicting exAMD progression, at the same time exhibiting less variability in agreement with each expert, compared to experts with each other.\nIt may not be enough for a system to simply provide a prediction: clinicians may also ideally seek information regarding the anatomic basis for predictions, which might be of significant use for further interpretation (for example, for designing studies or considering treatments). A benefit of our system is that it automatically segments each scan into known types of tissue. Extracting these anatomical and pathological features provides a systematic method to visualise the change in these tissues over time. The risk scores given by our system align with anatomical changes over time, and together give a richer picture of exAMD conversion.\nWe\u2019re excited by the potential to support clinicians and researchers by developing systems that can help detect retinal diseases earlier and inform the clinical understanding of their progression. A prediction system such as this could be used to inform appropriate follow-up intervals to effectively manage high-risk patients. Our work builds upon promising \nearly work\n to develop \npredictive models\n for \nexAMD\n based on \nretinal photographs\n and OCT \nscans\n. Since beginning our collaboration with Moorfields Eye Hospital in 2016, we\u2019ve published two promising studies highlighting the \npotential of AI\n to transform retinal healthcare. \nHowever, we know there\u2019s still a lot to do \u2013 this work does not yet represent a product that could be implemented in routine clinical practice. While our model can make better predictions than clinical experts, there are many other factors to consider for such systems to be impactful in a clinical setting. While the model was trained and evaluated on a population representative of the largest eye hospital in Europe, additional work would be needed to evaluate performance in the context of very different demographics. \nA recent study examining\n the use of a different AI system in a clinical setting highlighted just some of the sociotechnical issues for such systems in practice. Another difficult point to contend with is that any prediction system will have a certain rate of false positives: that is, when a patient is found to have a condition, or predicted to develop one, that they don\u2019t actually have. The tradeoff of adding an imprecise AI system to an early warning loop could be unnecessarily costly to patients who aren\u2019t actually at risk, and would need to be considered carefully in clinical studies of how such systems might be used in practice. In this paper, we propose two system operating points to balance sensitivity (a measure of how well it correctly identifies the disease) and specificity (a measure of how low the false positive rate is). For example, at a specificity of 90%, a sensitivity of 34% is achieved, meaning that the system correctly identified progression in one third of scans that did go on to progress within 6 months. This could identify a number of patients at high risk with a precision that may be sufficient to inform studies of novel treatment strategies that might mitigate vision loss and improve patient outcomes. \nWe would like to thank Moorfields Eye Hospital and the clinicians who helped curate the data and were involved in our benchmarking study. Please see the paper for all acknowledgements and further details on the work. In addition, we\u2019ve open-sourced the model code for future research, available \nhere\n, and Moorfields will be making the dataset available through the \nRyan Initiative for Macular Research.\nRead the Nature Medicine paper \nhere\n. \nCheck out the github repo \nhere\n.\nFigure design by Paulo Estriga and Adam Cain.\n"}
{"title": "Traffic prediction with advanced Graph Neural Networks", "contents": "By partnering with Google, DeepMind is able to bring the benefits of AI to billions of people all over the world. \u00a0From reuniting a speech-impaired user with his \noriginal voice\n, to helping users discover \npersonalised apps\n, we can apply breakthrough research to immediate real-world problems at a Google scale. Today we\u2019re delighted to share the results of our latest partnership, delivering a truly global impact for the more than one billion people that use Google Maps. \nPeople rely on Google Maps for accurate traffic predictions and estimated times of arrival (ETAs). These are critical tools that are especially useful when you need to be routed around a traffic jam, if you need to notify friends and family that you\u2019re running late, or if you need to leave in time to attend an important meeting. These features are also useful for businesses such as rideshare companies, which use Google Maps Platform to power their services with information about pickup and dropoff times, along with estimated prices based on trip duration. \nResearchers at DeepMind\n have partnered with the Google Maps team to improve the accuracy of real time ETAs by up to 50% in places like Berlin, Jakarta, S\u00e3o Paulo, Sydney, Tokyo, and Washington D.C. by using advanced machine learning techniques including Graph Neural Networks, as the graphic below shows:\nTo calculate ETAs, Google Maps analyses live traffic data for road segments around the world. While this data gives Google Maps an accurate picture of \ncurrent \ntraffic, it doesn\u2019t account for the traffic a driver can expect to see 10, 20, or even 50 minutes into their drive. To accurately predict\n future\n traffic, Google Maps uses machine learning to combine live traffic conditions with historical traffic patterns for roads worldwide. This process is complex for a number of reasons. For example - even though rush-hour inevitably happens every morning and evening, the exact time of rush hour can vary significantly from day to day and month to month. Additional factors like road quality, speed limits, accidents, and closures can also add to the complexity of the prediction model. \nDeepMind partnered with Google Maps to help improve the accuracy of their ETAs around the world. While Google Maps\u2019 predictive ETAs have been consistently accurate for over 97% of trips, we worked with the team to minimise the remaining inaccuracies even further - sometimes by more than 50% in cities like Taichung. To do this at a global scale, we used a generalised machine learning architecture called Graph Neural Networks that allows us to conduct spatiotemporal reasoning by incorporating relational learning biases to model the connectivity structure of real-world road networks. Here\u2019s how it works: \nWe divided road networks into \u201cSupersegments\u201d consisting of multiple adjacent segments of road that share significant traffic volume. Currently, the Google Maps traffic prediction system consists of the following components: (1) a route analyser that processes terabytes of traffic information to construct Supersegments and (2) a novel Graph Neural Network model, which is optimised with multiple objectives and predicts the travel time for each Supersegment.\nThe biggest challenge to solve when creating a machine learning system to estimate travel times using Supersegments is an architectural one. How do we represent dynamically sized examples of connected segments with arbitrary accuracy in such a way that a single model can achieve success?\nOur initial proof of concept began with a straight-forward approach that used the existing traffic system as much as possible, specifically the existing segmentation of road-networks and the associated real-time data pipeline. This meant that a Supersegment covered a set of road segments, where each segment has a specific length and corresponding speed features. At first we trained a single fully connected neural network model for every Supersegment. These initial results were promising, and demonstrated the potential in using neural networks for predicting travel time. However, given the dynamic sizes of the Supersegments, we required a separately trained neural network model for each one. To deploy this at scale, we would have to train millions of these models, which would have posed a considerable infrastructure challenge. This led us to look into models that could handle variable length sequences, such as Recurrent Neural Networks (RNNs). However, incorporating further structure from the road network proved difficult. Instead, we decided to use Graph Neural Networks. In modeling traffic, we\u2019re interested in how cars flow through a network of roads, and Graph Neural Networks can model network dynamics and information propagation.\nOur model treats the local road network as a graph, where each route segment corresponds to a node and edges exist between segments that are consecutive on the same road or connected through an intersection. In a Graph Neural Network, a message passing algorithm is executed where the messages and their effect on edge and node states are learned by neural networks. From this viewpoint, our Supersegments are road subgraphs, which were sampled at random in proportion to traffic density. A single model can therefore be trained using these sampled subgraphs, and can be deployed at scale.\nGraph Neural Networks extend the learning bias imposed by Convolutional Neural Networks and Recurrent Neural Networks by generalising the concept of \u201cproximity\u201d, allowing us to have arbitrarily complex connections to handle not only traffic ahead or behind us, but also along adjacent and intersecting roads. In a Graph Neural Network, adjacent nodes pass messages to each other. By keeping this structure, we impose a locality bias where nodes will find it easier to rely on adjacent nodes (this only requires one message passing step). These mechanisms allow Graph Neural Networks to capitalise on the connectivity structure of the road network more effectively. Our experiments have demonstrated gains in predictive power from expanding to include adjacent roads that are not part of the main road. For example, think of how a jam on a side street can spill over to affect traffic on a larger road. By spanning multiple intersections, the model gains the ability to natively predict delays at turns, delays due to merging, and the overall traversal time in stop-and-go traffic. This ability of Graph Neural Networks to generalise over combinatorial spaces is what grants our modeling technique its power. Each Supersegment, which can be of varying length and of varying complexity - from simple two-segment routes to longer routes containing hundreds of nodes - can nonetheless be processed by the \nsame\n Graph Neural Network model. \nA big challenge for a production machine learning system that is often overlooked in the academic setting involves the large variability that can exist across multiple training runs of the same model. While small differences in quality can simply be discarded as poor initialisations in more academic settings, these small inconsistencies can have a large impact when added together across millions of users. As such, making our Graph Neural Network robust to this variability in training took center stage as we pushed the model into production. We discovered that Graph Neural Networks are particularly sensitive to changes in the training curriculum - the primary cause of this instability being the large variability in graph structures used during training. A single batch of graphs could contain anywhere from small two-node graphs to large 100+ nodes graphs.\nAfter much trial and error, however, we developed an approach to solve this problem by adapting a novel reinforcement learning technique for use in a supervised setting.\nIn training a machine learning system, the learning rate of a system specifies how \u2018plastic\u2019 \u2013 or changeable to new information \u2013 it is. Researchers often reduce the learning rate of their models over time, as there is a tradeoff between learning new things, and forgetting important features already learned\u2013not unlike the progression from childhood to adulthood. We initially made use of an exponentially decaying learning rate schedule to stabilise our parameters after a pre-defined period of training. We also explored and analysed model ensembling techniques which have proven effective in previous work to see if we could reduce model variance between training runs.\nIn the end, the most successful approach to this problem was using \nMetaGradients\n to dynamically adapt the learning rate during training - effectively letting the system learn its own optimal learning rate schedule. By automatically adapting the learning rate while training, our model not only achieved higher quality than before, it also learned to decrease the learning rate automatically. This led to more stable results, enabling us to use our novel architecture in production. \nWhile the ultimate goal of our modeling system is to reduce errors in travel estimates, we found that making use of a linear combination of multiple loss functions (weighted appropriately) greatly increased the ability of the model to generalise. Specifically, we formulated a multi-loss objective making use of a regularising factor on the model weights, L_2 and L_1 losses on the global traversal times, as well as individual Huber and negative-log likelihood (NLL) losses for each node in the graph. By combining these losses we were able to guide our model and avoid overfitting on the training dataset. While our measurements of quality in training did not change, improvements seen during training translated more directly to held-out tests sets and to our end-to-end experiments.\nCurrently we are exploring whether the MetaGradient technique can also be used to vary the composition of the multi-component loss-function during training, using the reduction in travel estimate errors as a guiding metric. This work is inspired by the MetaGradient efforts that have found success in reinforcement learning, and early experiments show promising results.\nThanks to our close and fruitful collaboration with the Google Maps team, we were able to apply these novel and newly developed techniques at scale. Together, we were able to overcome both research challenges as well as production and scalability problems. In the end, the final model and techniques led to a successful launch, improving the accuracy of ETAs on Google Maps and Google Maps Platform APIs around the world.\nWorking at Google scale with cutting-edge research represents a unique set of challenges. If you\u2019re interested in applying cutting edge techniques such as Graph Neural Networks to address real-world problems, learn more about the team working on these problems \nhere\n.\nIn collaboration with: Marc Nunkesser, Seongjae Lee, Xueying Guo, Austin Derrow-Pinion, David Wong, Peter Battaglia, Todd Hester, Petar Veli\u010dkovi\u0107\u200e, Vishal Gupta, Ang Li, Zhongwen Xu, Geoff Hulten, Jeffrey Hightower, Luis C. Cobo, Praveen Srinivasan & Harish Chandran.\nFigures by Paulo Estriga & Adam Cain.\n"}
{"title": "Fast reinforcement learning through the composition of behaviours", "contents": "Imagine if you had to learn how to chop, peel and stir all over again every time you wanted to learn a new recipe. In many machine learning systems, agents often have to learn entirely from scratch when faced with new challenges. It\u2019s clear, however, that people learn more efficiently than this: they can combine abilities previously learned. In the same way that a finite dictionary of words can be reassembled into sentences of near infinite meanings, people repurpose and re-combine skills they already possess in order to tackle novel challenges.\nIn nature, learning arises as an animal explores and interacts with its environment in order to gather food and other rewards. This is the paradigm captured by \nreinforcement learning\n (RL): interactions with the environment reinforce or inhibit particular patterns of behavior depending on the resulting reward (or penalty). Recently, the combination of RL with \ndeep learning\n has led to impressive results, such as agents that can learn how to play boardgames like \nGo\n and \nchess\n, the full spectrum of \nAtari\n games, as well as more modern, difficult video games like \nDota\n and \nStarCraft II\n.\nA major limitation in RL is that current methods require vast amounts of training experience. For example, in order to learn how to play a single Atari game, an RL agent typically consumes an amount of data corresponding to several weeks of uninterrupted playing. A \nstudy\n led by researchers at MIT and Harvard indicated that in some cases, humans are able to reach the same performance level in just fifteen minutes of play.\nOne possible reason for this discrepancy is that, unlike humans, RL agents usually learn a new task from scratch. We would like our agents to leverage knowledge acquired in previous tasks to learn a new task more quickly, in the same way that a cook will have an easier time learning a new recipe than someone who has never prepared a dish before. In \nan article\n recently published in the Proceedings of the National Academy of Sciences (PNAS), we describe a framework aimed at endowing our RL agents with this ability.\nTwo ways of representing the world\nTo illustrate our approach, we will explore an example of an activity that is (or at least used to be) an everyday routine: the commute to work. Imagine the following scenario: an agent must commute every day from its home to its office, and it always gets a coffee on the way. There are two cafes between the agent's house and the office: one has great coffee but is on a longer path, and the other one has decent coffee but a shorter commute (Figure 1). Depending on how much the agent values the quality of the coffee versus how much of a rush it is in on a given day, it may choose one of two routes (the yellow and blue paths on the map shown in Figure 1).\nTraditionally, RL algorithms fall into two broad categories: \nmodel-based and model-free agents\n (Figures 2 & 3). A model-based agent (Figure 2) builds a representation of many aspects of the \u00a0environment. An agent of this type might know how the different locations are connected, the quality of the coffee in each cafe, and anything else that is considered relevant. A model-free agent (Figure 3) has a much more compact representation of its environment. For instance, a value-based model-free agent would have a single number associated with each possible route leaving its home; this is the expected \"value\" of each route, reflecting a specific weighing of coffee quality vs. commute length. Take the blue path shown in Figure 1 as an example. Say this path has length 4, and the coffee the agent gets by following it is rated 3 stars. If the agent cares about the commute distance 50% more than it cares about the quality of the coffee, the value of this path will be \u00a0(-1.5 x 4) + (1 x 3) = -3 \u00a0(we use a negative weight associated with the distance to indicate that longer commutes are undesirable).\nWe can interpret the relative weighting of the coffee quality versus the commute distance as the agent\u2019s \npreferences\n. For any fixed set of preferences, a model-free and a model-based agent would choose the same route. Why then have a more complicated representation of the world, like the one used by a model-based agent, if the end result is the same? Why learn so much about the environment if the agent ends up sipping the same coffee?\nPreferences can change day to day: an agent might take into account how hungry it is, or whether it\u2019s running late to a meeting, in planning its route to the office. One way for a model-free agent to handle this is to learn the best route associated with every possible set of preferences. This is not ideal because learning every possible combination of preferences will take a long time. It is also impossible to learn a route associated with every possible set of preferences if there are infinitely many of them.\nIn contrast, a model-based agent can adapt to any set of preferences, without any learning, by \"imagining\" all possible routes and asking how well they would fulfill its current mindset. However, this approach also has drawbacks. Firstly, \u201dmentally\u201d generating and evaluating all possible trajectories can be computationally demanding. Secondly, building a model of the entire world can be very difficult in complex environments.\nModel-free agents learn faster but are brittle to change. Model-based agents are flexible but can be slow to \u00a0learn. Is there an intermediate solution?\nA recent \nstudy\n in behavioural science and neuroscience suggests that in certain situations, humans and animals make decisions based on an algorithmic model that is a compromise between the model-free and model-based approaches (\nhere\n and \nhere\n). The hypothesis is that, like model-free agents, humans also compute the value of alternative strategies in the form of a number. But, instead of summarising a single quantity, humans summarise many different quantities describing the world around them, reminiscent of model-based agents.\nIt\u2019s possible to endow an RL agent with the same ability. In our example, such an agent would have, for each route, a number representing the expected quality of coffee and a number representing the distance to the office. It could also have numbers associated with things the agent is not deliberately trying to optimise but are nevertheless available to it for future reference (for example, the quality of the food in each cafe). The aspects of the world the agent cares about and keeps track of are sometimes referred to as \u201cfeatures\u201d. Because of that, this representation of the world is called \nsuccessor features\n (previously termed the \u201csuccessor representation\u201d in its \noriginal incarnation\n).\nSuccessor features can be thought of as a middle ground between the model-free and model-based representations. Like the latter, successor features summarise many different quantities, capturing the world beyond a single value. However, like in the model-free representation, the quantities the agent keeps track of are simple statistics summarising the features it cares about. In this way, successor features are like an \u201cunpacked\u201d version of the model-free agent. Figure 4 illustrates how an agent using successor features would see our example environment.\nSuccessor features are a useful representation because they allow for a route to be evaluated under different sets of preferences. Let\u2019s use the blue route in Figure 1 as an example again. Using successor features, the agent would have three numbers associated with this path: its length (4), the quality of the coffee (3) and the quality of the food (5). If the agent already ate breakfast it will probably not care much about the food; also, if it is late, it might care about the commute distance more than the quality of the coffee --say, 50% more, as before. In this scenario the value of the blue path would be \u00a0(-1.5 x 4) + (1 x 3) + (0 x 5) = -3, as in the example given above. But now, on a day when the agent is hungry, and thus cares about the food as much as it cares about the coffee, it can immediately update the value of this route to \u00a0(-1.5 x 4) + (1 x 3) + (1 x 5) = 2. Using the same strategy, the agent can evaluate any route according to any set of preferences.\nIn our example, the agent is choosing between routes. More generally, the agent will be searching for a policy: a prescription of what to do in every possible situation. Policies and routes are closely related: in our example, a policy that chooses to take the road to cafe A from home and then chooses the road to the office from cafe A would traverse the blue path. So, in this case, we can talk about policies and routes interchangeably (this would not be true if there were some randomness in the environment, but we will leave this detail aside). \u00a0We discussed how successor features allow a route (or policy) to be evaluated under different sets of preferences. We call this process \ngeneralised policy evaluation\n, or GPE.\nWhy is GPE useful? Suppose the agent has a dictionary of policies (for example, known routes to the office). Given a set of preferences, the agent can use GPE to immediately evaluate how well each policy in the dictionary would perform under those preferences. Now the really interesting part: based on this quick evaluation of known policies, \nthe agent can create entirely new policies on the fly.\n The way it does it is simple: every time the agent has to make a decision, it asks the following question: \u201cif I were to make this decision and then follow the policy with the maximum value thereafter, which decision would lead to the maximum overall value?\u201d Surprisingly, if the agent picks the decision leading to the maximum overall value in each situation, it ends up with a policy that is often better than the individual policies used to create it.\nThis process of \u201cstitching together\u201d a set of policies to create a better policy is called \ngeneralised policy improvement\n, or GPI. Figure 5 illustrates how GPI works using our running example.\nThe performance of a policy created through GPI will depend on how many policies the agent knows. For instance, in our running example, as long as the agent knows the blue and yellow \u00a0paths, it will find the best route for any preferences over coffee quality and commute length. But the GPI policy will not always find the best route. In Figure 1, the agent would never visit cafe A and then cafe B if it did not already know a policy that connected them in this way (like the orange route in the figure).\nA simple example to show GPE and GPI in action \nTo illustrate the benefits of GPE and GPI, we now give a glimpse of one of the experiments from our recent publication (see \npaper\n for full details). The experiment uses a simple environment that represents in an abstract way the type of problem in which our approach can be useful. As shown in Figure 6, the environment is a 10 x 10 grid with 10 objects spread across it. The agent only gets a non-zero reward if it picks up an object, in which case another object pops up in a random location. The reward associated with an object depends on its type. Object types are meant to represent concrete or abstract concepts; to connect with our running example, we will consider that each object is either \u201ccoffee\u201d or \u201cfood\u201d (these are the features the agent keeps track of).\nClearly, the best strategy for the agent depends on its current preferences over coffee or food. For example, in Figure 6, an agent that only cares about coffee may follow the path in red, while an agent focused exclusively on food would follow the blue path. We can also imagine intermediate situations in which the agent wants coffee and food with different weights, including the case in which the agent wants to avoid one of them. For example, if the agent wants coffee but really does not want food, the gray path in Figure 6 may be a better alternative to the red one.\nThe challenge in this problem is to quickly adapt to a new set of preferences (or a \u201ctask\u201d). In our experiments we showed how one can do so using GPE and GPI. Our agent learned two policies: one that seeks coffee and one that seeks food. We then tested how well the policy computed by GPE and GPI performed on tasks associated with different preferences. In figure 7 we compare our method with a model-free agent on the task whose goal is to seek coffee while avoiding food. Observe how the agent using GPE and GPI instantaneously synthesises a reasonable policy, even though it never learned how to deliberately avoid objects. Of course, the policy computed by GPE and GPI can be used as an initial solution to be later refined through learning, which means that it would match the final performance of a model-free agent but would probably get there faster.\nFigure 7 shows the performance of GPE and GPI on one specific task. We have also tested the same agent across many other tasks. Figure 8 shows what happens with the performance of the model-free and GPE-GPI agents when we change the relative importance of coffee and food. Note that, while the model-free agent has to learn each task separately, from scratch, the GPE-GPI agent only learns two policies and then quickly adapts to all of the tasks.\nThe experiments above used a simple environment designed to exhibit the properties needed by GPE and GPI without unnecessary confounding factors. But GPE and GPI have also been applied at scale. For example, in previous papers (\nhere\n and \nhere\n) we showed how the same strategy also works when we replace a grid world with a three dimensional environment in which the agent receives observations from a first-person perspective (see illustrative videos \nhere\n and \nhere\n). We have also used GPE and GPI to allow a four-legged simulated robot to navigate along any direction after having learned how to do so along three directions only (see paper \nhere\n and video \nhere\n).\nThe work on GPE and GPI is at the intersection of two separate branches of research related to these operations individually. The first, related to GPE, is the work on the successor representation, initiated with Dayan\u2019s \nseminal paper\n from 1993. Dayan\u2019s paper inaugurated a line of work in neuroscience that is very active to this day (see further reading: \"The successor representation in neuroscience\"). Recently, the successor representation reemerged in the context of RL (links \nhere\n and \nhere\n), where it is also referred to as \u201csuccessor features\u201d, and became an active line of research there as well (see further reading: \"GPE, successor features, and related approaches\"). Successor features are also closely related to \ngeneral value functions\n, a concept based on Sutton et al.\u2019s hypothesis that relevant knowledge can be expressed in the form of many predictions about the world (also discussed \nhere\n). The definition of successor features has independently emerged in \nother contexts\n within RL, and is also related to more \nrecent approaches\n normally associated with deep RL.\nThe second branch of research at the origins of GPE and GPI, related to the latter, is concerned with composing behaviours to create new behaviours. The idea of a decentralised controller that executes sub-controllers has come up multiple times over the years (e.g., \nBrooks, 1986\n), and its implementation using value functions can be traced back to at least as far as 1997, with \nHumphrys\u2019\n and \nKarlsson\u2019s\n PhD theses. GPI is also closely related to hierarchical RL, whose foundations were laid down in the 1990's and early 2000\u2019s in the works by \nDayan and Hinton\n, \nParr and Russell\n, \nSutton, Precup and Singh\n, and \nDietterich\n. Both the composition of behaviours and hierarchical RL are today dynamic areas of research (see further reading: \"GPI, hierarchical RL, and related approaches\").\nMehta \net al\n.\n were probably the first ones to jointly use GPE and GPI, although in the scenario they considered GPI reduces to a single choice at the outset (that is, there is no \u201cstitching\u201d of policies). The version of GPE and GPI discussed in this blog post was first \nproposed\n in 2016 as a mechanism to promote \ntransfer learning\n. Transfer in RL dates back to \nSingh\u2019s work\n in 1992 and has recently experienced a \nresurgence\n in the context of deep RL, where it continues to be an active area of research (see further reading: \"GPE + GPI, transfer learning, and related approaches\").\nSee more information about these works below, where we also provide a list of suggestions for further readings.\nIn summary, a model-free agent cannot easily adapt to new situations, for example to accommodate sets of preferences it has not experienced before. A model-based agent can adapt to any new situation, but in order to do so it first has to learn a model of the entire world. An agent based on GPE and GPI offers an intermediate solution: although the model of the world it learns is considerably smaller than that of a model-based agent, it can quickly adapt to certain situations, often with good performance.\nWe discussed specific instantiations of GPE and GPI, but these are in fact more general concepts. At an abstract level, an agent using GPE and GPI proceeds in two steps. First, when faced with a new task, it asks: \u201cHow well would solutions to known tasks perform on this new task?\u201d This is GPE. Then, based on this evaluation, the agent combines the previous solutions to construct a solution for the new task --that is, it performs GPI. The specific mechanics behind GPE and GPI are less important than the principle itself, and finding alternative ways to carry out these operations may be an exciting research direction. Interestingly, a new \nstudy\n in behavioural sciences provides preliminary evidence that humans make decisions in multitask scenarios following a principle that closely resembles GPE and GPI.\nThe fast adaptation provided by GPE and GPI is promising for building faster learning RL agents. More generally, it suggests a new approach to learning flexible solutions to problems. \u00a0Instead of tackling a problem as a single, monolithic, task, an agent can break it down into smaller, more manageable, sub-tasks. The solutions of the sub-tasks can then be reused and recombined to solve the overall task faster. This results in a compositional approach to RL that may lead to more scalable agents. At the very least, these agents will not be late because of a cup of coffee.\nRead the paper, as first published in PNAS, \nhere\n.\nWith thanks to Jim Kynvin, Adam Cain and Dominic Barlow for the figures, Kimberly Stachenfeld for the pointers to the neuroscience literature, and Kelly Clancy for the help with the text.\nGPE, successor features, and related approaches\nImproving Generalisation for Temporal Difference Learning: The Successor Representation.\n Peter Dayan. Neural Computation, 1993.\nApprenticeship Learning Via Inverse Reinforcement Learning.\n Pieter Abbeel and Andrew Y. Ng. Proceedings of the International Conference on Machine learning (ICML), 2004.\nHorde: A Scalable Real-time Architecture for Learning Knowledge from Unsupervised Sensorimotor Interaction.\n Richard S. Sutton, Joseph Modayil, Michael Delp, Thomas Degris, Patrick M. Pilarski, Adam White. Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2011.\nMulti-timescale Nexting in a Reinforcement Learning Robot.\n Joseph Modayil, Adam White, Richard S. Sutton. From Animals to Animats, 2012.\nUniversal Value Function Approximators.\n Tom Schaul, Dan Horgan, Karol Gregor, David Silver. Proceedings of the International Conference on Machine learning (ICML), 2015.\nDeep Successor Reinforcement Learning.\n Tejas D. Kulkarni, Ardavan Saeedi, Simanta Gautam, Samuel J. Gershman. arXiv, 2017.\nVisual Semantic Planning Using Deep Successor Representations.\n Yuke Zhu, Daniel Gordon, Eric Kolve, Dieter Fox, Li Fei-Fei, Abhinav Gupta, Roozbeh Mottaghi, Ali Farhadi. Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2017.\nDeep Reinforcement Learning with Successor Features for Navigation Across Similar Environments.\n Jingwei Zhang, Jost Tobias Springenberg, Joschka Boedecker, Wolfram Burgard. IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2017.\nUniversal Successor Representations for Transfer Reinforcement Learning.\n Chen Ma, Junfeng Wen, Yoshua Bengio. arXiv, 2018.\nEigenoption Discovery through the Deep Successor Representation.\n Marlos C. Machado, Clemens Rosenbaum, Xiaoxiao Guo, Miao Liu, Gerald Tesauro, Murray Campbell. International Conference on Learning Representations (ICLR), 2018.\nSuccessor Options: An Option Discovery Framework for Reinforcement Learning.\n Rahul Ramesh, Manan Tomar, Balaraman Ravindran. Proceedings of the \u00a0International Joint Conference on Artificial Intelligence (IJCAI), 2019.\nSuccessor Uncertainties: Exploration and Uncertainty in Temporal Difference Learning.\n David Janz, Jiri Hron, Przemys\u0142aw Mazur, Katja Hofmann, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Sebastian Tschiatschek. Advances in Neural Information Processing Systems (NeurIPS), 2019.\nSuccessor Features Combine Elements of Model-Free and Model-based Reinforcement Learning.\n Lucas Lehnert, Michael L. Littman. arXiv, 2019.\nCount-Based Exploration with the Successor Representation.\n Marlos C. Machado, Marc G. Bellemare, Michael Bowling. Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2020.\nGPI, hierarchical RL, and related approaches\nA Robust Layered Control System for a Mobile Robot.\n R. Brooks. IEEE Journal on Robotics and Automation, 1986.\nFeudal Reinforcement Learning.\n Peter Dayan and Geoffrey E. Hinton. Advances in Neural Information Processing Systems (NIPS), 1992.\nAction Selection Methods Using Reinforcement Learning.\n Mark Humphrys. PhD thesis, University of Cambridge, Cambridge, UK, 1997.\nLearning to Solve Multiple Goals.\n Jonas Karlsson. PhD thesis, University of Rochester, Rochester, New York, 1997.\nReinforcement Learning with Hierarchies of Machines.\n Ronald Parr and Stuart J. Russell. Advances in Neural Information Processing Systems (NIPS), 1997.\nBetween MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning.\n Richard S.Sutton, DoinaPrecup, Satinder Singh. Artificial Intelligence, 1999.\nHierarchical Reinforcement Learning with the MAXQ Value Function Decomposition. \nT. G. Dietterich. Journal of Artificial Intelligence Research, 2000.\nMultiple-Goal Reinforcement Learning with Modular Sarsa(O).\n Nathan Sprague and \u00a0Dana Ballard. Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), 2003.\nQ-decomposition for Reinforcement Learning Agents.\n Stuart J. Russell and Andrew Zimdars. \u00a0Proceedings of the International Conference on Machine Learning (ICML), 2003.\nCompositionality of Optimal Control Laws.\n E. Todorov. Advances in Neural Information Processing Systems (NIPS), 2009.\nLinear Bellman combination for control of character animation.\n M. da Silva, F. Durand, and J. Popovic. ACM Transactions on Graphics, 2009.\nHierarchy Through Composition with Multitask LMDPS.\n A. M. Saxe, A. C. Earle, and B. Rosman. Proceedings of the International Conference on Machine Learning (ICML), 2017.\nHybrid Reward Architecture for Reinforcement Learning.\n Harm van Seijen, Mehdi Fatemi, Joshua Romoff, Romain Laroche, Tavian Barnes, and Jeffrey Tsang. Advances in Neural Information Processing Systems (NIPS), 2017.\nFeudal Networks for Hierarchical Reinforcement Learning.\n Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max Jaderberg, David Silver, Koray Kavukcuoglu. \u00a0Proceedings of the International Conference on Machine Learning (ICML), 2017.\nComposable Deep Reinforcement Learning for Robotic Manipulation.\n T. Haarnoja, V. Pong, A. Zhou, M. Dalal, P. Abbeel, and S. Levine. IEEE International Conference on Robotics and Automation (ICRA), 2018.\nComposing Value Functions in Reinforcement Learning.\n Benjamin Van Niekerk, Steven James, Adam Earle, Benjamin Rosman. Proceedings of the International Conference on Machine Learning (ICML), 2019.\nPlanning in Hierarchical Reinforcement Learning: Guarantees for Using Local Policies.\n Tom Zahavy, Avinatan Hasidim, Haim Kaplan, Yishay Mansour. International Conference on Algorithmic Learning Theory (ALT), 2020.\nGPE + GPI, transfer learning, and related approaches\nTransfer of Learning by Composing Solutions of Elemental Sequential Tasks.\n Satinder Singh. Machine Learning, 1992.\nTransfer Learning for Reinforcement Learning Domains: A Survey.\n Matthew E. Taylor and Peter Stone. Journal of Machine Learning Research, 2009.\nTransfer in Variable-Reward Hierarchical Reinforcement Learning.\n Neville Mehta, Sriraam Natarajan, Prasad Tadepalli, Alan Fern. Machine Learning, 2008.\nLearning and Transfer of Modulated Locomotor Controllers.\n Nicolas Heess, Greg Wayne, Yuval Tassa, Timothy Lillicrap, Martin Riedmiller, David Silver. arXiv, 2016.\nLearning to Reinforcement Learn.\n Jane X. Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z. Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, Matt Botvinick. arXiv, 2016.\nRL2: Fast Reinforcement Learning via Slow Reinforcement Learning.\n Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, Pieter Abbeel. arXiv, 2016.\nModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\n Chelsea Finn, Pieter Abbeel, Sergey Levine. Proceedings of the International Conference on Machine Learning (ICML), 2017.\nSuccessor Features for Transfer in Reinforcement Learning.\n Andr\u00e9 Barreto, Will Dabney, R\u00e9mi Munos, Jonathan J. Hunt, Tom Schaul, Hado van Hasselt, David Silver. \u00a0Advances in Neural Information Processing Systems (NIPS), 2017.\nTransfer in Deep Reinforcement Learning Using Successor Features and Generalised Policy Improvement.\n Andr\u00e9 Barreto, Diana Borsa, John Quan, Tom Schaul, David Silver, Matteo Hessel, Daniel Mankowitz, Augustin \u017d\u00eddek, R\u00e9mi Munos. Proceedings of the International Conference on Machine Learning (ICML), 2018.\nComposing Entropic Policies Using Divergence Correction.\n Jonathan Hunt, Andr\u00e9 Barreto, Timothy Lillicrap, Nicolas Heess. Proceedings of the International Conference on Machine Learning (ICML), 2019.\nUniversal Successor Features Approximators.\n Diana Borsa, Andr\u00e9 Barreto, John Quan, Daniel Mankowitz, R\u00e9mi Munos, Hado van Hasselt, David Silver, Tom Schaul. International Conference on Learning Representations (ICLR), 2019.\nThe Option Keyboard: Combining Skills in Reinforcement Learning.\n Andr\u00e9 Barreto, Diana Borsa, \u00a0Shaobo Hou, Gheorghe Comanici, Eser Ayg\u00fcn, Philippe Hamel, Daniel Toyama, Jonathan J. Hunt, Shibl Mourad, David Silver, Doina Precup. Advances in Neural Information Processing Systems (NeurIPS), 2019.\nTransfer Learning in Deep Reinforcement Learning: A Survey.\n Zhuangdi Zhu, Kaixiang Lin, Jiayu Zhou, arXiv, 2020.\nFast Task Inference with Variational Intrinsic Successor Features.\n Steven Hansen, Will Dabney, Andr\u00e9 Barreto, Tom Van de Wiele, David Warde-Farley, Volodymyr Mnih. International Conference on Learning Representations (ICLR), 2020.\nFast Reinforcement Learning with Generalized Policy Updates.\n Andr\u00e9 Barreto, Shaobo Hou, Diana Borsa, David Silver, Doina Precup. Proceedings of the National Academy of Sciences, 2020.\nThe successor representation in neuroscience\nThe Hippocampus as a Predictive Map.\n Kimberly Stachenfeld, Matthew Botvinick, Samuel Gershman. Nature Neuroscience, 2017.\nThe Successor Representation in Human Reinforcement Learning.\n I. Momennejad, E. M. Russek, J. H. Cheong, M. M. Botvinick, N. D. Daw, S. J. Gershman. \u00a0Nature Human Behaviour, 2017.\nPredictive Representations Can Link Model-Based Reinforcement Learning to Model-Free Mechanisms.\n E. Russek, I. Momennejad, M. M. Botvinick, S. J. Gershman, N. D. Daw. PLOS Computational Biology, 2017.\nThe Successor Representation: Its Computational Logic and Neural Substrates.\n Samuel J. Gershman. Journal of Neuroscience, 2018.\nBetter Transfer Learning with Inferred Successor Maps.\n Tamas J. Madarasz, Timothy E. Behrens. Advances in Neural Information Processing Systems (NeurIPS), 2019.\nMulti-Task Reinforcement Learning in Humans.\n Momchil S. Tomov, Eric Schulz, and Samuel J. Gershman. bioRxiv, 2019.\nA neurally plausible model learns successor representations in partially observable environments.\n Eszter Vertes, Maneesh Sahani. Advances in Neural Information Processing Systems (NeurIPS), 2019.\nNeurobiological Successor Features for Spatial Navigation.\n William de Cothi, Caswell Barry. Hippocampus, 2020.\nLinear Reinforcement Learning: Flexible Reuse of Computation in Planning, Grid Fields, and Cognitive Control.\n Payam Piray, Nathaniel D. Daw. bioRxiv, 2020.\n"}
{"title": "Breaking down global barriers to access", "contents": "This week, we welcomed our biggest and most geographically diverse cohort of DeepMind scholars yet. We\u2019re excited to reflect on the journey so far, share more on the next chapter of the DeepMind scholarships \u2013 and welcome many more universities from around the world into the programme. \nAI could be one of the most useful and transformative technologies in history - and the mission to build safe and beneficial AI spans a broad community. We established our scholarship programme in 2017 in an effort to help build a stronger and more inclusive AI community, who can bring a wider range of experiences to the fields of AI and computer science. The scholarships provide financial support to students from underrepresented groups seeking to study graduate courses relating to AI and adjacent fields. But of course, financial barriers are not the only obstacles that students can face, so in addition, every scholar is matched with a personal DeepMind mentor, who can support their aspirations and help them to navigate academic life.\nWe started with eight fantastic scholars who were studying masters courses in the UK and US. The scholarships were awarded to academically excellent students who belong to groups currently underrepresented in AI. This week, we welcomed more than 50 scholars to our 2020 cohort alone.\nIncreasing representation in AI offers a huge opportunity to bring diverse values, hopes and concerns into conversations about the design and deployment of AI \u2013 and this is critical if AI is going to be a technology that benefits everyone. Take alumnus \nscholar\n Shaquille, who wanted to use machine learning to better understand sickle cell anaemia, a disease which disproportionately affects Black people.\nAs we celebrated our growing community, we considered the goal of the programme \u2013 to contribute to building a stronger and more inclusive AI ecosystem - and we reflected on who was excluded from it.\nOur scholarships aim to support underrepresented students \u2013 spanning gender, race, ethnicity, and socio-economic background. But imbalances in the field aren\u2019t just social, they\u2019re also geographical. Last year, 70% of all AI-related research was \npublished\n in Europe, the US, and China, while many other important regions and countries are significantly underrepresented. For instance, only \n0.3%\n of AI journal citations came from sub-Saharan Africa between 2014-2018, and a number of Eastern European countries are entirely absent from \npublication figures\n. This imbalance risks creating a technology that only accounts for the values, hopes and concerns of a narrow group, entrenching global inequalities while seeing large parts of the world miss out on the potential of AI to improve people\u2019s lives through innovation in science, healthcare, and education.\nThat\u2019s why today, we\u2019re delighted to announce that we\u2019re expanding our programme to support scholars in many more countries currently underrepresented in AI, including \u2013 Bulgaria, Colombia, Greece, Poland, Romania, South Africa, and Turkey. We are also establishing new scholarships in Canada and France, and continuing our support for scholars in the UK and the US. The full list of universities partnering in our scholarships programme is \nhere\n.\nThere are many initiatives actively working to increase regional participation in AI, such as the \nDeep Learning Indaba\n, \nKhipu AI\n, and the \nEastern European Machine Learning summer school\n. We hope to complement these efforts by enabling students to pursue further education in these regions with fewer financial barriers \u2013 contributing to regional hubs of excellence, while benefiting from the guidance of a DeepMind mentor and an international community of scholar peers.\nTo ensure AI is of global benefit, talent must be nurtured in regions which are currently underrepresented in AI research, and space for geographically and socially diverse, local contributions to the field must be made. We know that increasing access to further education is only one part of addressing the deep-seated structural imbalances in AI, but it is an important one and we are happy to be able to contribute.\nSo this week, as we reflect on the achievements of scholars past and present, and welcome the new 2020 cohort, we also look to the future, as we hope others will be inspired to take the next step on their AI career journeys too. To \nquote\n DeepMind alumna Benedetta, who studied at Oxford University: \u201cDon\u2019t underestimate the value of your unique background.\u201d It\u2019s these unique backgrounds and perspectives that will help make the AI community stronger, more diverse, and more inclusive in years to come.\nIf you\u2019re interested in becoming a DeepMind scholar, find out more about the programme on our \nwebsite\n, discover the universities that participated in the scholarship programme this year, and keep an eye out for upcoming announcements from universities offering DeepMind scholarships starting in 2021. \n"}
{"title": "FermiNet: Quantum Physics and Chemistry from First Principles", "contents": "In an \narticle\n recently published in Physical Review Research, we show how deep learning can help solve the fundamental equations of quantum mechanics for real-world systems. Not only is this an important fundamental scientific question, but it also could lead to practical uses in the future, allowing researchers to prototype new materials and chemical syntheses in silico before trying to make them in the lab. Today we are also releasing the \ncode\n from this study so that the computational physics and chemistry communities can build on our work and apply it to a wide range of problems. We\u2019ve developed a new neural network architecture, the Fermionic Neural Network or FermiNet, which is well-suited to modeling the quantum state of large collections of electrons, the fundamental building blocks of chemical bonds. The FermiNet was the first demonstration of deep learning for computing the energy of atoms and molecules from first principles that was accurate enough to be useful, and it remains the most accurate neural network method to date. We hope the tools and ideas developed in our AI research at DeepMind can help solve fundamental problems in the natural sciences, and the FermiNet joins our work on \nprotein folding\n, \nglassy dynamics\n, \nlattice quantum chromodynamics\n and many other projects in bringing that vision to life.\nMention \u201cquantum mechanics\u201d and you are more likely to inspire confusion than anything else. The phrase conjures up images of Schr\u00f6dinger\u2019s cat, which can paradoxically be both alive and dead, and fundamental particles that are also, somehow, waves. \u00a0In quantum systems, a particle such as an electron doesn\u2019t have an exact location, as it would in a classical description. Instead, its position is described by a probability cloud - it\u2019s smeared out in all places it\u2019s allowed to be. This counterintuitive state of affairs led Richard Feynman to declare: \u201cIf you think you understand quantum mechanics, you don\u2019t understand quantum mechanics.\u201d Despite this spooky weirdness, the meat of the theory can be reduced down to just a few straightforward equations. The most famous of these, the Schr\u00f6dinger equation, describes the behavior of particles at the quantum scale in the same way that Newton\u2019s laws describe the behavior of objects at our more familiar human scale. While the interpretation of this equation can cause endless head-scratching, the math is much easier to work with, leading to the common exhortation from professors to \u201cshut up and calculate\u201d when pressed with thorny philosophical questions from students.\nThese equations are sufficient to describe the behavior of all the familiar matter we see around us at the level of atoms and nuclei. Their counterintuitive nature leads to all sorts of exotic phenomena: superconductors, superfluids, lasers and semiconductors are only possible because of quantum effects. But even the humble covalent bond - the basic building block of chemistry - is a consequence of the quantum interactions of electrons. Once these rules were worked out in the 1920s, scientists realised that, for the first time, they had a detailed theory of how chemistry works. In principle, they could just set up these equations for different molecules, solve for the energy of the system, and figure out which molecules were stable and which reactions would happen spontaneously. But when they sat down to actually calculate the solutions to these equations, they found that they could do it exactly for the simplest atom (hydrogen) and virtually nothing else. Everything else was too complicated.\nThe heady optimism of those days was nicely summed up by Paul Dirac:\nMany took up Dirac\u2019s charge, and soon physicists built mathematical techniques that could approximate the qualitative behavior of molecular bonds and other chemical phenomena. These methods started from an approximate description of how electrons behave that may be familiar from introductory chemistry. In this description, each electron is assigned to a particular orbital, which gives the probability of a single electron being found at any point near an atomic nucleus. The shape of each orbital then depends on the average shape of all other orbitals. As this \u201cmean field\u201d description treats each electron as being assigned to just one orbital, it is a very incomplete picture of how electrons actually behave. Nevertheless, it is enough to estimate the total energy of a molecule with only about 0.5% error.\nUnfortunately, 0.5% error still isn\u2019t enough to be useful to the working chemist. The energy in molecular bonds is just a tiny fraction of the total energy of a system, and correctly predicting whether a molecule is stable can often depend on just 0.001% of the total energy of a system, or about 0.2% of the remaining \u201ccorrelation\u201d energy. For instance, while the total energy of the electrons in a \nbutadiene\n molecule is almost 100,000 kilocalories per mole, the difference in energy between different possible shapes of the molecule is just 1 kilocalorie per mole. That means that if you want to correctly predict butadiene\u2019s natural shape, then the same level of precision is needed as measuring the width of a football field down to the millimeter.\nWith the advent of digital computing after World War II, scientists developed a whole menagerie of computational methods that went beyond this mean field description of electrons. While these methods come in a bewildering alphabet soup of abbreviations, they all generally fall somewhere on an axis that trades off accuracy with efficiency. At one extreme, there are methods that are essentially exact, but scale worse than exponentially with the number of electrons, making them impractical for all but the smallest molecules. At the other extreme are methods that scale linearly, but are not very accurate. These computational methods have had an enormous impact on the practice of chemistry - the 1998 Nobel Prize in chemistry was awarded to the originators of many of these algorithms.\nDespite the breadth of existing computational quantum mechanical tools, we felt a new method was needed to address the problem of efficient representation. There\u2019s a reason that the largest quantum chemical calculations only run into the tens of thousands of electrons for even the most approximate methods, while classical chemical calculation techniques like molecular dynamics can handle millions of atoms. The state of a classical system can be described easily - we just have to track the position and momentum of each particle. Representing the state of a quantum system is far more challenging. A probability has to be assigned to every possible configuration of electron positions. This is encoded in the wavefunction, which assigns a positive or negative number to every configuration of electrons, and the wavefunction squared gives the probability of finding the system in that configuration. The space of all possible configurations is enormous - if you tried to represent it as a grid with 100 points along each dimension, then the number of possible electron configurations for the silicon atom would be larger than the number of atoms in the universe!\nThis is exactly where we thought deep neural networks could help. In the last several years, there have been huge advances in representing complex, high-dimensional probability distributions with neural networks. We now know how to train these networks efficiently and scalably. We surmised that, given these networks have already proven their mettle at fitting high-dimensional functions in artificial intelligence problems, maybe they could be used to represent quantum wavefunctions as well. We were not the first people to think of this - researchers such as \nGiuseppe Carleo and Matthias Troyer\n and others have shown how modern deep learning could be used for solving idealised quantum problems. We wanted to use deep neural networks to tackle more realistic problems in chemistry and condensed matter physics, and that meant including electrons in our calculations.\nThere is just one wrinkle when dealing with electrons. Electrons must obey the Pauli exclusion principle, which means that they can\u2019t be in the same space at the same time. This is because electrons are a type of particle known as fermions, which include the building blocks of most matter - protons, neutrons, quarks, neutrinos, etc. Their wavefunction must be antisymmetric - if you swap the position of two electrons, the wavefunction gets multiplied by -1. That means that if two electrons are on top of each other, the wavefunction (and the probability of that configuration) will be zero.\nThis meant we had to develop a new type of neural network that was antisymmetric with respect to its inputs, which we have dubbed the Fermionic Neural Network, or FermiNet. In most quantum chemistry methods, antisymmetry is introduced using a function called the determinant. The determinant of a matrix has the property that if you swap two rows, the output gets multiplied by -1, just like a wavefunction for fermions. So you can take a bunch of single-electron functions, evaluate them for every electron in your system, and pack all of the results into one matrix. The determinant of that matrix is then a properly antisymmetric wavefunction. The major limitation of this approach is that the resulting function - known as a Slater determinant - is not very general. Wavefunctions of real systems are usually far more complicated. The typical way to improve on this is to take a large linear combination of Slater determinants - sometimes millions or more - and add some simple corrections based on pairs of electrons. Even then, this may not be enough to accurately compute energies.\nDeep neural networks can often be far more efficient at representing complex functions than linear combinations of basis functions. In the FermiNet, this is achieved by making each function going into the determinant a function of all electrons (1). This goes far beyond methods that just use one- and two-electron functions. The FermiNet has a separate stream of information for each electron. Without any interaction between these streams, the network would be no more expressive than a conventional Slater determinant. To go beyond this, we average together information from across all streams at each layer of the network, and pass this information to each stream at the next layer. That way, these streams have the right symmetry properties to create an antisymmetric function. This is similar to how \ngraph neural networks\n aggregate information at each layer. Unlike the Slater determinants, FermiNets are \nuniversal\n function approximators\n, at least in the limit where the neural network layers become wide enough. That means that, if we can train these networks correctly, they should be able to fit the nearly-exact solution to the Schr\u00f6dinger equation.\nWe fit the FermiNet by minimising the energy of the system. To do that exactly, we would need to evaluate the wavefunction at all possible configurations of electrons, so we have to do it approximately instead. We pick a random selection of electron configurations, evaluate the energy locally at each arrangement of electrons, add up the contributions from each arrangement and minimise this instead of the true energy. This is known as a Monte Carlo method, because it\u2019s a bit like a gambler rolling dice over and over again. While it is approximate, if we need to make it more accurate we can always roll the dice again. Since the wavefunction squared gives the probability of observing an arrangement of particles in any location, it is most convenient to generate samples from the wavefunction itself - essentially, simulating the act of observing the particles. While most neural networks are trained from some external data, in our case the inputs used to train the neural network are generated by the neural network itself. It\u2019s a bit like pulling yourself up by your own bootstraps, and it means that we don\u2019t need any training data other than the positions of the atomic nuclei that the electrons are dancing around. The basic idea, known as variational quantum Monte Carlo (or VMC for short), has been around since the \u201860s, and it is generally considered a cheap but not very accurate way of computing the energy of a system. By replacing the simple wavefunctions based on Slater determinants with the FermiNet, we have dramatically increased the accuracy of this approach on every system we\u2019ve looked at.\nTo make sure that the FermiNet really does represent an advance in the state of the art, we started by investigating simple, well-studied systems, like atoms in the first row of the periodic table (hydrogen through neon). These are small systems - 10 electrons or fewer - and simple enough that they can be treated by the most accurate (but exponential scaling) methods. The FermiNet outperforms comparable VMC calculations by a wide margin - often cutting the error relative to the exponentially-scaling calculations by half or more. On larger systems, the exponentially-scaling methods become intractable, so instead we use the \u201ccoupled cluster\u201d method as a baseline. This method works well on molecules in their stable configuration, but struggles when bonds get stretched or broken, which is critical for understanding chemical reactions. While it scales much better than exponentially, the particular coupled cluster method we used still scales as the number of electrons raised to the seventh power, so it can only be used for medium-sized molecules. We applied the FermiNet to progressively larger molecules, starting with lithium hydride and working our way up to bicyclobutane, the largest system we looked at, with 30 electrons. On the smallest molecules, the FermiNet captured an astounding 99.8% of the difference between the coupled cluster energy and the energy you get from a single Slater determinant. On bicyclobutane, the FermiNet still captured 97% or more of this correlation energy - a huge accomplishment for a supposedly \u201ccheap but inaccurate\u201d approach.\nWhile coupled cluster methods work well for stable molecules, the real frontier in computational chemistry is in understanding how molecules stretch, twist and break. There, coupled cluster methods often struggle, so we have to compare against as many baselines as possible to make sure we get a consistent answer. We looked at two benchmark stretched systems - the nitrogen molecule (N2) and the hydrogen chain with 10 atoms, (H10). Nitrogen is an especially challenging molecular bond, because each nitrogen atom contributes 3 electrons. The hydrogen chain, meanwhile, is of \ninterest for understanding how electrons behave in materials\n, for instance predicting whether or not a material will conduct electricity. On both systems, coupled cluster did well at equilibrium, but had problems as the bonds were stretched. Conventional VMC calculations did poorly across the board. But the FermiNet was among the best methods investigated, no matter the bond length.\nWe think the FermiNet is the start of great things to come for the fusion of deep learning and computational quantum chemistry. Most of the systems we\u2019ve looked at so far are well-studied and well-understood. But just as the first good results with deep learning in other fields led to a burst of follow-up work and rapid progress, we hope that the FermiNet will inspire lots of work on scaling up and many ideas for new, even better network architectures. Already, since we first put our work on arXiv last year, \nother\n \ngroups\n have shared their approaches to applying deep learning to first-principles calculations on the many-electron problem. We have also just scratched the surface of computational quantum physics, and look forward to applying the FermiNet to tough problems in material science and condensed matter physics as well. Mostly, we hope that by releasing the source code used in our experiments, we can inspire other researchers to build on our work and try out new applications we haven\u2019t even dreamed of.\nRead the paper \nhere\n and view the code \nhere\n. With thanks to Jim Kynvin, Adam Cain and Dominic Barlow for the figures.\n(1) The FermiNet also has streams for every pair of electrons, and information from these streams is passed back to the single-electron streams. For simplicity, we chose not to visualise this in the blog post, but details can be found in the paper.\n"}
{"title": "AlphaFold: a solution to a 50-year-old grand challenge in biology", "contents": "In July 2022, we released AlphaFold protein structure predictions for nearly all catalogued proteins known to science. Read the latest blog \nhere\n.\nProteins are essential to life, supporting practically all its functions. They are large complex molecules, made up of chains of amino acids, and \nwhat a protein does largely depends on its unique 3D structure\n. Figuring out what shapes proteins fold into is known as the \n\u201cprotein folding problem\u201d\n, and has stood as a grand challenge in biology for the past 50 years. In a major scientific advance, the latest version of our AI system \nAlphaFold\n has been recognised as a solution to this grand challenge by the organisers of the biennial Critical Assessment of protein Structure Prediction (\nCASP\n). This breakthrough demonstrates the impact AI can have on scientific discovery and its potential to dramatically accelerate progress in some of the most fundamental fields that explain and shape our world.\nA protein\u2019s shape is closely linked with its function, and the ability to predict this structure unlocks a greater understanding of what it does and how it works. Many of the world\u2019s greatest challenges, like developing treatments for diseases or finding enzymes that break down industrial waste, are fundamentally tied to proteins and the role they play.\nThis has been a focus of intensive scientific research for many years, using a variety of experimental techniques to examine and determine protein structures, such as nuclear magnetic resonance and X-ray crystallography. These techniques, as well as newer methods like cryo-electron microscopy, depend on extensive trial and error, which can take years of painstaking and laborious work per structure, and require the use of multi-million dollar specialised equipment.\nIn his acceptance speech for the 1972 Nobel Prize in Chemistry, Christian Anfinsen \nfamously postulated\n that, in theory, a protein\u2019s \namino acid sequence\n should fully determine its structure. This hypothesis sparked a five decade quest to be able to computationally predict a protein\u2019s 3D structure based solely on its 1D amino acid sequence as a complementary alternative to these expensive and time consuming experimental methods. A major challenge, however, is that the number of ways a protein could theoretically fold before settling into its final 3D structure is astronomical. In 1969 Cyrus Levinthal noted that it would take longer than the age of the known universe to enumerate all possible configurations of a typical protein by brute force calculation \u2013 Levinthal estimated \n10^300 possible conformations\n for a typical protein. Yet in nature, proteins fold spontaneously, some within milliseconds \u2013 a dichotomy sometimes referred to as \nLevinthal\u2019s paradox\n.\nIn 1994, \nProfessor John Moult and Professor Krzysztof Fidelis founded CASP\n as a biennial blind assessment to catalyse research, monitor progress, and establish the state of the art in protein structure prediction. It is both the gold standard for assessing predictive techniques and a unique global community built on shared endeavour. Crucially, CASP chooses protein structures that have only very recently been experimentally determined (some were still awaiting determination at the time of the assessment) to be targets for teams to test their structure prediction methods against; they are not published in advance. Participants must blindly predict the structure of the proteins, and these predictions are subsequently compared to the ground truth experimental data when they become available. We\u2019re indebted to CASP\u2019s organisers and the whole community, not least the experimentalists whose structures enable this kind of rigorous assessment.\nThe main metric used by CASP to measure the accuracy of predictions is the \nGlobal Distance Test (GDT)\n which ranges from 0-100. In simple terms, GDT can be approximately thought of as the percentage of amino acid residues (beads in the protein chain) within a threshold distance from the correct position. According to \nProfessor Moult\n, a score of around 90 GDT is informally considered to be competitive with results obtained from experimental methods.\nIn \nthe results\n from the 14th CASP assessment, released today, our latest AlphaFold system achieves a median score of 92.4 GDT overall across all targets. This means that our predictions have an average error (\nRMSD\n) of approximately 1.6 \nAngstroms\n, which is comparable to the width of an atom (or 0.1 of a nanometer). Even for the very hardest protein targets, those in the most challenging \nfree-modelling category\n, AlphaFold achieves a median score of 87.0 GDT (\ndata available here\n).\nThese exciting results open up the potential for biologists to use computational structure prediction as a core tool in scientific research. Our methods may prove especially helpful for important classes of proteins, such as \nmembrane proteins\n, that are very difficult to crystallise and therefore challenging to experimentally determine.\nWe first entered \nCASP13\n in 2018 with our \ninitial version of AlphaFold\n, which achieved the highest accuracy among participants. Afterwards, we \npublished\n a paper on our CASP13 methods in Nature with associated \ncode\n, which has gone on to inspire \nother work\n and community-developed open source \nimplementations\n. Now, new deep learning architectures we\u2019ve developed have driven changes in our methods for CASP14, enabling us to achieve unparalleled levels of accuracy. These methods draw inspiration from the fields of biology, physics, and machine learning, as well as of course the work of many scientists in the protein-folding field over the past half-century.\nA folded protein can be thought of as a \u201cspatial graph\u201d, where residues are the nodes and edges connect the residues in close proximity. This graph is important for understanding the physical interactions within proteins, as well as their evolutionary history. For the latest version of AlphaFold, used at CASP14, we created an attention-based neural network system, trained end-to-end, that attempts to interpret the structure of this graph, while reasoning over the implicit graph that it\u2019s building. It uses evolutionarily related sequences, multiple sequence alignment (MSA), and a representation of amino acid residue pairs to refine this graph.\nBy iterating this process, the system develops strong predictions of the underlying physical structure of the protein and is able to determine highly-accurate structures in a matter of days. Additionally, AlphaFold can predict which parts of each predicted protein structure are reliable using an internal confidence measure.\nWe trained this system on publicly available data consisting of ~170,000 protein structures from the \nprotein data bank\n together with \nlarge databases\n containing protein sequences of unknown structure. It uses approximately 16 \nTPUv3s\n (which is 128 TPUv3 cores or roughly equivalent to ~100-200 GPUs) run over a few weeks, a relatively modest amount of compute in the context of most large state-of-the-art models used in machine learning today. As with our CASP13 AlphaFold system, we are preparing a paper on our system to submit to a peer-reviewed journal in due course.\nWhen DeepMind started a decade ago, we hoped that one day AI breakthroughs would help serve as a platform to advance our understanding of fundamental scientific problems. Now, after 4 years of effort building AlphaFold, we\u2019re starting to see that vision realised, with implications for areas like drug design and environmental sustainability.\nProfessor Andrei Lupas, Director of the Max Planck Institute for Developmental Biology and a CASP assessor, let us know that, \u201cAlphaFold\u2019s astonishingly accurate models have allowed us to solve a protein structure we were stuck on for close to a decade, relaunching our effort to understand how signals are transmitted across cell membranes.\u201d\nWe\u2019re optimistic about the impact AlphaFold can have on biological research and the wider world, and excited to collaborate with others to learn more about its potential in the years ahead. Alongside working on a peer-reviewed paper, we\u2019re exploring how best to provide broader access to the system in a scalable way.\nIn the meantime, we\u2019re also looking into how protein structure predictions could contribute to our understanding of specific diseases with a small number of specialist groups, for example by helping to identify proteins that have malfunctioned and to reason about how they interact. These insights could enable more precise work on drug development, complementing existing experimental methods to find promising treatments faster.\nWe\u2019ve also seen signs that protein structure prediction could be useful in future pandemic response efforts, as one of many tools developed by the scientific community. Earlier this year, we \npredicted several protein structures\n of the SARS-CoV-2 virus, including ORF3a, whose structures were previously unknown. At CASP14, we predicted the structure of another coronavirus protein, \nORF8\n. Impressively quick work by experimentalists has now confirmed the structures of both \nORF3a\n and \nORF8\n. Despite their challenging nature and having very few related sequences, we achieved a high degree of accuracy on both of our predictions when compared to their experimentally determined structures.\nAs well as accelerating understanding of known diseases, we\u2019re excited about the potential for these techniques to explore the hundreds of millions of proteins we don\u2019t currently have models for \u2013 a vast terrain of unknown biology. Since \nDNA specifies the amino acid sequences\n that comprise protein structures, the \ngenomics revolution\n has made it possible to read protein sequences from the natural world at massive scale \u2013 with 180 million protein sequences and counting in the Universal Protein database (\nUniProt\n). In contrast, given the experimental work needed to go from sequence to structure, only around 170,000 protein structures are in the Protein Data Bank (\nPDB\n). Among the undetermined proteins may be some with new and exciting functions and \u2013 just as a telescope helps us see deeper into the unknown universe \u2013 techniques like AlphaFold may help us find them.\nAlphaFold is one of our most significant advances to date but, as with all scientific research, there are still many questions to answer. Not every structure we predict will be perfect. There\u2019s still much to learn, including how multiple proteins form complexes, how they interact with \nDNA\n, \nRNA\n, or \nsmall molecules\n, and how we can determine the precise location of all amino acid side chains. In collaboration with others, there\u2019s also much to learn about how best to use these scientific discoveries in the development of new medicines, ways to manage the environment, and more.\nFor all of us working on computational and machine learning methods in science, systems like AlphaFold demonstrate the stunning potential for AI as a tool to aid fundamental discovery. Just as 50 years ago Anfinsen laid out a challenge far beyond science\u2019s reach at the time, there are many aspects of our universe that remain unknown. The progress announced today gives us further confidence that AI will become one of humanity\u2019s most useful tools in expanding the frontiers of scientific knowledge, and we\u2019re looking forward to the many years of hard work and discovery ahead!\nHigh Accuracy Protein Structure Prediction Using Deep Learning\nJohn Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Kathryn Tunyasuvunakool, Olaf Ronneberger, Russ Bates, Augustin \u017d\u00eddek, Alex Bridgland, Clemens Meyer, Simon A A Kohl, Anna Potapenko, Andrew J Ballard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen, David Reiman, Martin Steinegger, Michalina Pacholska, David Silver, Oriol Vinyals, Andrew W Senior, Koray Kavukcuoglu, Pushmeet Kohli, Demis Hassabis.\nIn Fourteenth Critical Assessment of Techniques for Protein Structure Prediction (Abstract Book), 30 November - 4 December 2020. Retrieved from \nhere\n.\nWe\u2019re right at the beginning of exploring how best to enable other groups to use our structure predictions, alongside preparing a peer-reviewed paper for publication. While our team won\u2019t be able to respond to every enquiry, if AlphaFold may be relevant to your work, please submit a few lines about it to \nalphafold@deepmind.com\n. We\u2019ll be in contact if there\u2019s scope for further exploration. \n"}
{"title": "Using JAX to accelerate our research", "contents": "DeepMind engineers accelerate our research by building tools, scaling up algorithms, and creating challenging virtual and physical worlds for training and testing artificial intelligence (AI) systems. As part of this work, we constantly evaluate new machine learning libraries and frameworks.\nRecently, we've found that an increasing number of projects are well served by \nJAX\n, a machine learning framework developed by \nGoogle Research\n teams. JAX resonates well with our engineering philosophy and has been widely adopted by our research community over the last year. Here we share our experience of working with JAX, outline why we find it useful for our AI research, and give an overview of the ecosystem we are building to support researchers everywhere.\nJAX is a Python library designed for high-performance numerical computing, especially machine learning research. Its API for numerical functions is based on \nNumPy\n, a collection of functions used in scientific computing. Both Python and NumPy are widely used and familiar, making JAX simple, flexible, and easy to adopt.\nIn addition to its NumPy API, JAX includes an extensible system of \ncomposable function transformations\n that help support machine learning research, including:\nWe have found that JAX has enabled rapid experimentation with novel algorithms and architectures and it now underpins many of our recent publications. To learn more please consider joining our JAX Roundtable, Wednesday December 9th 7:00pm GMT, at the\n NeurIPS\n virtual conference.\nSupporting state-of-the-art AI research means balancing rapid prototyping and quick iteration with the ability to deploy experiments at a scale traditionally associated with production systems. What makes these kinds of projects particularly challenging is that the research landscape evolves rapidly and is difficult to forecast. At any point, a new research breakthrough may, and regularly does, change the trajectory and requirements of entire teams. Within this ever-changing landscape, a core responsibility of our engineering team is to make sure that the lessons learned and the code written for one research project is reused effectively in the next.\nOne approach that has proven successful is modularisation: we extract the most important and critical building blocks developed in each research project into well tested and efficient \ncomponents\n. This empowers researchers to focus on their research while also benefiting from code reuse, bug fixes and performance improvements in the algorithmic ingredients implemented by our core libraries. We\u2019ve also found that it\u2019s important to make sure that each library has a clearly defined scope and to ensure that they\u2019re interoperable but independent. \nIncremental buy-in\n, the ability to pick and choose features without being locked into others, is critical to providing maximum flexibility for researchers and always supporting them in choosing the right tool for the job.\nOther considerations that have gone into the development of our JAX Ecosystem include making sure that it remains consistent (where possible) with the design of our existing \nTensorFlow\n libraries (e.g. \nSonnet\n and \nTRFL\n). We\u2019ve also aimed to build components that (where relevant) match their underlying mathematics as closely as possible, to be self-descriptive and minimise mental hops \"from paper to code\". Finally, we\u2019ve chosen to \nopen source\n our libraries to facilitate sharing of research outputs and to encourage the broader community to explore the JAX Ecosystem.\nThe JAX programming model of composable function transformations can make dealing with stateful objects complicated, e.g. neural networks with trainable parameters. Haiku is a neural network library that allows users to use familiar object-oriented programming models while harnessing the power and simplicity of JAX's pure functional paradigm.\nHaiku is actively used by hundreds of researchers across DeepMind and Google, and has already found adoption in several external projects (e.g. \nCoax\n, \nDeepChem\n, \nNumPyro\n). It builds on the API for \nSonnet\n, our module-based programming model for neural networks in TensorFlow, and we\u2019ve aimed to make porting from Sonnet to Haiku as simple as possible.\nFind out more on GitHub\nGradient-based optimisation is fundamental to ML. Optax provides a library of gradient transformations, together with composition operators (e.g. chain) that allow implementing many standard optimisers (e.g. RMSProp or Adam) in just a single line of code.\nThe compositional nature of Optax naturally supports recombining the same basic ingredients in custom optimisers. It additionally offers a number of utilities for stochastic gradient estimation and second order optimisation.\nMany Optax users have adopted Haiku but in line with our incremental buy-in philosophy, any library representing parameters as JAX tree structures is supported (e.g. \nElegy\n, \nFlax\n and \nStax\n). Please see \nhere\n for more information on this rich ecosystem of JAX libraries.\nFind out more on GitHub\nMany of our most successful projects are at the intersection of deep learning and reinforcement learning (RL), also known as \ndeep reinforcement learning\n. RLax is a library that provides useful building blocks for constructing RL agents.\nThe components in RLax cover a broad spectrum of algorithms and ideas: TD-learning, policy gradients, actor critics, MAP, proximal policy optimisation, non-linear value transformation, general value functions, and a number of exploration methods.\nAlthough some introductory \nexample agents\n are provided, RLax is not intended as a framework for building and deploying full RL agent systems. One example of a fully-featured agent framework that builds upon RLax components is \nAcme\n.\nFind out more on GitHub\nTesting is critical to software reliability and research code is no exception. Drawing scientific conclusions from research experiments requires being confident in the correctness of your code. Chex is a collection of testing utilities used by library authors to verify the common building blocks are correct and robust and by end-users to check their experimental code.\nChex provides an assortment of utilities including JAX-aware unit testing, assertions of properties of JAX datatypes, mocks and fakes, and multi-device test environments. Chex is used throughout DeepMind\u2019s JAX Ecosystem and by external projects such as \nCoax\n and \nMineRL\n.\nFind out more on GitHub\nGraph neural networks\n (GNNs) are an exciting area of research with many promising applications. See, for instance, our recent work on \ntraffic prediction\n in Google Maps and our work on \nphysics simulation\n. Jraph (pronounced \"giraffe\") is a lightweight library to support working with GNNs in JAX.\nJraph provides a standardised data structure for graphs, a set of utilities for working with graphs, and a 'zoo' of easily forkable and extensible graph neural network models. Other key features include: batching of GraphTuples that efficiently leverage hardware accelerators, JIT-compilation support of variable-shaped graphs via padding and masking, and losses defined over input partitions. Like Optax and our other libraries, Jraph places no constraints on the user's choice of a neural network library.\nLearn more about using the library from our rich collection of \nexamples\n.\nFind out more on GitHub\nOur JAX Ecosystem is constantly evolving and we encourage the ML research community to explore \nour libraries\n and the potential of JAX to accelerate their own research.\nCiting the DeepMind JAX Ecosystem\nIf you find the DeepMind JAX Ecosystem useful for your work, please use \nthis citation\n (hosted on GitHub).\n"}
{"title": "MuZero: Mastering Go, chess, shogi and Atari without rules", "contents": "In 2016, we introduced \nAlphaGo\n, the first artificial intelligence (AI) program to defeat humans at the ancient game of Go. Two years later, its successor - \nAlphaZero\n - learned from scratch to master Go, chess and shogi. Now, \nin a paper in the journal Nature\n, we describe MuZero, a significant step forward in the pursuit of general-purpose algorithms. MuZero masters Go, chess, shogi and Atari without needing to be told the rules, thanks to its ability to plan winning strategies in unknown environments.\nFor many years, researchers have sought methods that can both learn a model that explains their environment, and can then use that model to plan the best course of action. Until now, most approaches have struggled to plan effectively in domains, such as Atari, where the rules or dynamics are typically unknown and complex.\nMuZero, first introduced in a \npreliminary paper in 2019\n, solves this problem by learning a model that focuses only on the most important aspects of the environment for planning. By combining this model with AlphaZero\u2019s powerful lookahead tree search, MuZero set a new state of the art result on the Atari benchmark, while simultaneously matching the performance of AlphaZero in the classic planning challenges of Go, chess and shogi. In doing so, MuZero demonstrates a significant leap forward in the capabilities of reinforcement learning algorithms.\nThe ability to plan is an important part of human intelligence, allowing us to solve problems and make decisions about the future. For example, if we see dark clouds forming, we might predict it will rain and decide to take an umbrella with us before we venture out. Humans learn this ability quickly and can generalise to new scenarios, a trait we would also like our algorithms to have.\nResearchers have tried to tackle this major challenge in AI by using two main approaches: lookahead search or model-based planning.\nSystems that use lookahead search, such as AlphaZero, have achieved remarkable success in classic games such as checkers, chess and poker, but rely on being given knowledge of their environment\u2019s dynamics, such as the rules of the game or an accurate simulator. This makes it difficult to apply them to messy real world problems, which are typically complex and hard to distill into simple rules.\nModel-based systems aim to address this issue by learning an accurate model of an environment\u2019s dynamics, and then using it to plan. However, the complexity of modelling every aspect of an environment has meant these algorithms are unable to compete in visually rich domains, such as Atari. \u00a0Until now, the best results on Atari are from model-free systems, such as \nDQN\n, \nR2D2\n and \nAgent57\n. As the name suggests, model-free algorithms do not use a learned model and instead estimate what is the best action to take next.\nMuZero uses a different approach to overcome the limitations of previous approaches. Instead of trying to model the entire environment, MuZero just models aspects that are important to the agent\u2019s decision-making process. After all, knowing an umbrella will keep you dry is more useful to know than modelling the pattern of raindrops in the air.\nSpecifically, MuZero models three elements of the environment that are critical to planning:\nThese are all learned using a deep neural network and are all that is needed for MuZero to understand what happens when it takes a certain action and to plan accordingly.\nThis approach comes with another major benefit: MuZero can repeatedly use its learned model to improve its planning, rather than collecting new data from the environment. For example, in tests on the Atari suite, this variant - known as MuZero Reanalyze - used the learned model 90% of the time to re-plan what should have been done in past episodes.\nWe chose four different domains to test MuZeros capabilities. Go, chess and shogi were used to assess its performance on challenging planning problems, while we used the Atari suite as a benchmark for more visually complex problems. In all cases, MuZero set a new state of the art for reinforcement learning algorithms, outperforming all prior algorithms on the Atari suite and matching the superhuman performance of AlphaZero on Go, chess and shogi.\nWe also tested how well MuZero can plan with its learned model in more detail. We started with the classic precision planning challenge in Go, where a single move can mean the difference between winning and losing. To confirm the intuition that planning more should lead to better results, we measured how much stronger a fully trained version of MuZero can become when given more time to plan for each move (see left hand graph below). The results showed that playing strength increases by more than 1000 Elo (a measure of a player's relative skill) as we increase the time per move from one-tenth of a second to 50 seconds. This is similar to the difference between a strong amateur player and the strongest professional player.\nTo test whether planning also brings benefits throughout training, we ran a set of experiments on the Atari game Ms Pac-Man (right hand graph above) using separate trained instances of MuZero. Each one was allowed to consider a different number of planning simulations per move, ranging from five to 50. The results confirmed that increasing the amount of planning for each move allows MuZero to both learn faster and achieve better final performance.\nInterestingly, when MuZero was only allowed to consider six or seven simulations per move - a number too small to cover all the available actions in Ms Pac-Man - it still achieved good performance. This suggests MuZero is able to generalise between actions and situations, and does not need to exhaustively search all possibilities to learn effectively.\nMuZero\u2019s ability to both learn a model of its environment and use it to successfully plan demonstrates a significant advance in reinforcement learning and the pursuit of general purpose algorithms. Its predecessor, AlphaZero, has already been applied to a range of complex problems in \nchemistry\n, \nquantum physics\n and beyond. The ideas behind MuZero's powerful learning and planning algorithms may pave the way towards tackling new challenges in robotics, industrial systems and other messy real-world environments where the \u201crules of the game\u201d are not known.\n\u200d\nDesign by Adam Cain, Jim Kynvin and Aleksandrs Polozuns\n"}
{"title": "Game theory as an engine for large-scale data analysis", "contents": "EigenGame maps out a new approach to solve fundamental ML problems.\nModern AI systems approach tasks like \nrecognising objects in images\n and \npredicting the 3D structure of proteins\n as a diligent student would prepare for an exam. By training on many example problems, they minimise their mistakes over time until they achieve success. But this is a solitary endeavour and only one of the known forms of learning. Learning also takes place by interacting and playing with others. It\u2019s rare that a single individual can solve extremely complex problems alone. By allowing problem solving to take on these game-like qualities, previous DeepMind efforts have trained AI agents to play \nCapture the Flag\n and achieve \nGrandmaster level at Starcraft\n. This made us wonder if such a perspective modeled on game theory could help solve other fundamental machine learning problems.\nToday at \nICLR 2021\n (the International Conference on Learning Representations), we presented \u201c\nEigenGame: PCA as a Nash Equilibrium\n,\u201d which received an Outstanding Paper Award. Our research explored a new approach to an old problem: we reformulated principal component analysis (PCA), a type of \neigenvalue problem\n, as a competitive multi-agent game we call EigenGame. PCA is typically formulated as an optimisation problem (or single-agent problem); however, we found that the multi-agent perspective allowed us to develop new insights and algorithms which make use of the latest computational resources. This enabled us to scale to massive data sets that previously would have been too computationally demanding, and offers an alternative approach for future exploration.\nFirst described in the early 1900s, \nPCA\n is a long-standing technique for making sense of the structure of high-dimensional data. This approach is now ubiquitous as a first step in the data-processing pipeline and makes it easy to cluster and visualise data. It can also be a useful tool for learning low-dimensional representations for regression and classification. More than a century later, there are still compelling reasons to study PCA.\nFirstly, data was originally recorded by hand in paper notebooks, and now it is stored in data centres the size of warehouses. As a result, this familiar analysis has become a computational bottleneck. Researchers have explored \nrandomised algorithms\n and other directions to improve how PCA scales, but we found that these approaches have difficulty scaling to massive datasets because they are unable to fully harness recent deep-learning-centric advances in computation \u2014 namely access to many parallel GPUs or TPUs.\nSecondly, PCA shares a common solution with many important ML and engineering problems, namely the \nsingular value decomposition\n (SVD). By approaching the PCA problem in the right way, our insights and algorithms apply more broadly across the branches of the ML tree.\nAs with any board game, in order to reinvent PCA as a game we need a set of rules and objectives for players to follow. There are many possible ways to design such a game; however, important ideas come from PCA itself: the optimal solution consists of eigenvectors which capture the important variance in the data and are orthogonal to each other.\nIn EigenGame each player controls an eigenvector. Players increase their score by explaining variance within the data but are penalised if they\u2019re too closely aligned to other players. We also establish a hierarchy: Player 1 only cares about maximising variance, whereas other players also have to worry about minimising their alignment with players above them in the hierarchy. This combination of rewards and penalties defines each player\u2019s utility.\nWith appropriately designed\n Var \nand \nAlign\n terms, we can show that:\nThis independence property of simultaneous ascent is particularly important because it allows for the computation to be distributed across dozens of Google Cloud TPUs, enabling both data- and model-parallelism. This makes it possible for our algorithm to adapt to truly large-scale data. EigenGame finds the principal components in a matter of hours for hundred-terabyte datasets comprising millions of features or billions of rows.\nBy thinking about PCA from a multi-agent perspective, we were able to propose scalable algorithms and novel analyses. We also uncovered a surprising connection to \nHebbian Learning\n \u2014 or, how neurons adapt when learning. In EigenGame, each player maximising their utilities gives rise to update equations that are similar to \nupdate rules\n derived from Hebbian models of synaptic plasticity in the brain. Hebbian updates are known to converge to the PCA solution but are not derived as the gradient of any utility function. Game theory gives us a fresh lens to view Hebbian learning, and also suggests a continuum of approaches to machine learning problems.\nOn one end of the ML continuum is the well-developed path of proposing an objective function that can be optimised: Using the theory of convex and non-convex optimisation, researchers can reason about the global properties of the solution. On the other end, pure \nconnectionist\n methods and update rules inspired by neuroscience are specified directly, but analysis of the entire system can be more difficult, often invoking the study of complicated \ndynamical systems\n.\nGame theoretic approaches like EigenGame sit somewhere in between. Player updates are not constrained to be the gradient of a function, only a best response to the current strategies of the other players. We\u2019re free to design utilities and updates with desirable properties \u2014 for example, specifying updates which are unbiased or accelerated \u2014 while ensuring the Nash property still allows us to analyse the system as a whole.\nEigenGame represents a concrete example of designing the solution to a machine learning problem as the output of a large multi-agent system. More generally, designing machine learning problems as multi-agent games is a challenging \nmechanism design\n problem; however, researchers have already used the class of two-player, \nzero-sum\n games to solve machine learning problems. Most notably, the success of \ngenerative adversarial networks\n (GANs) as an approach to generative modelling has driven interest in the relationship between game theory and machine learning.\nEigenGame moves beyond this to the more complex many-player, general-sum setting. This enables more obvious parallelism for greater scale and speed. It also presents a quantitative benchmark for the community to test novel multi-agent algorithms alongside richer domains, such as \nDiplomacy\n and \nSoccer\n.\nWe hope our blueprint for designing utilities and updates will encourage others to explore this direction for designing new algorithms, agents, and systems. We\u2019re looking forward to seeing what other problems can be formulated as games and whether the insights we glean will further improve our understanding of the multi-agent nature of intelligence.\nFor more details see our paper \nEigenGame: PCA as a Nash Equilibrium\n and our follow-up work \nEigenGame Unloaded: When playing games is better than optimising\n. This blog post is based on joint work with Thore Graepel, a research group lead at DeepMind and Chair of Machine Learning at University College London.\nWe would like to thank Rob Fergus for their technical feedback on this post as well as Sean Carlson, Jon Fildes, Dominic Barlow, Mario Pinto, and Emma Yousif for pulling this all together.\nCustom figures by Jim Kynvin and Adam Cain.\n"}
{"title": "Advancing sports analytics through AI research", "contents": "Creating testing environments to help progress AI research out of the lab and into the real world is immensely challenging. Given AI\u2019s long association with games, it is perhaps no surprise that sports presents an exciting opportunity, offering researchers a testbed in which an AI-enabled system can assist humans in making complex, real-time decisions in a multiagent environment with dozens of dynamic, interacting individuals.\nThe rapid growth of sports data collection means we are in the midst of a remarkably important era for sports analytics. The availability of sports data is increasing in both quantity and granularity, transitioning from the days of aggregate high-level statistics and sabermetrics to more refined data such as event stream information (e.g., annotated passes or shots), high-fidelity player positional information, and \non-body sensors\n. However, the field of sports analytics has only recently started to harness machine learning and AI for both understanding and advising human decision-makers in sports. In our \nrecent paper\n published in collaboration with Liverpool Football Club (LFC) in JAIR, we envision the future landscape of sports analytics using a combination of statistical learning, video understanding, and game theory. We illustrate football, in particular, is a useful microcosm for studying AI research, offering benefits in the longer-term to decision-makers in sports in the form of an automated video-assistant coach (AVAC) system (Figure 1(A)).\nIn comparison to some other sports, football has been rather late with starting to systematically collect large sets of data for scientific analytics purposes aiming to progress teams\u2019 gameplay. This is for several reasons, with the most prominent being that there are far less controllable settings of the game compared to other sports (large outdoor pitch, dynamic game, etc.), and also the dominant credo to rely mainly on human specialists with track records and experience in professional football. On these lines, Arrigo Sacchi, a successful Italian football coach and manager who never played professional football in his career, responded to criticism over his lack of experience with his \nfamous quote\n when becoming a coach at Milan in 1987: \u201cI never realised that to be a jockey you had to be a horse first.\u201d\nFootball Analytics poses challenges that are well suited for a wide variety of AI techniques, coming from the intersection of 3 fields: computer vision, statistical learning and game theory (visualised in Figure 2). While these fields are individually useful for football analytics, their benefits become especially tangible when combined: players need to take sequential decision-making in the presence of other players (cooperative and adversarial) and as such game theory, a theory of interactive decision making, becomes highly relevant. Moreover, tactical solutions to particular in-game situations can be learnt based on in-game and specific player representations, which makes statistical learning a highly relevant area. Finally, players can be tracked and game scenarios can be recognised automatically from widely-available image and video inputs.\nThe AVAC system we envision is situated within the microcosm that is formed by the intersection of these three research fields (Figure 2). In our research in this exciting domain, we not only lay out a roadmap for scientific and engineering problems that can be tackled for years to come, but we also present new original results at the crossroads of game theoretic analysis, statistical learning, and computer vision to illustrate what this exciting area has to offer to football.\nGame theory plays an important role in the study of sports, enabling theoretical grounding of players\u2019 behavioral strategies. In the case of football, many of its scenarios can actually be modeled as zero-sum games, which have been studied extensively since the inception of game theory. For example, here we model the penalty kick situation as a two-player asymmetric game, where the kicker\u2019s strategies may be neatly categorised as left, center, or right shots. To study this problem, we augment game-theoretic analysis in the penalty kick scenario with \nPlayer Vectors\n, which summarise the playing styles of individual football players. With such representations of individual players, we are able to group kickers with similar playing styles, and then conduct game-theoretic analysis on the group-level (Figure 3). Our results show that the identified shooting strategies of different groups are statistically distinct. For example, we find that one group prefers to shoot to the left corner of the goal mouth, while another tends to shoot to the left and right corners more evenly. Such insights may help goalkeepers diversify their defense strategies when playing against different types of players. Building on this game-theoretic view, one can consider the durative nature of football by analysing it in the form of temporally-extended games, use this to advise tactics to individual players, or even go further to optimise the overall team strategy.\nOn the side of statistical learning, representation learning has yet to be fully exploited in sports analytics, which would enable informative summarisation of the behavior of individual players and football teams. Moreover, we believe that the interaction between game theory and statistical learning would catalyse advances in sports analytics further. In the above penalty kick scenario, for instance, augmenting the analysis with player-specific statistics (Player Vectors) provided deeper insights into how various types of players behave or make decisions about their actions in the penalty kick scenario. As another example of this, one can study '\nghosting\n', which refers to a particular data driven analysis of how players should have acted in hindsight in sports analytics (which bears connections to the notion of regret in online learning and game theory). The ghosting model suggests alternative player trajectories for a given play, e.g., based on the league average or a selected team. Predicted trajectories are usually visualised as a translucent layer over the original play, hence the term 'ghosting' (see Figure 4 for a visual example). Generative trajectory prediction models allow us to gain insights by analysing key situations of a game and how they might have played out differently. These models also bear potential in predicting the implications of a tactical change, a key player's injury, or substitution on the own team's performance along with the opposition's response to such a change.\nFinally, we consider computer vision to be one of the most promising avenues for advancing the boundaries of state of the art sports analytics research. By detecting events purely from video, a topic that has been well-studied in the computer vision community (e.g., see the following \nsurvey\n and our paper for additional references), the potential range of application is enormous. By associating events with particular frames, videos become searchable and ever more useful (e.g., automatic highlight generation becomes possible). Football video, in turn, offers an interesting application domain for computer vision. The large numbers of football videos satisfies a prerequisite for modern AI techniques. While each football video is different, the settings do not vary greatly, which makes the task ideal for sharpening AI algorithms. Third-party providers also exist to furnish hand-labelled event data that can be useful in training video models and are time consuming to generate, so both supervised and unsupervised algorithms can be used for football event detection. Figure 1(B), for example, provides a stylised visualisation of a deep learning model trained with supervised methods to recognise target events (e.g., kicks) purely from video.\nThe application of advanced AI techniques to football has the potential to revolutionise the game across many axes, for players, decision-makers, fans, and broadcasters. Such advances will also be important as they also bear potential to further democratise the sport itself (e.g., rather than relying on judgement calls from in-person scouts/experts, one may use techniques such as computer vision to quantify skillsets of players from under-represented regions, those from lower-level leagues, etc.). We believe that the development of increasingly advanced AI techniques afforded by the football microcosm might be applicable to broader domains. To this end, we are co-organising (with several external organisers) an \nIJCAI 2021 workshop on AI for Sports Analytics\n later this year, which we welcome interested researchers to attend. For researchers interested in this topic, publicly available datasets have been made available both by analytics companies such as StatsBomb (\ndataset link\n) and the wider research community (\ndataset link\n). Furthermore, the paper provides a comprehensive overview of research in this domain.\nPaper and related links:\nWork done as a collaboration with contributors: Karl Tuyls, Shayegan Omidshafiei, Paul Muller, Zhe Wang, Jerome Connor, Daniel Hennes, Ian Graham, William Spearman, Tim Waskett, Dafydd Steele, Pauline Luc, Adria Recasens, Alexandre Galashov, Gregory Thornton, Romuald Elie, Pablo Sprechmann, Pol Moreno, Kris Cao, Marta Garnelo, Praneet Dutta, Michal Valko, Nicolas Heess, Alex Bridgland, Julien Perolat, Bart De Vylder, Ali Eslami, Mark Rowland, Andrew Jaegle, Yi Yang, Remi Munos, Trevor Back, Razia Ahamed, Simon Bouton, Nathalie Beauguerlange, Jackson Broshear, Thore Graepel, and Demis Hassabis.\n"}
{"title": "An update on our racial justice efforts", "contents": "In June 2020, after George Floyd was killed in Minneapolis (USA) and the solidarity that followed as millions spoke out at Black Lives Matter protests around the world, \u00a0I \u2013 like many others \u2013 reflected on the situation and how our organisation could contribute. \u00a0I then shared some \nthoughts\n around DeepMind's intention to help combat racism and advance racial equity. \nWith other senior leaders at DeepMind, I spent time listening and talking to colleagues about how racism affects peoples\u2019 personal and professional lives and replicates itself in the systems and structures of our society. We also explored - and gathered feedback - on how we could best support racial justice in the communities DeepMind interacts with.\nToday I\u2019m pleased to share one of the outcomes of that process: putting resources directly in the hands of Black communities, so they can decide where they need them most. In the past months, we made donations to organisations that play a vital role supporting Black communities in the UK, US and Africa. Specifically, we\u2019ve supported organisations who are focused on impact in the AI/ML space, those supporting emerging regional tech communities, and those focused on broader societal impact. These donations are unrestricted, meaning each of these organisations can use the funds however they need to, to accelerate sustained impact.\nWe're delighted to support these organisations, and grateful to many of those who have shared with us an overview of their work:\nBlack Cultural Archives (UK) is the home of Black British History, situated within the iconic Windrush Square in Brixton. Since its inception in 1981, they've become the leading non-governmental and heritage institutional voice for the Windrush Generation, also leading in the heritage sector for their work on workforce diversity and interrogating decolonial archival practices. Their overall mission is to collect, preserve and celebrate the histories of people of African and Caribbean descent in the UK, inspiring and strengthening communities, individuals, and society alike.\nBlack in AI (USA) is a multi-institutional, transcontinental initiative creating a space for sharing ideas, fostering collaborations, and discussing initiatives to increase the presence of Black individuals in the field of AI.\nBlack Thrive Global (UK) is a partnership between communities, statutory organisations, and the voluntary and private sectors. They work together to improve the experiences and outcomes of Black people in mental health services in the UK, as well as address the social inequality and injustices that lead to the disproportionately high rate of mental illness amongst Black people.\nData Science Africa (Kenya) is a grassroots capacity building organisation that runs summer schools and workshops in the area of data science, AI and machine learning. Since 2015, they've run eight events in six countries in East and West Africa. They've also begun a \nresearch award program\n to support African researchers and a \nvisiting fellowship\n program to support research visits to DSA partner institutions.\nDeep Learning Indaba (across the African continent) supports Africa\u2019s community in AI to be owners and shapers of Machine Learning. Their mission is to strengthen Machine Learning on the African continent. One way they're doing this is by supporting AI communities through locally organised \nIndabaX\n events, which now includes 31 IndabaX ML communities across the continent. These events have been the best way to connect local ML researchers, engineers and enthusiasts to share their research, innovation and current challenges. Last year, they launched an innovative \nmentorship\n program and this year they're coming to the end of \n11 research\n projects and are looking to further deepen their collaboration with \nData Science Africa.\nStopWatch (UK) is a coalition of academics, lawyers, civil society organisations, and community stakeholders who aim to address excess and disproportionate stop and search, promote best practice and ensure fair, effective policing for all. This includes legal and policy analysis, media coverage and commentary, political advocacy, litigation, submissions to national and international organisations and community organising.\nUbele Initiative\u2019s (UK) work is grounded in community-based approaches to development, supporting Black and minoritised groups with their community assets through social action, enterprise development, and next-generation leadership initiatives.\nIt's an honour to support these organisations and to have the privilege of highlighting their efforts, but we recognise that this type of support is only one small part of the important work we need to do. At DeepMind, we want to build safe and ethical AI and deploy it in a way that is beneficial to society, which requires holding ourselves to a high standard of equity and fairness in our research and internal practices.\nThanks to the commitment and passion of many groups within DeepMind, this thinking is shaping our efforts to improve representation in AI and to ensure a fair and inclusive workplace.\nIt's also a perspective that, step-by-step, we're integrating into our research programmes, carefully assessing our research for potential harms, ensuring space for critical reflection via \nresearch\n and \ndiscussion\n on socio-technical topics. We're also regularly reviewing internal hiring, promotion and project assignment processes through the prism of equity. Externally, our \nscholarship & mentorship programme\n, which supports underrepresented groups to pursue postgraduate study, has expanded dramatically in the past two years.\nMy deep thanks to everyone who has committed time, energy and passion to these efforts so far.\n"}
{"title": "Generally capable agents emerge from open-ended play", "contents": "In recent years, artificial intelligence agents have succeeded in a range of complex game environments. For instance, \nAlphaZero\n beat world-champion programs in chess, shogi, and Go after starting out with knowing no more than the basic rules of how to play. Through \nreinforcement learning\n (RL), this single system learnt by playing round after round of games through a repetitive process of trial and error. But AlphaZero still trained separately on each game \u2014 unable to simply learn another game or task without repeating the RL process from scratch. The same is true for other successes of RL, such as \nAtari\n, \nCapture the Flag\n, \nStarCraft II\n, \nDota 2\n, and \nHide-and-Seek\n. DeepMind\u2019s mission of solving intelligence to advance science and humanity led us to explore how we could overcome this limitation to create AI agents with more general and adaptive behaviour. Instead of learning one game at a time, these agents would be able to react to completely new conditions and play a whole universe of games and tasks, including ones never seen before.\nToday, we published \"\nOpen-Ended Learning Leads to Generally Capable Agents\n,\" a preprint detailing our first steps to train an agent capable of playing many different games without needing human interaction data. We created a vast game environment we call XLand, which includes many multiplayer games within consistent, human-relatable 3D worlds. This environment makes it possible to formulate new learning algorithms, which dynamically control how an agent trains and the games on which it trains. The agent\u2019s capabilities improve iteratively as a response to the challenges that arise in training, with the learning process continually refining the training tasks so the agent never stops learning. The result is an agent with the ability to succeed at a wide spectrum of tasks \u2014 from simple object-finding problems to complex games like hide and seek and capture the flag, which were not encountered during training. We find the agent exhibits general, heuristic behaviours such as experimentation, behaviours that are widely applicable to many tasks rather than specialised to an individual task. This new approach marks an important step toward creating more general agents with the flexibility to adapt rapidly within constantly changing environments.\nA lack of training data \u2014 where \u201cdata\u201d points are different tasks \u2014 has been one of the major factors limiting RL-trained agents\u2019 behaviour being general enough to apply across games. Without being able to train agents on a vast enough set of tasks, agents trained with RL have been unable to adapt their learnt behaviours to new tasks. But by designing a simulated space to allow for \nprocedurally generated tasks\n, our team created a way to train on, and generate experience from, tasks that are created programmatically. This enables us to include billions of tasks in XLand, across varied games, worlds, and players.\nOur AI agents inhabit 3D first-person avatars in a multiplayer environment meant to simulate the physical world. The players sense their surroundings by observing RGB images and receive a text description of their goal, and they train on a range of games. These games are as simple as cooperative games to find objects and navigate worlds, where the goal for a player could be \u201cbe near the purple cube.\u201d More complex games can be based on choosing from multiple rewarding options, such as \u201cbe near the purple cube or put the yellow sphere on the red floor,\u201d and more competitive games include playing against co-players, such as symmetric hide and seek where each player has the goal, \u201csee the opponent and make the opponent not see me.\u201d Each game defines the rewards for the players, and each player\u2019s ultimate objective is to maximise the rewards.\nBecause XLand can be programmatically specified, the game space allows for data to be generated in an automated and algorithmic fashion. And because the tasks in XLand involve multiple players, the behaviour of co-players greatly influences the challenges faced by the AI agent. These complex, non-linear interactions create an ideal source of data to train on, since sometimes even small changes in the components of the environment can result in large changes in the challenges for the agents.\nCentral to our research is the role of \ndeep RL\n in training the neural networks of our agents. The neural network architecture we use provides an attention mechanism over the agent\u2019s internal recurrent state \u2014 helping guide the agent\u2019s attention with estimates of subgoals unique to the game the agent is playing. We\u2019ve found this goal-attentive agent (GOAT) learns more generally capable policies.\nWe also explored the question, what distribution of training tasks will produce the best possible agent, especially in such a vast environment? The dynamic task generation we use allows for continual changes to the distribution of the agent\u2019s training tasks: every task is generated to be neither too hard nor too easy, but just right for training. We then use \npopulation based training\n (PBT) to adjust the parameters of the dynamic task generation based on a fitness that aims to improve agents\u2019 general capability. And finally we chain together multiple training runs so each generation of agents can bootstrap off the previous generation.\nThis leads to a final training process with deep RL at the core updating the neural networks of agents with every step of experience:\nThe training process starts from scratch and iteratively builds complexity, constantly changing the learning problem to keep the agent learning. The iterative nature of the combined learning system, which does not optimise a bounded performance metric but rather the iteratively defined spectrum of general capability, leads to a potentially open-ended learning process for agents, limited only by the expressivity of the environment space and agent neural network.\nTo measure how agents perform within this vast universe, we create a set of evaluation tasks using games and worlds that remain separate from the data used for training. These \u201cheld-out\u201d tasks include specifically human-designed tasks like hide and seek and capture the flag.\nBecause of the size of XLand, understanding and characterising the performance of our agents can be a challenge. Each task involves different levels of complexity, different scales of achievable rewards, and different capabilities of the agent, so merely averaging the reward over held out tasks would hide the actual differences in complexity and rewards \u2014 and would effectively treat all tasks as equally interesting, which isn\u2019t necessarily true of procedurally generated environments.\nTo overcome these limitations, we take a different approach. Firstly, we normalise scores per task using the Nash equilibrium value computed using our current set of trained players. Secondly, we take into account the entire distribution of normalised scores \u2014 rather than looking at average normalised scores, we look at the different percentiles of normalised scores \u2014 as well as the percentage of tasks in which the agent scores at least one step of reward: participation. This means an agent is considered better than another agent only if it exceeds performance on all percentiles. This approach to measurement gives us a meaningful way to assess our agents\u2019 performance and robustness.\nAfter training our agents for five generations, we saw consistent improvements in learning and performance across our held-out evaluation space. Playing roughly 700,000 unique games in 4,000 unique worlds within XLand, each agent in the final generation experienced 200 billion training steps as a result of 3.4 million unique tasks. At this time, our agents have been able to participate in every procedurally generated evaluation task except for a handful that were impossible even for a human. And the results we\u2019re seeing clearly exhibit general, zero-shot behaviour across the task space \u2014 with the frontier of normalised score percentiles continually improving.\nLooking qualitatively at our agents, we often see general, heuristic behaviours emerge \u2014 rather than highly optimised, specific behaviours for individual tasks. Instead of agents knowing exactly the \u201cbest thing\u201d to do in a new situation, we see evidence of agents experimenting and changing the state of the world until they\u2019ve achieved a rewarding state. We also see agents rely on the use of other tools, including objects to occlude visibility, to create ramps, and to retrieve other objects. Because the environment is multiplayer, we can examine the progression of agent behaviours while training on held-out \nsocial dilemmas\n, such as in a game of \u201c\nchicken\n\u201d. As training progresses, our agents appear to exhibit more cooperative behaviour when playing with a copy of themselves. Given the nature of the environment, it is difficult to pinpoint intentionality \u2014 the behaviours we see often appear to be accidental, but still we see them occur consistently.\nAnalysing the agent\u2019s internal representations, we can say that by taking this approach to reinforcement learning in a vast task space, our agents are aware of the basics of their bodies and the passage of time and that they understand the high-level structure of the games they encounter. Perhaps even more interestingly, they clearly recognise the reward states of their environment. This generality and diversity of behaviour in new tasks hints toward the potential to fine-tune these agents on downstream tasks. For instance, we show in the technical paper that with just 30 minutes of focused training on a newly presented complex task, the agents can quickly adapt, whereas agents trained with RL from scratch cannot learn these tasks at all.\nBy developing an environment like XLand and new training algorithms that support the open-ended creation of complexity, we\u2019ve seen clear signs of zero-shot generalisation from RL agents. Whilst these agents are starting to be generally capable within this task space, we look forward to continuing our research and development to further improve their performance and create ever more adaptive agents.\nWe hope the \npreprint of our technical paper\n \u2014 and \nvideos of the results\n we\u2019ve seen \u2014 help other researchers likewise see a new path toward creating more adaptive, generally capable AI agents. And if you\u2019re excited by these advances, consider \njoining our team\n.\nFor more details, see our preprint \u201c\nOpen-Ended Learning Leads to Generally Capable Agents\n.\u201d\nThis blog post is based on joint work by the Open-Ended Learning Team (listed alphabetically by first name): Adam Stooke, Anuj Mahajan, Catarina Barros, Charlie Deck, Jakob Bauer, Jakub Sygnowski, Maja Trebacz, Max Jaderberg, Michael Mathieu, Nat McAleese, Nathalie Bradley-Schmieg, Nathaniel Wong, Nicolas Porcel, Roberta Raileanu, Steph Hughes-Fitt, Valentin Dalibard, Wojciech Marian Czarnecki.\n"}
{"title": "Putting the power of AlphaFold into the world\u2019s hands", "contents": "In July 2022, we released AlphaFold protein structure predictions for nearly all catalogued proteins known to science. Read the latest blog \nhere\n.\nToday, I\u2019m incredibly proud and excited to announce that DeepMind is making a significant contribution to humanity\u2019s understanding of biology.\nWhen we \nannounced AlphaFold 2\n last December, it was hailed as a solution to the 50-year old protein folding problem. Last week, we published the \nscientific paper\n and \nsource code\n explaining how we created this highly innovative system, and today we\u2019re sharing \nhigh-quality predictions\n for the shape of every single protein in the human body, as well as for the proteins of 20 additional organisms that scientists rely on for their research.\nAs researchers seek cures for diseases and pursue solutions to other big problems facing humankind \u2013 including antibiotic resistance, microplastic pollution, and climate change \u2013 they will benefit from fresh insights into the structure of proteins. Proteins are like tiny exquisite biological machines. The same way that the structure of a machine tells you what it does, so the structure of a protein helps us understand its function. Today, we are sharing \na trove of information\n that doubles \nhumanity\u2019s understanding of the human proteome\n, and reveals the protein structures found in 20 other biologically-significant organisms, from E.coli to yeast, and from the fruit fly to the mouse.\nAs a powerful tool that supports the efforts of researchers, we believe this is the most significant contribution AI has made to advancing scientific knowledge to date, and is a great example of the benefits AI can bring to humanity. \u00a0These insights will underpin many exciting future advances in our understanding of biology and medicine. Thanks to five tireless years of work and a lot of ingenuity from the AlphaFold team, and working closely for the past few months with our partners at \nEMBL\u2019s European Bioinformatics Institute (EMBL-EBI)\n, we are able to share this huge and valuable resource with the world.\n\u200d\nThis latest work builds on \nannouncements\n we made last December, at the CASP14 conference, when DeepMind unveiled a radical new version of our AlphaFold system, which was recognised by the organisers of the assessment as a solution to the 50-year old grand challenge to understand the 3D structure of proteins. Determining protein structures experimentally is a time-consuming and painstaking pursuit, but AlphaFold demonstrated that AI could accurately predict the shape of a protein, at scale and in minutes, down to atomic accuracy. At \nCASP\n, we pledged to share our methods and provide broad access to this body of knowledge.\nThis month, we\u2019ve finished the enormous amount of hard work to deliver on that commitment. We published two peer-reviewed papers in \nNature \n(\n1\n,\n2\n) and \nopen-sourced AlphaFold\u2019s code\n. Today, in partnership with \nEMBL-EBI\n, we\u2019re incredibly proud to be launching the \nAlphaFold Protein Structure Database\n, which offers the most complete and accurate picture of the human proteome to date, more than doubling humanity\u2019s accumulated knowledge of high-accuracy human protein structures.\nIn addition to the human proteome (all the ~20,000 proteins expressed by the human genome), we\u2019re providing open access to the proteomes of \n20 other biologically-significant organisms\n, totalling over 350,000 protein structures. Research into these organisms has been the subject of countless research papers and numerous major breakthroughs, and has resulted in a deeper understanding of life itself. In the coming months we plan to vastly expand the coverage \nto almost every sequenced protein known to science\n - over 100 million structures covering most of the \nUniProt reference database\n. It\u2019s a veritable protein almanac of the world. And the system and database will periodically be updated as we continue to invest in future improvements to AlphaFold.\nMost excitingly, in the hands of scientists around the world, this new protein almanac will enable and accelerate research that will advance our understanding of these building blocks of life. Already, through our early collaborations, we\u2019ve seen promising signals from researchers using AlphaFold in their own work. For instance, the \nDrugs for Neglected Diseases Initiative\n (DNDi) \nhas advanced their research into life-saving cures\n for diseases that disproportionately affect the poorer parts of the world, and the \nCentre for Enzyme Innovation\n at the University of Portsmouth (CEI) is using AlphaFold to help engineer faster enzymes for recycling some of our most polluting single-use plastics. For those scientists who rely on experimental protein structure determination, AlphaFold's predictions have helped accelerate their research. As another example, a team at the \nUniversity of Colorado Boulder\n is finding promise in using AlphaFold predictions to study antibiotic resistance, while a group at the \nUniversity of California San Francisco\n has used them to \nincrease their understanding of SARS-CoV-2 biology\n. And this is just the start of what we hope will be a revolution in structural bioinformatics. With AlphaFold out in the world, there is a treasure trove of data now waiting to be transformed into future advances.\nFor the AlphaFold team at DeepMind, this work represents the culmination of five years of enormous effort, including having to creatively overcome many challenging setbacks, resulting in a host of new sophisticated algorithmic innovations that were all needed to finally crack the problem. It builds on the discoveries of generations of scientists, from the early pioneers of protein imaging and crystallography, to the thousands of prediction specialists and structural biologists who\u2019ve spent years experimenting with proteins since. Our dream is that AlphaFold, by providing this foundational understanding, will aid countless more scientists in their work and open up completely new avenues of scientific discovery.\nAt DeepMind, our thesis has always been that artificial intelligence can dramatically accelerate breakthroughs in many fields of science, and in turn advance humanity. We built \nAlphaFold\n and the \nAlphaFold Protein Structure Database\n to support and elevate the efforts of scientists around the world in the important work they do. We believe AI has the potential to revolutionise how science is done in the 21st century, and we eagerly await the discoveries that AlphaFold might help the scientific community to unlock next.\nTo learn more, head over to Nature to read our peer-reviewed papers describing our \nfull method\n, and the \nhuman proteome\n. You can read more about them in our \nAuthors' Notes\n. If you want to explore our system, here\u2019s the \nopen-source code to AlphaFold\n and \nColab notebook\n to run individual sequences. To explore our structures, EMBL-EBI, the world leader in biological data, is hosting them in \na searchable database\n that is open and free to all. \nWe would love to hear your feedback and understand how AlphaFold has been useful in your research. Share your stories at \nalphafold@deepmind.com\n. \n"}
{"title": "Building architectures that can handle the world\u2019s data", "contents": "Most architectures used by AI systems today are specialists. A 2D residual network may be a good choice for processing images, but at best it\u2019s a loose fit for other kinds of data \u2014 such as the Lidar signals used in self-driving cars or the torques used in robotics. What\u2019s more, standard architectures are often designed with only one task in mind, often leading engineers to bend over backwards to reshape, distort, or otherwise modify their inputs and outputs in hopes that a standard architecture can learn to handle their problem correctly. Dealing with more than one kind of data, like the sounds and images that make up videos, is even more complicated and usually involves complex, hand-tuned systems built from many different parts, even for simple tasks. As part of DeepMind's mission of solving intelligence to advance science and humanity, we want to build systems that can solve problems that use many types of inputs and outputs, so we began to explore a more general and versatile architecture that can handle all types of data.\nIn a paper presented at \nICML 2021\n (the International Conference on Machine Learning) and \npublished as a preprint on arXiv\n, we introduced the Perceiver, a general-purpose architecture that can process data including images, point clouds, audio, video, and their combinations. While the Perceiver could handle many varieties of input data, it was limited to tasks with simple outputs, like classification. A \nnew preprint on arXiv\n describes Perceiver IO, a more general version of the Perceiver architecture. Perceiver IO can produce a wide variety of outputs from many different inputs, making it applicable to real-world domains like language, vision, and multimodal understanding as well as challenging games like StarCraft II. To help researchers and the machine learning community at large, we\u2019ve now \nopen sourced the code\n.\nPerceivers build on the \nTransformer\n, an architecture that uses an operation called \u201cattention\u201d to map inputs into outputs. By comparing all elements of the input, Transformers process inputs based on their relationships with each other and the task. Attention is simple and widely applicable, but Transformers use attention in a way that can quickly become expensive as the number of inputs grows. This means Transformers work well for inputs with at most a few thousand elements, but common forms of data like images, videos, and books can easily contain millions of elements. With the original Perceiver, we solved a major problem for a generalist architecture: scaling the Transformer\u2019s attention operation to very large inputs without introducing domain-specific assumptions. The Perceiver does this by using attention to first encode the inputs into a small latent array. This latent array can then be processed further at a cost independent of the input\u2019s size, enabling the Perceiver\u2019s memory and computational needs to grow gracefully as the input grows larger, even for especially deep models.\nThis \u201cgraceful growth\u201d allows the Perceiver to achieve an unprecedented level of generality \u2014 it\u2019s competitive with domain-specific models on benchmarks based on images, 3D point clouds, and audio and images together. But because the original Perceiver produced only one output per input, it wasn\u2019t as versatile as researchers needed. Perceiver IO fixes this problem by using attention not only to encode to a latent array but also to decode from it, which gives the network great flexibility. Perceiver IO now scales to large and diverse inputs \nand\n outputs, and can even deal with many tasks or types of data at once. This opens the door for all sorts of applications, like understanding the meaning of a text from each of its characters, tracking the movement of all points in an image, processing the sound, images, and labels that make up a video, and even playing games, all while using a single architecture that\u2019s simpler than the alternatives.\nIn our experiments, we\u2019ve seen Perceiver IO work across a wide range of benchmark domains \u2014 such as language, vision, multimodal data, and games \u2014 to provide an off-the-shelf way to handle many kinds of data. We hope \nour latest preprint \nand the code \navailable on Github\n help researchers and practitioners tackle problems without needing to invest the time and effort to build custom solutions using specialised systems. As we continue to learn from exploring new kinds of data, we look forward to further improving upon this general-purpose architecture and making it faster and easier to solve problems throughout science and machine learning.\n"}
{"title": "Nowcasting the next hour of rain", "contents": "Our lives are dependent on the weather. At any moment in the UK, according to \none study\n, one third of the country has talked about the weather in the past hour, reflecting the importance of weather in daily life. Amongst weather phenomena, rain is especially important because of its influence on our everyday decisions. Should I take an umbrella? How should we route vehicles experiencing heavy rain? What safety measures do we take for outdoor events? Will there be a flood? \nOur latest research\n and state-of-the-art model advances the science of \nPrecipitation Nowcasting\n, which is the prediction of rain (and other precipitation phenomena) within the next 1-2 hours. In a \npaper\n written in collaboration with the Met Office and published in Nature, we directly tackle this important \ngrand challenge\n in weather prediction. This collaboration between environmental science and AI focuses on value for decision-makers, opening up new avenues for the nowcasting of rain, and points to the opportunities for AI in supporting our response to the challenges of decision-making in an environment under constant change.\nThroughout history, the prediction of weather has held a place of importance for our communities and countries. \nMedieval meteorologists\n began by using the stars to make predictions. Slowly, tables recording seasons and rain patterns started to be kept. Centuries later, Lewis Fry imagined a \u2018\nForecast Factory\n\u2019 that used computation and the physical equations of the atmosphere to predict global weather. In this evolving book of weather prediction, we now add a story on the role of machine learning for forecasting.\nToday\u2019s weather predictions are driven by powerful \nnumerical weather prediction\n (NWP) systems. By solving physical equations, NWPs provide essential planet-scale predictions several days ahead. However, they struggle to generate high-resolution predictions for short lead times under two hours. Nowcasting fills the performance gap in this crucial time interval.\nNowcasting is essential for sectors like water management, agriculture, aviation, emergency planning, and \noutdoor events\n. Advances in weather sensing have made high-resolution radar data\u2013which measures the amount of precipitation at ground level\u2013available at high frequency (e.g., every 5 mins at 1 km resolution). This combination of a crucial area where existing methods struggle and the availability of high-quality data provides the opportunity for machine learning to make its contributions to nowcasting.\nWe focus on nowcasting rain: predictions up to 2 hours ahead that capture the amount, timing, and location of rainfall. We use an approach known as generative modelling to make detailed and plausible predictions of future radar based on past radar. Conceptually, this is a problem of generating radar movies. With such methods, we can both accurately capture large-scale events, while also generating many alternative rain scenarios (known as ensemble predictions), allowing rainfall uncertainty to be explored. We used radar data from both the UK and the US in our study results.\nWe were especially interested in the ability of these models to make predictions on medium to heavy-rain events, which are the events that most impact people and the economy, and we show statistically significant improvements in these regimes compared to competing methods. Importantly, we conducted a cognitive task assessment with more than 50 expert meteorologists at the Met Office, the UK\u2019s national meteorological service, \nwho rated our new approach as their first choice in 89% of cases when compared to widely-used nowcasting methods\n, demonstrating the ability of our approach to provide insight to real world decision-makers.\nBy using statistical, economic, and cognitive analyses we were able to demonstrate a new and competitive approach for precipitation nowcasting from radar. No method is without limitations, and more work is needed to improve the accuracy of long-term predictions and accuracy on rare and intense events. Future work will require us to develop additional ways of assessing performance, and further specialising these methods for specific real-world applications.\nWe think this is an exciting area of research and we hope our paper will serve as a foundation for new work by providing data and verification methods that make it possible to both provide competitive verification and operational utility. We also hope this collaboration with the Met Office will promote greater integration of machine learning and environmental science, and better support decision-making in our changing climate.\nRead the paper \nSkillful precipitation nowcasting using Deep Generative Models of Radar\n in the 30 September 2021 issue of Nature, which contains an extensive discussion of the model, data and verification approach. You can also explore the data we used for training and find a pre-trained model for the UK via \nGitHub\n.\nAcknowledgements. We are grateful to the Met Office and all our collaborators and advisors for their input to this work.\n"}
{"title": "Predicting gene expression with AI", "contents": "Based on Transformers, our new Enformer architecture advances genetic research by improving the ability to predict how DNA sequence influences gene expression.\nWhen the \nHuman Genome Project\n succeeded in mapping the DNA sequence of the human genome, the international research community were excited by the opportunity to better understand the genetic instructions that influence human health and development. DNA carries the genetic information that determines everything from eye colour to susceptibility to certain diseases and disorders. The roughly 20,000 sections of DNA in the human body known as genes contain instructions about the amino acid sequence of proteins, which perform numerous essential functions in our cells. Yet these genes make up less than 2% of the genome. The remaining base pairs \u2014 which account for 98% of the 3 billion \u201cletters\u201d in the genome \u2014 are called \u201cnon-coding\u201d and contain less well-understood instructions about when and where genes should be produced or expressed in the human body. At DeepMind, we believe that AI can unlock a deeper understanding of such complex domains, accelerating scientific progress and offering potential benefits to human health.\nToday Nature Methods published \u201c\nEffective gene expression prediction from sequence by integrating long-range interactions\n\u201d (first shared as a preprint on \nbioRxiv\n), in which we \u2014 in collaboration with our Alphabet colleagues at \nCalico\n \u2014 introduce a neural network architecture called Enformer that led to greatly increased accuracy in predicting gene expression from DNA sequence. To advance further study of gene regulation and causal factors in diseases, we also made our model and its initial predictions of common genetic variants \nopenly available here\n.\nPrevious work on gene expression has typically used convolutional neural networks as fundamental building blocks, but their limitations in modelling the influence of distal enhancers on gene expression have hindered their accuracy and application. Our initial explorations relied on \nBasenji2\n, which could predict regulatory activity from relatively long DNA sequences of 40,000 base pairs. Motivated by this work and the knowledge that regulatory DNA elements can influence expression at greater distances, we saw the need for a fundamental architectural change to capture long sequences.\nWe developed a new model based on \nTransformers\n, common in natural language processing, to make use of self-attention mechanisms that could integrate much greater DNA context. Because Transformers are ideal for looking at long passages of text, we adapted them to \u201cread\u201d vastly extended DNA sequences. By effectively processing sequences to consider interactions at distances that are more than 5 times (i.e., 200,000 base pairs) the length of previous methods, our architecture can model the influence of important regulatory elements called enhancers on gene expression from further away within the DNA sequence.\nTo better understand how Enformer interprets the DNA sequence to arrive at more accurate predictions, we used contribution scores to highlight which parts of the input sequence were most influential for the prediction. Matching the biological intuition, we observed that the model paid attention to enhancers even if located more than 50,000 base pairs away from the gene. Predicting which enhancers regulate which genes remains a major unsolved problem in genomics, so we were pleased to see the contribution scores of Enformer perform comparably with existing methods developed specifically for this task (using experimental data as input). Enformer also learned about insulator elements, which separate two independently regulated regions of DNA.\nAlthough it\u2019s now possible to study an organism's DNA in its entirety, complex experiments are required to understand the genome. Despite an enormous experimental effort, the vast majority of the DNA control over gene expression remains a mystery. With AI, we can explore new possibilities for finding patterns in the genome and provide mechanistic hypotheses about sequence changes. Similar to a spell checker, Enformer partially understands the vocabulary of the DNA sequence and can thereby highlight edits that could lead to altered gene expression.\nThe main application of this new model is to predict which changes to the DNA letters, also called genetic variants, will alter the expression of the gene. Compared to previous models, Enformer is significantly more accurate at predicting the effects of variants on gene expression, both in the case of natural genetic variants and synthetic variants that alter important regulatory sequences. This property is useful for interpreting the growing number of disease-associated variants obtained by genome-wide association studies. Variants associated with complex genetic diseases are predominantly located in the non-coding region of the genome, likely causing disease by altering gene expression. But due to inherent correlations among variants, many of these disease-associated variants are only spuriously correlated rather than causative. Computational tools can now help distinguish the true associations from false positives.\nWe\u2019re far from solving the untold puzzles that remain in the human genome, but Enformer is a step forward in understanding the complexity of genomic sequences. If you\u2019re interested in using AI to explore how fundamental cell processes work, how they\u2019re encoded in the DNA sequence, and how to build new systems to advance genomics and our understanding of disease, \nwe\u2019re hiring\n. We\u2019re also looking forward to expanding our collaborations with other researchers and organisations eager to explore computational models to help solve the open questions at the heart of genomics.\n"}
{"title": "The Podcast: Episode 3: Life is like a game", "contents": "Video games have become a favourite tool for AI researchers to test the abilities of their systems. In this episode, Hannah sits down to play StarCraft II - a challenging video game that requires players to control the onscreen action with as many as 800 clicks a minute.\nShe is guided by Oriol Vinyals, an ex-professional StarCraft player and research scientist at DeepMind, who explains how the program AlphaStar learnt to play the game and beat a top professional player. Elsewhere, she explores systems that are learning to cooperate in a digital version of the playground favourite \u2018Capture the Flag\u2019.\nInterviewees:\n Research scientists Max Jaderberg and Raia Hadsell; Lead researchers David Silver and Oriol Vinyals, and Director of Research Koray Kavukcuoglu.\nListen to this episode and subscribe to the whole series on\n Apple podcasts\n,\n \nGoogle podcasts\n,\n \nSpotify\n,\n \nDeezer\n or your favourite podcast app by searching for \u201cDeepMind: The Podcast\u201d.\nIf you know of other resources we should link to, please help other listeners by either replying to us on\n Twitter\n (#DMpodcast) or emailing us at podcast@deepmind.com. You can also use that address to send us questions or feedback on the series.\nPresenter:\n Hannah Fry\nEditor:\n David Prest\nSenior Producer:\n Louisa Field\nProducers:\n Amy Racs, Dan Hardoon\nBinaural Sound:\n Lucinda Mason-Brown\nMusic composition:\n Eleni Shaw (with help from Sander Dieleman and \nWaveNet\n)\n"}
{"title": "The Podcast: Episode 6: AI for everyone", "contents": "While there is a lot of excitement about AI research, there are also concerns about the way it might be implemented, used and abused.\nIn this episode Hannah investigates the more human side of the technology, some ethical issues around how it is developed and used, and the efforts to create a future of AI that works for everyone. \nInterviewees: \nVerity Harding, Co-Lead of DeepMind Ethics and Society; DeepMind\u2019s COO Lila Ibrahim, and research scientists William Isaac and Silvia Chiappa.\nListen to this episode and subscribe to the whole series on \nApple podcasts\n,\n Google podcasts\n,\n Spotify\n,\n Deezer\n or your favourite podcast app by searching for \u201cDeepMind: The Podcast\u201d.\nIf you know of other resources we should link to, please help other listeners by either replying to us on \nTwitter\n (#DMpodcast) or emailing us at podcast@deepmind.com. You can also use that address to send us questions or feedback on the series.\nPresenter: Hannah Fry\nEditor: David Prest\nSenior Producer: Louisa Field\nProducers: Amy Racs, Dan Hardoon\nBinaural Sound: Lucinda Mason-Brown\nMusic composition: Eleni Shaw (with help from Sander Dieleman and \nWaveNet\n)\n"}
{"title": "The Podcast: Episode 4: AI, Robot", "contents": "Forget what sci-fi has told you about superintelligent robots that are uncannily human-like; the reality is more prosaic. Inside DeepMind\u2019s robotics laboratory, Hannah explores what researchers call \u2018embodied AI\u2019: robot arms that are learning tasks like picking up plastic bricks, which humans find comparatively easy.\nDiscover the cutting-edge challenges of bringing AI and robotics together, and learning from scratch how to perform tasks. She also explores some of the key questions about using AI safely in the real world.\nInterviewees: \nSoftware engineer Jackie Kay and research scientists Murray Shanahan, Victoria Krakovna, Raia Hadsell and Jan Leike.\nListen to this episode and subscribe to the whole series on\n Apple podcasts\n,\n Google podcasts\n,\n Spotify\n,\n Deezer\n or your favourite podcast app by searching for \u201cDeepMind: The Podcast\u201d.\nIf you know of other resources we should link to, please help other listeners by either replying to us on\n Twitter\n (#DMpodcast) or emailing us at podcast@deepmind.com. You can also use that address to send us questions or feedback on the series.\nPresenter:\n Hannah Fry\nEditor:\n David Prest\nSenior Producer:\n Louisa Field\nProducers:\n Amy Racs, Dan Hardoon\nBinaural Sound:\n Lucinda Mason-Brown\nMusic composition:\n Eleni Shaw (with help from Sander Dieleman and \nWaveNet\n)\n"}
{"title": "The Podcast: Episode 5: Out of the lab", "contents": "The ambition of AI research is to create systems that can help to solve problems in the real world.\nIn this episode, Hannah Fry meets the people building systems that could be used to save the sight of thousands, help us solve one of the most fundamental problems in biology and reduce energy consumption in an effort to combat climate change. But whilst there is great potential, there are also important obstacles that will need to be tackled for AI to be used effectively, safely and fairly.\nInterviewees: \nPearse Keane, consultant ophthalmologist at Moorfields Eye Hospital; Sandy Nelson, Product Manager for DeepMind\u2019s Science Program; and DeepMind Program Manager Sims Witherspoon. Presented by Hannah Fry.\nListen to this episode and subscribe to the whole series on\n Apple podcasts\n,\n Google podcasts\n,\n Spotify\n,\n Deezer\n or your favourite podcast app by searching for \u201cDeepMind: The Podcast\u201d.\nIf you know of other resources we should link to, please help other listeners by either replying to us on\n Twitter\n (#DMpodcast) or emailing us at podcast@deepmind.com. You can also use that address to send us questions or feedback on the series.\nPresenter:\n Hannah Fry\nEditor:\n David Prest\nSenior Producer:\n Louisa Field\nProducers:\n Amy Racs, Dan Hardoon\nBinaural Sound:\n Lucinda Mason-Brown\nMusic composition:\n Eleni Shaw (with help from Sander Dieleman and \nWaveNet\n)\n"}
{"title": "Replay in biological and artificial neural networks", "contents": "One of a series of posts explaining the theories underpinning our research.\nOur waking and sleeping lives are punctuated by fragments of recalled memories: a sudden connection in the shower between seemingly disparate thoughts, or an ill-fated choice decades ago that haunts us as we struggle to fall asleep. By measuring memory retrieval directly in the brain, neuroscientists have noticed something remarkable: spontaneous recollections, measured directly in the brain, often occur as very fast \nsequences\n of multiple memories. These so-called 'replay' sequences play out in a fraction of a second\u2013so fast that we're not necessarily aware of the sequence.\nIn parallel, AI researchers discovered that incorporating a similar kind of \nexperience replay\n improved the efficiency of learning in artificial neural networks. Over the last three decades, the AI and neuroscientific studies of replay have grown up together. Machine learning offers hypotheses sophisticated enough to push forward our expanding knowledge of the brain; and insights from neuroscience guide and inspire AI development. Replay is a key point of contact between the two fields because like the brain, AI uses experience to learn and improve. And each piece of experience offers much more potential for learning than can be absorbed in real-time\u2013so continued offline learning is crucial for both brains and artificial neural nets.\nNeural replay sequences were originally discovered by studying the hippocampus in rats. As we know from the Nobel prize winning work of \nJohn O\u2019Keefe\n and others, many hippocampal cells fire only when the animal is physically located in a specific \nplace\n. In early experiments, \nrats\n ran the length of a single corridor or circular track, so researchers could easily determine which neuron coded for each position within the corridor.\nAfterwards, the scientists recorded from the same neurons while the rats rested. During rest, the cells sometimes spontaneously fired in rapid sequences demarking the same path the animal ran earlier, but at a greatly accelerated speed. These sequences are called replay. An entire replay sequence only lasts a fraction of a second, but plays through several seconds worth of real experience.\nWe now know replay is essential for learning. In a number of more recent experiments, researchers recorded from hippocampus to detect a signature of replay events in real time. By disrupting brain activity during replay events (either during \nsleep\n or \nwakeful\n resting), scientists significantly impaired rodents\u2019 ability to learn a new task. The same disruption applied 200 milliseconds out of sync with replay events had no effect on learning.\nWhile these experiments have been revealing, a significant limitation of rodent experiments is the difficulty of studying more sophisticated aspects of cognition, such as abstract concepts. In the last few years, replay-like sequences have also been \ndetected\n in human brains, supporting the idea that replay is pervasive, and expanding the kinds of questions we can ask about it.\nIncorporating replay\n in silico \nhas been beneficial to advancing \nartificial intelligence\n. Deep learning often depends upon a ready supply of large datasets. In \nreinforcement learning\n, these data come through direct interaction with the environment, which takes time. The technique of \nexperience\n \nreplay\n allows the agent to repeatedly rehearse previous interactions, making the most of each interaction. This method proved crucial for combining deep neural networks with reinforcement learning in the \nDQN\n agent that first mastered multiple Atari games.\nSince the introduction of DQN, the efficiency of replay has been improved by \npreferentially\n \nreplaying\n \nthe\n most salient experiences from memory, rather than simply choosing experiences at random for replay. And recently, a variant of preferential replay has been \napplied\n as a model in neuroscience to successfully explain empirical data from brain recordings.\nFurther improvements in agent performance have come from \ncombining experiences\n across multiple agents, learning about a variety of \ndifferent behaviours\n from the same set of experiences, and replaying not only the trajectory of events in the world, but also the agent's corresponding \ninternal memory states\n. Each of these methods makes interesting predictions for neuroscience that remain largely untested.\nAs mentioned above, research into experience replay has unfolded along parallel tracks in artificial intelligence and neuroscience, with each field providing ideas and inspiration for the other. In particular, there is a central distinction, which has been studied in both fields, between two versions of replay.\nSuppose you come home and, to your surprise and dismay, discover water pooling on your beautiful wooden floors. Stepping into the dining room, you find a broken vase. Then you hear a whimper, and you glance out the patio door to see your dog looking very guilty.\nIn the first version of replay, which we could call the \"movie\" version, when you sit down on the couch and take a rest, replay faithfully rehearses the actual experiences of the past. This theory says that your brain will replay the sequence: \"water, vase, dog\". In AI terms, the past experience was stored in a replay buffer, and trajectories for offline learning were drawn directly from the buffer.\n\u200d\nIn the second version, which we might call \"imagination,\" replay doesn\u2019t literally rehearse events in the order they were experienced. Instead, it infers or imagines the \nreal relationships\n between events, and synthesises sequences that make sense given an understanding of how the world works. In AI terms, these replay sequences are \ngenerated\n using a \nlearned\n \nmodel\n of the environment.\nThe imagination theory makes a different prediction about how replay will look: when you rest on the couch, your brain should replay the sequence \"dog, vase, water\". You know from past experience that dogs are more likely to cause broken vases than broken vases are to cause dogs\u2013and this knowledge can be used to reorganise experience into a more meaningful order.\nIn deep RL, the large majority of agents have used movie-like replay, because it is easy to implement (the system can simply store events in memory, and play them back as they happened). However, RL researchers have continued to study the possibilities around imagination replay.\nMeanwhile in neuroscience, classic theories of replay postulated that movie replay would be useful to strengthen the connections between neurons that represent different events or locations in the order they were experienced. However, there have been \nhints\n from experimental neuroscience that replay might be able to imagine new sequences. The most compelling \nobservation\n is that even when rats only experienced two arms of a maze separately, subsequent replay sequences sometimes followed trajectories from one arm into the other.\nBut studies like this leave open the question of whether replay simply stitches together chunks of experienced sequences, or if it can synthesise new trajectories from whole cloth. Also, rodent experiments have been primarily limited to \nspatial\n sequences, but it would be fascinating to know whether humans' ability to imagine sequences is enriched by our vast reserve of abstract \nconceptual\n knowledge.\nWe asked these questions in a set of recent \nexperiments\n performed jointly between UCL, Oxford, and DeepMind.\nIn these experiments, we first taught people a rule that defined how a set of objects could interact. The exact rule we used can be found in the paper. But to continue in the language of the \"water, vase, dog\" example, we can think of the rule as the knowledge that dogs can cause broken vases, and broken vases can cause water on the floor. We then presented these objects to people in a \nscrambled\n order (like \"water, vase, dog\"). That way, we could ask whether their brains replayed the items in the scrambled order that they experienced, or in the unscrambled order that meaningfully connected the items. They were shown the scrambled sequence and then given five minutes to rest, while sitting in an \nMEG\n brain scanner.\nAs in previous experiments, fast replay sequences of the objects were evident in the brain recordings. (In yet another example of the \nvirtuous circle\n between neuroscience and AI, we used machine learning to read out these signatures from cortical activity.) These spontaneous sequences played out rapidly over about a sixth of a second, and contained up to four objects in a row. However, the sequences did not play out in the \nexperienced\n order (i.e., the scrambled order: spilled water \u2013> vase \u2013> dog). Instead, they played out the \nunscrambled\n, meaningful order: dog \u2013> vase \u2013> spilled water. This answers\u2013in the affirmative\u2013the questions of whether replay can imagine new sequences from whole cloth, and whether these sequences are shaped by abstract knowledge.\nHowever, this finding still leaves open the important question of \nhow\n the brain builds these unscrambled sequences. To try to answer this, we played a second sequence for participants. In this sequence, you walk into your factory and see spilled oil on the floor. You then see a knocked over oil barrel. Finally, you turn to see a guilty robot. To unscramble this sequence, you can use the same kind of knowledge as in the \"water, vase, dog\" sequence: knowledge that a mobile agent can knock over containers, and those knocked-over containers can spill liquid. Using that knowledge, the second sequence can also be unscrambled: robot \u2013> barrel \u2013> spilled oil.\nBy showing people multiple sequences with the same structure, we could examine two new types of neural representation. First, the part of the representation that is \ncommon\n between spilled water and spilled oil. This is an \nabstract\n code for \"a spilled liquid\", invariant over whether we're in the home sequence or the factory sequence. And second, the part of the representation that is common between water, vase and dog. This is an abstract code for \"the home sequence,\" invariant over which object we're considering.\nWe found both of these types of abstract codes in the brain data. And to our surprise, during rest they played out in fast sequences that were precisely coordinated with the spontaneous replay sequences mentioned above. Each object in a replay sequence was preceded slightly by both abstract codes. For example, during a dog, vase, water replay sequence, the representation of \"water\" was preceded by the codes for \"home sequence\" and \"spilled liquid\".\nThese abstract codes, which incorporate the conceptual knowledge that lets us unscramble the sequences, may help the brain to retrieve the correct item for the next slot in the replay sequence. This paints an interesting picture of a system where the brain slots new information into an \nabstract\n \nframework\n \nbuilt\n from past experiences, keeping it organized using precise relative timings within very fast replay sequences. Each position within a sequence could be thought of as a role in an \nanalogy\n (as in the above figure). Finally, we speculate that during rest, the brain may explore novel implications of previously-learned knowledge by placing an item into an analogy in which it's never been experienced, and examining the consequences.\nComing back to the virtuous circle, analogy and abstraction are relatively underused in current neural network architectures. The new results described above both indicate that the imagination style of replay may be a fruitful avenue to continue pursuing in AI research, and suggest directions for neuroscience research to learn more about the brain mechanisms underlying analogy and abstraction. It's exciting to think about how data from the brain will continue helping with the advance toward better and more human-like artificial intelligence.\nThe \nnew work reported in Cell\n was done by \nYunzhe Liu\n, \nRay Dolan\n, \nZeb Kurth-Nelson\n and \nTim Behrens\n, and was a collaboration between DeepMind, the Wellcome Centre for Human Neuroimaging (UCL), the Max Planck-UCL Centre for Computational Psychiatry and Ageing Research, and the Wellcome Centre For Integrative Neuroimaging (Oxford).\n"}
{"title": "The Podcast: Episode 7: Towards the future", "contents": "AI researchers around the world are trying to create a general purpose learning system that can learn to solve a broad range of problems without being taught how.\nKoray Kavukcuoglu, DeepMind\u2019s Director of Research, describes the journey to get there, and takes Hannah on a whistle-stop tour of DeepMind\u2019s HQ and its research.\nInterviewees:\n Koray Kavukcuoglu, Director of Research; Trevor Back, Product Manager for DeepMind\u2019s science research; research scientists Raia Hadsell and Murray Shanahan; and DeepMind CEO and co-founder, Demis Hassabis.\nListen to this episode and subscribe to the whole series on\n Apple podcasts\n,\n Google podcasts\n,\n Spotify\n,\n Deezer\n or your favourite podcast app by searching for \u201cDeepMind: The Podcast\u201d.\nIf you know of other resources we should link to, please help other listeners by either replying to us on \nTwitter\n (#DMpodcast) or emailing us at podcast@deepmind.com. You can also use that address to send us questions or feedback on the series.\nPresenter: Hannah Fry\nEditor: David Prest\nSenior Producer: Louisa Field\nProducers: Amy Racs, Dan Hardoon\nBinaural Sound: Lucinda Mason-Brown\nMusic composition: Eleni Shaw (with help from Sander Dieleman and \nWaveNet\n)\n"}
{"title": "The Podcast: Episode 8: Demis Hassabis - The interview", "contents": "In this special extended episode, Hannah Fry meets Demis Hassabis, the CEO and co-founder of DeepMind.\nShe digs into his former life as a chess player, games designer and neuroscientist and explores how his love of chess helped him to get start-up funding, what drives him and his vision, and why AI keeps him up at night. \nInterviewees: \nDeepmind CEO and co-founder, Demis Hassabis\nListen to this episode and subscribe to the whole series on\n Apple podcasts\n,\n Google podcasts\n,\n Spotify\n,\n Deezer\n or your favourite podcast app by searching for \u201cDeepMind: The Podcast\u201d.\nIf you know of other resources we should link to, please help other listeners by either replying to us on \nTwitter\n (#DMpodcast) or emailing us at podcast@deepmind.com. You can also use that address to send us questions or feedback on the series.\nPresenter: Hannah Fry\nEditor: David Prest\nSenior Producer: Louisa Field\nProducers: Amy Racs, Dan Hardoon\nBinaural Sound: Lucinda Mason-Brown\nMusic composition: Eleni Shaw (with help from Sander Dieleman and \nWaveNet\n)\n"}
{"title": "DeepMind\u2019s health team joins Google Health", "contents": "Over the last three years, DeepMind has built a team to tackle some of healthcare\u2019s most complex problems\u2014developing AI research and mobile tools that are already having a positive impact on patients and care teams. Today, with our healthcare partners, the team is excited to officially join the \nGoogle Health\n family. Under the leadership of \nDr. David Feinberg\n, and alongside other teams at Google, we\u2019ll now be able to tap into global expertise in areas like app development, data security, cloud storage and user design to build products that support care teams and improve patient outcomes. \nDuring my time working in the UK National Health Service (NHS) as a surgeon and researcher, I saw first-hand how technology could help, or hinder, the important work of nurses and doctors. It\u2019s remarkable that many frontline clinicians, even in the most world\u2019s most advanced hospitals, are still reliant on clunky desktop systems and pagers that make delivering fast and safe patient care challenging. Thousands of people die in hospitals every year from avoidable conditions like sepsis and acute kidney injury and we believe that better tools could save lives. That\u2019s why I joined DeepMind, and why I will continue this work with Google Health. \nWe\u2019ve already seen how our \nmobile medical assistant\n for clinicians is helping patients and the clinicians looking after them, and we are looking forward to continuing our partnerships with The Royal Free London NHS Foundation Trust, Imperial College Healthcare NHS Trust and Taunton and Somerset NHS Foundation Trust.\nOn the research side, we\u2019ve seen major advances with Moorfields Eye Hospital NHS Foundation Trust \nin detecting eye disease from scans \nas accurately as experts; with University College London Hospitals NHS Foundation Trust on planning \ncancer radiotherapy treatment\n; and with the US Department of Veterans Affairs to \npredict patient deterioration\n up to 48 hours earlier than currently possible. We see enormous potential in continuing, and scaling, our work with all three partners in the coming years as part of Google Health. \nIt\u2019s clear that a \ntransition like this\n takes time. Health data is sensitive, and we gave proper time and care to make sure that we had the full consent and cooperation of our partners. This included giving them the time to ask questions and fully understand our plans and to choose whether to continue our partnerships. As has always been the case, our partners are in full control of all patient data and we will only use patient data to help improve care, under their oversight and instructions.\nI know DeepMind is proud of our healthcare work to date. With the expertise and reach of Google behind us, we\u2019ll now be able to develop tools and technology capable of helping millions of patients around the world. \nThis blog also appears on \nThe Keyword\n"}
{"title": "Causal Bayesian Networks: A flexible tool to enable fairer machine learning", "contents": "Decisions based on machine learning (ML) are potentially advantageous over human decisions, as they do not suffer from the same subjectivity, and can be more accurate and easier to analyse. At the same time, data used to train ML systems often contain human and societal biases that can lead to harmful decisions: extensive evidence in areas such as hiring, criminal justice, surveillance, and healthcare suggests that ML decision systems can treat individuals unfavorably (\nunfairly\n) on the basis of characteristics such as race, gender, disabilities, and sexual orientation \u2013 referred to as \nsensitive attributes\n. \u00a0\nCurrently, most fairness criteria used for evaluating and designing ML decision systems focus on the relationships between the sensitive attribute and the system output. However, the training data can display different patterns of unfairness depending on how and why the sensitive attribute influences other variables. Using such criteria without fully accounting for this could be problematic: it could, for example, lead to the erroneous conclusion that a model exhibiting harmful biases is fair and, vice-versa, that a model exhibiting harmless biases is unfair. The development of technical solutions to fairness also requires considering the different, potentially intricate, ways in which unfairness can appear in the data. \nUnderstanding how and why a sensitive attribute influences other variables in a dataset can be a challenging task, requiring both a technical and sociological analysis. The visual, yet mathematically precise, framework of \nCausal\n \nBayesian\n \nnetworks\n (CBNs) represents a flexible useful tool in this respect as it can be used to formalize, measure, and deal with different unfairness scenarios underlying a dataset. A CBN (Figure 1) is a graph formed by nodes representing random variables, connected by links denoting causal influence. By defining unfairness as the presence of a harmful influence from the sensitive attribute in the graph, CBNs provide us with a simple and intuitive \nvisual tool\n for describing different possible unfairness scenarios underlying a dataset. In addition, CBNs provide us with a powerful \nquantitative tool \nto measure unfairness in a dataset and to help researchers develop techniques for addressing it. \nConsider a hypothetical college admission example (\ninspired by the Berkeley case\n) in which applicants are admitted based on qualifications Q, choice of department D, and gender G; and in which female applicants apply more often to certain departments (for simplicity\u2019s sake, we consider gender as binary, but this is not a necessary restriction imposed by the framework). \nDefinition: In a CBN, a path from node X to node Z is defined as a sequence of linked nodes starting at X and ending at Z. X is a cause of (has an influence on) Z if there exists a causal path from X to Z, namely a path whose links are pointing from the preceding nodes toward the following nodes in the sequence. For example, in Figure 1, the path G\u2192D\u2192A is causal, whilst the path G\u2192D\u2192A\u2190Q is non causal.\nThe admission process is represented by the CBN in Figure 1. Gender has a direct influence on admission through the causal path G\u2192A and an indirect influence through the causal path G\u2192D\u2192A. The direct influence captures the fact that individuals with the same qualifications who are applying to the same department might be treated differently based on their gender. The indirect influence captures differing admission rates between female and male applicants due to their differing department choices. \nWhilst the direct influence of the sensitive attribute on admission is considered unfair for social and legal reasons, the indirect influence could be considered fair or unfair depending on contextual factors. In Figure 2a, 2b and 2c, we depict three possible scenarios, where total or partial red paths are used to indicate unfair and and partially-unfair links, respectively.\nThis simplified example shows how CBNs can provide us with a visual framework for describing different possible unfairness scenarios. Understanding which scenario underlies a dataset can be challenging or even impossible, and might require expert knowledge. It is nevertheless necessary to avoid pitfalls when evaluating or designing a decision system. \nAs an example, let\u2019s assume that a university uses historical data to train a decision system to decide whether a prospective applicant should be admitted, and that a regulator wants to evaluate its fairness. Two popular fairness criteria are \nstatistical parity \n(requiring the same \nadmission rates \namong female and male applicants) and \nequal false positive or negative rates\n (EFPRs/EFNRs, requiring the same \nerror rates \namong female and male applicants: i.e., the percentage of accepted applicants erroneously predicted as rejected, and vice-versa). In other words, statistical parity and EFPRs/EFNRs require all the predictions and the incorrect predictions to be independent of gender.\nFrom the discussion above, we can deduce that whether such criteria are appropriate or not strictly depends on the nature of the data pathways. Due to the presence of the unfair direct influence of gender on admission, it would be inappropriate for the regulator to use EFPRs/EFNRs to gauge fairness, because this criterion considers the influence that gender has on admission in the data as legitimate. This means that it would be possible for the system to be deemed fair, even if it carries the unfair influence: this would automatically be the case for an error-free decision system. On the other hand, if the path G\u2192D\u2192A was considered fair, it would be inappropriate to use statistical parity. In this case, it would be possible for the system to be deemed unfair, even if it does not contain the unfair direct influence of gender on admission through the path G\u2192A and only contains the fair indirect influence through the path G\u2192D\u2192A. In \nour first paper\n, we raise these concerns in the context of the fairness debate surrounding the COMPAS pretrial risk assessment tool, which has been central to the dialogue around the risks of using ML decision systems.\nCBNs can also be used to quantify unfairness in a dataset and to design techniques for alleviating unfairness in the case of complex relationships in the data. \nPath-specific techniques enable us to estimate the influence that a sensitive attribute has on other variables along specific sets of causal paths. This can be used to measure the degree of unfairness on a given dataset in complex scenarios in which some causal paths are considered unfair whilst other causal paths are considered fair. In the college admission example in which the path G\u2192D\u2192A is considered fair, path-specific techniques would enable us to measure the influence of G on A restricted to the direct path G\u2192A over the whole \npopulation\n, in order to obtain an estimate of the degree of unfairness contained in the dataset.\nSidenote: It's worth noting that, in our simple example, we do not consider the presence of confounders for the influence of G on A. In this case, as there are no unfair causal paths from G to A except the direct one, the degree of unfairness could simply be obtained by measuring the discrepancy between p(A | G=0, Q, D) and p(A | G=1, Q, D), where p(A | G=0, Q, D) indicates the distribution of A conditioned on the candidate being male, their qualifications, and department choice.\nThe additional use of counterfactual inference techniques would enable us to ask if a specific\n individual\n was treated unfairly, for example by asking whether a rejected female applicant (G=1, Q=q, D=d, A=0) would have obtained the same decision in a counterfactual world in which her gender were male along the direct path G\u2192A. In this simple example, assuming that the admission decision is obtained as the deterministic function f of G, Q, and D, i.e., A = f(G, Q, D), this corresponds to asking if f(G=0, Q=q, D=d) = 0, namely if a male applicant with the same department choice and qualifications would have also been rejected. We exemplify this in Figure 3 by re-computing the admission decision after changing the female candidate's photo to a male one in the profile. \nHowever, path-specific counterfactual inference is generally more complex to achieve, if some variables are unfairly influenced by G. Assume that G also has an influence on Q through a direct path G\u2192Q which is considered unfair. In this case, the CBN contains both variables that are fairly and unfairly influenced by G. Path-specific counterfactual inference would consist in performing a counterfactual correction of q, q_0, i.e of the variable which is unfairly influenced by G, and then computing the counterfactual decision as f(G=0, Q=q_0, D=d). The counterfactual correction q_0 is obtained by first using the information of the female applicant (G=1, Q=q, D=d, A=0) and knowledge about the CBN to get an estimate of the specific latent randomness in the makeup of the applicant, and then using this estimate to re-compute the value of Q as if G=0 along G\u2192Q. \nIn addition to answering questions of fairness in a dataset, path-specific counterfactual inference could be used to design methods to alleviate the unfairness of an ML system. In our \nsecond paper\n, we propose a method to perform path-specific counterfactual inference and suggest that it can be used to post-process the unfair decisions of a trained ML system by replacing them with counterfactual decisions. The resulting system is said to satisfy path-specific counterfactual fairness.\nAs machine learning continues to be embedded in more systems which have a significant impact on people\u2019s lives and safety, it is incumbent on researchers and practitioners to identify and address potential biases embedded in how training data sets are generated. Causal Bayesian Networks offer a powerful visual and quantitative tool for expressing the relationships among random variables in a dataset. While it is important to acknowledge the limitations and difficulties of using this tool \u2013 such as identifying a CBN that accurately describes the dataset\u2019s generation, dealing with confounding variables, and performing counterfactual inference in complex settings \u2013 this unique combination of capabilities could enable a deeper understanding of complex systems and allow us to better align decision systems with society's values.\nThis post is based on the following papers: \nA Causal Bayesian Networks Viewpoint on Fairness\nPath-Specific Counterfactual Fairness\nWith thanks to Niki Kilbertus, Ben Coppin, Tom Stepleton, \u00a0Ray Jiang, Christina Heinze-Deml, Tom Everitt, and Shira Mitchell.\n"}
{"title": "AlphaStar: Grandmaster level in StarCraft II using multi-agent reinforcement learning", "contents": "TL;DR: \nAlphaStar is the first AI to reach the top league of a widely popular esport without any game restrictions. \nThis January\n, a preliminary version of AlphaStar challenged two of the world's top players in StarCraft II, one of the most enduring and popular real-time strategy video games of all time. Since then, we have taken on a much greater challenge: playing the full game at a Grandmaster level under professionally approved conditions. \nWe chose to use general-purpose machine learning techniques \u2013 including neural networks, self-play via reinforcement learning, multi-agent learning, and imitation learning \u2013 to learn directly from game data with general purpose techniques. Using the advances described in our \nNature paper\n, AlphaStar was ranked above 99.8% of active players on Battle.net, and achieved a Grandmaster level for all three StarCraft II races: Protoss, Terran, and Zerg. We expect these methods could be applied to many other domains.\nLearning-based systems and self-play are elegant research concepts which have facilitated remarkable advances in artificial intelligence. In 1992, researchers at IBM developed TD-Gammon, combining a learning-based system with a neural network to play the game of backgammon. Instead of playing according to hard-coded rules or heuristics, TD-Gammon was designed to use \nreinforcement learning\n to figure out, through trial-and-error, how to play the game in a way that maximises its probability of winning. Its developers used the notion of \nself-play \nto make the system more robust: by playing against versions of itself, the system grew increasingly proficient at the game. When combined, the notions of learning-based systems and self-play provide a powerful paradigm of open-ended learning.\nMany advances since then have demonstrated that these approaches can be scaled to progressively challenging domains. For example, AlphaGo and AlphaZero established that it was possible for a system to \nlearn\n to achieve superhuman performance at Go, chess, and shogi, and \nOpenAI Five\n and \nDeepMind\u2019s FTW\n demonstrated the power of self-play in the modern games of Dota 2 and Quake III. \nAt DeepMind, we\u2019re interested in understanding the potential \u2013 and limitations \u2013 of open-ended learning, which enables us to develop robust and flexible agents that can cope with complex, real-world domains. Games like StarCraft are an excellent training ground to advance these approaches, as players must use limited information to make dynamic and difficult decisions that have ramifications on multiple levels and timescales. \nDespite its successes, self-play suffers from well known drawbacks. The most salient one is forgetting: an agent playing against itself may keep improving, but it also may forget how to win against a previous version of itself. Forgetting can create a cycle of an agent \u201cchasing its tail\u201d, and never converging or making real progress. For example, in the game rock-paper-scissors, an agent may currently prefer to play rock over other options. As self-play progresses, a new agent will then choose to switch to paper, as it wins against rock. Later, the agent will switch to scissors, and eventually back to rock, creating a cycle. Fictitious self-play - playing against a mixture of all previous strategies - is one solution to cope with this challenge.\nAfter first \nopen-sourcing StarCraft II\n as a research environment, we found that even fictitious self-play techniques were insufficient to produce strong agents, so we set out to develop a better, general-purpose solution. A central idea of our recently published Nature paper extends the notion of fictitious self-play to a group of agents \u2013 the League. Normally in self-play, every agent maximises its probability of winning against its opponents; however, this was only part of the solution. In the real world, a player trying to improve at StarCraft may choose to do so by partnering with friends so that they can train particular strategies. As such, their training partners are not playing to win against every possible opponent, but are instead exposing the flaws of their friend, to help them become a better and more robust player. The key insight of the League is that playing to win is insufficient: instead, we need both main agents whose goal is to win versus everyone, and also exploiter agents that focus on helping the main agent grow stronger by exposing its flaws, rather than maximising their own win rate against all players. Using this training method, the League learns all its complex StarCraft II strategy in an end-to-end, fully automated fashion.\nExploration is another key challenge in complex environments such as StarCraft. There are up to 1026 possible actions available to one of our agents at each time step, and the agent must make thousands of actions before learning if it has won or lost the game. Finding winning strategies is challenging in such a massive solution space. Even with a strong self-play system and a diverse league of main and exploiter agents, there would be almost no chance of a system developing successful strategies in such a complex environment without some prior knowledge. Learning human strategies, and ensuring that the agents keep exploring those strategies throughout self-play, was key to unlocking AlphaStar\u2019s performance. To do this, we used imitation learning \u2013 combined with advanced neural network architectures and techniques used for language modelling \u2013 to create an initial policy which played the game better than 84% of active players. We also used a latent variable which conditions the policy and encodes the distribution of opening moves from human games, which helped to preserve high-level strategies. AlphaStar then used a form of distillation throughout self-play to bias exploration towards human strategies. This approach enabled AlphaStar to represent many strategies within a single neural network (one for each race). During evaluation, the neural network was not conditioned on any specific opening moves.\nIn addition, we found that many prior approaches to reinforcement learning are ineffective in StarCraft, due to its enormous action space. In particular, AlphaStar uses a new algorithm for off-policy reinforcement learning, which allows it to efficiently update its policy from games played by an older policy.\nOpen-ended learning systems that utilise learning-based agents and self-play have achieved impressive results in increasingly challenging domains. Thanks to advances in imitation learning, reinforcement learning, and the League, we were able to train AlphaStar Final, an agent that reached Grandmaster level at the full game of StarCraft II without any modifications, as shown in the above video. This agent played online anonymously, using the gaming platform Battle.net, and achieved a Grandmaster level using all three StarCraft II races. AlphaStar played using a camera interface, with similar information to what human players would have, and with restrictions on its action rate to make it comparable with human players. The interface and restrictions were approved by a professional player. Ultimately, these results provide strong evidence that general-purpose learning techniques can scale AI systems to work in complex, dynamic environments involving multiple actors. The techniques we used to develop AlphaStar will help further the safety and robustness of AI systems in general, and, we hope, may serve to advance our research in real-world domains.\nRead more about this work\n in Nature\nPublicly available paper\n here\nView all\n Battle.net game replays\nAlphaStar team:\nOriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Micha\u00ebl Mathieu, Andrew Dudzik, Junyoung Chung, David H. Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, John P. Agapiou, Max Jaderberg, Alexander S. Vezhnevets, R\u00e9mi Leblond, Tobias Pohlen, Valentin Dalibard, David Budden, Yury Sulsky, James Molloy, Tom L. Paine, Caglar Gulcerhe, Ziyu Wang, Tobias Pfaff, Yuhuai Wu, Roman Ring, Dani Yogatama, Dario W\u00fcnsch, Katrina McKinney, Oliver Smith, Tom Schaul, Timothy Lillicrap, Koray Kavukcuoglu, Demis Hassabis, Chris Apps, David Silver\nAcknowledgements:\nWe\u2019re grateful to\n Dario W\u00fcnsch (TLO)\n ,\n Grzegorz Komincz (MaNa)\n, and\n Diego Schwimer (Kelazhur)\n for their advice, guidance, and immense skill. We are also grateful for the continued support of Blizzard and the StarCraft gaming and AI community for making this work possible\u2013especially those who played against AlphaStar on Battle.net. Thanks to Ali Razavi, Daniel Toyama, David Balduzzi, Doug Fritz, Eser Ayg\u00fcn, Florian Strub, Guillaume Alain, Haoran Tang, Jaume Sanchez, Jonathan Fildes, Julian Schrittwieser, Justin Novosad, Karen Simonyan, Karol Kurach, Philippe Hamel, \u00a0Ricardo Barreira, Scott Reed, Sergey Bartunov, Shibl Mourad, Steve Gaffney, Thomas Hubert, the\n team that created PySC2,\n and the whole DeepMind team, with special thanks to the research platform team, comms and events teams.\n*Agents were capped at a max of 22 agent actions per 5 seconds, where one agent action corresponds to a selection, an ability and a target unit or point, which counts as up to 3 actions towards the in-game APM counter. Moving the camera also counts as an agent action, despite not being counted towards APM.\n"}
{"title": "Advanced machine learning helps Play Store users discover personalised apps", "contents": "Over the past few years we've applied DeepMind's technology to Google products and infrastructure, with notable successes like reducing the amount of energy needed for \ncooling data centers\n, and extending \nAndroid battery performance\n. We're excited to share more about our work in the coming months.\nWe know users get the most out of their phone when they have apps and games they love, and that it\u2019s exciting to discover new favourites. In collaboration with Google Play, \nour team that leads on collaborations with Google\n has driven significant improvements in the Play Store's discovery systems, helping to deliver a more personalised and intuitive Play Store experience for users. \nEvery month, billions of users come to the \nGoogle Play Store\n to download apps for their mobile devices \u2013 the Play Store supports one of the largest recommendation systems in the world. \u00a0While some are looking for specific apps, like Snapchat, others are browsing the store to discover what\u2019s new and interesting. The Google Play discovery team strives to help users discover the most relevant apps and games by providing them with helpful app recommendations. To deliver a richer, personalised experience, apps are suggested according to past user preferences. This, however, requires nuance \u2013 both for understanding what an app does, and its relevance to a particular user. \u00a0For example, to an avid sci-fi gamer, similar game recommendations may be of interest, but if a user installs a travel app, recommending a translation app may be more relevant than five more travel apps. The collection and use of these user preferences is governed by \nGoogle's privacy policies\n.\nWe started collaborating with the Play store to help develop and improve systems that determine the relevance of an app with respect to the user. \u00a0In this post, we\u2019ll explore some of the cutting-edge machine learning techniques we developed to achieve this. Today, Google Play\u2019s recommendation system contains three main models: a candidate generator, a reranker, and a model to optimise for multiple objectives. \u00a0The candidate generator is a deep retrieval model that can analyse more than a million apps and retrieve the most suitable ones. For each app, a reranker, i.e. a user preference model, predicts the user's preferences along multiple dimensions. Next these predictions are the input to a multi-objective optimisation model whose solution gives the most suitable candidates to the user.\nTo improve how Google Play\u2019s recommendation system learns users\u2019 preferences, our first approach was to use an \nLSTM (Long Short-Term Memory)\n model, a recurrent neural network that performs well in real-world scenarios, owing to a powerful update equation and backpropagation dynamics. Whilst the LSTM led to significant accuracy gains, it also introduced a serving delay, because LSTMs can be computationally expensive when processing long sequences. To address this, we \u00a0replaced the LSTM with \na Transformer model\n, which is well-equipped for sequence-to-sequence prediction and has previously yielded strong results in natural language processing, as it\u2019s able to capture longer dependencies between words than other commonly used models. The Transformer improved the model performance, but also increased the training cost. Our third and final solution was to implement an efficient additive attention model that works for any combination of sequence features, while incurring low computational cost. \nOur model (called a candidate generator) learns what apps a user is more likely to install based on previous \u00a0apps they\u2019ve installed from the Play store. However, this can introduce a recommendation bias problem. For instance, if app A is shown in the Play store 10 times more than app B, it\u2019s more likely to be installed by the user, and thus more likely to be recommended by our model. \u00a0The model therefore learns a bias that favours the apps that are shown \u2013 and thus installed \u2013 more often.\nTo help correct for this bias, we introduced importance weighting in our model. An importance weight is based on the impression-to-install rate of each individual app in comparison with the median impression-to-install rate across the Play store. \u00a0An app with a below-median install rate will have an importance weight less than one. However, even \u201cniche\u201d apps that are installed less frequently can have a high importance weight if their install rate is higher than the median rate. Through importance weighting, our candidate generator can downweight or upweight apps based on their install rates, which mitigates the recommendation bias problem.\nRecommendation systems often provide a range of possibilities to a user, and present them in an order with the best or most relevant options at the top. But how do we ensure the most relevant apps make it to the top of the list, so the user doesn\u2019t have to scroll for pages, or potentially miss the best option? Many recommendation systems treat the ranking problem as a binary classification problem, where the training data is labeled with either a positive or negative class, and the ranker learns to predict a probability from this binary label alone. However, this type of \u201cpointwise\u201d model \u2013 which only ranks one item at a time \u2013 fails to capture the context of how apps perform relative to one another. To deliver a better user experience, the ranker could predict the relative order of presented items based on the context of other candidate apps.\nOur solution to this, the reranker model, learns the relative importance of a pair of apps that have been shown to the user at the same time. We built our reranker model on a core insight: if a user is presented with two apps in the store, the app that the user chooses to \u00a0install is more relevant to the user than the app that they didn't install. We can then assign each of the pair a positive or negative label, and the model tries to minimise the number of inversions in ranking, thus improving the relative ranking of the apps. This kind of \u201cpairwise\u201d model works better in practice than pointwise models because predicting relative order is closer to the nature of ranking than predicting class labels or install probabilities.\nMany recommendation systems must optimise for multiple objectives at the same time, such as relevance, popularity, or personal preferences. We formulated the multi-objective optimisation problem as a constrained optimisation problem: the overall objective is to maximise the expected value of a primary metric, subject to constraints in terms of expected values of secondary metrics. During online serving, the objectives may shift according to user\u2019s needs \u2013 for example, a user that had previously been interested in housing search apps might have found a new flat, and so is now interested in home decor apps \u2013 so we worked toward a dynamic solution.\nRather than solving the problem offline and bringing a fixed model online, we solved this problem on-line, per-request, based on the actual values of the objectives during serving time. We define the constraints to be relative constraints, meaning we would like to improve the secondary objective by a percentage rather than an absolute value. This way, any shifts in the secondary objectives didn\u2019t affect our solver.\nThe algorithm that we developed can be used to find tradeoffs between a number of metrics. Finding suitable points along the tradeoff curve, our algorithm can significantly raise secondary metrics with only minor effects on the primary metric.\nOne of our key takeaways from this \ncollaboration\n is that when implementing advanced machine learning techniques for use in the real world, we need to work within many practical constraints. Because the Play Store and DeepMind teams worked so closely together and communicated on a daily basis, we were able to take product requirements and constraints into consideration throughout the algorithm design, implementation, and final testing phases, resulting in a more successful product.\nOur collaborations with Google have so far reduced the electricity needed for \ncooling Google\u2019s data centres\n by up to 30%, boosted the value of Google\u2019s \nwind energy\n by roughly 20%, and created on-device learning systems to optimise \nAndroid\n battery performance. \nWaveNet\n is now in the hands of Google Assistant and Google Cloud Platform users around the world, and \nour research collaboration\n with Waymo has helped improve the performance of its models, as well as the efficiency of training its neural networks.\nWorking at Google scale presents a unique set of research challenges, and the opportunity to take our breakthroughs beyond the lab to address global, complex challenges. If you\u2019re interested in working on applying cutting edge research to real world problems, learn more about the team that led this project \nhere\n.\nIn collaboration with: Dj Dvijotham, Amogh Asgekar, Will Zhou, Sanjeev Jagannatha Rao, Xueliang Lu, Carlton Chu, Arun Nair, Timothy Mann, Bruce Chia, Ruiyang Wu, Natarajan Chendrashekar, Tyler Brabham, Amy Miao, Shelly Bensal, Natalie Mackraz, Praveen Srinivasan & Harish Chandran\n"}
{"title": "Learning human objectives by evaluating hypothetical behaviours", "contents": "TL;DR: We present a method for training reinforcement learning agents from human feedback in the presence of unknown unsafe states.\nWhen we train reinforcement learning (RL) agents in the real world, we don\u2019t want them to explore \nunsafe states\n, such as driving a mobile robot into a ditch or writing an embarrassing email to one\u2019s boss. Training RL agents in the presence of unsafe states is known as the \nsafe exploration\n problem\n. We tackle the hardest version of this problem, in which the agent initially doesn\u2019t know how the environment works or where the unsafe states are. The agent has one source of information: feedback about unsafe states from a human user.\nExisting\n \nmethods\n \nfor\n \ntraining\n \nagents\n from human feedback ask the user to evaluate data of the agent acting in the environment. That is \u2013 in order to learn about unsafe states, the agent first needs to visit these states, so the user can provide feedback on them. This makes prior work inapplicable to tasks that require safe exploration.\nIn our \nlatest paper\n, we propose a method for \nreward modeling\n that operates in two phases. First, the system is encouraged to explore a wide range of states through synthetically-generated, hypothetical behaviour. The user provides feedback on this hypothetical behaviour, and the system interactively learns a model of the user's reward function. Only after the model has successfully learned to predict rewards and unsafe states, we deploy an RL agent that safely performs the desired task.\nWe start with a generative model of initial states and a forward dynamics model, trained on off-policy data like random trajectories or safe expert demonstrations. Our method uses these models to synthesise hypothetical behaviours, asks the user to label the behaviours with rewards, and trains a neural network to predict these rewards. The key idea is to actively synthesise the hypothetical behaviours from scratch to make them as informative as possible, \nwithout interacting with the environment\n. We call this method \nreward query synthesis via trajectory optimisation\n (ReQueST).\nFor this approach to work, we need the system to simulate and explore a wide range of behaviours, in order to effectively train the reward model. To encourage exploration during reward model training, ReQueST synthesises \nfour different types of hypothetical behaviours\n using gradient descent trajectory optimisation. The first type of hypothetical behaviour \nmaximises the uncertainty of an ensemble of reward models\n, eliciting user labels for behaviours that have the highest information value. The second type of hypothetical behaviour \nmaximises predicted rewards\n, surfacing behaviours for which the reward model might be incorrectly predicting high rewards; i.e., \nreward hacking\n. The third type of hypothetical behaviour \nminimises predicted rewards\n, adding potentially unsafe hypothetical behaviours to the training data. This data enables the reward model to learn about unsafe states. The fourth type of hypothetical behaviour \nmaximises the novelty of trajectories\n, encouraging exploration of a wide range of states, regardless of predicted rewards. \nEach hypothetical behaviour consists of a sequence of state transitions (s, a, s\u2019). We ask the user to label each state transition with a reward, r. Then, given the labeled dataset of transitions (s, a, r, s\u2019), we train a neural network to predict rewards using a maximum-likelihood objective. We use standard supervised learning techniques based on gradient descent.\nOnce the user is satisfied with the reward model, we deploy a planning-based agent that uses model-predictive control (MPC) to pick actions that optimise the learned rewards. Unlike \nmodel-free\n RL algorithms like Q-learning or policy gradient methods that learn through trial and error, \nmodel-based\n RL algorithms like MPC enable the agent to avoid unsafe states during deployment by using the dynamics model to anticipate the consequences of its actions.\nWe evaluate ReQueST with simulated users on a state-based 2D navigation task and the image-based Car Racing video game. Our results show that ReQueST satisfies three important safety properties: it can \ntrain a reward model to detect unsafe states without visiting them\n; it can \ncorrect reward hacking \nbefore deploying the agent; and it tends to learn robust reward models that\n perform well when transferred to new environments\n.\nTo test the generalisation of the reward model, we set up a 2D navigation task with separate training and test environments.\nWe intentionally introduce a significant shift in the initial state distribution: the agent starts at the lower left corner (0, 0) in the training environment, and at the upper right corner (1, 1) in the test environment. Prior methods that collect data by deploying an agent in the training environment are unlikely to learn about the trap in the upper right corner, because they immediately find the goal, then fail to continue exploring. ReQueST synthesizes a variety of hypothetical states, including states in and around the trap. The user labels these states with rewards, using which ReQueST learns a robust reward model that enables the agent to navigate around the trap in the test environment.\nTo test whether ReQueST scales to domains with high-dimensional, continuous states like images, we use the \nCar Racing\n video game from the OpenAI Gym.\nIn addition to benchmarking ReQueST against prior methods, we ran a hyperparameter sweep and ablation study, where we varied the regularization strength of the dynamics model during trajectory optimisation as well as the subset of hypotheticals synthesized in order to measure ReQueST\u2019s sensitivity to these settings. We found that ReQueST can trade off between producing realistic vs. informative queries, and that the optimal trade-off varies across domains. We also found that the usefulness of each of the four hypothetical behaviours depends on the domain and the amount of training data collected.\nTo our knowledge, ReQueST is the first reward modeling algorithm that safely learns about unsafe states and scales to training neural network reward models in environments with high-dimensional, continuous states.\nReQueST relies on a generative model of initial states and a forward dynamics model, which can be hard to acquire for visual domains with complex dynamics. So far, we have only demonstrated the effectiveness of ReQueST in simulated domains with relatively simple dynamics. One direction for future work is to test ReQueST in 3D domains with more realistic physics and other agents acting in the environment.\nIf you want to learn more, check out our\n preprint on arXiv\n: \nSiddharth Reddy, Anca D. Dragan, Sergey Levine, Shane Legg, Jan Leike, Learning Human Objectives by Evaluating Hypothetical Behavior, arXiv, 2019.\nTo encourage replication and extensions, we have released our\n code\n.\nListen to our\n podcast\n to learn more about DeepMind's commitment to building safe AI.\nThanks to Zac Kenton and Kelly Clancy for feedback on early drafts of this post, and to Paulo Estriga for his design work.\n\u200d\n"}
{"title": "From unlikely start-up to major scientific organisation: Entering our tenth year at DeepMind", "contents": "Since we started DeepMind nearly 10 years ago, our mission has been to unlock answers to the world\u2019s biggest questions by understanding and recreating intelligence itself.\nAs we approach the end of 2019, we\u2019ve come a long way in building the organisation we need to achieve this long-term mission - from our environment for research, to our collaborations with other Alphabet companies, to our increasingly interdisciplinary and diverse team.\nA mission this ambitious requires pioneering research on many fronts over many years. We\u2019ve been privileged to make some significant advances over the past twelve months, from \nwinning a major international contest \nto predict the shapes of proteins - the building blocks of life - at the end of 2018, to developing AI agents that cooperate with each other and people in our \nCapture the Flag\n paper published by Science, to our latest work on mastering the complex strategy game StarCraft II, which led to a \nNature cover article\n last month. \nBeyond the moments that capture widespread imagination, we have a broad programme of \nfundamental research\n with hundreds of papers published each year. Many are substantial and exciting contributions to their domains, such as \nMuZero\n, which can not only master chess, Go, Shogi but also extend to Atari, without knowing the rules first. We\u2019ve developed new ways to open up and explain our work, such as our dedicated \nAI safety research\n blog, thematic summaries of \u00a0research concepts such as \nunsupervised learning\n, and our many \nopen source\n projects, which bring new frameworks and environments to the field. \u00a0\nAs our research matures, we\u2019ve been finding more opportunities to partner with others for social and commercial impact, often with our colleagues across Alphabet. This year, we demonstrated how AI could predict potentially fatal \npatient deterioration\n two days before existing tests, used machine learning to accelerate ecological research in the \nSerengeti\n, collaborated with \nWaymo\n on evolutionary selection to train more capable self-driving cars, learned how to boost the value of Google\u2019s \nwind energy\n, \nhelped Play Store users\n discover more relevant apps, and more.\nAs I discussed with Wired\n in the summer, this year feels like the start of a new phase for DeepMind as an established scientific organisation. We\u2019re no longer the tiny start-up working in a very unfashionable area, as was the case in 2010. Fuelled by breakthroughs like \nDQN \nand \nAlphaGo\n, and many exciting advances from colleagues at other labs, AI is once again one of the most vibrant areas of scientific research. \nOver the past year, we\u2019ve also been formalising a leadership team with the seasoned experience and skills for our second decade. We want to ensure DeepMind continues to be the best place in the world for fundamental breakthroughs in AI, and that we conduct this work thoughtfully and responsibly. Much of this work is led by Chief Operating Officer Lila Ibrahim and VP of Research Koray Kavukcuoglu. \nLila joined DeepMind in April 2018\n following a distinguished, global career including a long stint at Intel where she started as an engineer and took on management roles including Chief of Staff to the CEO and Chairman, and then leadership roles at investment firm Kleiner Perkins and education startup Coursera. Koray is one of the foremost research scientists in AI and deep learning, and a key contributor to many of DeepMind\u2019s biggest breakthroughs, from the \nDQN system that mastered Atari\n in 2013 to the \nresearch and deployment of\n \nWaveNet\n, which improves the experience of Google users around the world.\nAs we enter this next phase, Mustafa Suleyman is leaving DeepMind. I founded DeepMind back in 2010 along with Shane Legg (our Chief Scientist) and Mustafa. As a serial entrepreneur, Mustafa played a key role over the past decade helping to get DeepMind off the ground, and launched a series of innovative collaborations with Google to \nreduce energy consumption in data centres\n, improve \nAndroid battery performance\n, \noptimise Google Play\n, and find ways to \nimprove the lives of patients, nurses and doctors\n alike. Mustafa leaves DeepMind having helped set us up for long-term success, and I\u2019m looking forward to what he\u2019ll achieve in the years ahead as he joins Google in a new role. \nOf course, there\u2019s a long way left to go for DeepMind and for the AI field overall, and many grand challenges remain. Right back to our origins blending neuroscience with machine learning, we\u2019ve found that breakthroughs happen faster when different disciplines come together. In our offices across the world - soon to include our incredible new \npurpose-built HQ\n in London - we recruit and develop brilliant people with backgrounds in research and engineering, program management, games design, operations, ethics and safety research and beyond. You can get to know just a fraction of our team by listening to the \nDeepMind podcast\n. Increasing the diversity of our workforce - and doing what we can to \nimprove access to science \namong under-represented groups - remains a personal and organisational priority.\nThank you to the hundreds of amazing colleagues who have made DeepMind what it is today, and to all those at Alphabet and in the wider AI community for your long-term support and collaboration. I can\u2019t wait to show you what we have coming up\u2026 all I can say for now is, keep your eyes peeled for some very exciting advances in 2020!\n"}
{"title": "Using WaveNet technology to reunite speech-impaired users with their original voices", "contents": "This post details a recent project we undertook with Google and ALS campaigner Tim Shaw, as part of Google\u2019s Euphonia project. We demonstrate an early proof of concept of how text-to-speech technologies can synthesise a high-quality, natural sounding voice using minimal recorded speech data. \u00a0\nAs a teenager, Tim Shaw put everything he had into football practice: his dream was to join the NFL. After playing for Penn State in college, his ambitions were finally realised: the Carolina Panthers drafted him at age 23, and he went on to play for the Chicago Bears and Tennessee Titans, where he broke records as a linebacker. After six years in the NFL, on the cusp of greatness, his performance began to falter. He couldn\u2019t tackle like he once had; his arms slid off the pullup bar. At home, he dropped bags of groceries, and his legs began to buckle underneath him. In 2013 Tim was cut from the Titans but he resolved to make it onto another team. Tim practiced harder than ever, yet his performance continued to decline. Five months later, he finally discovered the reason: he was diagnosed with \nAmyotrophic lateral sclerosis\n (ALS, commonly known as Lou Gehrig\u2019s disease). In ALS, the neurons that control a person\u2019s voluntary muscles die, eventually leading to a total loss of control over one\u2019s body. ALS has no known cause, and, as of today, has no cure. \u00a0\nToday, Tim is a \npowerful advocate\n for ALS research. Earlier this year, he published a \nletter to his younger self\n advising acceptance\u2013\u201cotherwise, you\u2019ll grieve yourself to death.\u201d Now a wheelchair user, he lives under the constant care of his parents. People with ALS have trouble moving, and the disease makes speaking, swallowing, and even breathing on their own difficult and then impossible. Not being able to communicate can be one of the hardest aspects for people with ALS and their families. As Tim put it: \u201cit\u2019s beyond frustrating not to be able to express what\u2019s going on in my mind. I\u2019m smarter than ever but I just can\u2019t get it out.\u201d\nLosing one\u2019s voice can be socially devastating. Today, the main option available to people to preserve their voice is \nmessage banking,\n wherein people with ALS can digitally record and store personally meaningful phrases using their natural inflection and intonation. Message banking is a source of great comfort for people with ALS and their families, helping to preserve a core part of their identity - their voice - through a deeply challenging time. But message banking lacks flexibility, resulting in a static dataset of phrases. Imagine being told you will never be able to speak again. Now imagine that you were given the chance to preserve your voice by recording as much of it as possible. How would you decide what to record? How would you capture what you most want to be able to say in the future? \u00a0Would it be a meaningful story, a favorite phrase or a simple \u201cI love you\u201d? The process can be time consuming and emotionally draining, especially as someone\u2019s voice degrades. And people who aren\u2019t able to record phrases in time are left to choose a generic computer synthesized voice that lacks the same power of connection as their own.\nAt DeepMind, we\u2019ve been collaborating with Google and people like Tim Shaw to help develop technologies that can make it easier for people with speech difficulties to communicate. The challenges of this are two-fold. Firstly, we must have technology that can recognise the speech of people with non-standard pronunciation\u2013something Google AI has been researching through \nProject Euphonia\n. Secondly, we\u2019d ideally like people to be able to communicate using their original voice. Stephen Hawking, who also suffered from ALS, communicated with a famously unnatural sounding text-to-speech synthesiser. Thus, the second challenge is customising text-to-speech technology to the user\u2019s natural speaking voice. \nCreating natural sounding speech is considered a \u201c\ngrand challenge\n\u201d in the field of AI. With \u00a0\nWaveNet\n and \nTacotron\n, we\u2019ve seen tremendous breakthroughs in the quality of text-to-speech systems. However, whilst it is possible to create natural sounding voices that sound like specific people in certain contexts \u2013 as we demonstrated in collaboration with \nJohn Legend\n last year \u2013 \u00a0developing synthetic voices requires many hours of studio recording time with a very specific script \u2013 a luxury that many people with ALS simply don\u2019t have. Creating machine learning models that require less training data is an active area of research at DeepMind, and is crucial for use cases such as this where we need to recreate a voice with just a handful of audio recordings. \u00a0We\u2019ve helped do this by harnessing our \nWaveNet\n work and the novel approaches demonstrated in our paper, \nSample Efficient Adaptive Text-to-Speech\n (TTS) - where we showed that it\u2019s possible to create a high quality voice using small amounts of speech data. \nWhich brings us back to Tim. Tim and his family were instrumental in our recent research. Our goal was to provide Tim and his family an opportunity to hear his original speaking voice again. Thanks to Tim\u2019s time in the media spotlight, resulting in about thirty minutes of high-quality audio recordings, we were able to apply the methodologies from WaveNet and TTS to recreate his former voice. \nFollowing a six-month effort, Google\u2019s AI team visited Tim and his family to show him the results of their work. The meeting was captured for the new YouTube Originals learning series, \u201c\nThe Age of A.I.\n\u201d hosted by Robert Downey Jr. \u00a0Tim and his family were able to hear his old voice for the first time in years, as the model \u2013 trained on Tim\u2019s NFL audio recordings \u2013 read out the \nletter he\u2019d recently written to his younger self\n.\n\u201cI don\u2019t remember that voice,\u201d Tim remarked. His father responded, \u201cwe do.\u201d Later, Tim recounted\u2013\"it has been so long since I've sounded like that, I feel like a new person. I felt like a missing part was put back in place. It's amazing. I'm just thankful that there are people in this world that will push the envelope to help other people.\"\nYou can learn more about our project with Tim and the vital role he played in our research in \u201c\nThe Age of A.I.\n\u201d now available on \nYouTube.com/Learning\n.\nTo understand how the technology works, it\u2019s important to first understand \nWaveNet\n. WaveNet is a generative model trained on many hours of speech and text data from diverse speakers. It can then be fed arbitrary new text to be synthesized into a natural-sounding spoken sentence. \nLast year, in our \nSample Efficient Adaptive Text-to-Speech\n paper, we illustrated that it\u2019s possible to train a new voice with minutes, rather than hours, of voice recordings through a process called fine-tuning. This involves first training a large WaveNet model on up to thousands of speakers, which takes a few days, until it can produce the basics of natural sounding speech. Then, we take the small corpus of data for the target speaker and intelligently adapt the model, adjusting the weights so that we can create a single model that matches the target speaker. The concept of fine-tuning is similar to how people learn. For example, if you are attempting to learn calculus, you should first understand the foundations of basic algebra, and then apply these simpler concepts to help solve more complex equations. \nAfter this publication, we continued to iterate on our models. First, we migrated from WaveNet to \nWaveRNN\n, which is a more efficient text to speech model co-developed by Google AI and DeepMind. WaveNet requires a second distillation step to speed it up to serve requests in real-time, which makes fine-tuning more challenging. WaveRNN, on the other hand, does not require a second training step and can synthesize speech much faster than a WaveNet model that has not been distilled.\nIn addition to speeding up the models by switching to WaveRNN, we collaborated with Google AI to improve the quality of the models. Google AI researchers demonstrated that a similar fine-tuning approach could be applied to the related Google \nTacotron\n model, which we use in conjunction with WaveRNN to synthesise realistic voices. By combining these technologies trained on audio clips of Tim Shaw from his NFL days, we were able to generate an authentic sounding voice that resembles how Tim sounded before his speech degraded. While the voice is not yet perfect \u2013 lacking the expressiveness, quirks, and controllability of a real voice \u2013 we\u2019re excited that the combination of WaveRNN and Tacotron may help people like Tim preserve an important part of their identity, and we would like to one day integrate it into speech-generation devices. \u00a0 \nWe\u2019re honored to have briefly reunited Tim with his voice. At this stage, it\u2019s too early to know where our research will take us, but we are looking at ways to combine the Euphonia speech recognition systems with the speech synthesis technology so that people like Tim can more easily communicate. We hope that our research can eventually be shared more widely with those who need it most in order to communicate with their loved ones\u2013there are thousands of people in the world who this work might one day benefit. As Tim wrote in his letter to his younger self\u2013what matters, in the end, is \u201cthe relationships and the people you have in your life who love you and care about you.\u201d\nIn collaboration with: \nZachary Gleicher, Luis C. Cobo, Yannis Assael, Brendan Shillingford, Nando de Freitas, Julie Cattiau\u200e, Philip Nelson, Ye Jia, Heiga Zen, Ron Weiss, Zhifeng Chen, Yonghui Wu, Tejas Iyer\u200e, Hadar Shemtov, Tim Shaw, Fernando Vieira, Maeve McNally, John Shaw, Sharon Shaw, John Costello\n"}
{"title": "Strengthening the AI community", "contents": "Most people have at least one crossroads moment in their life - when the choice they make defines their personal or professional trajectory. For me, it was being awarded an internship at Intel, the first one ever through Purdue\u2019s Co-Op Engineering program in 1990.\nAt first, I questioned whether the then little-known Intel, California, or this internship (on the Pentium processor) was the right choice for me. I just didn\u2019t know if I had the right technical skills for the work, or if engineering was really my path. But I took a leap of faith because I didn\u2019t want to waste the opportunity. That internship gave me a fantastic insight into the day to day work of engineers, including a chance to prove to myself that I could do engineering! It grew into a very successful 18-year career at Intel and a 25-year career in tech. At that moment, I could have easily said \u201cengineering isn\u2019t for me\u201d had I not had the nudge in the right direction, the vote of confidence, that this practical experience provided. Nearly 30 years later, I returned to Purdue last week, for their Distinguished Engineering Lectures programme, and had a chance to talk about the incredible career journey I\u2019ve been on - in the hopes of inspiring a new generation of engineers as to what is possible.\nSometimes that extra support is the difference between saying yes or saying no - between following a path in STEM, or doing something completely different. Whether it\u2019s inspiring self-confidence, offering reassurance or providing a financial safety net, showing support and removing the barriers that prevent individuals achieving their full potential can have a powerful impact. \nAt DeepMind we want to build advanced AI to expand our knowledge and find answers to some of the fundamental questions facing society. It is an ambitious and long-term goal, and we will only achieve it if we can bring people together with different experiences, knowledge, backgrounds. This is a field where diversity is paramount, not only for the innovative work that diverse teams produce, but because it\u2019s vital that we mitigate the risks of bias in the development of algorithms and applications. We need as many perspectives as possible to make sure the important questions are being asked when it matters. \nThe DeepMind scholarship programme is one way we seek to broaden participation in science and AI. It gives talented students from underrepresented backgrounds the support they need to study at leading universities, and connect with our researchers and engineers. Scholars get their Masters' fees paid in full, as well as guidance and support from personal DeepMind mentors. \nThis week we announced\n the renewal and expansion of our scholarship programme with the University College London. Four more DeepMind graduate scholarships for students wishing to pursue a master\u2019s degree in the Department of Computer Science will be available for students starting courses in 2020\u201321. But UCL is just one example. We also work with numerous other universities, such as \nOxford\n, \nQueen Mary University London\n, the \nUniversity of Cambridge\n and \nNYU\n, to broaden participation in AI and computer science. \nI\u2019ve seen many examples of the impact that diverse perspectives can have in practice. Take Shaquille Momoh, one of our DeepMind scholars, who was inspired to research protein folding prediction while studying at UCL. Nearly every function our body performs\u2014contracting muscles, sensing light, or turning food into energy\u2014can be traced back to proteins and how they move and change. \u00a0Predicting their structure is fundamental to understanding the body, as well as diagnosing and treating diseases believed to be caused by misfolded proteins. Shaquille had a specific motivation for studying protein folding. He wanted to better understand sickle-cell anaemia, a painful inherited condition much more prevalent in black communities \u2013 and for which there is no current cure. \nTo ensure the next generation of researchers reach their full potential, protecting and strengthening the research and teaching capacity of our academic institutions is vital too. \nDeepMind partners with a range of world-leading universities with the aim of extending research excellence and teaching capacity. We\u2019ve established academic chairs in machine learning at the \nUniversity of Alberta\n, \nUniversity College London\n, and the \nUniversity of Cambridge\n, offering unrestricted funding for world-renowned researchers to freely pursue their academic interests. These chairs will be supported in their research and teaching efforts by PhDs students. And many of our researchers hold dual affiliations, allowing them to continue teaching or supervising students at Cambridge, Oxford, Imperial, MIT, McGill and elsewhere (you can access some of these courses on \nYouTube\n). \nOnly by investing the right way across the ecosystem will we able to ensure the highest quality AI research that benefits everyone. It\u2019s also why we partner with charities like \nChess in Schools and Communities\n and In2Science, and have become founding partners of the \nDeep Learning Indaba\n in Africa, \u00a0\nKhipu AI\n in South America, the \nEastern European Machine Learning Summer School\n, the \nSoutheast Asia Machine Learning School (SEAMLS)\n, and the \nAI4Good Summer Lab\n in Canada. \nAnd to research how the lack of diversity affects the development of AI \u2013 how companies work, what products get built, and who benefits \u2013 last month \nwe announced\n a research fellowship with the \nPartnership on AI\n to explore the pervasive challenge of developing AI for the benefit of people and society.\nHistory has shown us that hard problems are best solved with collective effort. Innovation happens when people with different experiences, knowledge, and backgrounds join together, break down boundaries, openly share ideas and collaborate for a common goal. Building advanced AI responsibly may be one of the hardest scientific challenges to solve. If our sector can provide the right support for researchers and foster an open, collaborative and diverse academic culture, the impact could be truly transformative.\n"}
{"title": "AlphaFold: Using AI for scientific discovery", "contents": "UPDATE: In July 2022, we released AlphaFold protein structure predictions for nearly all catalogued proteins known to science. Read the latest blog \nhere\n.\nIn our study \npublished in Nature\n, we demonstrate how artificial intelligence research can drive and accelerate new scientific discoveries. We\u2019ve built a dedicated, interdisciplinary team in hopes of using AI to push basic research forward: bringing together experts from the fields of structural biology, physics, and machine learning to apply cutting-edge techniques to predict the 3D structure of a protein based solely on its genetic sequence. \nOur system, AlphaFold \u2013 described in peer-reviewed papers now published in \nNature\n and \nPROTEINS\n \u2013 is the culmination of several years of work, and builds on decades of prior research using large genomic datasets to predict protein structure. The 3D models of proteins that AlphaFold generates are far more accurate than any that have come before - marking significant progress on one of the core challenges in biology. The AlphaFold code used at CASP13 is available on Github \nhere\n for anyone interested in learning more or replicating our results. We\u2019re also excited by the fact that this work has already inspired other, independent implementations, including the model described in \nthis paper\n, and a community - built, \nopen source implementation\n, described \nhere\n. \nProteins are large, complex molecules essential to all of life. Nearly every function that our body performs - contracting muscles, sensing light, or turning food into energy - relies on proteins, and how they move and change. What any given protein can do depends on its unique 3D structure. For example, antibody proteins utilised by our immune systems are \u2018Y-shaped\u2019, and form unique hooks. By latching on to viruses and bacteria, these antibody proteins are able to detect and tag disease - causing microorganisms for elimination. Collagen proteins are shaped like cords, which transmit tension between cartilage, ligaments, bones, and skin. Other types of proteins include Cas9, which, using CRISPR sequences as a guide, act like scissors to cut and paste sections of DNA; antifreeze proteins, whose 3D structure allows them to bind to ice crystals and prevent organisms from freezing; and ribosomes, which act like a programmed assembly line, helping to build proteins themselves.\nThe recipes for those proteins - called genes - are encoded in our DNA. An error in the genetic recipe may result in a malformed protein, which could result in disease or death for an organism. Many diseases, therefore, are fundamentally linked to proteins. But just because you know the genetic recipe for a protein doesn\u2019t mean you automatically know its shape. Proteins are comprised of chains of amino acids (also referred to as amino acid residues). But DNA only contains information about the \nsequence\n of amino acids - not how they fold into shape. The bigger the protein, the more difficult it is to model, because there are more interactions between amino acids to take into account. As demonstrated by \nLevinthal\u2019s paradox\n, it would take longer than the age of the known universe to randomly enumerate all possible configurations of a typical protein before reaching the true 3D structure - yet proteins themselves fold spontaneously, within milliseconds. Predicting how these chains will fold into the intricate 3D structure of a protein is what\u2019s known as the \u201cprotein-folding problem\u201d - a challenge that scientists have worked on for decades. This unsolved problem has already inspired countless developments, from spurring IBM\u2019s efforts in supercomputing (\nBlueGene\n), to novel citizen science efforts (\nFolding@Home\n and \nFoldIt\n) to new engineering realms, such as rational protein design.\nScientists have long been interested in determining the structures of proteins because a protein\u2019s form is thought to dictate its function. Once a protein\u2019s shape is understood, its role within the cell can be guessed at, and scientists can develop drugs that work with the protein\u2019s unique shape. \nOver the past five decades, researchers have been able to determine shapes of proteins in labs using experimental techniques like \ncryo-electron microscopy\n, \nnuclear magnetic resonance\n and \nX-ray crystallography\n, but each method depends on a lot of trial and error, which can take years of work, and cost tens or hundreds of thousands of dollars per protein structure. This is why biologists are turning to AI methods as an alternative to this long and laborious process for difficult proteins. The ability to predict a protein\u2019s shape computationally from its genetic code alone \u2013 rather than determining it through costly experimentation \u2013 could help accelerate research. \nFortunately, the field of genomics is quite rich in data thanks to the rapid reduction in the cost of genetic sequencing. As a result, deep learning \napproaches\n to the prediction problem that rely on genomic data have become increasingly popular in the last few years. To catalyse research and measure progress on the newest methods for improving the accuracy of predictions, a biennial global competition called CASP (\nCritical Assessment of protein Structure Prediction\n) was established in 1994, and has become the gold standard for assessing predictive techniques. We\u2019re indebted to decades of prior work by the CASP organisers, as well as to the thousands of experimentalists whose structures enable this kind of assessment.\nDeepMind\u2019s work on this problem resulted in AlphaFold, which we submitted to CASP13. We\u2019re proud to be part of what the CASP organisers have called \u201cunprecedented progress in the ability of computational methods to predict protein structure,\u201d placing \nfirst\n in rankings among the teams that entered (our entry is A7D).\nOur team focused specifically on the problem of modelling target shapes from scratch, without using previously solved proteins as templates. We achieved a high degree of accuracy when predicting the physical properties of a protein structure, and then used two distinct methods to construct predictions of full protein structures.\nBoth of these methods relied on deep neural networks that are trained to predict properties of the protein from its genetic sequence. The properties our networks predict are: (a) the distances between pairs of amino acids and (b) the angles between chemical bonds that connect those amino acids. The first development is an advance on commonly used techniques that estimate whether pairs of amino acids are near each other.\nWe trained a neural network to predict a distribution of distances between every pair of residues in a protein (visualised in Figure 2). These probabilities were then combined into a score that estimates how accurate a proposed protein structure is. We also trained a separate neural network that uses all distances in aggregate to estimate how close the proposed structure is to the right answer.\nBoth of these methods relied on deep neural networks that are trained to predict properties of the protein from its genetic sequence. The properties our networks predict are: (a) the distances between pairs of amino acids and (b) the angles between chemical bonds that connect those amino acids. The first development is an advance on commonly used techniques that estimate whether pairs of amino acids are near each other.\nWe trained a neural network to predict a distribution of distances between every pair of residues in a protein (visualised in Figure 2). These probabilities were then combined into a score that estimates how accurate a proposed protein structure is. We also trained a separate neural network that uses all distances in aggregate to estimate how close the proposed structure is to the right answer.\nUsing these scoring functions, we were able to search the protein landscape to find structures that matched our predictions. Our first method built on techniques commonly used in structural biology, and repeatedly replaced pieces of a protein structure with new protein fragments. We trained a generative neural network to invent new fragments, which were used to continually improve the score of the proposed protein structure.\nThe second method optimised scores through \ngradient descent \n- a mathematical technique commonly used in machine learning for making small, incremental improvements - which resulted in highly accurate structures. This technique was applied to entire protein chains rather than to pieces that must be folded separately before being assembled into a larger structure, to simplify the prediction process.\nThe AlphaFold version used at CASP13 is available \non Github\n for anyone interested in learning more, or replicating our protein-folding results. \nWhile we\u2019re thrilled by the success of our protein-folding model, there\u2019s still much to be done in the realm of protein biology, and we\u2019re excited to continue our efforts in this field. We\u2019re committed to establishing ways that AI can contribute to basic scientific discovery, with the hope of making real-world impact. This approach might serve to ultimately improve our understanding of the body and how it works, enabling scientists to target and design new, effective cures for diseases more efficiently. Scientists have only mapped structures for about half of all the proteins made by human cells. Some rare diseases involve mutations in a single gene, resulting in a malformed protein which can have profound effects on the health of an entire organism. A tool like AlphaFold might help rare disease researchers predict the shape of a protein of interest rapidly and economically. As scientists acquire more knowledge about the shapes of proteins and how they operate through simulations and models, this method may eventually help us contribute to efficient drug discovery, while also reducing the costs associated with experimentation. Our hope is that AI will be useful for disease research, and ultimately improve the quality of life for millions of patients around the world. \nBut potential benefits aren\u2019t restricted to health alone - understanding protein folding will assist in protein design, which could unlock a tremendous \nnumber of benefits\n. For example, advances in biodegradable enzymes - which can be enabled by protein design - could help manage pollutants like plastic and oil, helping us break down waste in ways that are more friendly to our environment. In fact, researchers have already begun \nengineering bacteria\n to secrete proteins that will make waste biodegradable, and easier to process.\nThe success of our first foray into protein folding is indicative of how machine learning systems can integrate diverse sources of information to help scientists come up with creative solutions to complex problems at speed. Just as we\u2019ve seen how AI can help people master complex games through systems like \nAlphaGo\n and \nAlphaZero\n, we similarly hope that one day, AI breakthroughs will help serve as a platform to advance our understanding of fundamental scientific problems, too.\nIt\u2019s exciting to see these early signs of progress in protein folding, demonstrating the utility of AI for scientific discovery. Even though there\u2019s a lot more work to do before we\u2019re able to have a quantifiable impact on treating diseases, managing waste, and more, we know the potential is enormous. With a \ndedicated team\n focused on delving into how machine learning can advance the world of science, we\u2019re looking forward to seeing the many ways our technology can make a difference.\nListen to our \npodcast\n featuring the researchers behind this work.\nThis blog post is based on the following work:\nAlphaFold: Improved protein structure prediction using potentials from deep learning\n (Nature)\nProtein structure prediction using multiple deep neural networks in CASP13\n (PROTEINS)\nThe AlphaFold version used at CASP13 is available \non Github\n for anyone interested in learning more, or replicating our protein folding results.\nThis work was done in collaboration with Andrew Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green, Chongli Qin, Augustin \u017d\u00eddek, Sandy Nelson, Alex Bridgland, Hugo Penedones, Stig Petersen, Karen Simonyan, Steve Crossan, Pushmeet Kohli, David Jones, David Silver, Koray Kavukcuoglu and Demis Hassabis\n\u200d\n"}
{"title": "WaveNet", "contents": "For decades, computer scientists have tried to mimic these abilities and make computers sound more \u201cnatural.\u201d Yet, despite incredible progress, artificial speech has struggled to match the qualities of the human voice.\nWhen we first started working on \nWaveNet\n, most text-to-speech systems relied on \u201cconcatenative synthesis\u201d \u2014 a pain-staking process of cutting voice recordings into phonetic sounds and recombining them to form new words and sentences.\nThe resulting voices often sound mechanical, and making changes requires entirely new recordings \u2014 an expensive and time-consuming process.\nWaveNet addresses these limitations, offering a technology that finally allows people to interact more naturally with the products they use.\nThey can be \ntrained\n on images, videos, or sounds and, once trained, should be able to create new, realistic examples based on what they have learned.\nFor example, if we train a generative model on a dataset of landscape drawings, it should learn to create entirely novel images of landscapes not seen in the dataset.\nThe more accurate the results, the more it suggests the model has learned the underlying structure of the dataset, rather than just simply memorising examples.\nIt creates the waveforms of speech patterns by predicting which sounds likely follow each other. Each waveform is built one sample at a time, with up to 24,000 samples per second of sound.\nAnd because the model learns from human speech, WaveNet automatically incorporates natural-sounding elements left out of earlier text-to-speech systems, such as lip-smacking and breathing patterns.\nBy including intonation, accents, emotion, and other vital layers of communication overlooked by earlier systems, WaveNet delivers a richness and depth to computer-generated voices.\nFor example, when we \nfirst introduced WaveNet\n, we created American English and Mandarin Chinese voices that narrowed the gap between human and computer-generated voices by 50%.\nUsing a technique called distillation \u2014 transferring knowledge from a large model to a smaller model \u2014 we \nreengineered WaveNet\n to run 1,000 times faster than our research prototype, creating one second of speech in just 50 milliseconds.\nIn parallel, we also developed \nWaveRNN\n \u2014 a simpler, faster, and more computationally efficient model that could run on mobile phones rather than in a data centre.\nThe following year, we partnered with the Google speech team to launch \nWaveNet as the voice of Google Assistant\n.\nAfter improving the experience for users in American English and Japanese, WaveNet was rolled out to create dozens of voices in different languages for millions of people using the Assistant through their smart-home and mobile devices.\nIn another demonstration\n, WaveNet was used to recreate the voices of two celebrities who featured as cameos on the Assistant. Using only a few hours of speech samples from each celebrity, we integrated the voices of singer John Legend and actress Issa Rae.\nOn the latest Android devices, WaveRNN also now powers the Assistant voice.\nDiagnosed with ALS in 2014, former NFL linebacker Tim Shaw watched as his strength, and his voice, deteriorated. To help, Google AI, \nthe ALS Therapy Institute\n, and \nProject Euphonia\n (a Google program applying AI to help people with atypical speech) developed a service to better understand Shaw\u2019s impaired speech.\nWaveRNN was combined with other speech technologies and a dataset of media interviews previously recorded to create a natural-sounding version of Shaw\u2019s voice, empowering him to read aloud a letter \nwritten to his younger self\n.\nAnd, through Google Cloud, businesses can now choose from hundreds of lifelike voices in over 30 languages or use a WaveRNN \nservice\n to make a custom voice from only 30 minutes of speech, to improve customer service and device interactions.\nExtensions of WaveNet are also helping create entirely new product experiences.\nFor example, \nWaveNetEQ\n and \nLyra\n help fill in lost information and improve the quality of calls on weak connections for \nGoogle\u2019s video-calling app Duo\n.\nThe same technology that made it possible for Tim Shaw to regain a voice lost to a degenerative disease also helps answer some of the one billion queries asked of the Google Assistant every day.\nIt also has the potential to help millions more people to communicate successfully, translate instantly across multiple languages, expand small businesses with custom audio content, and much more.\nWaveNet is helping unlock barriers in communication, culture, and commerce for people around the world everyday. And its journey is just beginning.\n"}
{"title": "A new model and dataset for long-range memory", "contents": "This blog introduces a new long-range memory model, the \nCompressive Transformer\n, alongside a new benchmark for book-level language modelling, \nPG19\n. We provide the conceptual tools needed to understand this new research in the context of recent developments in memory models and language modelling.\nThroughout our lives, we build up memories that are retained over a diverse array of timescales, from minutes to months to years to decades. When reading a book, we can recall characters who were introduced many chapters ago, or in an earlier book in a series, and reason about their motivations and likely actions in the current context. We can even put the book down during a busy week, and pick up from where we left off without forgetting the plotline. \nWe do not achieve such feats by storing every detail of sensory input we receive about the world throughout our lifetimes. \nOur brains select, filter, and integrate\n input stimuli based on factors of relevance, surprise, perceived danger, and repetition. In other words, we \ncompress\n lifelong experience to a set of salient memories which help us understand the past, and better anticipate the future. A major goal of AI researchers is discovering ways of implementing such abilities in computational systems and benchmarks which require complex reasoning over long time-spans. \nMemory systems for artificial neural networks have advanced considerably in the past two decades. In this post, we look to past advances to explore why this is such a difficult task and consider how natural language modelling could offer an effective means of designing better long range memory systems? We reflect on the necessity for better compressive memory architectures, and sparse memory access mechanisms, to work towards the goal of incorporating lifelong reasoning in our computational systems.\nOne of the earliest and most widely-used memory architectures in present day is a recurrent neural network (RNN) called the \nLong Short-Term-Memory\n (LSTM). The LSTM maintains a compact memory in the form of a vector of numbers, which it accesses and modifies with gated read, write, and forget operations. It was originally developed on a suite of synthetic tasks that involved learning logical operations on a stream of bits. However, it has since become a ubiquitous model of sequential data: from recognising handwritten notes to predicting the early onset of kidney injury.\nOne weakness of the LSTM, and of many contemporary RNNs, is capacity. They are designed so that each unit of memory can influence every other unit in memory with a learnable weight. But this results in a computationally inefficient system: \u00a0the number of learnable parameters in the model grows quadratically with the memory size. For example, an LSTM with a memory of size 64KB results in parameters of size 8GB. Circumventing this memory capacity bottleneck has been an active research area.\nResearchers at DeepMind proposed a novel architecture, the \nDifferentiable Neural Computer\n (DNC), which augments an LSTM with a much larger memory matrix to address these deficits. The DNC uses an \nattention\n operation to read from this memory matrix. In visual attention, our eyes are drawn by pertinent objects in a visual scene\u2013for example, one might typically spend more time observing a friend\u2019s face during an emotional conversation than on noticing their shoes. Here, memory models can attend to particular events/data in the past. This attention operation requires a fixed number of parameters, independent of the memory size, and so the memory capacity of the model can be significantly increased.\nAlongside the DNC, recurrent neural networks with an additional attention mechanism were showing promise in the domains of \ntranslation\n \u00a0and \nquestion answering\n. These models were able to reason over time using two memory structures: a small and compact LSTM memory and a large external memory. However, more recently researchers at Google Brain Team proposed \nthe\n \nTransformer\n which removes the LSTM, and only uses attention to transmit \ninformation across time\n. \nThe Transformer was originally shown to significantly outperform recurrent neural networks for machine translation. However it has since been applied to a range of applications in natural language processing, from question answering, document summarisation, sentiment classification and the modelling of natural language \u2013 a task that has seen particular exciting developments over the past year.\nFinding machine learning tasks which both drive the development of better memory architectures and push us further towards artificial general intelligence is challenging. Statistical \nlanguage modelling\n is one such task that we believe could be valuable for both purposes. Language models work by sequentially predicting the next word in a stream of text. They can be used to model existing texts and also to generate novel texts. As they get better at modelling the past, their predictions become more accurate, and the texts they generate become more realistic.\nIn Claude Shannon\u2019s seminal article \u201c\nA Mathematical Theory of Communication\n\u201d\n published in 1948, which founded the field of information theory, he discussed primitive language models and illustrated how adding more context improves the quality and realism of generated text. He does this by introducing the most simple model of English text, which has no contextual modelling at all \u2013 \u00a0a character-level model which treats each character independently. By sampling characters with their relative frequencies (8% of the time for \u2018a\u2019, 1.5% for \u2018b\u2019 etc.) we arrive with a nonsensical string :\nHowever, he remarks at the improvement in sample quality if one instead models the probability of words independently. Now the modelled context is approximately 7X larger (the average number of characters in a word): \nBy modelling the probability of word pairs, a further 2X in context length, even more realistic text emerges:\nIn other words, an increase in the length of context leads to an improvement in the quality of text generated. Shannon remarks on the quality of his produced samples and conjectures that natural text samples may emerge from a sufficiently complex statistical model, \n\u201cThe particular sequence of ten words \u201cattack on an English writer that the character of this\u201d is not at all unreasonable. It appears then that a sufficiently complex stochastic process will give a satisfactory representation of a discrete source\u201d\n. \nOne criticism of language modelling as a task for long-range reasoning is that models can capture a large portion of their predictions from the local context. Neural language models have traditionally ignored the wider context, focusing mostly on the short term. For example, in 2017 \nDailuk et al.\n found their neural language model rarely attends beyond the preceding five words. However in the past year large Transformer models have been shown to make use of hundreds of words of context to generate ever-more realistic text with a longer range of coherence. A demo from \nOpenAI\u2019s GPT-2\n, a 1.5B parameter Transformer, indicate that the model is able to generate realistic text and retain key entities (e.g. Dr Jorge P\u00e9rez and unicorns) across multiple paragraphs: \nSuch samples would likely astound Shannon, 70 years on from his early language model experiments. However the real benefit of powerful neural language models \u2013 and their relevance to the goal of AGI \u2013 is their ability to transfer knowledge to a suite of tasks. In the process of learning how to model text, neural language models appear to build up a knowledge-base of associations, and a plethora of skills. \nFor instance, researchers at OpenAI showed that GPT-2 can be applied to natural-language processing tasks such as question answering, paraphrasing, or sentiment analysis with surprisingly good performance \u2013 especially for a model that has never been explicitly trained to perform such tasks. When large Transformer language models are fine-tuned on particular tasks such as question answering, the resulting performance is significantly better than models that were designed and trained solely for question answering. Google\u2019s prominent natural language model, \nBERT\n, achieves state-of-the-art performance on a wide array of NLP benchmarks, and is now \na part of Google Search\n. And more recently, it was shown that GPT-2 can learn to play rudimentary chess by training it on strings of \ngame moves\n. \nA popular long-range language model benchmark is \nWikiText-103\n, which is comprised of English-language Wikipedia articles, and was developed by researchers at \nSalesforce AI\n. Articles are around 3,600 words on average, which, at the time of creation, was far beyond the memory window of state-of-the-art models. \nHowever researchers at Google recently showed that a Transformer variant called the TransformerXL \u2013 which maintains a memory of past network activations and recently obtained state-of-the-art results on WikiText-103 \u2013 can make use of contexts spanning over \none thousand words\n. This raises the question: will models soon saturate these benchmarks? As such, we\u2019ve compiled and released a new, longer-range language model benchmark based on books.\nTo support growing interest in long-range sequence models, we are releasing a new language modelling benchmark, \nPG-19\n, which is derived from books in the \nProject Gutenberg online library\n. \nBooks provide a rich context for the development of long-range memory models. \u00a0We selected a subset of approximately 28,000 books from Project Gutenberg published before 1919. Unlike prior language modeling dataset releases, we apply very little pre-processing to the text. For example, we do not limit the vocabulary size of the data or censor numbers, to avoid the filtering of useful information. \nPG-19 is over double the size of prior language modelling benchmarks, such as the \nBillion Word Benchmark\n, and contains text that is over 10X longer in context than the prior long-range language model benchmark, WikiText-103. We provide a comparative table of existing language modelling benchmarks, below:\nAlongside a new benchmark, we propose a long-range memory model called the \nCompressive Transformer\n. We take inspiration from the role of sleep in the formation of \nconsolidated episodic memories\n. Sleep is known to be crucial for memory, and it\u2019s thought that sleep serves to compress and consolidate memories, thereby improving reasoning abilities for memory tasks. In the Compressive Transformer, granular memories akin to episodic memories are collected online as the model passes over a sequence of inputs; over time, they are eventually compacted. \nThe Compressive Transformer uses attention to select information from the past, like the Transformer. It maintains a short-term memory of past activations, in the same style as the recently-proposed \nTransformerXL\n. Where the TransformerXL discards past activations when they become older, the Compressive Transformer instead compacts them into a \ncompressed memory\n. \u00a0The \ncompression\n is performed by a neural network guided by an auxiliary loss that guides it to keep around task-relevant information. It can learn to filter out irrelevant memories, as well as combine memories so that the salient information is preserved and retrievable over a longer period of time.\nWe find the Compressive Transformer has state-of-the-art performance in the modelling of natural language for two widely-used long-range benchmarks, \nWikiText-103\n and \nEnwik8, \ncompared to \npublished results\n that do not use additional sources of training data. We also show it can be used effectively to model speech, handles rare words especially well, and can be used within a reinforcement learning agent to solve a memory task. \nWe find the Compressive Transformer produces the largest performance gain in modelling long-context book text from the PG-19 benchmark. The model\u2019s conditional samples can be used to write book-like extracts. Below we show a sample that is fed a paragraph of text to be used as context, taken from \u201cThe Patrol of the Sun Dance\u201d by Ralph Connor, which the model has not previously seen.\nContext from \nThe Patrol of the Sun Dance\n Trail by Ralph Connor\n \nContinuation by the Compressive Transformer\nThe Compressive Transformer is able to produce narrative in a variety of styles, from multi-character dialogue, first-person diary entries, or third-person prose. Although the model does not have an understanding of language that\u2019s grounded in the real world, or the events that take place in it \u2013 by capturing longer-range correlations, we see the emergence of more coherent text. \nAs we strive to create agents that operate over days, weeks or even years, it will be impractical to compute over all raw input data at each timestep. Even with the current growth in computing power, we will need to develop compressive and sparse architectures for memory to build representations and reason about actions. \nModels which are able to capture relevant correlations across days, months, or years\u2019 worth of experience are on the horizon. We believe the route to more powerful reasoning over time will emerge from better selective attention of the past, and more effective mechanisms to compress it. As we explore ideas in this space, we need tasks and datasets that span longer and longer time intervals. The PG-19 dataset can help researchers move in this direction, presenting textual data in the longest form that we typically consume as humans: full-length books. We hope that its release will spur interest in new models that compress the past in order to predict the future and act effectively in the present.\nRead more\n"}
{"title": "Towards understanding glasses with graph neural networks", "contents": "Under a microscope, a pane of window glass doesn\u2019t look like a collection of orderly molecules, as a crystal would, but rather a jumble with no discernable structure. Glass is made by starting with a glowing mixture of high-temperature melted sand and minerals. Once cooled, its viscosity (a measure of the friction in the fluid) increases a trillion-fold, and it becomes a solid, resisting tension from stretching or pulling. Yet the molecules in the glass remain in a seemingly disordered state, much like the original molten liquid \u2013 almost as though the disordered liquid state had been flash-frozen in place. The \nglass transition\n, then, first appears to be a dramatic arrest in the movement of the glass molecules. Whether this process corresponds to a structural phase transition (as in water freezing, or the \nsuperconducting transition\n) is a major open question in the field. Understanding the nature of the dynamics of glass is fundamental to understanding how the atomic-scale properties define the visible features of many solid materials. \u00a0In the words of the \nrecently deceased\n Nobel Prize laureate \nPhilip W. Anderson\n, whose pioneering work shaped the field of solid-state physics:\nThe glass transition is a \nubiquitous phenomenon\n which manifests in more than window (silica) glasses. For instance, when \nironing\n, polymers in a fabric are heated, become mobile, and then oriented by the weight of the iron. More broadly, a similar and related transition, the \njamming\n transition, can be found in colloidal suspensions (such as ice cream), \ngranular materials \n(such as a static pile of sand), and also biological systems (e.g., for modelling \ncell migration\n during embryonic development) as well as social behaviours (for instance traffic jams). These systems all operate under local constraints where the position of some elements inhibits the motion of others (termed\n frustration\n). Their dynamics are complex and cooperative, taking the form of large-scale, collective rearrangements which propagate through space in a heterogeneous manner. Glasses are considered to be archetypal of these kinds of complex systems, and so better understanding them will have implications across many research areas. This understanding might yield practical benefits \u2013 for example, creating materials that have a more stable glass structure, instead of a crystalline one, would allow them to dissolve quickly, which could lead to new \ndrug delivery\n methods. \u00a0Understanding the glass transition may result in other applications of disordered materials, in fields as diverse as biorenewable polymers and food processing. The study of glasses has also already led to insights in apparently very different domains such as \nconstraint satisfaction problems\n in computer science and, more recently, the training dynamics of \nunder-parameterized neural networks\n. \nA deeper understanding of glasses may lead to practical advances in the future, but their mysterious properties also raise many fundamental research questions. Though humans have been making silica glasses for at least four thousand years, they remain enigmatic to scientists: there are many unknowns about the underlying physical correlates of, for example, the trillion-fold increase in viscosity that happens over the cooling process. Our interest in this field was also motivated by the fact that glasses are also an excellent testbed for applying modern machine learning methods to physical problems: they\u2019re easy to simulate, and easy to input to particle-based machine learning models. Crucially, we can then go in and examine these models to understand what they\u2019ve learned about the system, to gain deeper qualitative insights about the nature of glass, and the structural quantities which underpin its mysterious dynamical qualities. Our \nnew work\n, published in Nature Physics, could help us gain an understanding of the structural changes that may occur near the glass transition. More practically, this research could lead to insights about the mechanical constraints of glasses (e.g., where a glass will break). \nGlasses can be modelled as particles interacting via a short-range repulsive potential which essentially prevents particles from getting too close to each other. This potential is \nrelational\n (only pairs of particles interact) and \nlocal\n (only nearby particles interact with each other), which suggests that a model that respects this local and relational structure should be effective. In other words, given the system is underpinned by a graph-like structure, we reasoned it would be best modeled by a graph structured network, and set out to apply \nGraph Neural Networks\n to predict physical aspects of a glass. \nWe first created an input graph where the nodes represent particles, and edges represent interactions between particles, and are labelled with their relative distance. \u00a0A particle was connected to its neighboring particles within a certain radius (in this case, 2 particle diameters). We then trained a neural network, described below, to predict a single real number for each node of the graph. This prediction was ultimately regressed towards the \nmobilities \nof particles obtained from computer simulations of glasses. Mobility is a measure of how much a particle \ntypically \nmoves (more technically, this corresponds to the average distance travelled when averaging over initial velocities). \nOur network architecture was a typical graph network architecture, consisting of several neural networks. We first embedded the node and edge labels in a high-dimensional vector-space using two encoder networks (we used standard \nmulti-layer perceptrons\n). Next, we iteratively updated the embedded node and edge labels using two update networks visualized in Fig. 2b. At first, each edge updated based on its previous embedding and the embeddings of the two nodes it connected to. After all edges were updated in parallel using the same network, the nodes were also updated based on the sum of their neighboring edge embeddings and their previous embeddings, using a second network. We repeated this procedure several times (typically 7), allowing local information to propagate throughout the graph, as shown in Fig. 2c. Finally, we extracted the mobility for each particle from the final embeddings of the corresponding node using a decoder network. The resulting network has all the required properties: it is inherently relational, it is invariant under permutation of the nodes and edges of the graph, and it updates embedding in a way that is a composition of local operations. The network parameter training was done via stochastic gradient descent.\nTo study the full dynamical evolution of glasses, we constructed several datasets corresponding to predictions of mobilities on different time horizons and for different temperatures. We note that each particle will have collided several thousands of times over those timescales. Thus, the network must find a way to coarsely represent the long-term dynamics of the system.\nAfter applying graph networks to the three dimensional glasses that we simulated, we found that they strongly outperformed existing models, ranging from standard \nphysics-inspired baselines\n to \nstate-of-the-art machine learning models\n. Comparing the predicted mobilities (colour gradients, Figure 3) with the ground truth simulation (dots, Figure 3), we see that the agreement is extremely good on short times and remains well matched up to the relaxation time of the glass. Looking at a glass over the timescale of its relaxation time \u2013 for actual glass, this would be thousands of years \u2013 is like looking at a liquid over about a picosecond (10-12): the relaxation time is loosely when particles have collided enough to start losing information about their initial position. In numbers, the correlation between our prediction and the simulation's ground truth is 96% for very short timescales, and remains high at 64% for the relaxation time of the glass (an improvement of 40% compared to the previous state of the art).\nWe don\u2019t want to simply model glass, however: we want to understand it. \u00a0We therefore explored what factors were important to our model\u2019s success in order to infer what properties are important in the underlying system. A central unsolved question in the dynamics of glass is how particles influence one another as a function of distance, and how this evolves over time. We investigated this by designing an experiment leveraging the specific architecture of the graph network. Recall that repeated applications of the edge and node updates define shells of particles around any given particle: the first shell consists of all particles one step away from this \"marked\" particle, the second shell consists of all particles one step away from the first shell, and so on (see the different shades of blue on Figure 2c). By measuring the sensitivity of the prediction that the network makes for the central particle when the \nn-\nth shell is modified, we can measure how large an area the network uses to extract its prediction, which provides an estimate of the distance over which particles influence each other in the physical system.\nWe found that when predicting what happens in the near future or in the liquid phase, drastic modifications of the third shell (for instance, removing it altogether, Figure 4, left) did \nnot\n modify the prediction that the network would make for the marked particle. On the other hand, when making predictions at low temperature and in the far future, after the glass starts to relax, even tiny perturbations (Figure 4, right) of the 5-th shell affect the prediction for the marked particle. These findings are consistent with a physical picture where a \ncorrelation length\n (a measure of the distance over which particles influence each other) grows upon approaching the glass transition. The definition and study of correlation lengths is a cornerstone of the study of phase transition in physics, and one that is still an open point of debate when studying glasses. While this \"machine learned\" correlation length cannot be directly transformed into a physically measurable quantity, it provides compelling evidence that growing spatial correlations are present in the system upon approaching the glass transition, and that our network has learned to extract them.\nOur results show that graph networks constitute a powerful tool to predict the long term dynamics of glassy systems, leveraging the structure hidden in a local neighborhood of particles. We expect our technique to be useful for predicting other physical quantities of interest in glasses, and hope that it will lead to more insights for glassy system theorists \u2013 we are \nopen-sourcing our models and trained networks\n to aid this effort. More generally, graph networks are a versatile tool that are being applied to many other physical systems that consist of many-body interactions, in contexts including \ntraffic\n, crowd simulations, and cosmology. The network analysis methods used here also yield a deeper understanding in other fields: graph networks may not only help us make better predictions for a range of systems, but indicate what physical correlates are important for modeling them \u2013 in this work, how interactions between local particles in a glassy material evolve over time. \nWe believe that our results advocate using structured models when applying machine learning to the physical sciences; in our case, the ability to analyse the inner workings of a neural network indicated that it had discovered a quantity that correlates with an elusive physical quantity. This demonstrates that machine learning can be used not only to make quantitative predictions, but also to gain qualitative understanding of physical systems. This could mean that machine learning systems might be able to eventually assist researchers in deriving fundamental physical theories, ultimately helping to augment, rather than replace, human understanding.\n\u200d\nRead more\nWe\u2019re continuing to develop methods for applying machine learning to a broad range of fundamental science questions. We\u2019re always looking to hire more scientists\u2013read about our \nscience programme\n and \nopenings\n for more information.\nWork done in collaboration with: E. D. Cubuk, S. S. Schoenholz, A. Obika, A. W. R. Nelson, T. Back, D. Hassabis and P. Kohli\nFigure design by Paulo Estriga and Adam Cain\n"}
{"title": "Stacking our way to more general robots", "contents": "Introducing RGB-Stacking as a new benchmark for vision-based robotic manipulation.\nPicking up a stick and balancing it atop a log or stacking a pebble on a stone may seem like simple \u2014 and quite similar \u2014 actions for a person. However, most robots struggle with handling more than one such task at a time. Manipulating a stick requires a different set of behaviours than stacking stones, never mind piling various dishes on top of one another or assembling furniture. Before we can teach robots how to perform these kinds of tasks, they first need to learn how to interact with a far greater range of objects. As part of \nDeepMind\u2019s mission\n and as a step toward making more generalisable and useful robots, we\u2019re exploring how to enable robots to better understand the interactions of objects with diverse geometries.\nIn a paper to be presented at \nCoRL 2021\n (Conference on Robot Learning) and available now as a preprint on \nOpenReview\n, we introduce RGB-Stacking as a new benchmark for vision-based robotic manipulation. In this benchmark, a robot has to learn how to grasp different objects and balance them on top of one another. What sets our research apart from prior work is the diversity of objects used and the large number of empirical evaluations performed to validate our findings. Our results demonstrate that a combination of simulation and real-world data can be used to learn complex multi-object manipulation and suggest a strong baseline for the open problem of generalising to novel objects. To support other researchers, we\u2019re \nopen-sourcing\n a version of our simulated environment, and releasing the \ndesigns\n for building our real-robot RGB-stacking environment, along with the RGB-object models and information for 3D printing them. We are also open-sourcing \na collection of libraries and tools\n used in our robotics research more broadly.\nWith RGB-Stacking, our goal is to train a robotic arm via reinforcement learning to stack objects of different shapes. We place a parallel gripper attached to a robot arm above a basket, and three objects in the basket \u2014 one red, one green, and one blue, hence the name RGB. The task is simple: stack the red object on top of the blue object within 20 seconds, while the green object serves as an obstacle and distraction. The learning process ensures that the agent acquires generalised skills through training on multiple object sets. We intentionally vary the grasp and stack affordances \u2014 the qualities that define how the agent can grasp and stack each object. This design principle forces the agent to exhibit behaviours that go beyond a simple pick-and-place strategy.\nOur RGB-Stacking benchmark includes two task versions with different levels of difficulty. In \u201cSkill Mastery,\u201d our goal is to train a single agent that\u2019s skilled in stacking a predefined set of five triplets. In \u201cSkill Generalisation,\u201d we use the same triplets for evaluation, but train the agent on a large set of training objects \u2014 totalling more than a million possible triplets. To test for generalisation, these training objects exclude the family of objects from which the test triplets were chosen. In both versions, we decouple our learning pipeline into three stages:\nDecoupling our learning pipeline in such a way proves crucial for two main reasons. Firstly, it allows us to solve the problem at all, since it would simply take too long if we were to start from scratch on the robots directly. Secondly, it increases our research velocity, since different people in our team can work on different parts of the pipeline before we combine these changes for an overall improvement.\nIn recent years, there has been much work on applying learning algorithms to solving difficult real-robot manipulation problems at scale, but the focus of such work has largely been on tasks such as grasping, pushing, or other forms of manipulating single objects. The approach to RGB-Stacking we describe in our paper, accompanied by \nour robotics resources now available on GitHub\n, results in surprising stacking strategies and mastery of stacking a subset of these objects. Still, this step only scratches the surface of what\u2019s possible \u2013 and the generalisation challenge remains not fully solved. As researchers keep working to solve the open challenge of true generalisation in robotics, we hope this new benchmark, along with the environment, designs, and tools we have released, contribute to new ideas and methods that can make manipulation even easier and robots more capable.\n\u200d\n"}
{"title": "Dopamine and temporal difference learning: A fruitful relationship between neuroscience and AI", "contents": "Learning and motivation are driven by internal and external rewards. Many of our day-to-day behaviours are guided by predicting, or anticipating, whether a given action will result in a positive (that is, rewarding) outcome. The study of how organisms learn from experience to correctly anticipate rewards has been a productive research field for well over a century, since Ivan Pavlov's seminal psychological work. In his most famous experiment, dogs were trained to expect food some time after a buzzer sounded. These dogs began salivating as soon as they heard the sound, before the food had arrived, indicating they'd learned to predict the reward. In the original experiment, Pavlov estimated the dogs\u2019 anticipation by measuring the volume of saliva they produced. But in recent decades, scientists have begun to decipher the inner workings of how the brain learns these expectations. Meanwhile, in close contact with this study of reward learning in animals, computer scientists have developed algorithms for reinforcement learning in artificial systems. These algorithms enable AI systems to learn complex strategies without external instruction, guided instead by reward predictions. \nThe contribution of our new work, \npublished in Nature\n (\nPDF\n), is finding that a recent development in computer science \u2013 which yields significant improvements in performance on reinforcement learning problems \u2013 may provide a deep, parsimonious explanation for several previously unexplained features of reward learning in the brain, and opens up new avenues of research into the brain\u2019s dopamine system, with potential implications for learning and motivation disorders.\nReinforcement learning is one of the oldest and most powerful ideas linking neuroscience and AI. In the late 1980s, computer science researchers were trying to develop algorithms that could learn how to perform complex behaviours on their own, using only rewards and punishments as a teaching signal. These rewards would serve to reinforce whatever behaviours led to their acquisition. To solve a given problem, it\u2019s necessary to understand how current actions result in future rewards. For example, a student might learn by reinforcement that studying for an exam leads to better scores on tests. In order to predict the total future reward that will result from an action, it's often necessary to reason many steps into the future. \nAn important breakthrough in solving the problem of reward prediction was the \ntemporal difference learning (TD) algorithm\n. TD uses a mathematical trick to replace complex reasoning about the future with a very simple learning procedure that can produce the same results. This is the trick: instead of trying to calculate total future reward, TD simply tries to predict the combination of immediate reward and \nits own reward prediction at the next moment in time\n. Then, when the next moment comes, bearing new information, the new prediction is compared against what it was expected to be. If they\u2019re different, the algorithm calculates how different they are, and uses this \u201ctemporal difference\u201d to adjust the old prediction toward the new prediction. By always striving to bring these numbers closer together at every moment in time \u2013 matching expectations to reality \u2013 the entire chain of prediction gradually becomes more accurate. \nAround the same time, in the late 80s and early 90s, neuroscientists were \nstruggling\n to understand the behaviour of dopamine neurons. Dopamine neurons are clustered in the midbrain, but send projections to many brain areas, potentially broadcasting some globally relevant message. It was clear that the firing of these neurons had some relationship to reward, but their responses also depended on sensory input, and changed as the animals became more experienced in a given task.\nFortuitously, some researchers were versed in the recent developments of both neuroscience and AI. These scientists \nnoticed\n, in the mid-1990s, that responses in some dopamine neurons represented reward prediction errors\u2013their firing signalled when the animal got more reward, or less reward, than it was trained to expect. These researchers therefore proposed that the brain uses a TD learning algorithm: a reward prediction error is calculated, broadcast to the brain via the dopamine signal, and used to drive learning. Since then, the \nreward prediction error theory of dopamine\n has been tested and validated in thousands of experiments, and has become one of the most successful quantitative theories in neuroscience.\nComputer scientists have continued to improve the algorithms for learning from rewards and punishments. \nSince 2013\n, there\u2019s been a focus on \ndeep\n reinforcement learning: using deep neural networks to learn powerful representations in reinforcement learning. This has enabled reinforcement learning algorithms to \nsolve\n \ntremendously\n \nmore\n \nsophisticated\n and \nuseful\n problems.\nOne of the algorithmic developments that has made reinforcement learning work better with neural networks is \ndistributional reinforcement learning\n. In many situations (\nespecially\n in the \nreal world\n), the amount of future reward that will result from a particular action is not a perfectly known quantity, but instead involves some randomness. An example is shown in Figure 1. This is a stylised representation of a situation where a computer-controlled avatar, \ntrained\n to traverse an obstacle course, jumps across a gap. The agent is uncertain about whether it will fall, or reach the other side. Therefore, the distribution of predicted rewards has two bumps: one representing the possibility of falling, and one representing the possibility of successfully reaching the other side.\nIn such situations, a standard TD algorithm learns to predict the future reward that will be received \non average\n\u2013in this case, failing to capture the two-peaked distribution of potential returns. A distributional reinforcement learning algorithm, on the other hand, learns to predict the full spectrum of future rewards. Figure 1 depicts the reward prediction learned by a distributional agent. \u00a0\nOne of the simplest distributional reinforcement learning algorithms is very closely related to standard TD, and is called distributional TD. Whereas standard TD learns a single prediction \u2013 the average expected reward \u2013 a distributional TD network learns a set of distinct predictions. Each of these is learned through the same method as standard TD \u2013 by computing a reward prediction error that describes the difference between consecutive predictions. But the crucial ingredient is that each predictor applies a different transformation to its reward prediction errors. Some predictors \"amplify\" or \"overweight\" their reward prediction errors (RPE) selectively when the reward prediction error is positive (Figure 2a). This causes the predictor to learn a more \noptimistic\n reward prediction, corresponding to a higher part of the reward distribution. Other predictors amplify their negative reward prediction errors (Figure 2a), and so learn more \npessimistic\n predictions. All together, a set of predictors with a diverse set of pessimistic and optimistic weightings map out the full reward distribution (Figure 2b, 2c).\nAside from its simplicity, another benefit of distributional reinforcement learning is that it\u2019s very powerful when combined with deep neural networks. In the last 5 years, there\u2019s been a great deal of progress in algorithms based around the original deep reinforcement learning \nDQN agent\n, and these are frequently evaluated on the \nAtari-57 benchmark\n set of \nAtari 2600 games\n. Figure 3 compares many standard and distributional RL algorithms, trained and evaluated under the same conditions, on this benchmark. Distributional reinforcement learning agents are shown in blue, and illustrate the significant pattern of improvements. Three of these algorithms (QR-DQN, IQN, and FQF) are variants of the distributional TD algorithm we\u2019ve been discussing.\nWhy are distributional reinforcement learning algorithms so effective? Although this is still an active topic of research, a key ingredient is that learning about the distribution of rewards gives the neural network a more powerful signal for \nshaping its representation\n in a way that\u2019s robust to changes in the environment or changes in the policy. \nBecause distributional TD is so powerful in artificial neural networks, a natural question arises: Is distributional TD used in the brain? This was the driving question behind our paper recently published in \nNature\n.\nIn this work, we collaborated with an \nexperimental lab at Harvard\n to analyse their recordings of dopamine cells in mice. The recordings were made while the mice performed a well-learned task in which they received rewards of unpredictable magnitude (indicated by the dice illustration in Figure 4). We evaluated whether the activity of dopamine neurons was more consistent with standard TD or distributional TD.\nAs described above, distributional TD relies on a set of distinct reward predictions. Our first question was whether we could see such genuinely diverse reward predictions in the neural data.\nFrom previous work, we know that dopamine cells change their firing rate to indicate a prediction error \u2013 that is, if an animal receives more or less reward than it expected. We know that there should be zero prediction error when a reward is received that is the exact size as what a cell had predicted, and therefore no change in firing rate. For each dopamine cell, we determined the reward size for which it didn\u2019t change its baseline firing rate. We call this the cell's \"reversal point\". We wanted to know whether these reversal points were different between cells. In Figure 4c, we show that there were marked differences between cells, with some cells predicting very large amounts of reward, and other cells predicting very little reward. These differences were above and beyond the amount of difference we would expect to see from random variability inherent in the recordings. \nIn distributional TD, these differences in reward prediction arise from selective amplification of positive or negative reward prediction errors. Amplifying positive reward prediction errors causes more optimistic reward predictions to be learned; amplifying negative reward prediction errors causes pessimistic predictions. So we next measured the degree to which different dopamine cells exhibited different relative amplifications of positive versus negative expectations. Between cells, we found reliable diversity which, again, could not be explained by noise. And, crucially, we found that the \nsame cells\n which amplified their positive reward prediction errors also had higher reversal points (Figure 4c, bottom-right panels) \u2013 that is, they were apparently tuned to expect higher reward volumes. \nFinally, distributional TD theory predicts that these diverse reversal points and diverse asymmetries, across cells, should collectively encode the learned reward distribution. So our final question was if we could \ndecode\n the reward distribution from the firing rates of dopamine cells. As shown in Figure 5, we found that it was indeed possible, using only the firing rates of dopamine cells, to reconstruct a reward distribution (blue trace) which was a very close match to the actual distribution of rewards (grey area) in the task that the mice were engaged in. This reconstruction relied on interpreting the firing rates of dopamine cells as the reward prediction errors of a distributional TD model, and performing inference to determine what distribution that model had learned about.\nIn summary, we found that dopamine neurons in the brain were each tuned to different levels of pessimism or optimism. If they were a choir, they wouldn\u2019t all be singing the same note, but harmonizing \u2013 each with a consistent vocal register, like bass and soprano singers. In artificial reinforcement learning systems, this diverse tuning creates a richer training signal that greatly speeds learning in neural networks, and we speculate that the brain might use it for the same reason.\nThe existence of distributional reinforcement learning in the brain has interesting implications both for AI and neuroscience. Firstly, this discovery validates distributional reinforcement learning \u2013 it gives us increased confidence that AI research is on the right track, since this algorithm is already being used in the most intelligent entity we're aware of: the brain. \nSecondly, it raises new questions for neuroscience, and new insights for understanding mental health and motivation. What happens if an individual's brain \u201clistens\u201d selectively to optimistic versus pessimistic dopamine neurons? Does this give rise to impulsivity, or depression? A strength of the brain is its powerful representations \u2013 how are these sculpted by distributional learning? Once an animal learns about the distribution of rewards, how is that representation used downstream? How does the variability of optimism across dopamine cells relate to \nother\n \nknown\n \nforms\n of diversity in the brain? \nFinally, we hope that asking and answering these questions will stimulate progress in neuroscience that will feed back to benefit AI research, completing the \nvirtuous circle\n. \nRead the paper \nhere\n.\nListen to our \npodcast\n on the virtuous circle between AI and neuroscience.\n"}
{"title": "Opening up a physics simulator for robotics", "contents": "When you walk, your feet make contact with the ground. When you write, your fingers make contact with the pen. Physical contacts are what makes interaction with the world possible. Yet, for such a common occurrence, contact is a surprisingly complex phenomenon. Taking place at microscopic scales at the interface of two bodies, contacts can be soft or stiff, bouncy or spongy, slippery or sticky. It\u2019s no wonder our fingertips have \nfour different types\n of touch-sensors. This subtle complexity makes simulating physical contact \u2014 a vital component of robotics research \u2014 a tricky task.\nThe rich-yet-efficient contact model of the \nMuJoCo physics simulator\n has made it a leading choice by robotics researchers and today, we're proud to announce that, as part of \nDeepMind's mission\n of advancing science, we've acquired MuJoCo and are making it \nfreely available\n for everyone, to support research everywhere. Already widely used within the robotics community, including as the physics simulator of choice for DeepMind\u2019s robotics team, MuJoCo features a rich contact model, powerful scene description language, and a well-designed API. Together with the community, we will continue to improve MuJoCo as open-source software under a permissive licence. As we work to prepare the codebase, we are making MuJoCo \nfreely available\n as a precompiled library.\nA balanced model of contact. MuJoCo, which stands for \nMu\nlti-\nJo\nint Dynamics with \nCo\nntact, hits a sweet spot with its contact model, which accurately and efficiently captures the salient features of contacting objects. Like other rigid-body simulators, it avoids the fine details of deformations at the contact site, and often runs much faster than real time. Unlike other simulators, MuJoCo resolves contact forces using the convex \nGauss Principle\n. Convexity ensures unique solutions and well-defined inverse dynamics. The model is also flexible, providing multiple parameters which can be tuned to approximate a wide range of contact phenomena.\nReal physics, no shortcuts. \nBecause many simulators were initially designed for purposes like gaming and cinema, they sometimes take shortcuts that prioritise stability over accuracy. For instance, they may ignore gyroscopic forces or directly modify velocities. This can be particularly harmful in the context of optimisation: as \nfirst observed\n by artist and researcher Karl Sims, an optimising agent can quickly discover and exploit these deviations from reality. In contrast, MuJoCo is a second-order continuous-time simulator, implementing the full Equations of Motion. Familiar yet non-trivial physical phenomena like \nNewton\u2019s Cradle\n, as well as unintuitive ones like the \nDzhanibekov effect\n, emerge naturally. Ultimately, MuJoCo closely adheres to the equations that govern our world.\nPortable code, clean API.\n MuJoCo\u2019s core engine is written in pure C, which makes it easily portable to various architectures. The library produces deterministic results, with the scene description and simulation state fully encapsulated within two data structures. These constitute all the information needed to recreate a simulation, including results from intermediate stages, providing easy access to the internals. The library also provides fast and convenient computations of commonly used quantities, like kinematic Jacobians and inertia matrices.\nPowerful scene description. \nThe MJCF scene-description format uses cascading defaults \u2014 avoiding multiple repeated values \u200b\u200b\u2014 and contains elements for real-world robotic components like equality constraints, motion-capture markers, tendons, actuators, and sensors. Our long-term roadmap includes standardising MJCF as an open format, to extend its usefulness beyond the MuJoCo ecosystem.\nBiomechanical simulation.\n MuJoCo includes two powerful features that support musculoskeletal models of humans and animals. Spatial tendon routing, including wrapping around bones, means that applied forces can be distributed correctly to the joints, describing complicated effects like the variable \nmoment-arm\n in the \nknee enabled by the tibia\n. MuJoCo\u2019s muscle model captures the complexity of biological muscles, including activation states and force-length-velocity curves.\nA \nrecent \nPNAS\n perspective\n exploring the state of simulation in robotics identifies open source tools as critical for advancing research. The authors\u2019 recommendations are to develop and validate open source simulation platforms as well as to establish open and community-curated libraries of validated models. In line with these aims, we\u2019re committed to developing and maintaining MuJoCo as a free, open-source, community-driven project with best-in-class capabilities. We\u2019re currently hard at work preparing MuJoCo for full open sourcing, and we encourage you to download the software from the \nnew homepage\n and visit the \nGitHub repository\n if you'd like to contribute. \nEmail us\n if you have any questions or suggestions, and if you're also excited to push the boundaries of realistic physics simulation, \nwe're hiring\n. We can\u2019t promise we\u2019ll be able to address everything right away, but we\u2019re eager to work together to make MuJoCo the physics simulator we\u2019ve all been waiting for.\nMuJoCo in DeepMind. \nOur robotics team has been using MuJoCo as a simulation platform for various projects, mostly via our \ndm_control\n Python stack. In the carousel below, we highlight a few examples to showcase what can be simulated in MuJoCo. Of course, these clips represent only a tiny fraction of the vast possibilities for how researchers might use the simulator. For higher quality versions of these clips, please click \nhere\n.\n"}
{"title": "Agent57: Outperforming the human Atari benchmark", "contents": "The Atari57 suite of games is a long-standing benchmark to gauge agent performance across a wide range of tasks. We\u2019ve developed \nAgent57,\n the first deep reinforcement learning agent to obtain a score that is above the \u00a0human baseline on all 57 Atari 2600 games. Agent57 combines an algorithm for efficient exploration with a meta-controller that adapts the exploration and long vs. short-term behaviour of the agent.\nAt DeepMind, \u00a0we\u2019re interested in building agents that do well on a wide range of tasks. An agent that performs \nsufficiently well\n on a \nsufficiently wide\n range of tasks is classified as \nintelligent\n. Games are an excellent testing ground for building adaptive algorithms: they provide a rich suite of tasks which players must develop sophisticated behavioural strategies to master, but they also provide an easy progress metric \u2013 game score \u2013 to optimise against. The ultimate goal is not to develop systems that excel at games, but rather to use games as a stepping stone for developing systems that learn to excel at a broad set of challenges. Typically, human performance is taken as a baseline for what doing \u201csufficiently well\u201d on a task means: the score obtained by an agent on each task can be measured relative to representative human performance, providing a human normalised score: \u00a00% indicates that an agent performs at random, while 100% or above indicates the agent is performing at human level or better.\nIn 2012, \nthe Arcade Learning environment\n \u2013 a suite of 57 Atari 2600 games (dubbed Atari57) \u2013 was proposed as a benchmark set of tasks: these canonical Atari games pose a broad range of challenges for an agent to master. The research community commonly uses this benchmark to measure progress in building successively more intelligent agents. \u00a0It\u2019s often desirable to summarise the performance of an agent on a wide range of tasks as a single number, and so average performance (either mean or median score across all games) on the Atari57 benchmark is often used to summarise an agents\u2019 abilities. Average scores have progressively increased over time. Unfortunately, the average performance can fail to capture how many tasks an agent is doing well on, and so is not a good statistic for determining how \ngeneral\n an agent is: it captures that an agent is doing \nsufficiently well,\n but not that it is doing sufficiently well on a \nsufficiently wide set of tasks\n. So although average scores have increased, until now, the number of above human games has not. As an illustrative example, consider a benchmark consisting of twenty tasks. Suppose agent A obtains a score of 500% on eight tasks, 200% on four tasks, and 0% on eight tasks \u00a0(mean = 240%, median = 200%), while agent B obtains a score of 150% on all tasks (mean = median = 150%). On average, agent A performs better than agent B. However, agent B possesses a more general ability: it obtains human-level performance on more tasks than agent A.\nThis issue is exacerbated if some tasks are much easier than others. By performing very well on very easy tasks, agent A can apparently outperform agent B, which performs well on both easy and hard tasks.\nThe median is less distorted by exceptional performance on a few easy games \u2013 it\u2019s a more \nrobust statistic\n than the mean for indicating the \ncenter of a distribution\n. However, in measuring generality, the tails of the distribution become more pertinent, particularly as the number of tasks becomes larger. For example, the measure of performance on the hardest 5th percentile of games can be much more representative of an agent\u2019s degree of generality.\nResearchers have focused on maximising agents\u2019 average performance on the Atari57 benchmark since its inception, and average performance has significantly increased over the past eight years. But, like the illustrative example above, not all Atari games are equal, with some games being much easier than others. Instead of examining the average performance, if we examine the performance of agents on the bottom 5% of games, we see that not much has changed since 2012: in fact, agents published in 2019 were struggling on the same games with which agents published in 2012 struggled. \u00a0Agent57 changes this, and is a more general agent in Atari57 than any agent since the inception of the benchmark. Agent57 finally obtains above human-level performance on the very hardest games in the benchmark set, as well as the easiest ones.\nBack in 2012, DeepMind developed the \nDeep Q-network agent \n(DQN) to tackle the Atari57 suite. Since then, the research community has developed many extensions and alternatives to DQN. Despite these advancements, however, all deep reinforcement learning agents have consistently failed to score in four games: Montezuma\u2019s Revenge, Pitfall, Solaris and Skiing. \nMontezuma\u2019s Revenge and Pitfall require extensive exploration to obtain good performance. A core dilemma in learning is the \nexploration-exploitation problem\n: should one keep performing behaviours one knows works (exploit), or should one try something new (explore) to discover new strategies that might be even more successful? For example, should one always order their same favourite dish at a local restaurant, or try something new that might surpass the old favourite? Exploration involves taking many suboptimal actions to gather the information necessary to discover an ultimately stronger behaviour.\nSolaris and Skiing are long-term credit assignment problems: in these games, it\u2019s challenging to match the consequences of an agents\u2019 actions to the rewards it receives. Agents must collect information over long time scales to get the feedback necessary to learn.\nPlaylist: Agent57 playing the four most challenging Atari57 games \u2013 Montezuma's Revenge, Pitfall, Solaris and Skiing\nFor Agent57 to tackle these four challenging games in addition to the other Atari57 games, several changes to DQN were necessary.\nEarly improvements to DQN enhanced its learning efficiency and stability, including \ndouble DQN\n, \nprioritised experience replay\n and \ndueling architecture\n. These changes allowed agents to make more efficient and effective use of their experience.\nNext, researchers introduced \ndistributed\n variants of DQN, \nGorila DQN\n and \nApeX\n, \u00a0that could be run on many computers simultaneously. This allowed agents to acquire and learn from experience more quickly, enabling researchers to rapidly iterate on ideas. Agent57 is also a distributed RL agent that decouples the data collection and the learning processes. Many actors interact with independent copies of the environment, feeding data to a central \u2018memory bank\u2019 in the form of a prioritized replay buffer. A learner then samples training data from this replay buffer, as shown in Figure 4, similar to how a person might recall memories to better learn from them. \u00a0The learner uses these replayed experiences to construct loss functions, by which it estimates the cost of actions or events. Then, it updates the parameters of its neural network by minimizing losses. Finally, each actor shares the same network architecture as the learner, but with its own copy of the weights. The learner weights are sent to the actors frequently, allowing them to update their own weights in a manner determined by their individual priorities, as we\u2019ll discuss later. \nAgents need to have memory in order to take into account previous observations into their decision making. This allows the agent to not only base its decisions on the present observation (which is usually partial, that is, an agent only sees some of its world), but also on past observations, which can reveal more information about the environment as a whole. Imagine, for example, a task where an agent goes from room to room in order to count the number of chairs in a building. Without memory, the agent can only rely on the observation of one room. With memory, the agent can remember the number of chairs in previous rooms and simply add the number of chairs it observes in the present room to solve the task. Therefore the role of memory is to aggregate information from past observations to improve the decision making process. In deep RL and deep learning, recurrent neural networks such as \nLong-Short Term Memory\n (LSTM) are used as short term memories.\nInterfacing memory with behaviour is crucial for building systems that self-learn. In reinforcement learning, an agent can be an on-policy learner, which can only learn the value of its direct actions, or an off-policy learner, which can learn about optimal actions even when not performing those actions \u2013 e.g., it might be taking random actions, but can still learn what the best possible action would be. \u00a0Off-policy learning is therefore a desirable property for agents, helping them learn the best course of action to take while thoroughly exploring their environment. Combining off-policy learning with memory is challenging because you need to know what you might remember when executing a different behaviour. For example, what you might choose to remember when looking for an apple (e.g., where the apple is located), is different to what you might choose to remember if looking for an orange. But if you were looking for an orange, you could still learn how to find the apple if you came across the apple by chance, in case you need to find it in the future. The first deep RL agent combining memory and off-policy learning was\n Deep Recurrent Q-Network\n (DRQN). More recently, a significant speciation in the lineage of Agent57 occurred with \nRecurrent Replay Distributed DQN\n (R2D2), combining a neural network model of short-term memory with off-policy learning and distributed training, and achieving a very strong average performance on Atari57. \u00a0R2D2 modifies the replay mechanism for learning from past experiences to work with short term memory. All together, this helped R2D2 efficiently learn profitable behaviours, and \nexploit\n them for reward.\nWe designed \nNever Give Up\n (NGU) to augment R2D2 with another form of memory: episodic memory. This enables NGU to detect when new parts of a game are encountered, so the agent can explore these newer parts of the game in case they yield rewards. This makes the agent\u2019s behaviour (\nexploration\n) deviate significantly from the policy the agent is trying to learn (obtaining a high score in the game); thus, off-policy learning again plays a critical role here. NGU was the first agent to obtain positive rewards, without domain knowledge, on Pitfall, a game on which no agent had scored any points since the introduction of the Atari57 benchmark, and other challenging Atari games. Unfortunately, NGU sacrifices performance on what have historically been the \u201ceasier\u201d games and so, on average, underperforms relative to R2D2. \nIn order to discover the most successful strategies, agents must explore their environment\u2013but some exploration strategies are more efficient than others. With DQN, researchers attempted to address the exploration problem by using an undirected exploration strategy known as epsilon-greedy: with a fixed probability (epsilon), take a random action, otherwise pick the current best action. However, this family of techniques do not scale well to hard exploration problems: in the absence of rewards, they require a prohibitive amount of time to explore large state-action spaces, as they rely on undirected random action choices to discover unseen states. In order to overcome this limitation, many directed exploration strategies have been proposed. Among these, one strand has focused on developing \nintrinsic motivation\n \nrewards\n that encourage an agent to explore and visit as many states as possible by providing more dense \u201cinternal\u201d rewards for novelty-seeking behaviours. Within that strand, we distinguish two types of rewards: firstly, \nlong-term novelty \nrewards encourage visiting many states throughout training, across many episodes. Secondly, \nshort-term novelty \nrewards encourage visiting many states over a short span of time (e.g., within a single episode of a game).\nLong-term novelty rewards\n signal when a previously unseen state is encountered in the agent\u2019s lifetime, and is a function of the density of states seen so far in training: that is, it\u2019s adjusted by how often the agent has seen a state similar to the current one relative to states seen overall. When the density is high (indicating that the state is \nfamiliar\n), the long term novelty reward is low, and vice versa. When all the states are familiar, the agent resorts to an undirected exploration strategy. However, learning density models of high dimensional spaces is fraught with problems due to the \ncurse of dimensionality\n. In practice, when agents use deep learning models to learn a density model, they suffer from \ncatastrophic forgetting\n (forgetting information seen previously as they encounter new experiences), as well as an inability to produce precise outputs for all inputs. For example, in Montezuma\u2019s Revenge, unlike undirected exploration strategies, long-term novelty rewards allow the agent to surpass the human baseline. However, even the\n best performing methods on Montezuma\u2019s Revenge\n need to carefully train a density model at the \nright\n speed: when the density model indicates that the states in the first room are \nfamiliar\n, the agent should be able to consistently get to unfamiliar territory.\nPlaylist: DQN vs. Agent57 playing Montezuma's revenge\nShort-term novelty rewards\n can be used to encourage an agent to explore states that have not been encountered in its recent past. Recently, neural networks that mimic some properties of \nepisodic memory\n have been used \u00a0to speed up learning in reinforcement learning agents. Because episodic memories are also thought to be important for \nrecognising novel experiences\n, we adapted these models to give Never Give Up a notion of short-term novelty. Episodic memory models are efficient and reliable candidates for computing short-term novelty rewards, as they can quickly learn a non-parametric density model that can be adapted on the fly (without needing to learn or adapt parameters of the model). In this case, the magnitude of the reward is determined by measuring the distance between the present state and previous states recorded in episodic memory. \nHowever, not all notions of distance encourage meaningful forms of exploration. For example, consider the task of navigating a busy city with many pedestrians and vehicles. If an agent is programmed to use a notion of distance wherein every tiny visual variation is taken into account, that agent would visit a large number of different states simply by passively observing the environment, even standing still \u2013 a fruitless form of exploration. To avoid this scenario, the agent should instead learn features that are seen as important for exploration, such as controllability, and compute a distance with respect to those features only. Such models have previously been used for exploration, and combining them with episodic memory is one of the main advancements of the \nNever Give Up exploration method\n, which resulted in above-human performance in Pitfall!\nPlaylist: NGU vs. Agent57 playing Pitfall!\nNever Give Up (NGU) used this short-term novelty reward based on \ncontrollable states\n, mixed with a long term novelty reward, using \nRandom Network Distillation\n. The mix was achieved by multiplying both rewards, where the long term novelty is bounded. This way the short-term novelty reward\u2019s effect is preserved, but can be down-modulated as the agent becomes more familiar with the game over its lifetime. The other core idea of NGU is that it learns a family of policies that range from purely exploitative to highly exploratory. This is achieved by leveraging a distributed setup: by building on top of \nR2D2\n, actors produce experience with different policies based on different importance weighting on the total novelty reward. This experience is produced uniformly with respect to each weighting in the family.\nAgent57 is built on the following observation: what if an agent can learn when it\u2019s better to exploit, and when it\u2019s better to explore? We introduced the notion of a meta-controller that adapts the exploration-exploitation trade-off, as well as a time horizon that can be adjusted for games requiring longer temporal credit assignment. With this change, Agent57 is able to get the best of both worlds: above human-level performance on both easy games and hard games.\nSpecifically, intrinsic motivation methods have two shortcomings:\nThis motivated the use of an online adaptation mechanism that controls the amount of experience produced with different policies, with a variable-length time horizon and importance attributed to novelty. Researchers have tried tackling this with multiple methods, including \ntraining a population of agents\n with different hyperparameter values, \ndirectly learning the values of the hyperparameters by gradient descent\n, or using a \ncentralized bandit to learn the value of hyperparameters\n.\nWe used a bandit algorithm to select which policy our agent should use to generate experience. Specifically, we trained a \nsliding-window UCB bandit\n for each actor to select the degree of preference for exploration and time horizon its policy should have.\nPlaylist: NGU vs. Agent57 playing Skiing\nTo achieve Agent57, we combined our previous exploration agent, Never Give Up, with a meta-controller. This agent computes a mixture of long and short term intrinsic motivation to explore and learn a family of policies, where the choice of policy is selected by the meta-controller. The meta-controller allows each actor of the agent to choose a different trade-off between near vs. long term performance, as well as exploring new states vs. exploiting what\u2019s already known (Figure 4). Reinforcement learning is a feedback loop: the actions chosen determine the training data. Therefore, the meta-controller also determines what data the agent learns from.\nWith Agent57, we have succeeded in building a more generally intelligent agent that has above-human performance on all tasks in the Atari57 benchmark. It builds on our previous agent Never Give Up, and instantiates an adaptive meta-controller that helps the agent to know when to explore and when to exploit, as well as what time-horizon it would be useful to learn with. A wide range of tasks will naturally require different choices of both of these trade-offs, therefore the meta-controller provides a way to dynamically adapt such choices.\nAgent57 was able to scale with increasing amounts of computation: the longer it trained, the higher its score got. While this enabled Agent57 to achieve strong general performance, it takes a lot of computation and time; the data efficiency can certainly be improved. Additionally, this agent shows better 5th percentile performance on the set of Atari57 games. This by no means marks the end of Atari research, not only in terms of data efficiency, but also in terms of general performance. We offer two views on this: firstly, analyzing the performance among percentiles gives us new insights on how general algorithms are. While Agent57 achieves strong results on the first percentiles of the 57 games and holds better mean and median performance than NGU or R2D2, as illustrated by \nMuZero\n, it could still obtain a higher average performance. Secondly, all current algorithms are \nfar from achieving optimal performance\n in some games. To that end, key improvements to use might be enhancements in the representations that Agent57 uses for exploration, planning, and credit assignment.\nRead the paper \nhere\n.\nWork done by: Adri\u00e0 Puigdom\u00e8nech, Bilal Piot, Steven Kapturowski, Pablo Sprechmann, Alex Vitvitskyi, Daniel Guo, Charles Blundell\nFigure design by Paulo Estriga and Adam Cain\n"}
{"title": "AlphaGo", "contents": "Despite decades of work, the strongest Go computer programs could only play at the level of human amateurs. Standard AI methods, which test all possible moves and positions using a search tree, can\u2019t handle the sheer number of possible Go moves or evaluate the strength of each possible board position.\nTwo players, using either white or black stones, take turns placing their stones on a board. The goal is to surround and capture their opponent's stones or strategically create spaces of territory. Once all possible moves have been played, both the stones on the board and the empty points are tallied. The highest number wins. \nAs simple as the rules may seem, Go is profoundly complex. There are an astonishing 10 to the power of 170 possible board configurations - more than the number of atoms in the known universe. This makes the game of Go a googol times more complex than chess.\nWe created AlphaGo, a computer program that combines advanced search tree with deep neural networks. These neural networks take a description of the Go board as an input and process it through a number of different network layers containing millions of neuron-like connections. \nOne neural network, the \u201cpolicy network\u201d, selects the next move to play. The other neural network, the \u201cvalue network\u201d, predicts the winner of the game. We introduced AlphaGo to numerous amateur games to help it develop an understanding of reasonable human play. Then we had it play against different versions of itself thousands of times, each time learning from its mistakes. \nOver time, AlphaGo improved and became increasingly stronger and better at learning and decision-making. This process is known as reinforcement learning. AlphaGo went on to defeat Go world champions in different global arenas and arguably became the greatest Go player of all time.\nAlphaGo then competed against legendary Go player Mr Lee Sedol, the winner of 18 world titles, who is widely considered the greatest player of the past decade. AlphaGo's 4-1 victory in Seoul, South Korea, on March 2016 was watched by over 200 million people worldwide. This landmark achievement was a decade ahead of its time. \n\u200d\nThe game earned AlphaGo a 9 dan professional ranking, the highest certification. This was the first time a computer Go player had ever received the accolade. During the games, AlphaGo played several inventive winning moves, several of which - including move 37 in game two - were so surprising that they upended hundreds of years of wisdom. Players of all levels have extensively examined these moves ever since. \n\u200d\nIn January 2017, we revealed an improved, online version of AlphaGo called Master. This online player achieved 60 straight wins in time-control games against top international players. \n\u200d\nFour months later, AlphaGo took part in the Future of Go Summit in China, the birthplace of Go. The five-day festival created an opportunity to explore the mysteries of Go in a spirit of mutual collaboration with the country\u2019s top players. Designed to help unearth even more strategic moves, the summit included various game formats such as pair Go, team Go, and a match with the world\u2019s number one player Ke Jie.\nThis powerful technique is no longer constrained by the limits of human knowledge. Instead, the computer program accumulated thousands of years of human knowledge during a period of just a few days and learned to play Go from the strongest player in the world, AlphaGo. \nAlphaGo Zero quickly surpassed the performance of all previous versions and also discovered new knowledge, developing unconventional strategies and creative new moves, including those which beat the World Go Champions Lee Sedol and Ke Jie. These creative moments give us confidence that AI can be used as a positive multiplier for human ingenuity.\nAlphaZero replaces hand-crafted heuristics with a deep neural network and algorithms that are given nothing beyond the basic rules of the game. By teaching itself, AlphaZero developed its own unique and creative style of play in all three games.\n In its chess games, for example, players saw it had developed a highly dynamic and \u201c\nunconventional\n\u201d style of play that differed from any previous chess playing engine. Many of its \n\u201cgame changing\u201d ideas\n have since been taken up at the highest level of play. \nIt does this by learning a model of its environment and combining it with AlphaZero\u2019s powerful lookahead tree search. This allows it to plan winning strategies in unknown domains, a significant leap forward in the capabilities of reinforcement learning algorithms and an important step towards our mission of building general-purpose learning systems. \nWhile it is still early days, the ideas behind MuZero's powerful learning and planning algorithms may pave the way towards tackling new problems in messy real-world environments where the \u201crules of the game\u201d are unknown.\n"}
{"title": "DeepMind Blog", "contents": ""}
{"title": "AlphaFold", "contents": "These \nexquisite, intricate machines\n are \nproteins\n. They underpin not just the biological processes in your body but every biological process in every living thing. They\u2019re the building blocks of life.\nCurrently, there are over \n200 million known proteins\n, with many more found every year. Each one has a unique 3D shape that determines how it works and what it does.\nBut figuring out the exact structure of a protein remains an expensive and often time-consuming process \u2013 and until now \u2013 scientists have only been able to study the \nexact 3D structure\n of a tiny fraction of the proteins known to science.\nFinding ways to close this rapidly expanding gap and predict the structure of millions of unknown proteins can not only help us tackle disease, and more quickly find new medicines, but perhaps, also unlock the mysteries of how life itself works.\nThese sequences are assembled according to the genetic instructions of an organism's \nDNA\n.\nAttraction and repulsion between the 20 different types of \namino acids\n cause the string to fold in a feat of \u2018spontaneous \norigami\n\u2019, forming the intricate curls, loops, and pleats of a protein\u2019s 3D structure.\nFor \ndecades, scientists\n have been trying to find a method to reliably determine a protein\u2019s structure just from its sequence of amino acids.\nThis grand scientific challenge is known as the \nprotein-folding\n problem.\nIt was taught by showing it the sequences and structures of around \n100,000 known proteins\n.\nExperimental techniques for determining structures have been painstakingly laborious and time consuming (sometimes taking years and millions of dollars).\nOur \nlatest system\n can now predict the shape of a protein, at scale and in minutes, down to atomic accuracy.\nThis is a significant breakthrough and highlights the impact AI can have on science.\nCASP\n is a community forum that allows researchers to share progress on the protein-folding problem. The community also organises a biennial challenge for research groups to test the accuracy of their predictions against real experimental data.\nTeams are given a selection of amino acid sequences for proteins which have had their exact 3D shape mapped but have not yet been released into the public domain. Groups must submit their best predictions to see how close they are to the subsequently revealed structures.\u00a0\u00a0\nAmong the teams that participated in \nCASP13\n (2018), AlphaFold \nplaced first in the protein structure prediction challenge\n. At \nCASP14\n (2020), we presented our latest version of \nAlphaFold\n, which has now reached a level of accuracy considered to solve the protein structure prediction problem.\nOur work builds upon decades of research by CASP\u2019s organisers and the protein-folding community, and we\u2019re indebted to the countless number of people who have contributed protein structures over the years, making such rigorous evaluations possible.\nThe \nAlphaFold Protein Structure Database\n, created in partnership with Europe\u2019s flagship laboratory for life sciences (\nEMBL\u2019s European Bioinformatics Institute\n), builds upon decades of painstaking work done by scientists, using traditional methods to determine the structure of proteins.\nOur \nfirst release\n, on 22 July, 2021, covers over 350,000 structures, including the human proteome \u2013 all of the ~20,000 known proteins expressed in the human body \u2013 along with the proteomes of 20 additional organisms important for biological research, including yeast, the fruit fly, and the mouse.\nThese organisms are central to modern biological research, including Nobel Prize winning discoveries and life-saving drug development.\nThis release dramatically expanded our knowledge of protein structures and more than doubled the number of high-accuracy human protein structures available to scientists around the world.\nOur latest release, announced on 28 July, 2022, expands this database from nearly 1 million structures to over 200 million structures \u2013 including nearly all catalogued proteins known to science.\nRead \nthe blog\n about our latest release.\nOur partners are already using AlphaFold to accelerate progress on important real-world problems.\nFor instance, the Drugs for Neglected Diseases initiative (DNDi) is advancing drug discovery for neglected diseases, such as Chagas disease and leishmaniasis, which impact millions within poor and vulnerable communities.\u00a0\nMeanwhile, a scientist at the life and material science company, Schr\u00f6dinger, is searching for ways to improve medicine by creating selective drugs, which can focus on one target rather than many.\nAt the Centre for Enzyme Innovation (CEI), researchers are discovering and engineering enzymes for breaking down single-use plastics, while teams from universities across Norway and the USA mapped the structure of honey bee Vitellogenin (Vg), a central protein for understanding the immune systems of egg-laying animals.\u00a0\nLooking at how changes in our DNA result in changes in our traits, a professor at ETH Zurich is studying the evolution of proteins. And at the University of Colorado, Boulder, another team is studying antibiotic resistance, a problem which causes 2.8M infections in the US alone each year.\n"}
{"title": "Predicting eye disease with Moorfields Eye Hospital", "contents": "In August, we \nannounced the first stage\n of our \njoint research partnership with Moorfields Eye Hospital\n, which showed how AI could match world-leading doctors at recommending the correct course of treatment for over 50 eye diseases, and also explain how it arrives at its recommendations.\nNow we\u2019re excited to start working on the next research challenge \u2013 whether we can help clinicians predict eye diseases before symptoms set in.\nThere are two types of age-related macular degeneration (AMD), one of the most common blinding eye diseases, with 170 million sufferers worldwide. The \u2018dry\u2019 form is relatively common among those over 65, and often only causes mild sight loss. However, about 15% of patients with dry AMD go on to develop the more serious form of the disease \u2013 \u2018wet\u2019 AMD \u2013 which can cause permanent, blinding sight loss.\nCurrently, ophthalmologists diagnose wet AMD by analysing highly detailed 3D scans of the back of the eye, called OCT scans. The first phase of our research suggested that our AI technology could help clinicians analyse these scans more quickly to detect the symptoms of patients who need urgent treatment \u2013 ultimately saving their sight.\nThat could help patients who already display symptoms: but what about those who haven\u2019t yet developed them? If AI could help predict severe eye diseases in advance, that could help clinicians prevent sight loss before it even occurs.\nCollaborating with the clinicians at Moorfields Eye Hospital, we\u2019ll be analysing de-identified scans of up to 7,000 patients at Moorfields who had previously received treatment for wet AMD in one eye, to try to predict deterioration in the other, seemingly healthy eye.\nPredicting potential indicators of disease is a much more complicated \u2013 and computationally intense \u2013 task than identifying existing known symptoms. To carry out this research reliably, efficiently and at scale, we have agreed with Moorfields to use Google\u2019s world-class cloud computing infrastructure, which is already being used in our partnership with Cancer Research UK on research to improve the diagnosis of breast cancer. Moorfields and DeepMind researchers will initially access this in the UK and US, but it may one day include cloud facilities around the world.\nThe benefits of using the cloud have been endorsed by \nNHS Digital\n as well as other regulatory bodies. We believe that using this infrastructure \u2013 with its greater size and scalability, reliability and processing power \u2013 will help us achieve the best possible results, which could take us one step closer to improving patient care.\nAs with all of our work, we\u2019re committed to treating the data used in this research with the utmost care and respect. Data is encrypted and de-identified, accessible only to a limited number of researchers who are conducting this research. All access to data is automatically audited and logged, and granted only for officially approved research purposes, with Moorfields\u2019 permission.\nWe\u2019re thrilled to be working with our partners at Moorfields to take this research a step further. This is the first phase of a number of new and exciting research projects that we will be working on in the coming months. We will keep you updated as we make progress.\n"}
{"title": "Using AI to plan head and neck cancer treatments", "contents": "Early results\n from our partnership with \nthe Radiotherapy Department at University College London Hospitals NHS Foundation Trust\n suggest that we are well on our way to developing an artificial intelligence (AI) system that can analyse and segment medical scans of head and neck cancer to a similar standard as expert clinicians. This segmentation process is an essential but time-consuming step when planning radiotherapy treatment. \nThe findings\n also show that our system can complete this process in a fraction of the time.\nMore than half a million people are diagnosed each year with cancers of the head and neck worldwide. Radiotherapy is a key part of treatment, but clinical staff have to plan meticulously so that healthy tissue doesn\u2019t get damaged by radiation: a process which involves radiographers, oncologists and/or dosimetrists manually outlining the areas of anatomy that need radiotherapy, and those areas that should be avoided.\nAlthough our work is still at an early stage, we hope it could one day reduce the waiting time between diagnosis and treatment, which could potentially improve outcomes for cancer patients. We also hope that accurate auto-segmentation could speed up the \nadaptive radiotherapy process\n, whereby radiotherapy treatments are adapted as the tumour shrinks - although more work is needed to investigate how this would work in practice.\nAs well as changing patients\u2019 lives, this research could also free up time for the clinicians who treat them, meaning they get to spend more time on patient care, education and research.\nWe\u2019ve taken steps to ensure our work is clinically applicable. This includes the development of a \nnew performance metric\n used to assess model performance that we believe is more representative of clinical processes, and a test set with new \nhigh-quality segmentations\n of scans selected from sites previously unseen to the model which demonstrates generalisability. Both of these have been open sourced to the research community. But for our system to have an impact on real people diagnosed with cancer, we need to expand it and demonstrate that it works in real clinical environments.\nThat\u2019s why we\u2019re looking forward to moving into the next phase of work with UCLH, where we will be exploring a human evaluation of these AI algorithms to test how they might perform in a clinical environment.\nAt DeepMind Health, we think it is important to share our work with others in the community. As such, Professor Olaf Ronneberger, Senior Research Scientist at DeepMind Health, will be presenting these initial findings at \nMICCAI\n, the world leading conference on medical imaging, this Sunday.\nUltimately, we believe that advanced technologies can and should help change lives, and we\u2019re excited for the next steps of this project. We\u2019ll continue to keep you updated as we make progress.\n"}
{"title": "Expanding our research on breast cancer screening to Japan", "contents": "Six months ago, we joined a groundbreaking new research partnership led by the Cancer Research UK Imperial Centre at Imperial College London to explore whether AI technology could help clinicians diagnose breast cancers on mammograms quicker and more effectively.\nBreast cancer is a huge global health problem. Around the world, over 1.6 million people are diagnosed with the disease every single year, and 500,000 lose their life to it \u2013 partly because accurately detecting and diagnosing breast cancer still remains a huge challenge.\nWorking alongside leading breast cancer experts, clinicians and academics in the UK, we\u2019ve been exploring whether machine learning (a form of AI) could help address this issue.\nToday, we\u2019re delighted to announce that this project is expanding internationally, with The Jikei University Hospital, one of Japan\u2019s foremost medical institutions, \njoining the collaboration\n as part of a wider five year partnership they have signed with DeepMind Health.\nFor the purposes of this research, they will be working with us to analyse historic, de-identified mammograms from around 30,000 women taken at the hospital between 2007 and 2018. These will be analysed with AI technology alongside the historic de-identified mammograms already provided by the UK OPTIMAM mammography database in order to investigate whether the technology can spot signs of cancerous tissue on these X-rays more effectively than current screening techniques allow. De-identified breast ultrasounds from around 30,000 women and 3,500 de-identified breast MRI scans will also be shared by The Jikei University Hospital during the course of the project.\nWorking with partners and data from multiple countries in a single project is a first for DeepMind Health. We hope that doing so will help us work towards our ambition to create technology that works for everyone around the world, because it will help us minimise bias.\nBias can occur when you train an AI system on data which doesn\u2019t accurately reflect the people it is being designed for, and it\u2019s a serious problem. By under-representing or even excluding certain groups from a dataset \u2013 be it by age, ethnicity, or gender \u2013 you create technology which doesn\u2019t best meet their needs.\nIn health, where genetic and biological differences between certain groups are commonplace, this could have a huge impact on patient care. In the field of mammography, for example, there can be considerable variations in breast density between ethnic groups. Bias in our AI system could therefore result in breast cancers being misidentified or even missed altogether if the technology is not set up to reflect these differences.\nTraining our algorithm on representative datasets from the UK and Japan is one of a number of ways we\u2019re looking to overcome this. In time, we hope to extend this research further to other international partners too.\nAs with all of our research, DeepMind is committed to treating the data from The Jikei University Hospital with the utmost care and respect. As is standard practice, the data being used in the research remains in the full control of our partners, and is being stored to world-class standards of security and encryption. Additionally, all medical information will be de-identified before it is transferred, with any information that could identify an individual being removed before researchers can conduct their analysis. More details on the de-identification process that has been applied to this data can be found \nhere\n.\nProfessor Ara Darzi, Director of the Cancer Research UK Imperial Centre, said: \u201cThe involvement of The Jikei University Hospital in a global research partnership will help take us one step closer to developing technology that could ultimately transform care for the millions of people who develop breast cancer around the world every year.\u201d\nIt\u2019s early days for this work, but we\u2019re optimistic about the long-term potential for AI technology in this area and hope, in time, to explore how it could help in the analysis of other forms of breast imaging. It\u2019s a hugely exciting opportunity to make a difference to breast cancer treatment across the world, and we\u2019ll keep you updated as we continue on this journey.\n\u534a\u5e74\u524d\u3001DeepMind \u306f\u3001\u30a4\u30f3\u30da\u30ea\u30a2\u30eb\u30fb\u30ab\u30ec\u30c3\u30b8\u30fb\u30ed\u30f3\u30c9\u30f3\u306e Cancer Research UK Imperial Centre \u304c\u4e3b\u5c0e\u3059\u308b\u9769\u65b0\u7684\u306a\u30ea\u30b5\u30fc\u30c1\u30d1\u30fc\u30c8\u30ca\u30fc\u30b7\u30c3\u30d7\u3078\u306e\u53c2\u753b\u3092\u767a\u8868\u3057\u307e\u3057\u305f\u3002\u672c\u7814\u7a76\u306f\u3001\u81e8\u5e8a\u533b\u306b\u3088\u308b\u30de\u30f3\u30e2\u30b0\u30e9\u30d5\u30a3\u3092\u7528\u3044\u305f\u4e73\u304c\u3093\u30b9\u30af\u30ea\u30fc\u30cb\u30f3\u30b0\u306b\u304a\u3044\u3066\u3001AI \u6280\u8853\u304c\u30d7\u30ed\u30bb\u30b9\u306e\u8fc5\u901f\u5316\u3084\u52b9\u7387\u5316\u306b\u3044\u304b\u306b\u8ca2\u732e\u3067\u304d\u308b\u304b\u3092\u63a2\u308b\u3053\u3068\u3092\u76ee\u7684\u3068\u3057\u3066\u3044\u307e\u3059\u3002\n\u4e73\u304c\u3093\u306f\u4e16\u754c\u898f\u6a21\u306e\u91cd\u8981\u306a\u5065\u5eb7\u554f\u984c\u3067\u3059\u3002\u305d\u306e\u6b63\u78ba\u306a\u691c\u51fa\u3068\u8a3a\u65ad\u306b\u306f\u672a\u3060\u5927\u304d\u306a\u8ab2\u984c\u304c\u6b8b\u3063\u3066\u304a\u308a\u3001\u4e16\u754c\u3067\u306f\u6bce\u5e74 160 \u4e07\u4eba\u3092\u8d85\u3048\u308b\u4eba\u3005\u304c\u4e73\u304c\u3093\u3068\u8a3a\u65ad\u3055\u308c\u300150 \u4e07\u4eba\u304c\u547d\u3092\u843d\u3068\u3057\u3066\u3044\u307e\u3059\u3002\n\u79c1\u305f\u3061\u306f\u3001\u82f1\u56fd\u306b\u304a\u3051\u308b\u4e73\u304c\u3093\u306e\u512a\u308c\u305f\u5c02\u9580\u533b\u3084\u81e8\u5e8a\u533b\u3001\u7814\u7a76\u8005\u3068\u5354\u529b\u3057\u3001\u3053\u306e\u8ab2\u984c\u89e3\u6c7a\u306b\u5bfe\u3057\u3066\u3001\u6a5f\u68b0\u5b66\u7fd2\uff08AI \u306e\u4e00\u5f62\u5f0f\uff09\u306f\u3069\u306e\u3088\u3046\u306b\u8ca2\u732e\u3067\u304d\u308b\u306e\u304b\u3092\u63a2\u308b\u7814\u7a76\u3092\u884c\u3063\u3066\u3044\u307e\u3059\u3002\n\u672c\u65e5\u3001\u65e5\u672c\u306e\u5148\u9032\u7684\u306a\u533b\u7642\u6a5f\u95a2\u306e\u4e00\u3064\u3067\u3042\u308b\u6771\u4eac\u6148\u6075\u4f1a\u533b\u79d1\u5927\u5b66\u9644\u5c5e\u75c5\u9662\u304c\u3001\u672c\u7814\u7a76\u3078\u306e\u53c2\u52a0\u3092\u767a\u8868\u3055\u308c\u307e\u3057\u305f\u3002\u3053\u308c\u306f\u3001DeepMind Health \u3068\u6771\u4eac\u6148\u6075\u4f1a\u533b\u79d1\u5927\u5b66\u9644\u5c5e\u75c5\u9662\u306e 5 \u5e74\u9593\u306e\u30d1\u30fc\u30c8\u30ca\u30fc\u30b7\u30c3\u30d7\u5951\u7d04\u306b\u57fa\u3065\u304f\u3082\u306e\u3067\u3001\u540c\u75c5\u9662\u306e\u53c2\u753b\u306b\u3088\u308a\u672c\u7814\u7a76\u306e\u56fd\u969b\u5316\u304c\u9032\u3080\u3053\u3068\u3092\u559c\u3070\u3057\u304f\u601d\u3063\u3066\u3044\u307e\u3059\u3002\n\u672c\u7814\u7a76\u3067\u306f\u3001\u6771\u4eac\u6148\u6075\u4f1a\u533b\u79d1\u5927\u5b66\u9644\u5c5e\u75c5\u9662\u3068 DeepMind \u304c\u5171\u540c\u3067\u30012007 \u5e74\uff5e 2018 \u5e74\u306b\u540c\u75c5\u9662\u3067\u3001\u904e\u53bb\u306b\u64ae\u5f71\u3055\u308c\u3001\u304b\u3064\u533f\u540d\u52a0\u5de5\u3055\u308c\u305f \u7d04 30,000 \u4eba\u306e\u5973\u6027\u306e\u30de\u30f3\u30e2\u30b0\u30e9\u30d5\u30a3\u306e\u5206\u6790\u3092\u884c\u3044\u307e\u3059\u3002\u3053\u308c\u3089\u306e\u30c7\u30fc\u30bf\u306f\u3001UK OPTIMAM \uff08\u82f1\u56fd\uff09\u304c\u4fdd\u6709\u3059\u308b\u30de\u30f3\u30e2\u30b0\u30e9\u30d5\u30a3\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u306b\u4fdd\u7ba1\u3055\u308c\u3066\u3044\u308b\u904e\u53bb\u306b\u64ae\u5f71\u3055\u308c\u3001\u533f\u540d\u52a0\u5de5\u3055\u308c\u305f\u30de\u30f3\u30e2\u30b0\u30e9\u30d5\u30a3\u306e\u30c7\u30fc\u30bf\u3068\u5408\u308f\u305b\u3066 AI \u6280\u8853\u306b\u3088\u308b\u89e3\u6790\u3092\u884c\u3044\u3001AI \u6280\u8853\u304c\u73fe\u5728\u306e\u30b9\u30af\u30ea\u30fc\u30cb\u30f3\u30b0\u6280\u8853\u3088\u308a\u3082\u52b9\u679c\u7684\u306b \u00a0X \u7dda\u753b\u50cf\u4e0a\u3067\u304c\u3093\u6027\u7d44\u7e54\u306e\u5146\u5019\u3092\u691c\u51fa\u3067\u304d\u308b\u304b\u691c\u8a0e\u3057\u307e\u3059\u3002\u307e\u305f\u3001\u672c\u7814\u7a76\u306e\u904e\u7a0b\u3067\u306f\u3001\u6771\u4eac\u6148\u6075\u4f1a\u533b\u79d1\u5927\u5b66\u9644\u5c5e\u75c5\u9662\u3088\u308a\u3001\u7d04 3 \u4e07\u4eba\u306e\u5973\u6027\u306e\u533f\u540d\u52a0\u5de5\u3055\u308c\u305f\u4e73\u623f\u8d85\u97f3\u6ce2\u691c\u67fb\u753b\u50cf\u304a\u3088\u3073 3,500 \u306e\u533f\u540d\u52a0\u5de5\u3055\u308c\u305f\u4e73\u623f MRI \u30b9\u30ad\u30e3\u30f3\u753b\u50cf\u306e\u5171\u6709\u3092\u4e88\u5b9a\u3057\u3066\u3044\u307e\u3059\u3002\nDeepMind \u306f\u300c\u4e16\u754c\u4e2d\u306e\u4eba\u3005\u306b\u3068\u3063\u3066\u5f79\u7acb\u3064\u30c6\u30af\u30ce\u30ed\u30b8\u30fc\u3092\u5275\u9020\u3059\u308b\u300d\u3053\u3068\u3092\u305d\u306e\u30df\u30c3\u30b7\u30e7\u30f3\u306b\u63b2\u3052\u3066\u3044\u307e\u3059\u3002DeepMind Health \u306b\u3068\u3063\u3066\u3001\u30d1\u30fc\u30c8\u30ca\u30fc\u3084\u30c7\u30fc\u30bf\u304c\u8907\u6570\u56fd\u306b\u307e\u305f\u304c\u308b\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306f\u521d\u306e\u8a66\u307f\u3067\u3059\u304c\u3001\u307e\u3055\u306b\u3053\u306e\u3088\u3046\u306a\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u304c\u30d0\u30a4\u30a2\u30b9\uff08\u504f\u308a\uff09\u306e\u6975\u5c0f\u5316\u3092\u52a9\u3051\u3001\u76ee\u6a19\u306e\u5b9f\u73fe\u3078\u3068\u524d\u9032\u3055\u305b\u308b\u3053\u3068\u306b\u7e4b\u304c\u308b\u3082\u306e\u3068\u8003\u3048\u3066\u3044\u307e\u3059\u3002\n\u300c\u30d0\u30a4\u30a2\u30b9\u300d\u306f\u3001\u6b63\u3057\u304f\u5bfe\u8c61\u3092\u53cd\u6620\u3057\u306a\u3044\u30c7\u30fc\u30bf\uff08\u504f\u3063\u305f\u30c7\u30fc\u30bf\uff09\u3092\u7528\u3044\u3066 AI \u30b7\u30b9\u30c6\u30e0\u3092\u8a13\u7df4\u3057\u305f\u5834\u5408\u306b\u8d77\u304d\u308b\u6df1\u523b\u306a\u554f\u984c\u3067\u3059\u3002\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u304b\u3089\u5e74\u9f62\u3001\u4eba\u7a2e\uff08\u6c11\u65cf\uff09\u3001\u6027\u5225\u7b49\u306e\u7279\u5b9a\u306e\u30b0\u30eb\u30fc\u30d7\u3092\u904e\u5c0f\u8a55\u4fa1\u3057\u305f\u308a\u3001\u9664\u5916\u3057\u305f\u308a\u3059\u308b\u3068\u3001\u30cb\u30fc\u30ba\u306b\u5408\u308f\u306a\u3044\u6280\u8853\u3092\u751f\u307f\u51fa\u3059\u3053\u3068\u306b\u306a\u308a\u304b\u306d\u307e\u305b\u3093\u3002\n\u533b\u7642\u306e\u4e16\u754c\u306b\u304a\u3044\u3066\u3001\u7279\u5b9a\u30b0\u30eb\u30fc\u30d7\u9593\u306b\u907a\u4f1d\u7684\u304a\u3088\u3073\u751f\u7269\u5b66\u7684\u306a\u76f8\u9055\u304c\u3042\u308b\u3053\u3068\u306f\u5f53\u7136\u3067\u3001\u60a3\u8005\u306e\u30b1\u30a2\u306b\u3082\u5927\u304d\u306a\u5f71\u97ff\u3092\u4e0e\u3048\u307e\u3059\u3002\u30de\u30f3\u30e2\u30b0\u30e9\u30d5\u30a3\u306e\u5206\u91ce\u3092\u3068\u3063\u3066\u307f\u308b\u3068\u3001\u4eba\u7a2e\uff08\u6c11\u65cf\uff09\u306b\u3088\u3063\u3066\u4e73\u817a\u6fc3\u5ea6\u306b\u304b\u306a\u308a\u306e\u5909\u52d5\u304c\u3042\u308b\u3053\u3068\u304c\u308f\u304b\u3063\u3066\u3044\u307e\u3059\u3002\u3064\u307e\u308a\u3001\u3053\u308c\u3089\u306e\u9055\u3044\u304c AI \u30b7\u30b9\u30c6\u30e0\u306b\u53cd\u6620\u3055\u308c\u306a\u3051\u308c\u3070\u3001\u305d\u306e\u30b7\u30b9\u30c6\u30e0\u306f\u4e73\u304c\u3093\u3092\u6b63\u3057\u304f\u691c\u51fa\u3067\u304d\u306a\u304b\u3063\u305f\u308a\u3001\u898b\u9003\u3059\u3068\u3044\u3063\u305f\u7d50\u679c\u3092\u62db\u304f\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002\n\u4eca\u56de\u306e\u767a\u8868\u306e\u3088\u3046\u306b\u3001\u82f1\u56fd\u3068\u65e5\u672c\u306e\u4ee3\u8868\u7684\u306a\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u7528\u3044\u3066\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u8a13\u7df4\u3059\u308b\u65b9\u6cd5\u306f\u3001\u30d0\u30a4\u30a2\u30b9\u3092\u514b\u670d\u3059\u308b\u305f\u3081\u306e\u6709\u52b9\u306a\u624b\u6bb5\u306e\u4e00\u3064\u3067\u3042\u308b\u3068\u8003\u3048\u3066\u3044\u307e\u3059\u3002\u4eca\u5f8c\u3001\u672c\u7814\u7a76\u3092\u4ed6\u56fd\u306e\u30d1\u30fc\u30c8\u30ca\u30fc\u6a5f\u95a2\u306b\u3082\u62e1\u5927\u3057\u3066\u3044\u304d\u305f\u3044\u3068\u8003\u3048\u3066\u3044\u307e\u3059\u3002\nDeepMind \u306b\u304a\u3051\u308b\u5168\u3066\u306e\u7814\u7a76\u3068\u540c\u69d8\u306b\u3001\u79c1\u305f\u3061\u306f\u6771\u4eac\u6148\u6075\u4f1a\u533b\u79d1\u5927\u5b66\u9644\u5c5e\u75c5\u9662\u306e\u30c7\u30fc\u30bf\u3092\u7d30\u5fc3\u306e\u6ce8\u610f\u3068\u656c\u610f\u306e\u3082\u3068\u306b\u53d6\u6271\u3046\u3053\u3068\u3092\u304a\u7d04\u675f\u3057\u307e\u3059\u3002\u6a19\u6e96\u7684\u306a\u624b\u9806\u306b\u5f93\u3044\u3001\u7814\u7a76\u3067\u4f7f\u7528\u3055\u308c\u3066\u3044\u308b\u30c7\u30fc\u30bf\u306f\u3001\u5f15\u304d\u7d9a\u304d\u5b8c\u5168\u306b\u30d1\u30fc\u30c8\u30ca\u30fc\u6a5f\u95a2\u306e\u7ba1\u7406\u4e0b\u306b\u3042\u308a\u3001\u4e16\u754c\u6a19\u6e96\u306e\u30bb\u30ad\u30e5\u30ea\u30c6\u30a3\u3068\u6697\u53f7\u5316\u306b\u3088\u308a\u4fdd\u8b77\u3055\u308c\u307e\u3059\u3002\u307e\u305f\u3001\u3059\u3079\u3066\u306e\u533b\u7642\u60c5\u5831\u306f\u3001DeepMind \u3078\u306e\u63d0\u4f9b\u524d\u306b\u533f\u540d\u52a0\u5de5\u51e6\u7406\u304c\u884c\u308f\u308c\u3001\u500b\u4eba\u304c\u8b58\u5225\u3067\u304d\u308b\u60c5\u5831\u306f\u7814\u7a76\u8005\u304c\u5206\u6790\u3092\u884c\u3046\u524d\u306b\u524a\u9664\u3055\u308c\u307e\u3059\u3002\u3053\u306e\u30c7\u30fc\u30bf\u306b\u9069\u7528\u3055\u308c\u308b\u533f\u540d\u52a0\u5de5\u306e\u30d7\u30ed\u30bb\u30b9\u306b\u3064\u3044\u3066\u3001\u8a73\u7d30\u306f \nPIPA\n\u3092\u3054\u53c2\u7167\u304f\u3060\u3055\u3044\u3002\nCancer Research UK Imperial Centre \u30c7\u30a3\u30ec\u30af\u30bf\u30fc\u3067\u3042\u308b Ara Darzi \u6559\u6388\u306f\u3001\u6b21\u306e\u3088\u3046\u306b\u8ff0\u3079\u3066\u3044\u307e\u3059\u3002\u300c\u30b0\u30ed\u30fc\u30d0\u30eb\u306a\u30ea\u30b5\u30fc\u30c1\u30d1\u30fc\u30c8\u30ca\u30fc\u30b7\u30c3\u30d7\u3078\u306e\u6771\u4eac\u6148\u6075\u4f1a\u533b\u79d1\u5927\u5b66\u9644\u5c5e\u75c5\u9662\u306e\u53c2\u753b\u306f\u3001\u6bce\u5e74\u4e16\u754c\u4e2d\u3067\u4e73\u304c\u3093\u3092\u767a\u75c7\u3059\u308b\u6570\u767e\u4e07\u4eba\u306e\u4eba\u3005\u306e\u6cbb\u7642\u3092\u5909\u9769\u3059\u308b\u304b\u3082\u3057\u308c\u306a\u3044\u6280\u8853\u306e\u958b\u767a\u306b\u4e00\u6b69\u8fd1\u3065\u304f\u3053\u3068\u3092\u610f\u5473\u3057\u3066\u3044\u307e\u3059\u3002\n\u672c\u53d6\u7d44\u307f\u306f\u7dd2\u306b\u5c31\u3044\u305f\u3070\u304b\u308a\u3067\u3059\u304c\u3001\u3053\u306e\u5206\u91ce\u306b\u304a\u3051\u308b AI \u30c6\u30af\u30ce\u30ed\u30b8\u30fc\u306e\u9577\u671f\u7684\u306a\u53ef\u80fd\u6027\u306b\u3064\u3044\u3066\u3001\u79c1\u305f\u3061\u306f\u697d\u89b3\u7684\u3067\u3042\u308a\u3001\u5c06\u6765\u7684\u306b\u306f\u4ed6\u306e\u30bf\u30a4\u30d7\u306e\u4e73\u623f\u753b\u50cf\u8a3a\u65ad\u306b\u304a\u3044\u3066\u3082\u5f79\u7acb\u3064\u65b9\u6cd5\u3092\u63a2\u6c42\u3067\u304d\u308b\u53ef\u80fd\u6027\u306b\u671f\u5f85\u3092\u6301\u3063\u3066\u3044\u307e\u3059\u3002\u3053\u308c\u306f\u4e16\u754c\u306e\u4e73\u304c\u3093\u6cbb\u7642\u3092\u5909\u3048\u308b\u304b\u3082\u3057\u308c\u306a\u3044\u6311\u6226\u3067\u3059\u3002\u7814\u7a76\u306e\u9032\u6357\u306b\u3042\u308f\u305b\u3066\u3001\u7686\u69d8\u306b\u6700\u65b0\u306e\u60c5\u5831\u3092\u304a\u5c4a\u3051\u3057\u3066\u53c2\u308a\u307e\u3059\u3002\n"}
{"title": "Scaling Streams with Google", "contents": "We\u2019re excited to announce that the team behind \nStreams - \nour mobile app that supports doctors and nurses to deliver faster, better care to patients - will be joining Google.\nIt\u2019s been a phenomenal journey to see Streams go from initial idea to live deployment, and to hear how it\u2019s helped \nchange the lives of patients\n and the nurses and doctors who treat them. The arrival of world-leading health expert \nDr. David Feinberg at Google\n will accelerate these efforts, helping to make a difference to the lives of millions of patients around the world.\nThis is a major milestone for DeepMind! One of the reasons for joining forces with Google in 2014 was the opportunity to use Google\u2019s scale and experience in building billion-user products to bring our breakthroughs more rapidly to the wider world. It\u2019s been amazing to put this into practice in \ndata centre efficiency\n, \nAndroid battery life\n, \ntext-to-speech applications\n, and now the work of our Streams team.\nOver the past three years we\u2019ve built a team of experts in what it takes to deploy clinical tools in practice - engineers, clinicians, translational researchers and more. In that time, we\u2019ve also made major advances in healthcare AI research: \ndetecting eye disease more quickly\n and accurately than experts; \nplanning cancer radiotherapy treatment\n in seconds rather than hours; and working to \ndetect patient deterioration from electronic records\n.\nOur vision is for Streams to now become an AI-powered assistant for nurses and doctors everywhere - combining the best algorithms with intuitive design, all backed up by rigorous evidence. The team working within Google, alongside brilliant colleagues from across the organisation, will help make this vision a reality.\nThe Streams team will remain in London, under the leadership of former NHS surgeon and researcher Dr Dominic King. We\u2019re fully committed to all our NHS partners, and to delivering on our current projects and more. We\u2019ll be working closely with them as we plan for the team\u2019s transition, and information governance and safety remain our top priorities. Patient data remains under our partners\u2019 strict control, and all decisions about its use will continue to lie with them.\nAs a research organisation, DeepMind will continue to lead the way in fundamental research applying AI to important science and medical research questions, in collaboration with academic partners, to accelerate scientific progress for the benefit of everyone.\nOver the coming years, we expect AI to help scientists make transformative advances on problems ranging from protein folding to image analysis, potentially improving medical diagnosis, drug discovery and much more. We\u2019re excited to play our part in that journey both at DeepMind and at Google, in the service of patients and clinicians around the world.\nUpdate: On September 18 2019, we confirmed DeepMind Health had joined the Google Health family. Find out more \nhere\n.\n"}
{"title": "Open sourcing TRFL: a library of reinforcement learning building blocks", "contents": "Today we are open sourcing a \nnew library\n of useful building blocks for writing reinforcement learning (RL) agents in TensorFlow. Named TRFL (pronounced \u2018truffle\u2019), it represents a collection of key algorithmic components that we have used internally for a large number of our most successful agents such as DQN, DDPG and the Importance Weighted Actor Learner Architecture.\nA typical deep reinforcement learning agent consists of a large number of interacting components: at the very least, these include the environment and some deep network representing values or policies, but they often also include components such as a learned model of the environment, pseudo-reward functions or a replay system.\nThese parts tend to interact in subtle ways (often not well-documented in papers, as highlighted by \nHenderson and colleagues\n), thus making it difficult to identify bugs in such large computational graphs. A recent \nblog post by OpenAI\n highlighted this issue by analysing some of the most popular open-source implementations of reinforcement learning agents and finding that six out of 10 \u201chad subtle bugs found by a community member and confirmed by the author\u201d.\nOne approach to addressing this issue, and helping those in the research community attempting to reproduce results from papers, is through open-sourcing complete agent implementations. For example, this is what we did recently with our \nscalable distributed implementation of the v-trace agent\n. These large agent codebases can be very useful for reproducing research, but also hard to modify and extend. A different and complementary approach is to provide reliable, well-tested implementations of common building blocks, that can be used in a variety of different RL agents. Moreover, having these core components abstracted away in a single library, with a consistent API, makes it simpler to combine ideas originating from various different publications.\nThe TRFL library includes functions to implement both classical RL algorithms as well as more cutting-edge techniques. \u00a0The loss functions and other operations provided here are implemented in pure TensorFlow. They are not complete algorithms, but implementations of RL-specific mathematical operations needed when building fully-functional RL agents.\nFor value-based reinforcement learning we provide TensorFlow ops for learning in discrete action spaces, such as TD-learning, Sarsa, Q-learning and their variants, as well as ops for implementing continuous control algorithms, such as DPG. We also include ops for learning distributional value functions. These ops support batches, and return a loss that can be minimised by feeding it to a TensorFlow Optimiser. Some losses operate over batches of transitions (e.g. Sarsa, Q learning, ...), and others over batches of trajectories (e.g. Q lambda, Retrace, \u2026). For policy-based methods, we have utilities to easily implement both online methods such as A2C, as well as supporting off-policy correction techniques, such as v-trace. The computation of policy gradients in continuous action spaces is also supported. \u00a0Finally, TRFL also provides an implementation of the auxiliary pseudo-reward functions used by UNREAL, which we have found to improve data efficiency in a variety of domains.\nThis is not a one-time release. Since this library is used extensively within DeepMind, we will continue to maintain it as well as add new functionalities over time. We are also eager to \nreceive contributions\n to the library by the wider RL community.\nThis library was created by the Research Engineering team at DeepMind.\n"}
{"title": "AlphaFold: Using AI for scientific discovery", "contents": "In July 2022, we released AlphaFold protein structure predictions for nearly all catalogued proteins known to science. Read the latest blog \nhere\n.\nWe\u2019re excited to share DeepMind\u2019s first significant milestone in demonstrating how artificial intelligence research can drive and accelerate new scientific discoveries. With a strongly interdisciplinary approach to our work, DeepMind has brought together experts from the fields of structural biology, physics, and machine learning to apply cutting-edge techniques to predict the 3D structure of a protein based solely on its genetic sequence.\nOur system, \nAlphaFold\n, which we have been working on for the past two years, builds on years of prior research in using vast genomic data to predict protein structure. The 3D models of proteins that AlphaFold generates are far more accurate than any that have come before\u2014making significant progress on one of the core challenges in biology.\nProteins are large, complex molecules essential in sustaining life. Nearly every function our body performs\u2014contracting muscles, sensing light, or turning food into energy\u2014can be traced back to one or more proteins and how they move and change. The recipes for those proteins\u2014called genes\u2014are encoded in our DNA.\nWhat any given protein can do depends on its unique 3D structure. For example, antibody proteins that make up our immune systems are \u2018Y-shaped\u2019, and are akin to unique hooks. By latching on to viruses and bacteria, antibody proteins are able to detect and tag disease-causing microorganisms for extermination. Similarly, collagen proteins are shaped like cords, which transmit tension between cartilage, ligaments, bones, and skin. Other types of proteins include Cas9, which, using CRISPR sequences as a guide, act like scissors to cut and paste sections of DNA; antifreeze proteins, whose 3D structure allows them to bind to ice crystals and prevent organisms from freezing; and ribosomes that act like a programmed assembly line, which help build proteins themselves.\nBut figuring out the 3D shape of a protein purely from its genetic sequence is a complex task that scientists have found challenging for decades. The challenge is that DNA only contains information about the sequence of a protein\u2019s building blocks called amino acid residues, which form long chains. Predicting how those chains will fold into the intricate 3D structure of a protein is what\u2019s known as the \u201cprotein-folding problem\u201d.\nThe bigger the protein, the more complicated and difficult it is to model because there are more interactions between amino acids to take into account. As noted in \nLevinthal\u2019s paradox\n, it would take longer than the age of the universe to enumerate all the possible configurations of a typical protein before reaching the right 3D structure.\nThe ability to predict a protein\u2019s shape is useful to scientists because it is fundamental to understanding its role within the body, as well as diagnosing and treating diseases believed to be caused by misfolded proteins, such as \nAlzheimer\u2019s\n, \nParkinson\u2019s\n, \nHuntington\u2019s\n and \ncystic fibrosis\n.\nWe are especially excited about how it might improve our understanding of the body and how it works, enabling scientists to design new, effective cures for diseases more efficiently. As we acquire more knowledge about the shapes of proteins and how they operate through simulations and models, it opens up new potential within drug discovery while also reducing the costs associated with experimentation. That could ultimately improve the quality of life for millions of patients around the world.\nAn understanding of protein folding will also assist in protein design, which could unlock a tremendous number of benefits. For example, advances in biodegradable enzymes\u2014which can be enabled by protein design\u2014could help manage pollutants like plastic and oil, helping us break down waste in ways that are more friendly to our environment. In fact, researchers have already begun \nengineering bacteria \nto secrete proteins that will make waste biodegradable, and easier to process.\nTo catalyse research and measure progress on the newest methods for improving the accuracy of predictions, a global biennial competition called CASP (\nCritical Assessment of protein Structure Prediction\n) was established in 1994, and has become the gold standard for assessing techniques.\nOver the past five decades, scientists have been able to determine shapes of proteins in labs using experimental techniques like \ncryo-electron microscopy\n, \nnuclear magnetic resonance\n or \nX-ray crystallography\n, but each method depends on a lot of trial and error, which can take years and cost tens of thousands of dollars per structure. This is why biologists are turning to AI methods as an alternative to this long and laborious process for difficult proteins.\nFortunately, the field of genomics is quite rich in data thanks to the rapid reduction in the cost of genetic sequencing. As a result, deep learning \napproaches\n to the prediction problem that rely on genomic data have become increasingly popular in the last few years. DeepMind\u2019s work on this problem resulted in AlphaFold, which we submitted to CASP this year. We\u2019re proud to be part of what the CASP organisers have called \u201cunprecedented progress in the ability of computational methods to predict protein structure,\u201d placing \nfirst\n in rankings among the teams that entered (our entry is A7D).\nOur team focused specifically on the hard problem of modelling target shapes from scratch, without using previously solved proteins as templates. We achieved a high degree of accuracy when predicting the physical properties of a protein structure, and then used two distinct methods to construct predictions of full protein structures.\nBoth of these methods relied on deep neural networks that are trained to predict properties of the protein from its genetic sequence. The properties our networks predict are: (a) the distances between pairs of amino acids and (b) the angles between chemical bonds that connect those amino acids. The first development is an advance on commonly used techniques that estimate whether pairs of amino acids are near each other.\nWe trained a neural network to predict a separate distribution of distances between every pair of residues in a protein. These probabilities were then combined into a score that estimates how accurate a proposed protein structure is. We also trained a separate neural network that uses all distances in aggregate to estimate how close the proposed structure is to the right answer.\nUsing these scoring functions, we were able to search the protein landscape to find structures that matched our predictions. Our first method built on techniques commonly used in structural biology, and repeatedly replaced pieces of a protein structure with new protein fragments. We trained a generative neural network to invent new fragments, which were used to continually improve the score of the proposed protein structure. \u00a0 \u00a0 \nThe second method optimised scores through \ngradient descent\n\u2014a mathematical technique commonly used in machine learning for making small, incremental improvements\u2014which resulted in highly accurate structures. This technique was applied to entire protein chains rather than to pieces that must be folded separately before being assembled, reducing the complexity of the prediction process.\nThe success of our first foray into protein folding is indicative of how machine learning systems can integrate diverse sources of information to help scientists come up with creative solutions to complex problems at speed. Just as we\u2019ve seen how AI can help people master complex games through systems like \nAlphaGo\n and \nAlphaZero\n, we similarly hope that one day, AI breakthroughs will help us master fundamental scientific problems, too.\nIt\u2019s exciting to see these early signs of progress in protein folding, demonstrating the utility of AI for scientific discovery. Even though there\u2019s a lot more work to do before we\u2019re able to have a quantifiable impact on treating diseases, managing the environment, and more, we know the potential is enormous. With a dedicated team focused on delving into how machine learning can advance the world of science, we\u2019re looking forward to seeing the many ways our technology can make a difference.\nUntil we have published a paper on this work, please cite it as:\nDe novo structure prediction with deep-learning based scoring \nR.Evans, \u00a0J.Jumper, J.Kirkpatrick, L.Sifre, T.F.G.Green, C.Qin, A.Zidek, A.Nelson, A.Bridgland, H.Penedones, S.Petersen, K.Simonyan, S.Crossan, D.T.Jones, D.Silver, K.Kavukcuoglu, D.Hassabis, A.W.Senior\nIn Thirteenth Critical Assessment of Techniques for Protein Structure Prediction (Abstracts) 1-4 December 2018. Retrieved from here \nhere\n.\nThis work was done in collaboration with Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green, Chongli Qin, Augustin Zidek, Sandy Nelson, Alex Bridgland, Hugo Penedones, Stig Petersen, Karen Simonyan, Steve Crossan, David Jones, David Silver, Koray Kavukcuoglu, Demis Hassabis, and Andrew Senior.\n"}
{"title": "AlphaZero: Shedding new light on chess, shogi, and Go", "contents": "In late 2017 we \nintroduced AlphaZero\n, a single system that taught itself from scratch how to master the games of chess, \nshogi\n(Japanese chess), and \nGo\n, beating a world-champion program in each case. We were excited by the preliminary results and thrilled to see the response from members of the chess community, who saw in AlphaZero\u2019s games a ground-breaking, highly dynamic and \u201c\nunconventional\n\u201d style of play that differed from any chess playing engine that came before it.\nToday, we are delighted to introduce the full evaluation of AlphaZero, \npublished in the journal Science\n (\nOpen Access version here\n), that confirms and updates those preliminary results. It describes how AlphaZero quickly learns each game to become the strongest player in history for each, despite starting its training from random play, with no in-built domain knowledge but the basic rules of the game.\nThis ability to learn each game afresh, unconstrained by the norms of human play, results in a distinctive, unorthodox, yet creative and dynamic playing style. Chess Grandmaster Matthew Sadler and Women\u2019s International Master Natasha Regan, who have analysed thousands of AlphaZero\u2019s chess games for \ntheir forthcoming book Game Changer\n (New in Chess, January 2019), say its style is unlike any traditional chess engine.\u201d It\u2019s like discovering the secret notebooks of some great player from the past,\u201d says Matthew.\nTraditional chess engines \u00a0\u2013 including the world computer chess champion \nStockfish\n and \nIBM\u2019s ground-breaking Deep Blue\n \u2013 rely on thousands of rules and heuristics handcrafted by strong human players that try to account for every eventuality in a game. Shogi programs are also game specific, using similar search engines and algorithms to chess programs.\nAlphaZero takes a totally different approach, replacing these hand-crafted rules with a deep \nneural network\n and general purpose algorithms that know nothing about the game beyond the basic rules.\nTo learn each game, an untrained neural network plays millions of games against itself via a process of trial and error called \nreinforcement learning\n. At first, it plays completely randomly, but over time the system learns from wins, losses, and draws to adjust the parameters of the neural network, making it more likely to choose advantageous moves in the future. The amount of training the network needs depends on the style and complexity of the game, taking approximately 9 hours for chess, 12 hours for shogi, and 13 days for Go.\nThe trained network is used to guide a search algorithm \u2013 known as Monte-Carlo Tree Search (MCTS) \u2013 to select the most promising moves in games. For each move, AlphaZero searches only a small fraction of the positions considered by traditional chess engines. In Chess, for example, it searches only 60 thousand positions per second in chess, compared to roughly 60 million for Stockfish.\nThe fully trained systems were tested against the strongest hand-crafted engines for chess (\nStockfish\n) and shogi (\nElmo\n), along with our previous self-taught system \nAlphaGo Zero\n, the strongest Go player known.\nIn each evaluation, AlphaZero convincingly beat its opponent:\nHowever, it was the style in which AlphaZero plays these games that players may find most fascinating. In Chess, for example, AlphaZero independently discovered and played common human motifs during its self-play training such as openings, king safety and pawn structure. But, being self-taught and therefore unconstrained by conventional wisdom about the game, it also developed its own intuitions and strategies adding a new and expansive set of exciting and novel ideas that augment centuries of thinking about chess strategy.\nThe first thing that players will notice is AlphaZero's style, says Matthew Sadler \u2013 \u201cthe way its pieces swarm around the opponent\u2019s king with purpose and power\u201d. Underpinning that, he says, is AlphaZero\u2019s highly dynamic game play that maximises the activity and mobility of its own pieces while minimising the activity and mobility of its opponent\u2019s pieces. Counterintuitively, AlphaZero also seems to place less value on \u201cmaterial\u201d, an idea that underpins the modern game where each piece has a value and if one player has a greater value of pieces on the board than the other, then they have a material advantage. Instead, AlphaZero is willing to sacrifice material early in a game for gains that will only be recouped in the long-term.\n\u201cImpressively, it manages to impose its style of play across a very wide range of positions and openings,\u201d says Matthew, who also observes that it plays in a very deliberate style from its first move with a \u201cvery human sense of consistent purpose\u201d.\n\u201cTraditional engines are exceptionally strong and make few obvious mistakes, but can drift when faced with positions with no concrete and calculable solution,\u201d he says. \u201cIt's precisely in such positions where \u2018feeling\u2019, \u2018insight\u2019 or \u2018intuition\u2019 is required that AlphaZero comes into its own.\"\nThis unique ability, not seen in other traditional chess engines, has already been harnessed to give chess fans \nfresh insight and commentary\n on the recent\n World Chess Championship\n match between\n Magnus Carlsen\n and\n Fabiano Caruana\n and will be explored further in \nGame Changer\n. \u201cIt was fascinating to see how AlphaZero's analysis differed from that of top chess engines and even top grandmaster play,\u201d says Natasha Regan. \"AlphaZero could be a powerful teaching tool for the whole community.\"\nAlphaZero\u2019s teachings echo what we saw when \nAlphaGo\n played the legendary champion \nLee Sedol\n in 2016. During \nthe games\n, AlphaGo played a number of \nhighly inventive winning moves,\n including move 37 in game two, which overturned hundreds of years of thinking. These moves - and many others - have since been studied by players at all levels including Lee Sedol himself, who said of Move 37: \u201cI thought AlphaGo was based on probability calculation and it was merely a machine. But when I saw this move I changed my mind. Surely AlphaGo is creative.\u201d\nAs with Go, we are excited about AlphaZero\u2019s creative response to chess, which has been a grand challenge for artificial intelligence since the dawn of the computing age with early pioneers including Babbage, Turing, Shannon, and von Neumann all trying their hand at designing chess programs. But AlphaZero is about more than chess, shogi or Go. To create intelligent systems capable of solving a wide range of real-world problems we need them to be flexible and generalise to new situations. While there has been some progress towards this goal, it remains a major challenge in AI research with systems capable of mastering specific skills to a very high standard, but often failing when presented with even slightly modified tasks.\nAlphaZero\u2019s ability to master three different complex games \u2013 and potentially any perfect information game \u2013 is an important step towards overcoming this problem. It demonstrates that a single algorithm can learn how to discover new knowledge in a range of settings. And, while it is still early days, AlphaZero\u2019s creative insights coupled with the encouraging results we see in other projects such as \nAlphaFold\n, \u00a0give us confidence in \nour mission\n to create general purpose learning systems that will one day help us find novel solutions to some of the most important and complex scientific problems.\nThis work was done by David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, and Demis Hassabis.\n"}
{"title": "Preserving Outputs Precisely while Adaptively Rescaling Targets", "contents": "Multi-task learning - allowing a single agent to learn how to solve many different tasks - is a longstanding objective for artificial intelligence research. Recently, there has been a lot of excellent progress, with agents like \nDQN\n able to use the same algorithm to learn to play multiple games including Breakout and Pong. These algorithms were used to train individual expert agents for each task. As artificial intelligence research advances to more complex real world domains, building a single general agent - as opposed to multiple expert agents - to learn to perform multiple tasks will be crucial. However, so far, this has proven to be a significant challenge. \nOne reason is that there are often differences in the reward scales our reinforcement learning agents use to judge success, leading them to focus on tasks where the reward is arbitrarily higher. For example, in the Atari game Pong, the agent receives a reward of either -1, 0, or +1 per step. In contrast, an agent playing Ms. Pac-Man can obtain hundreds or thousands of points in a single step. Even if the size of individual rewards is comparable, the frequency of rewards can change over time as the agent gets better. This means agents tend to focus on those tasks which have large scores, leading to better performance on certain tasks, and far worse on others.\nTo resolve these kinds of issues, we developed \nPopArt\n, a technique that can adapt the scale of scores in each game so the agent judges the games to be of equal learning value, no matter the scale of rewards available in each specific game. We applied a PopArt normalisation to a state-of-the-art reinforcement learning agent, resulting in a single agent that can play a whole set of 57 diverse Atari video games, with above-human median performance across the set.\nBroadly speaking, deep learning relies on the weights of a neural network being updated so that its output moves closer to the desired target output. \u00a0This also applies when neural networks are used in the context of deep reinforcement learning. PopArt works by estimating the mean and the spread of these targets (such as the score in a game). It then uses these statistics to normalise the targets before they are used to update the network\u2019s weights. Using normalised targets makes learning more stable and robust to changes in scale and shift. To obtain accurate estimates - of expected future scores for example - the outputs of the network can then be rescaled back to the true target range by inverting the normalisation process. If done naively, each update to the statistics would change all unnormalised outputs, including those that were already very good. We prevent this from happening by updating the network in the opposite direction whenever we update the statistics, this can be done exactly. This means we get the benefit of well-scaled updates, while keeping the previously learnt outputs intact. It is for these reasons that we call our method PopArt: it works by Preserving Outputs Precisely while Adaptively Rescaling Targets.\nTraditionally, researchers have overcome the problem of varying reward scales by using reward clipping in their reinforcement learning algorithms. This clips big and small scores at 1 or -1, roughly normalising the expected rewards. Although this makes learning easier, it also changes the goal of the agent. For instance, in Ms. Pac-Man the goal is to collect pellets, each of which is worth 10 points each, and eat ghosts worth between 200 and 1600 points. With clipped rewards, there is no apparent difference for the agent between eating a pellet or eating a ghost and results \u00a0in agents that only eat pellets, and never bothers to chase ghosts, as \nthis video\n shows. \u00a0When we remove reward clipping and use PopArt\u2019s adaptive normalisation to stabilise learning, it results in quite different behaviour, with the agent chasing ghosts, and achieving a higher score, as shown in \nthis video\n.\nWe applied PopArt to the \nImportance-weighted Actor-Learner Architecture\n (IMPALA), one of the most popular deep reinforcement learning agents used at DeepMind. In our experiments, PopArt greatly improved the performance of the agent compared to the baseline agent without PopArt. Both with clipped and unclipped rewards, the median score of the PopArt agent across games was above the human median. This is much higher than the baseline with clipped rewards, while the baseline with unclipped rewards fails to reach meaningful performance at all because it cannot effectively deal with the large variation in reward scales across games.\nThis is the first time we\u2019ve seen superhuman performance on this kind of multi-task environment using a single agent, suggesting PopArt could provide some answers to the open research question of how to balance varied objectives without manually clipping or scaling them. Its ability to adapt the normalisation automatically while learning may become important as we apply AI to more complex multi-modal domains where an agent must learn to trade-off a number of different objectives with varying rewards.\nFor more details, please see our latest\n paper\n and the original \npaper\n. \nThis work was done by Matteo Hessel, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki, Simon Schmitt, and Hado van Hasselt. \n"}
{"title": "AlphaStar: Mastering the real-time strategy game StarCraft II", "contents": "Games have been used for decades as an important way to test and evaluate the performance of artificial intelligence systems. As capabilities have increased, the research community has sought games with increasing complexity that capture different elements of intelligence required to solve scientific and real-world problems. In recent years, StarCraft, considered to be one of the most challenging Real-Time Strategy (RTS) games and one of the longest-played esports of all time, has emerged by consensus as a \u201cgrand challenge\u201d for AI research.\nNow, we introduce our \nStarCraft II\n program AlphaStar, the first Artificial Intelligence to defeat a top professional player. In a series of test matches held on 19 December, AlphaStar decisively beat \nTeam Liquid\u2019s\n Grzegorz \"\nMaNa\n\" Komincz, \none of the world\u2019s strongest professional StarCraft players\n, 5-0, following a successful benchmark match against his team-mate Dario \u201c\nTLO\n\u201d W\u00fcnsch. The matches took place under professional match conditions on a competitive ladder \nmap\n and without any game restrictions.\nAlthough there have been significant successes in video games such as \nAtari\n, \nMario\n, \nQuake III Arena Capture the Flag\n, and \nDota 2\n, until now, AI techniques have struggled to cope with the complexity of StarCraft. The best \nresults\n were made possible by hand-crafting major elements of the system, imposing significant restrictions on the game rules, giving systems superhuman capabilities, or by playing on simplified maps. Even with these modifications, no system has come anywhere close to rivalling the skill of professional players. In contrast, AlphaStar plays the full game of StarCraft II, using a deep neural network that is trained directly from raw game data by \nsupervised learning \nand \nreinforcement learning\n.\nStarCraft II, created by \nBlizzard Entertainment\n, is set in a fictional sci-fi universe and features rich, multi-layered gameplay designed to challenge human intellect. Along with the original title, it is among the biggest and most successful games of all time, with players competing in esports tournaments for more than 20 years.\nThere are several different ways to play the game, but in esports the most common is a 1v1 tournament played over five games. To start, a player must choose to play one of three different alien \u201craces\u201d - Zerg, Protoss or Terran, all of which have distinctive characteristics and abilities (although professional players tend to specialise in one race). Each player starts with a number of worker units, which gather basic resources to build more units and structures and create new technologies. These in turn allow a player to harvest other resources, build more sophisticated bases and structures, and develop new capabilities that can be used to outwit the opponent. To win, a player must carefully balance big-picture management of their economy - known as macro - along with low-level control of their individual units - known as micro.\nThe need to balance short and long-term goals and adapt to unexpected situations, poses a huge challenge for systems that have often tended to be brittle and inflexible. Mastering this problem requires breakthroughs in several AI research challenges including:\nDue to these immense challenges, StarCraft has emerged as a \u201cgrand challenge\u201d for AI research. Ongoing competitions in both StarCraft and StarCraft II have assessed progress since the launch of the BroodWar API in 2009, including the \nAIIDE StarCraft AI Competition\n, CIG StarCraft Competition, \nStudent StarCraft AI Tournament\n, and the \nStarcraft II AI Ladder\n. To help the community explore these problems further, \nwe worked with Blizzard in 2016 and 2017 to release an open-source set of tools known as PySC2\n, including the largest set of anonymised game replays ever released. We have now built on this work, combining engineering and algorithmic breakthroughs to produce AlphaStar.\nAlphaStar\u2019s behaviour is generated by a deep \nneural network\n that receives input data from the raw game interface (a list of units and their properties), and outputs a sequence of instructions that constitute an action within the game. More specifically, the neural network architecture applies a \ntransformer\n torso to the units (similar to \nrelational deep reinforcement learning\n), combined with a \ndeep LSTM core\n, an \nauto-regressive policy head\n with a \npointer network\n, and a \ncentralised value baseline\n. We believe that this advanced model will help with many other challenges in machine learning research that involve long-term sequence modelling and large output spaces such as translation, language modelling and visual representations.\nAlphaStar also uses a novel multi-agent learning algorithm. The neural network was initially trained by supervised learning from anonymised human games \nreleased by Blizzard\n. This allowed AlphaStar to learn, by imitation, the basic micro and macro-strategies used by players on the StarCraft ladder. This initial agent defeated the built-in \u201cElite\u201d level AI - around gold level for a human player - in 95% of games.\nThese were then used to seed a multi-agent reinforcement learning process. A continuous league was created, with the agents of the league - competitors - playing games against each other, akin to how humans experience the game of StarCraft by playing on the \nStarCraft ladder\n. New competitors were dynamically added to the league, by branching from existing competitors; each agent then learns from games against other competitors. This new form of training takes the ideas of \npopulation-based\n and \nmulti-agent\n reinforcement learning further, creating a process that continually explores the huge strategic space of StarCraft gameplay, while ensuring that each competitor performs well against the strongest strategies, and does not forget how to defeat earlier ones.\nAs the league progresses and new competitors are created, new counter-strategies emerge that are able to defeat the earlier strategies. While some new competitors execute a strategy that is merely a refinement of a previous strategy, others discover drastically new strategies consisting of entirely new build orders, unit compositions, and micro-management plans. For example, early on in the AlphaStar league, \u201ccheesy\u201d strategies such as very quick rushes with \nPhoton Cannons\n or \nDark Templars\n were favoured. These risky strategies were discarded as training progressed, leading to other strategies: for example, gaining economic strength by over-extending a base with more workers, or sacrificing two \nOracles\n to disrupt an opponent's workers and economy. This process is similar to the way in which players have discovered new strategies, and were able to defeat previously favoured approaches, over the years since StarCraft was released.\nTo encourage diversity in the league, each agent has its own learning objective: for example, which competitors should this agent aim to beat, and any additional internal motivations that bias how the agent plays. One agent may have an objective to beat one specific competitor, while another agent may have to beat a whole distribution of competitors, but do so by building more of a particular game unit. These learning objectives are adapted during training.\nThe neural network weights of each agent are updated by reinforcement learning from its games against competitors, to optimise its personal learning objective. The weight update rule is an efficient and novel \noff-policy actor-critic\n reinforcement learning algorithm with \nexperience replay\n, \u00a0\nself-imitation learning\n and \npolicy distillation\n.\nIn order to train AlphaStar, we built a highly scalable distributed training setup using \nGoogle's v3 TPUs that\n supports a population of agents learning from many thousands of parallel instances of StarCraft II. The AlphaStar league was run for 14 days, using 16 TPUs for each agent. During training, each agent experienced up to 200 years of real-time StarCraft play. The final AlphaStar agent consists of the components of the \nNash distribution of the league\n - in other words, the most effective mixture of strategies that have been discovered - that run on a single desktop GPU.\nA full technical description of this work is being prepared for publication in a peer-reviewed journal.\nProfessional StarCraft players such as TLO and MaNa are able to issue hundreds of \nactions per minute\n (APM) on average. This is far fewer than the majority of \nexisting bots\n, which control each unit independently and consistently maintain thousands or even tens of thousands of APMs.\nIn its games against TLO and MaNa, AlphaStar had an average APM of around 280, significantly lower than the professional players, although its actions may be more precise. This lower APM is, in part, because AlphaStar starts its training using replays and thus mimics the way humans play the game. Additionally, AlphaStar reacts with a delay between observation and action of 350ms on average.\nDuring the matches against TLO and MaNa, AlphaStar interacted with the StarCraft game engine directly via its raw interface, meaning that it could observe the attributes of its own and its opponent\u2019s visible units on the map directly, without having to move the camera - effectively playing with a zoomed out view of the game. In contrast, human players must explicitly manage an \"economy of attention\" to decide where to focus the camera. However, analysis of AlphaStar\u2019s games suggests that it manages an implicit focus of attention. On average, agents \u201cswitched context\u201d about 30 times per minute, similar to MaNa or TLO.\nAdditionally, and subsequent to the matches, we developed a second version of AlphaStar. Like human players, this version of AlphaStar chooses when and where to move the camera, its perception is restricted to on-screen information, and action locations are restricted to its viewable region.\nWe trained two new agents, one using the raw interface and one that must learn to control the camera, against the AlphaStar league. Each agent was initially trained by supervised learning from human data followed by the reinforcement learning procedure outlined above. The version of AlphaStar using the camera interface was almost as strong as the raw interface, exceeding 7000 MMR on our internal leaderboard. In an exhibition match, MaNa defeated a prototype version of AlphaStar using the camera interface, that was trained for just 7 days. We hope to evaluate a fully trained instance of the camera interface in the near future. \nThese results suggest that AlphaStar\u2019s success against MaNa and TLO was in fact due to superior macro and micro-strategic decision-making, rather than superior click-rate, faster reaction times, or the raw interface.\nThe game of StarCraft allows players to select one of three alien races: Terran, Zerg or Protoss. We elected for AlphaStar to specialise in playing a single race for now - Protoss - to reduce training time and variance when reporting results from our internal league. Note that the same training pipeline could be applied to any race. Our agents were trained to play StarCraft II (v4.6.2) in Protoss v Protoss games, on the CatalystLE ladder map. To evaluate AlphaStar\u2019s performance, we initially tested our agents against \nTLO\n: a \ntop professional Zerg player\n and a GrandMaster level Protoss player. AlphaStar won the match 5-0, using a wide variety of units and build orders. \u201cI was surprised by how strong the agent was,\u201d he said. \u201cAlphaStar takes well-known strategies and turns them on their head. The agent demonstrated strategies I hadn\u2019t thought of before, which means there may still be new ways of playing the game that we haven\u2019t fully explored yet.\u201d\nAfter training our agents for an additional week, we played against MaNa, \none of the world\u2019s strongest StarCraft II players\n, and among the 10 strongest Protoss players. AlphaStar again won by 5 games to 0, demonstrating strong micro and macro-strategic skills. \u201cI was impressed to see AlphaStar pull off advanced moves and different strategies across almost every game, using a very human style of gameplay I wouldn\u2019t have expected,\u201d he said. \u201cI\u2019ve realised how much my gameplay relies on forcing mistakes and being able to exploit human reactions, so this has put the game in a whole new light for me. We\u2019re all excited to see what comes next.\u201d\nWhile StarCraft is just a game, albeit a complex one, we think that the techniques behind AlphaStar could be useful in solving other problems. For example, its neural network architecture is capable of modelling very long sequences of likely actions - with games often lasting up to an hour with tens of thousands of moves - based on imperfect information. Each frame of StarCraft is used as one step of input, with the neural network predicting the expected sequence of actions for the rest of the game after every frame. The fundamental problem of making complex predictions over very long sequences of data appears in many real world challenges, such as weather prediction, climate modelling, language understanding and more. We\u2019re very excited about the potential to make significant advances in these domains using learnings and developments from the AlphaStar project.\nWe also think some of our training methods may prove useful in the study of safe and robust AI. One of the great challenges in AI is the number of ways in which systems could go wrong, and StarCraft pros have previously found it easy to beat AI systems by finding inventive ways to provoke these mistakes. AlphaStar\u2019s innovative league-based training process finds the approaches that are most reliable and least likely to go wrong. We\u2019re excited by the potential for this kind of approach to help improve the safety and robustness of AI systems in general, particularly in safety-critical domains like energy, where it\u2019s essential to address complex edge cases.\nAchieving the highest levels of StarCraft play represents a major breakthrough in one of the most complex video games ever created. We believe that these advances, alongside other recent progress in projects such as \nAlphaZero\n and \nAlphaFold\n, represent a step forward in our mission to create intelligent systems that will one day help us unlock novel solutions to some of the world\u2019s most important and fundamental scientific problems.\nWe are thankful for the support and immense skill of Team Liquid\u2019s TLO and MaNa. We are also grateful for the continued support of Blizzard and the StarCraft community for making this work possible.\nAlphaStar Team:\nOriol Vinyals, Igor Babuschkin, Junyoung Chung, Michael Mathieu, Max Jaderberg, Wojtek Czarnecki, Andrew Dudzik, Aja Huang, Petko Georgiev, Richard Powell, Timo Ewalds, Dan Horgan, Manuel Kroiss, Ivo Danihelka, John Agapiou, Junhyuk Oh, Valentin Dalibard, David Choi, Laurent Sifre, Yury Sulsky, Sasha Vezhnevets, James Molloy, Trevor Cai, David Budden, Tom Paine, Caglar Gulcehre, Ziyu Wang, Tobias Pfaff, Toby Pohlen, Yuhuai Wu, Dani Yogatama, Julia Cohen, Katrina McKinney, Oliver Smith, Tom Schaul, Timothy Lillicrap, Chris Apps, Koray Kavukcuoglu, Demis Hassabis, David Silver\nWith thanks to:\nAli Razavi, Daniel Toyama, David Balduzzi, Doug Fritz, Eser Ayg\u00fcn, Florian Strub, Guillaume Alain, Haoran Tang, Jaume Sanchez, Jonathan Fildes, Julian Schrittwieser, Justin Novosad, Karen Simonyan, Karol Kurach, Philippe Hamel, Remi Leblond, Ricardo Barreira, Scott Reed, Sergey Bartunov, Shibl Mourad, Steve Gaffney, Thomas Hubert, the \nteam that created PySC2\n and the whole DeepMind Team, with special thanks to the research platform team, comms and events teams.\n"}
{"title": "TF-Replicator: Distributed Machine Learning for Researchers", "contents": "At DeepMind, the Research Platform Team builds infrastructure to empower and accelerate our AI research. Today, we are excited to share how we developed TF-Replicator, a software library that helps researchers deploy their TensorFlow models on GPUs and \nCloud TPUs\n with minimal effort and no previous experience with distributed systems. TF-Replicator\u2019s programming model has now been open sourced as part of TensorFlow\u2019s \ntf.distribute.Strategy\n. This blog post gives an overview of the ideas and technical challenges underlying TF-Replicator. For a more comprehensive description, please read our \narXiv paper\n.\nA recurring theme in recent AI breakthroughs - from \nAlphaFold\n to \nBigGAN\n to \nAlphaStar\n - \u00a0is the need for effortless and reliable scalability. Increasing amounts of computational capacity allow researchers to train ever-larger neural networks with new capabilities. To address this, the Research Platform Team developed TF-Replicator, which allows researchers to target different hardware accelerators for Machine Learning, scale up workloads to many devices, and seamlessly switch between different types of accelerators. While it was initially developed as a library on top of TensorFlow, TF-Replicator\u2019s API has since been integrated into TensorFlow 2.0\u2019s new \ntf.distribute.Strategy\n.\nWhile TensorFlow provides direct support for CPU, GPU, and TPU (\nTensor Processing Unit\n) devices, switching between targets requires substantial effort from the user. This typically involves specialising code for a particular hardware target, constraining research ideas to the capabilities of that platform. Some existing frameworks built on top of TensorFlow, e.g. \nEstimators\n, seek to address this problem. However, they are typically targeted at production use cases and lack the expressivity and flexibility required for rapid iteration of research ideas.\nOur original motivation for developing TF-Replicator was to provide a simple API for DeepMind researchers to use \nTPUs\n. TPUs provide scalability for Machine Learning workloads, enabling research breakthroughs such as state-of-the-art image synthesis with our \nBigGAN\n model. \nTensorFlow\u2019s native API for TPUs\n differs from how GPUs are targeted, forming a barrier to TPU adoption. TF-Replicator provides a simpler, more user-friendly API that hides the complexity of TensorFlow\u2019s TPU API. Critically, the Research Platform Team developed the TF-Replicator API in close collaboration with researchers across various machine learning disciplines to ensure the necessary flexibility and ease-of-use.\nCode written using TF-Replicator looks similar to code written in TensorFlow for a single device, allowing users the freedom to define their own model run loop. The user simply needs to define (1) an input function that exposes a Dataset, and (2) a step function that defines the logic of their model (e.g. a single step of gradient descent):\nScaling computation to multiple devices requires the devices to communicate with each other. In the context of training Machine Learning models, the most common form of communication is to accumulate gradients for use in optimisation algorithms such as \nStochastic Gradient Descent\n. We therefore provide a convenient method to wrap \nTensorFlow Optimizers\n, so that gradients are accumulated across devices before updating the model\u2019s parameters. For more general communication patterns we provide \nMPI\n-like primitives, such as `all_reduce` and `broadcast`. These make it trivial to implement operations such as global batch normalisation, a technique that is crucial to scale up training of our \nBigGAN\n models (see Section 3 of the paper).\nFor multi-GPU computation TF-Replicator relies on an \u201cin-graph replication\u201d pattern, where the computation for each device is replicated in the same TensorFlow graph. Communication between devices is achieved by connecting nodes from the devices\u2019 corresponding sub-graphs. Implementing this in TF-Replicator was challenging, as communication can occur at any point in the data-flow graph. The order in which computations are constructed is therefore critical.\nOur first idea was to build each device\u2019s sub-graph concurrently in a separate Python thread. When encountering a communication primitive, the threads synchronise and the main thread inserts the required cross-device computation. After that, each thread would continue building its device\u2019s computation. However, at the time we considered this approach, TensorFlow\u2019s graph building API was not thread-safe which made concurrently building sub-graphs in different threads very difficult. Instead, we used \ngraph rewriting\n to insert the communication after all devices\u2019 sub-graphs had been built. When constructing the sub-graphs, placeholders are inserted in places where communication is required. We then collect all matching placeholders across devices and replace them with the appropriate cross-device computation.\nBy collaborating closely with researchers throughout the design and implementation of TF-Replicator, we were able to build a library that allows users to easily scale computation across many hardware accelerators, while leaving them with the control and flexibility required to do cutting-edge AI research. For example, we added MPI-style communication primitives such as all-reduce following discussion with researchers. TF-Replicator and other shared infrastructure allows us to build increasingly complex experiments on robust foundations and quickly spread best practices throughout DeepMind.\nAt the time of writing, TF-Replicator is the most widely used interface for TPU programming at DeepMind. While the library itself is not constrained to training neural networks, it is most commonly used for training on large batches of data. The \nBigGAN\n model, for example, was trained on batches of size 2048 across up to 512 cores of a TPUv3 pod. In Reinforcement Learning agents with a distributed actor-learner setup, such as \nour importance weighted actor-learner architectures\n, scalability is achieved by having many actors generating new experiences by interacting with the environment. This data is then processed by the learner to improve the agent\u2019s policy, represented as a neural network. To cope with an increasing number of actors, TF-Replicator can be used to easily distribute the learner across many hardware accelerators. These and other examples are described in more detail in \nour arXiv paper\n.\nTF-Replicator is just one of many examples of impactful technology built by DeepMind\u2019s Research Platform Team. Many of DeepMind\u2019s breakthroughs in AI, from AlphaGo to AlphaStar, were enabled by the team. If you share our mission and are excited about accelerating state-of-the-art AI research, look out for open Software Engineering positions in Research Platform at \nhttps://deepmind.com/careers\n \n(machine learning experience is optional for these roles).\nThis work was completed by the Research Platform Team at DeepMind. We\u2019d like to thank Frederic Besse, Fabio Viola, John Aslanides, Andy Brock, Aidan Clark, Sergio G\u00f3mez Colmenarejo, Karen Simonyan, Sander Dieleman, Lasse Espeholt, Akihiro Matsukawa, Tim Harley, Jean-Baptiste Lespiau, Koray Kavukcuoglu, Dan Belov and many others at DeepMind for their valuable feedback throughout the development of TF-Replicator. We'd also like to thank Priya Gupta, Jonathan Hseu, Josh Levenberg, Martin Wicke and others at Google for making these ideas available to all TensorFlow users as part of tf.distribute.Strategy.\n"}
{"title": "Capture the Flag: the emergence of complex cooperative agents", "contents": "Mastering the strategy, tactical understanding, and team play involved in multiplayer video games represents a critical challenge for AI research. In our latest paper, \nnow published in the journal Science\n, we present new developments in reinforcement learning, resulting in human-level performance in Quake III Arena Capture the Flag. This is a complex, multi-agent environment and one of the canonical 3D first-person multiplayer games. The agents successfully cooperate with both artificial and human teammates, and demonstrate high performance even when trained with reaction times comparable to human players. Furthermore, we show how these methods have managed to scale beyond research Capture the Flag environments to the full game of Quake III Arena.\nBillions of people inhabit the planet, each with their own individual goals and actions, but still capable of coming together through teams, organisations and societies in impressive displays of collective intelligence. This is a setting we call multi-agent learning: many individual agents must act independently, yet learn to interact and cooperate with other agents. This is an immensely difficult problem - because with co-adapting agents the world is constantly changing.\nTo investigate this problem, we look at 3D first-person multiplayer video games. These games represent the most popular genre of video game, and have captured the imagination of millions of gamers because of their immersive game play, as well as the challenges they pose in terms of strategy, tactics, hand-eye coordination, and team play. The challenge for our agents is to learn directly from raw pixels to produce actions. This complexity makes first-person multiplayer games a fruitful and active area of research within the AI community.\nThe game we focused on in this work is Quake III Arena (which we aesthetically modified, though all game mechanics remain the same). Quake III Arena has laid the foundations for many modern first-person video games, and has attracted a long-standing competitive e-sports scene. We train agents that learn and act as individuals, but which must be able to play on teams with and against any other agents, artificial or human.\nThe rules of CTF are simple, but the dynamics are complex. Two teams of individual players compete on a given map with the goal of capturing the opponent team\u2019s flag while protecting their own. To gain tactical advantage they can tag the opponent team members to send them back to their spawn points. The team with the most flag captures after five minutes wins.\nFrom a multi-agent perspective, CTF requires players to both successfully cooperate with their teammates as well as compete with the opposing team, while remaining robust to any playing style they might encounter.\nTo make things even more interesting, we consider a variant of CTF in which the map layout changes from match to match. As a consequence, our agents are forced to acquire general strategies rather than memorising the map layout. Additionally, to level the playing field, our learning agents experience the world of CTF in a similar way to humans: they observe a stream of pixel images and issue actions through an emulated game controller.\nOur agents must learn from scratch how to see, act, cooperate, and compete in unseen environments, all from a single reinforcement signal per match: whether their team won or not. This is a challenging learning problem, and its solution is based on three general ideas for reinforcement learning:\nThe resulting agent, dubbed the For The Win (FTW) agent, learns to play CTF to a very high standard. Crucially, the learned agent policies are robust to the size of the maps, the number of teammates, and the other players on their team. Below, you can explore some games on both the outdoor procedural environments, where FTW agents play against each other, as well as games in which humans and agents play together on indoor procedural environments.\nWe ran a tournament including 40 human players, in which humans and agents are randomly matched up in games - both as opponents and as teammates.\nThe FTW agents learn to become much stronger than the strong baseline methods, and exceed the win-rate of the human players. In fact, in a survey among participants they were rated more collaborative than human participants.\nGoing beyond mere performance evaluation, it is important to understand the emergent complexity in the behaviours and internal representations of these agents.\nTo understand how agents represent game state, we look at activation patterns of the agents\u2019 neural networks plotted on a plane. Dots in the figure below represent situations during play with close by dots representing similar activation patterns. These dots are coloured according to the high-level CTF game state in which the agent finds itself: In which room is the agent? What is the status of the flags? What teammates and opponents can be seen? We observe clusters of the same colour, indicating that the agent represents similar high-level game states in a similar manner.\nThe agents are never told anything about the rules of the game, yet learn about fundamental game concepts and effectively develop an intuition for CTF. In fact, we can find particular neurons that code directly for some of the most important game states, such as a neuron that activates when the agent\u2019s flag is taken, or a neuron that activates when an agent\u2019s teammate is holding a flag. The paper provides further analysis covering the agents\u2019 use of memory and visual attention.\nHow did our agents perform as well as they did? First, we noticed that the agents had very fast reaction times and were very accurate taggers, which might explain their performance (tagging is a tactical action that sends opponents back to their starting point). Humans are comparatively slow to process and act on sensory input, due to our slower biological signalling. \u00a0\nHere\u2019s an example of a reaction time test you can try yourself\n. Thus, our agents\u2019 superior performance might be a result of their faster visual processing and motor control. However, by artificially reducing this accuracy and reaction time, we saw that this was only one factor in their success. In a further study, we trained agents which have an inbuilt delay of a quarter of a second (267 ms) \u2013 that is, agents have a 267ms lag before observing the world \u2013 comparable with reported reaction times of human video game players. These response-delayed agents still outperformed human participants, with strong humans only winning 21% of the time.\nThrough unsupervised learning we established the prototypical behaviours of agents and humans to discover that agents in fact learn human-like behaviours, such as following teammates and camping in the opponent\u2019s base.\nThese behaviours emerge in the course of training, through reinforcement learning and population-level evolution, with behaviours - such as teammate following - falling out of favour as agents learn to cooperate in a more complementary manner.\nThe training progression of a population of FTW agents. Top left: the 30 agents\u2019 Elo ratings as they train and evolve from each other. Top right: the genetic tree of these evolution events. The lower graph shows the progression of knowledge, some of the internal rewards, and behaviour probability throughout the training of the agents.\nWhile this paper focuses on Capture the Flag, the research contributions are general and we are excited to see how others build upon our techniques in different complex environments. Since initially publishing these results, we have found success in extending these methods to the full game of Quake III Arena, which includes professionally played maps, more multiplayer game modes in addition to Capture the Flag, and more gadgets and pickups. Initial results indicate that agents can play multiple game modes and multiple maps competitively, and are starting to challenge the skills of our human researchers in test matches. Indeed, ideas introduced in this work, such as population based multi-agent RL, form a foundation of the \nAlphaStar agent in our work on StarCraft II\n.\nIn general, this work highlights the potential of multi-agent training to advance the development of artificial intelligence: exploiting the natural curriculum provided by multi-agent training, and forcing the development of robust agents that can even team up with humans.\nFor more details, please see the \npaper\n (\nPDF\n) and the \nfull supplementary video\n.\nThis work was done by Max Jaderberg, Wojciech M. Czarnecki, Iain Dunning, Luke Marris, Brendan Tracey, Guy Lever, Antonio Garcia Castaneda, Charles Beattie, Neil Rabinowitz, Ari Morcos, Avraham Ruderman, Nicolas Sonnerat, Tim Green, Louise Deason, Joel Z. Leibo, David Silver, Demis Hassabis, Koray Kavukcuoglu, and Thore Graepel.\nVisualisations were created by Adam Cain, Damien Boudot, Doug Fritz, Jaume Sanchez Elias, Paul Lewis, Max Jaderberg, Wojciech M. Czarnecki, and Luke Marris.\nWe would like to thank Patrick Howard and Dan \u201cScancode\u201d Gold for allowing us to use the Quake III Arena maps they designed.\nUpdated 30/5/19. Read about our new work below, in \u201cHuman Comparable Agents\u201d and \u201cGoing Further\u201d.\n"}
{"title": "Using AI to give doctors a 48-hour head start on life-threatening illness", "contents": "Artificial intelligence can now predict one of the leading causes of avoidable patient harm up to two days before it happens, as demonstrated by \nour latest research published in Nature\n. Working alongside experts from the US Department of Veterans Affairs (VA), we have developed technology that, in the future, could give doctors a 48-hour head start in treating acute kidney injury (AKI), a condition that is associated with over \n100,000 people \nin the UK every year. These findings come alongside a peer-reviewed service evaluation of Streams, our mobile assistant for clinicians, which shows that patient care can be improved, and health care costs reduced, through the use of digital tools. Together, they form the foundation for a transformative advance in medicine, helping to move from reactive to preventative models of care.\nMillions of people die every year from diseases that could have been prevented with earlier detection. One such disease is acute kidney injury (AKI), a condition where a patient\u2019s kidney suddenly stops working properly. Affecting up to one in five hospitalised patients in \nthe UK\n and \nthe US\n, the condition is notoriously difficult to spot, and deterioration can happen quickly. \nExperts\n believe that up to 30% of cases could be prevented if a doctor intervenes early enough.\nOver the last few years, our team at DeepMind has focused on finding an answer to the complex problem of avoidable patient harm, building digital tools that can spot serious conditions earlier and helping doctors and nurses deliver faster, better care to patients in need. This is our team\u2019s biggest healthcare research breakthrough to date, demonstrating the ability to not only spot deterioration more effectively, but actually predict it before it happens.\nWorking with the VA, the DeepMind team applied AI technology to a comprehensive de-identified electronic health record dataset collected from a network of over a hundred VA sites. The \nresearch\n shows that the AI could accurately predict AKI in patients up to 48 hours earlier than it is currently diagnosed. Importantly, the model correctly predicted 9 out of 10 patients whose condition deteriorated so severely that they then required dialysis. This could provide a window in the future for earlier preventative treatment and avoid the need for more invasive procedures like kidney dialysis. The model has also been designed so that it might, in the future, generalise to other major causes of diseases and deterioration such as sepsis, a life-threatening infection.\nTo address the \u2018black box\u2019 problem \u2013 one of the key barriers for the implementation of AI in clinical practice \u2013 the model also provides the clinical information that was most important in making its predictions of deteriorating kidney function. It also provides predicted future results for several relevant blood tests. This information may help clinicians understand the reasoning behind the AI-enabled alert and anticipate future patient deterioration.\nHowever, these predictions can\u2019t help real patients without the right tools to \u00a0alert specialists. Clinicians routinely use pagers, paper records and fax machines to communicate with each other, but better technology is desperately needed so that critical information can be delivered to the right specialist at the right time. That\u2019s why we\u2019re also pleased to report that the results of a peer-reviewed evaluation of our mobile medical assistant Streams have also been published today. This work was conducted by researchers at University College London.\nStreams is a mobile medical assistant for clinicians, and has been in use at the Royal Free London NHS Foundation Trust since early 2017. The app uses the existing national AKI algorithm to flag patient deterioration, supports the review of medical information at the bedside, and enables instant communication between clinical teams. Shortly after rolling out at the Royal Free, clinicians \nsaid\n that Streams was saving them up to two hours a day. We also heard about patients, like \nAfia Ahmad\n, whose treatment was escalated thanks to the app. But we wanted to quantify these benefits through robust clinical evaluation. Today\u2019s results show that the app saved clinicians\u2019 time, improved care and reduced the number of AKI cases being missed at the hospital.\nBy using Streams, \nspecialists reviewed urgent cases within 15 minutes or less\n(a process that might otherwise have taken several hours) and fewer cases of AKI were missed (3.3% rather than 12.4%). The app also \nreduced the average cost of admission\n for a patient with AKI by 17%, demonstrating a huge potential cost saving for hospitals in the future, considering that AKI costs the NHS \nmore than \u00a31 billion \neach year.\nFeedback from \nthe qualitative study\n was positive, with healthcare professionals emphasising the ways in which the app accelerated the detection of patients in need, saved them time in performing administrative tasks, and improved team communication. One respondent said the app \u201cstreamlines care, and speeds up the time in which they get a specialist renal review.\u201d Another clinician from the nephrology team stated that \u201cBeing able to look up the blood results for anyone in the hospital wherever you are is unparalleled...it must save at least \u2013 I don\u2019t know if you could analyse it \u2013 but it must save at least a couple of hours in a day.\u201d\nGetting the right information about the right patient at the right time is a huge problem for healthcare systems across the globe. Critically, these early findings from the Royal Free suggest that, in order to improve patient outcomes even further, clinicians need to be able to intervene before AKI can be detected by the current NHS algorithm \u2013 which is why our research on AKI is so promising. These results comprise the building blocks for our long-term vision of preventative healthcare, helping doctors to intervene in a proactive, rather than reactive, manner.\nStreams doesn\u2019t use artificial intelligence at the moment, but the team now intends to find ways to safely integrate predictive AI models into Streams in order to provide clinicians with intelligent insights into patient deterioration.\nThis is a major milestone for the DeepMind Health team, who will be carrying this work forward as part of Google Health, led by Dr David Feinberg. As we \nannounced\n in November 2018, the Streams team, and colleagues working on translational research in healthcare, will be joining Google in order to make a positive impact on a global scale. The combined experience, infrastructure and expertise of DeepMind Health teams alongside Google\u2019s will help us continue to develop mobile tools that can support more clinicians, address critical patient safety issues and could, we hope, save thousands of lives globally.\n"}
{"title": "How evolutionary selection can train more capable self-driving cars", "contents": "Waymo\u2019s self-driving vehicles employ neural networks to perform many driving tasks, from detecting objects and predicting how others will behave, to planning a car's next moves. Training an individual neural net has traditionally required weeks of fine-tuning and experimentation, as well as enormous amounts of computational power. Now, Waymo, in a research collaboration with DeepMind, has taken inspiration from Darwin\u2019s insights into evolution to make this training more effective and efficient.\nAt a high level, neural nets learn through trial and error. A network is presented with a task, and is \u201cgraded\u201d on whether it performs the task correctly or not. The network learns by continually attempting these tasks and adjusting itself based on its grades, such that it becomes more likely to perform correctly in the future.\nA network\u2019s performance depends heavily on its training regimen. For example, a researcher can tweak how much a network adjusts itself after each task\u2013referred to as its learning rate. The higher the learning rate, the more dramatic the adjustments. The goal is to find a learning rate high enough that the network gets better after each iteration, but not so high that the network's performance fluctuates wildly.\nFinding the best training regimen (or \u201chyperparameter schedule\u201d) is commonly achieved through an engineer\u2019s experience and intuition, or through extensive searching. In random search, researchers apply many random hyperparameter schedules over multiple types of hyperparameters in order to train different networks independently and in parallel\u2013after which it\u2019s possible to settle on the best performing model. This\n blog post\n covers how Waymo engineers apply reinforcement learning to the search for better neural net architectures.\nBecause training numerous models in parallel is computationally expensive, researchers typically hand-tune random search by monitoring networks while they\u2019re training, periodically culling the weakest performers and freeing resources to train new networks from scratch with new random hyperparameters. This type of manual tuning produces better results faster, but it\u2019s labor intensive.\nTo make this process more efficient, researchers at DeepMind devised a way to automatically determine good hyperparameter schedules based on evolutionary competition (called \u201cPopulation Based Training\u201d or PBT), which combines the advantages of \nhand-tuning and random search\n.\nLike random search, PBT also starts with multiple networks initiated with random hyperparameters. Networks are evaluated periodically and compete with each other for \u201csurvival\u201d in an evolutionary fashion. If a member of the population is underperforming, it\u2019s replaced with the \u201cprogeny\u201d of a better performing member. The progeny is a copy of the better performing member, with slightly mutated hyperparameters. PBT doesn\u2019t require us to restart training from scratch, because each progeny inherits the full state of its parent network, and hyperparameters are updated actively throughout training, not at the end of training. Compared to random search, PBT spends more of its resources training with good hyperparameter values.\n\u200d\nThe first experiments that DeepMind and Waymo collaborated on involved training a network that generates boxes around pedestrians, bicyclists, and motorcyclists detected by our sensors\u2013named a \u201cregion proposal network.\u201d The aim was to investigate whether PBT could improve a neural net's ability to detect pedestrians along two measures: recall (the fraction of pedestrians identified by the neural net over total number of pedestrians in the scene) and precision (the fraction of detected pedestrians that are actually pedestrians, and not spurious \u201cfalse positives\u201d). Waymo\u2019s vehicles detect these road users using multiple neural nets and other methods, but the goal of this experiment was to train this single neural net to maintain recall over 99%, while reducing false positives using population-based training.\nWe learned a lot from this experiment. Firstly, we discovered that we needed to create a realistic and robust evaluation for the networks so that we\u2019d know if a neural net would truly perform better when deployed across a variety of situations in the real world. This evaluation formed the basis of the competition that PBT employs to pick one winning neural net over another. To ensure neural nets perform well generally, and don\u2019t simply memorise answers to examples they've seen during training, our PBT competition evaluation uses a set of examples (the \"validation set\") that is different from those used in training (the \"training set.\") To verify final performance, we also use a third set of examples (the \"evaluation set\") that the neural nets have never seen in training or competition.\nSecondly, we learned that we needed fast evaluation to support frequent evolutionary competition. Researchers seldom evaluate their models during training, and when they do, the evaluation is done infrequently. PBT required models be evaluated every 15 minutes. To achieve this, we took advantage of Google\u2019s data centres to parallelise the evaluation across hundreds of distributed machines.\nDuring these experiments, we noticed that one of PBT\u2019s strengths\u2013allocating more resources to the progeny of better performing networks\u2013can also be a weakness, because PBT optimises for the present and fails to consider long-term outcomes. This can be a problem because it disadvantages late-bloomers, so neural nets with hyperparameters that perform better over the long term don\u2019t have the chance to mature and succeed. One way to combat this is to increase population diversity, which can be achieved by simply training a larger population. If the population is large enough, there is a greater chance for networks with late-blooming hyperparameters to survive and catch up in later generations.\nIn these experiments, we were able to increase diversity by creating sub-populations called \u201cniches,\u201d where neural nets were only allowed to compete within their own sub-groups\u2013similar to how species evolve when isolated on islands. We also tried to directly reward diversity through a technique called \u201cfitness sharing,\u201d where we measure the difference between members of the population and give more unique neural nets an edge in the competition. Greater diversity allows PBT to explore a larger hyperparameter space.\nPBT enabled dramatic improvements in model performance. For the experiment above, our PBT models were able to achieve higher precision by reducing false positives by 24% compared to its hand-tuned equivalent, while maintaining a high recall rate. A chief advantage of evolutionary methods such as PBT is that they can optimise arbitrarily complex metrics. Traditionally, neural nets can only be trained using simple and smooth loss functions, which act as a proxy for what we really care about. PBT enabled us to go beyond the update rule used for training neural nets, and towards the more complex metrics optimising for features we care about, such as maximising precision under high recall rates.\nPBT also saves time and resources. The hyperparameter schedule discovered with PBT-trained nets outperformed Waymo\u2019s previous net with half the training time and resources. Overall, PBT uses half the computational resources used by random parallel search to efficiently discover better hyperparameter schedules. \u00a0It also saves time for researchers\u2013by incorporating PBT directly into Waymo\u2019s technical infrastructure, researchers from across the company can apply this method with the click of a button, and spend less time tuning their learning rates. Since the completion of these experiments, PBT has been applied to many different Waymo models, and holds a lot of promise for helping to create more capable vehicles for the road.\nContributors: The work described here was a research collaboration between Yu-hsin Chen and Matthieu Devin of Waymo, and Ali Razavi, Ang Li, Sibon Li, Ola Spyra, Pramod Gupta and Oriol Vinyals of DeepMind. Advisors to the project include Max Jaderberg, Valentin Dalibard, Meire Fortunato and Jackson Broshear from DeepMind.\n"}
{"title": "Identifying and eliminating bugs in learned predictive models", "contents": "One in a series of posts explaining the theories underpinning our research. \nBugs and software have gone hand in hand since the beginning of computer programming. Over time, software developers have established a set of best practices for testing and debugging before deployment, but these practices are not suited for modern deep learning systems. Today, the prevailing practice in machine learning is to train a system on a training data set, and then test it on another set. While this reveals the average-case performance of models, it is also crucial to ensure robustness, or acceptably high performance even in the worst case. \u00a0In this article, we describe three approaches for rigorously identifying and eliminating bugs in learned predictive models: adversarial testing, robust learning, and formal verification.\nMachine learning systems are not robust by default. Even systems that outperform humans in a particular domain can fail at solving simple problems if subtle differences are introduced. For example, consider the problem of image perturbations: a neural network that can classify images better than a human can be easily fooled into believing that sloth is a race car if a small amount of carefully calculated noise is added to the input image.\nThis is not an entirely new problem. Computer programs have always had bugs. Over decades, software engineers have assembled an impressive toolkit of techniques, ranging from unit testing to formal verification. These methods work well on traditional software, but adapting these approaches to rigorously test machine learning models like neural networks is extremely challenging due to the scale and lack of structure in these models, which may contain hundreds of millions of parameters. This necessitates the need for developing novel approaches for ensuring that machine learning systems are robust at deployment.\nFrom a programmer\u2019s perspective, a bug is any behaviour that is inconsistent with the \nspecification\n, i.e. the intended functionality, of a system. As part of our mission of solving intelligence, we conduct research into techniques for evaluating whether machine learning systems are consistent not only with the train and test set, but also with a list of specifications describing desirable properties of a system. Such properties might include robustness to sufficiently small perturbations in inputs, safety constraints to avoid catastrophic failures, or producing predictions consistent with the laws of physics.\nIn this article, we discuss three important technical challenges for the machine learning community to take on, as we collectively work towards rigorous development and deployment of machine learning systems that are reliably consistent with desired specifications:\nRobustness to adversarial examples is a relatively well-studied problem in deep learning. One major theme that has come out of this work is the importance of evaluating against strong attacks, and designing transparent models which can be efficiently analysed. Alongside other researchers from the community, we have found that many models appear robust when evaluated against weak adversaries. However, they show essentially 0% adversarial accuracy when evaluated against stronger adversaries (\nAthalye et al., 2018\n, \nUesato et al., 2018\n, \nCarlini and Wagner, 2017\n).\nWhile most work has focused on rare failures in the context of supervised learning (largely image classification), there is a need to extend these ideas to other settings. In recent work on adversarial approaches for uncovering catastrophic failures, we apply these ideas towards testing reinforcement learning agents intended for use in safety-critical settings. One challenge in developing autonomous systems is that because a single mistake may have large consequences, very small failure probabilities are unacceptable.\nOur objective is to design an \u201cadversary\u201d to allow us to detect such failures in advance (e.g., in a controlled environment). If the adversary can efficiently identify the worst-case input for a given model, this allows us to catch rare failure cases before deploying a model. As with image classifiers, evaluating against a weak adversary provides a false sense of security during deployment. This is similar to the software practice of red-teaming, though extends beyond failures caused by malicious adversaries, and also includes failures which arise naturally, for example due to lack of generalization.\nWe developed two complementary approaches for adversarial testing of RL agents. In the first, we use a derivative-free optimisation to directly minimise the expected reward of an agent. In the second, we learn an adversarial value function which predicts from experience which situations are most likely to cause failures for the agent. We then use this learned function for optimisation to focus the evaluation on the most problematic inputs. These approaches form only a small part of a rich, growing space of potential algorithms, and we are excited about future development in rigorous evaluation of agents.\nAlready, both approaches result in large improvements over random testing. Using our method, failures that would have taken days to uncover, or even gone undetected entirely, can be detected in minutes (\nUesato et al., 2018b\n). We also found that adversarial testing may uncover qualitatively different behaviour in our agents from what might be expected from evaluation on a random test set. In particular, using adversarial environment construction we found that agents performing a 3D navigation task, which match human-level performance on average, still failed to find the goal completely on surprisingly simple mazes (\nRuderman et al., 2018\n). Our work also highlights that we need to design systems that are secure against natural failures, not only against adversaries.\nAdversarial testing aims to find a counter example that violates specifications. As such, it often leads to overestimating the consistency of models with respect to these specifications. Mathematically, a specification is some relationship that has to hold between the inputs and outputs of a neural network. This can take the form of upper and lower bounds on certain key input and output parameters.\nMotivated by this observation, several researchers (\nRaghunathan et al., 2018\n; \nWong et al., 2018\n; \nMirman et al., 2018\n; \nWang et al., 2018\n) including our team at DeepMind (\nDvijotham et al., 2018\n; \nGowal et al., 2018\n), have worked on algorithms that are agnostic to the adversarial testing procedure (used to assess consistency with the specification). This can be understood geometrically - we can bound (e.g., using interval bound propagation; \nEhlers 2017\n, \nKatz et al. 2017\n, \nMirman et al., 2018\n) the worst violation of a specification by bounding the space of outputs given a set of inputs. If this bound is differentiable with respect to network parameters and can be computed quickly, it can be used during training. The original bounding box can then be propagated through each layer of the network.\nWe show that interval bound propagation is fast, efficient, and \u2014 contrary to prior belief \u2014 can achieve strong results (\nGowal et al., 2018\n). In particular, we demonstrate that it can decrease the provable error rate (i.e., maximal error rate achievable by any adversary) over state-of-the-art in image classification on both MNIST and CIFAR-10 datasets.\nGoing forward, the next frontier will be to learn the right geometric abstractions to compute \ntighter overapproximations\n of the space of outputs. We also want to train networks to be consistent with more complex specifications capturing desirable behavior, such as above mentioned invariances and consistency with physical laws.\nRigorous testing and training can go a long way towards building robust machine learning systems. However, no amount of testing can formally guarantee that a system will behave as we want. In large-scale models, enumerating all possible outputs for a given set of inputs (for example, infinitesimal perturbations to an image) is intractable due to the astronomical number of choices for the input perturbation. However, as in the case of training, we can find more efficient approaches by setting geometric bounds on the set of outputs. Formal verification is a subject of ongoing research at DeepMind.\nThe machine learning community has developed several interesting ideas on how to compute precise geometric bounds on the space of outputs of the network (\nKatz et al. 2017\n, \nWeng et al., 2018\n; \nSingh et al., 2018\n). Our approach (\nDvijotham et al., 2018\n), based on optimisation and duality, consists of formulating the verification problem as an optimisation problem that tries to find the largest violation of the property being verified. By using ideas from duality in optimisation, the problem becomes computationally tractable. This results in additional constraints that refine the bounding boxes computed by interval bound propagation, using so-called cutting planes. This approach is sound but incomplete: there may be cases where the property of interest is true, but the bound computed by this algorithm is not tight enough to prove the property. However, once we obtain a bound, this formally guarantees that there can be no violation of the property. The figure below graphically illustrates the approach.\nThis approach enables us to extend the applicability of verification algorithms to more general networks (activation functions, architectures), general specifications and more sophisticated deep learning models (generative models, neural processes, etc.) and specifications beyond adversarial robustness (\nQin, 2018\n).\nDeployment of machine learning in high-stakes situations presents unique challenges, and requires the development of evaluation techniques that reliably detect unlikely failure modes. More broadly, we believe that learning consistency with specifications can provide large efficiency improvements over approaches where specifications only arise implicitly from training data. We are excited about ongoing research into adversarial evaluation, learning robust models, and verification of formal specifications.\nMuch more work is needed to build automated tools for ensuring that AI systems in the real world will do the \u201cright thing\u201d. In particular, we are excited about progress in the following directions:\nDeepMind is dedicated to positive social impact through responsible development and deployment of machine learning systems. To make sure that the contributions of developers are reliably positive, we need to tackle many technical challenges. We are committed to taking part in this effort and are excited to work with the community on solving these challenges.\nThis post describes the work of the Robust and Verified Deep Learning group (Pushmeet Kohli, DJ Krishnamurthy, Jonathan Uesato, Sven Gowal, Chongli Qin, Robert Stanforth, Po-Sen Huang) performed in collaboration with various contributors across DeepMind including Avraham Ruderman, Alhussein Fawzi, Ananya Kumar, Brendan O'Donoghue, Bristy Sikder, Chenglong Wang, Csaba Szepesvari, Hubert Soyer, Relja Arandjelovic, Richard Everett, Rudy Bunel, Timothy Mann, Grzegorz Swirszcz, and Tom Erez.\nThanks to Vishal Maini, Ale\u0161 Flidr, Damien Boudot, and Jan Leike for their contributions to the post.\n"}
{"title": "Using machine learning to accelerate ecological research", "contents": "The Serengeti is one of the last remaining sites in the world that hosts an intact community of large mammals. These animals roam over vast swaths of land, some migrating thousands of miles across multiple countries following seasonal rainfall. As human encroachment around the region becomes more intense, these species are forced to alter their behaviours in order to survive. Increasing agriculture, poaching, and climate abnormalities contribute to changes in animal behaviours and population dynamics, but these changes have occurred at spatial and temporal scales which are difficult to monitor using traditional research methods. There is a great urgency to understand how these animal communities function as human pressures grow, both in order to understand the dynamics of these last pristine ecosystems, and to formulate effective management plans to conserve and protect the integrity of this unique biodiversity hotspot.\nTo this end, DeepMind is collaborating with ecologists and conservationists to develop machine learning methods to help study the behavioural dynamics of an entire African animal community in the Serengeti National Park and Grumeti Reserve in Tanzania. The Serengeti-Mara ecosystem is globally unparalleled in its biodiversity, hosting an estimated 70 large mammal species and 500 bird species, thanks in part to its unique geology and varied habitat types. Almost a decade ago, the Serengeti Lion Research program installed hundreds of motion-sensitive cameras within the core of the protected area. The cameras are triggered by passing wildlife, capturing animal images frequently, across vast spatial scales, allowing researchers to study animal behaviour, distribution, and demography with great spatial and temporal resolution.\nOver the last nine years, the team has collected and stored millions of photos like the one above. Until now, volunteers from across the world have helped to identify and count the species in the photos by hand using the \nZooniverse\n web-based platform, which hosts many similar projects for citizen-scientists. This has resulted in a rich \ndataset\n, \nSnapshot Serengeti\n, featuring labels and counts for around 50 different species. Currently, the annotation process is labor intensive and time-consuming: it takes up to a year from the time a camera is triggered until labels are collected from volunteers. This bottleneck has not only impeded scientists\u2019 ability to perform basic research, but has made it hard for conservationists to react adaptively to challenges and perturbations disrupting the ecosystem. To help researchers unlock this data with greater efficiency, we\u2019ve used the \nSnapshot Serengeti dataset\n to train machine learning models to automatically detect, identify, and count animals.\nUsing machine learning for conservation is not new. For example, researchers have previously leveraged \ntourist photos\n and \nYouTube videos\n to track animals, and \naudio recordings\n to identify species by their calls. Camera trap data can be hard to work with\u2013animals may appear out of focus, and can be at many different distances and positions with respect to the camera (as in the image above). With expert input from leading ecologist and conservationist Dr. Meredith Palmer, our project quickly took shape, and we now have a model that can perform on par with, or better than, human annotators for most of the species in the region. Importantly, this method shortens the data processing pipeline by up to 9 months, which has immense potential to help researchers in the field.\nOf course, field work is challenging, and fraught with unexpected hazards such as failing power lines and limited or no internet access. We are currently preparing the software for deployment in the field, and looking at ways to safely run our pre-trained model with modest hardware requirements and little Internet access. We\u2019ve worked closely with our collaborators in the field to be sure that our technology is used \nresponsibly\n. Once in place, researchers in the Serengeti will be able to make direct use of this tool, helping provide them with up-to-date species information to better support their conservation efforts.\nWe will be talking further about the project and related work at the \nDeep Learning Indaba\n in Kenya later this August. DeepMind is a founding partner of the Deep Learning Indaba, a continent-wide movement to strengthen the research and application of ML and AI in Africa, and several DeepMind researchers serve as key organisers of this unique event. \u201cIndaba\u201d is a Zulu word indicating an important community gathering. This year, community-led IndabaX AI conferences were held in 26 African countries as part of the runup to the main \nDeep Learning Indaba\n at Kenyatta University in Kenya in late August. For a week, researchers, students and community members will meet to share their knowledge and best practices, and experts will host panels, workshops and discussions covering many topics in machine learning and AI. During this meeting, DeepMind and other Indaba volunteers will co-host a hackathon for anyone interested in ML and conservation to develop their own models using the \nSnapshot Serengeti\n dataset. Ecology students will be equipped to understand and use ML models for conservation, and taught how to develop their own models. Through gatherings like Indaba, we hope to empower more local experts to use AI techniques for addressing problems in their own communities. The AI community in Africa is growing, and the hackathon will help to train local experts, centering on conservation as part of the core dialogue.\nThe DeepMind Science Team works to leverage AI to tackle key scientific challenges that impact the world. We\u2019ve developed a robust model for detecting and analysing animal populations in field data, and have helped to consolidate data to enable the growing machine learning community in Africa to build AI systems for conservation which, we hope, will scale to other parks. We\u2019ll next be validating our models by deploying them in the field and tracking their progress. Our hope is to contribute towards making AI research more inclusive\u2013both in terms of the kinds of domains we apply it to, and the people developing it. Hence, participating in meetings like Indaba are key for helping build a global team of AI practitioners who can deploy machine learning for diverse projects.\nProject credits:\nJean-baptiste Alayrac, Sam Blackwell, Joao Carreira, Reena Chopra, Sander Dieleman, Brian McWilliams, Sofia Mi\u00f1ano, Sanjana Narayanan, Meredith Palmer, Ulrich Paquet, Stig Petersen, Roman Werpachowski, Michal Zielinski.\nAdditional Credits:\nRazia Ahamed, Andrea Banino, Pushmeet Kohli, Drew Purves, Andrew Zisserman\nThis work was made possible by data from Snapshot Serengeti. Images are available via a \nCreative Commons Attribution 4.0 International License\n and can be found \nhere\n. Please contact \nDr Meredith Palmer\n with data inquiries.\nSwanson AB, Kosmala M, Lintott CJ, Simpson RJ, Smith A, Packer C (2015) Snapshot Serengeti, high-frequency annotated camera trap images of 40 mammalian species in an African savanna. Scientific Data 2: 150026\n"}
{"title": "Unsupervised learning: The curious pupil", "contents": "One in a series of posts explaining the theories underpinning our research.\nOver the last decade, machine learning has made unprecedented progress in areas as diverse as image recognition, self-driving cars and playing complex games like Go. These successes have been largely realised by training deep neural networks with one of two learning paradigms\u2014supervised learning and reinforcement learning. Both paradigms require training signals to be designed by a human and passed to the computer. In the case of supervised learning, these are the \u201ctargets\u201d (such as the correct label for an image); in the case of reinforcement learning, they are the \u201crewards\u201d for successful behaviour (such as getting a high score in an Atari game). The limits of learning are therefore defined by the human trainers.\nWhile some scientists contend that a sufficiently inclusive training regime\u2014for example, the ability to complete a very wide variety of tasks\u2014should be enough to give rise to general intelligence, others believe that true intelligence will require more independent learning strategies. Consider how a toddler learns, for instance. Her grandmother might sit with her and patiently point out examples of ducks (acting as the instructive signal in supervised learning), or reward her with applause for solving a woodblock puzzle (as in reinforcement learning). But the vast majority of a toddler\u2019s time is spent naively exploring the world, making sense of her surroundings through curiosity, play, and observation. Unsupervised learning is a paradigm designed to create autonomous intelligence by rewarding agents (that is, computer programs) for learning about the data they observe without a particular task in mind. In other words, the agent learns for the sake of learning.\nA key motivation for unsupervised learning is that, while the data passed to learning algorithms is extremely rich in internal structure (e.g., images, videos and text), the targets and rewards used for training are typically very sparse (e.g., the label \u2018dog\u2019 referring to that particularly protean species, or a single one or zero to denote success or failure in a game). This suggests that the bulk of what is learned by an algorithm must consist of understanding the data itself, rather than applying that understanding to particular tasks.\n2012 was a landmark year for deep learning, when AlexNet (named after its lead architect Alex Krizhnevsky) swept the \nImageNet classification competition\n. AlexNet\u2019s abilities to recognize images were unprecedented, but even more striking is what was happening under the hood. When researchers analysed what AlexNet was doing, they discovered that it interprets images by building increasingly complex \ninternal representations of its inputs\n. Low-level features, such as textures and edges, are represented in the bottom layers, and these are then combined to form high-level concepts such as wheels and dogs in higher layers.\nThis is remarkably similar to how information is processed in our brains, where simple edges and textures in primary sensory processing areas are assembled into complex objects like faces in higher areas. The representation of a complex scene can therefore be built out of visual primitives, in much the same way that meaning emerges from the individual words comprising a sentence. Without explicit guidance to do so, the layers of AlexNet had discovered a fundamental \u2018vocabulary\u2019 of vision in order to solve its task. In a sense, it had learned to play what Wittgenstein called a \n\u2018language game\u2019\n that iteratively translates from pixels to labels.\nFrom the perspective of general intelligence, the most interesting thing about AlexNet\u2019s vocabulary is that it can be reused, or transferred, to visual tasks other than the one it was trained on, such as recognising \nwhole scenes rather than individual objects\n. Transfer is essential in an ever-changing world, and humans excel at it: we are able to rapidly adapt the skills and understanding we\u2019ve gleaned from our experiences (our \u2018world model\u2019) to whatever situation is at hand. For example, a classically-trained pianist can pick up jazz piano with relative ease. Artificial agents that form the right internal representations of the world, the reasoning goes, should be able to do similarly.\nNonetheless, the representations learned by classifiers such as AlexNet have limitations. In particular, as the network was only trained to label images with a single class (cat, dog, car, volcano), any information not required to infer the label\u2014no matter how useful it might be for other tasks\u2014is liable to be ignored. For example, the representations may fail to capture the background of the image if the label always refers to the foreground. A possible solution is to provide more comprehensive training signals, like \ndetailed captions describing the images\n: not just \u201cdog,\u201d but \u201cA Corgi catching a frisbee in a sunny park.\u201d However, such targets are laborious to provide, especially at scale, and still may be insufficient to capture all the information needed to complete a task. The basic premise of unsupervised learning is that the best way to learn rich, broadly transferable representations is to attempt to learn everything that can be learned about the data.\nIf the notion of transfer through representation learning seems too abstract, consider a child who has learned to draw people as stick figures. She has discovered a representation of the human form that is both highly compact and rapidly adaptable. By augmenting each stick figure with specifics, she can create portraits of all her classmates: glasses for her best friend, her deskmate in his favorite red tee-shirt. And she has developed this skill not in order to complete a specific task or receive a reward, but rather in response to her basic urge to reflect the world around her.\nPerhaps the simplest objective for unsupervised learning is to train an algorithm to generate its own instances of data. So-called generative models should not simply reproduce the data they are trained on (an uninteresting act of memorisation), but rather build a model of the underlying class from which that data was drawn: not a particular photograph of a horse or a rainbow, but the set of all photographs of horses and rainbows; not a specific utterance from a specific speaker, but the general distribution of spoken utterances. The guiding principle of generative models is that being able to construct a convincing example of the data is the strongest evidence of having understood it: as Richard Feynman put it, \"what I cannot create, I do not understand.\"\nFor images, the most successful generative model so far has been the \nGenerative Adversarial Network\n (GAN for short), in which two networks\u2014a generator and a discriminator\u2014engage in a contest of discernment akin to that of an artistic forger and a detective. The generator produces images with the goal of tricking the discriminator into believing they are real; the discriminator, meanwhile, is rewarded for spotting the fakes. The generated images, first messy and random, are refined over many iterations, and the ongoing dynamic between the networks leads to ever-more realistic images that are in many cases \nindistinguishable from real photographs\n. Generative adversarial networks can also dream details of landscapes \ndefined by the rough sketches of users\n.\nA glance at the images below is enough to convince us that the network has learned to represent many of the key features of the photographs they were trained on, such as the structure of animal\u2019s bodies, the texture of grass, and detailed effects of light and shade (even when refracted through a soap bubble). Close inspection reveals slight anomalies, such as the white dog\u2019s apparent extra leg and the oddly right-angled flow of one of the jets in the fountain. While the creators of generative models strive to avoid such imperfections, their visibility highlights one of the benefits of recreating familiar data such as images: by inspecting the samples, researchers can infer what the model has and hasn\u2019t learned.\nAnother notable family within unsupervised learning are autoregressive models, in which the data is split into a sequence of small pieces, each of which is predicted in turn. Such models can be used to generate data by successively guessing what will come next, feeding in a guess as input and guessing again. Language models, where each word is predicted from the words before it, are perhaps the best known example: these models power the text predictions that pop up on some email and messaging apps. Recent advances in language modelling have enabled the generation of strikingly plausible passages, such as the one shown below from \nOpenAI\u2019s GPT-2\n.\nOne interesting inconsistency in the text is that the unicorns are described as \u201cfour-horned\u201d: again, it is fascinating to probe the limitations of the network\u2019s understanding.\nBy controlling the input sequence used to condition the out predictions, autoregressive models can also be used to transform one sequence into another. This \ndemo\n uses a conditional autoregressive model to transform text into realistic handwriting. \nWaveNet\n transforms text into natural sounding speech, and is now used to \ngenerate voices for Google Assistant\n. A similar process of conditioning and autoregressive generation can be used to \ntranslate from one language to another\n.\nAutoregressive models learn about data by attempting to predict each piece of it in a particular order. A more general class of unsupervised learning algorithms can be built by predicting any part of the data from any other. For example, this could mean removing a word from a sentence, and \nattempting to predict it from whatever remains\n. By learning to make lots of localised predictions, the system is forced to learn about the data as a whole.\nOne concern around generative models is their potential for misuse. While manipulating evidence with photo, video, and audio editing has been possible for a long time, generative models could make it even easier to edit media with malicious intent. We have already seen demonstrations of so-called \u2018deepfakes\u2019\u2014for instance, this\n fabricated video footage of President Obama\n. It\u2019s encouraging to see that several major efforts to address these challenges are already underway, including using statistical techniques to help\n detect\n synthetic media\n and verify authentic media, \nraising public awareness\n, and discussions around limiting the availability of trained generative models. Furthermore, generative models can themselves be used to detect synthetic media and anomalous data\u2014for example when \ndetecting fake speech\n or identifying payment abnormalities to protect customers against fraud. Researchers need to work on generative models in order to better understand them and mitigate downstream risks.\nGenerative models are fascinating in their own right, but our principal interest in them at DeepMind is as a stepping stone towards general intelligence. Endowing an agent with the ability to generate data is a way of giving it an imagination, and hence the ability to \nplan and reason about the future\n. Even without explicit generation, our studies show that \nlearning to predict\n different \naspects of the environment\n enriches the agent\u2019s \nworld model\n, and thereby improves its ability to solve problems.\nThese results resonate with our intuitions about the human mind. Our ability to learn about the world without explicit supervision is fundamental to what we regard as intelligence. On a train ride we might listlessly gaze through the window, drag our fingers over the velvet of the seat, regard the passengers sitting across from us. We have no agenda in these studies: we almost can\u2019t help but gather information, our brains ceaselessly working to understand the world around us, and our place within it.\n"}
{"title": "Machine learning can boost the value of wind energy", "contents": "Carbon-free technologies like renewable energy help combat climate change, but many of them have not reached their full potential. Consider wind power: over the past decade, wind farms have become an important source of carbon-free electricity as the cost of turbines has plummeted and adoption has surged. However, the variable nature of wind itself makes it an unpredictable energy source\u2014less useful than one that can reliably deliver power at a set time.\nIn search of a solution to this problem, last year, DeepMind and Google started applying machine learning algorithms to 700 megawatts of wind power capacity in the central United States. These wind farms\u2014part of Google\u2019s global fleet of \nrenewable energy projects\n\u2014collectively generate as much electricity as is needed by a medium-sized city.\nUsing a neural network trained on widely available weather forecasts and historical turbine data, we configured the DeepMind system to predict wind power output 36 hours ahead of actual generation. Based on these predictions, our model recommends how to make optimal hourly delivery commitments to the power grid a full day in advance. This is important, because energy sources that can be scheduled (i.e. can deliver a set amount of electricity at a set time) are often more valuable to the grid.\nAlthough we continue to refine our algorithm, our use of machine learning across our wind farms has produced positive results. To date, machine learning has boosted the value of our wind energy by roughly 20 percent, compared to the baseline scenario of no time-based commitments to the grid.\nWe can\u2019t eliminate the variability of the wind, but our early results suggest that we can use machine learning to make wind power sufficiently more predictable and valuable. This approach also helps bring greater data rigor to wind farm operations, as machine learning can help wind farm operators make smarter, faster and more data-driven assessments of how their power output can meet electricity demand.\nOur hope is that this kind of machine learning approach can strengthen the business case for wind power and drive further adoption of carbon-free energy on electric grids worldwide. Researchers and practitioners across the energy industry are developing novel ideas for how society can make the most of variable power sources like solar and wind. We\u2019re eager to join them in exploring general availability of these cloud-based machine learning strategies.\nGoogle recently achieved \n100 percent renewable energy purchasing\n and is now striving to \nsource carbon-free energy \non a 24x7 basis. The partnership with DeepMind to make wind power more predictable and valuable is a concrete step toward that aspiration. While much remains to be done, this step is a meaningful one\u2014for Google, and more importantly, for the environment.\nThis article is cross-posted from \nThe Keyword\n.\n"}
{"title": "Welcome to the DeepMind podcast", "contents": "What\u2019s AI? What can it be used for? Is it safe? And how do I get involved? These are the kinds of questions we often get asked at public events like science festivals, talks and workshops. We love answering them and really value the conversations and thinking they provoke.\nSadly, we can\u2019t have face-to-face conversations with everyone who is interested in AI. So, to help us bridge that gap, we\u2019re now launching DeepMind: The Podcast, a new series that we hope will answer these questions and more, while also giving listeners an inside look at how AI research is done at an organisation like DeepMind. You can subscribe now on your favourite podcast app.\nCommunicating a complex topic like AI research is not easy, and we\u2019re grateful that this series is hosted by the brilliant \nDr Hannah Fry\n, a mathematician and broadcaster with a talent for making technical topics accessible and interesting. We\u2019ve worked together over the last 12 months to choose topics that we hope will convey the excitement of AI research, whilst also highlighting some of the questions and challenges the whole field is wrestling with today. The result is an eight-part series that explores topics such as the link between neuroscience and AI, why we use games in our research, building safe AI and how AI can be used to solve scientific problems.\nIn it, you will hear from our researchers, engineers, program managers and collaborators, many of whom are talking to the public about their work for the very first time.\nSubscribe on\n \nApple podcasts\n, \nGoogle podcasts\n, \nSpotify\n, \nDeezer\n \nor your favourite podcast app by searching for \u201cDeepMind: The Podcast\u201d.\nWe\u2019re really proud of the programmes and hope they'll spark the curiosity of listeners to explore the world of AI further. To help, we have compiled a list of further reading for each episode in the show notes, drawing on the work of other labs and organisations in the AI community. We want to make this useful to anyone, so if you know of other resources we should link to, please help other listeners by either replying to us on Twitter (#DMpodcast) or emailing us at \npodcast@deepmind.com.\n You can also use that address to send us feedback on the series.\nThis is our first foray into podcasting and we've wrestled with lots of questions, some of which we thought it would be useful to share: \nWe\u2019d like to say a huge thank you to all of the brilliant contributors, creative producers, and our talented presenter, Hannah, all of whom helped craft our initial idea into the final programmes you'll hear over the coming weeks. \nWe really enjoyed making this podcast and hope you enjoy listening too. \nCredits:\nPresenter:\n Hannah Fry\nEditor:\n David Prest\nSenior Producer:\n Louisa Field\nProducers:\n Amy Racs, Dan Hardoon\nBinaural Sound:\n Lucinda Mason-Brown\nMusic composition:\n Eleni Shaw (with help from Sander Dieleman and WaveNet)\n"}
{"title": "The Podcast: Episode 2: Go to Zero", "contents": "In March 2016, more than 200 million people watched AlphaGo become first computer program to defeat a professional human player at the game of Go, a milestone in AI research that was considered to be a decade ahead of its time.\nSince then the team has continued to develop the system and recently unveiled \u00a0AlphaZero: a program that has taught itself how to play chess, Go, and shogi. Hannah explores the inside story of both with Lead Researcher David Silver and finds out why games are a useful proving ground for AI researchers. She also meets Chess Grandmaster Matthew Sadler and women\u2019s international master Natasha Regan, who have written a book on AlphaZero and its unique gameplay.\nInterviewees: \nDeepMind CEO Demis Hassabis, Matthew Sadler, chess Grandmaster; Lead Researcher David Silver, Matt Botvinick, Director of Neuroscience Research; and Natasha Regan, women\u2019s international chess master.\nIf you know of other resources we should link to, please help other listeners by either replying to us on \nTwitter\n (#DMpodcast) or emailing us at \npodcast@deepmind.com.\n You can also use that address to send us questions or feedback on the series.\nPresenter:\n Hannah Fry\nEditor:\n David Prest\nSenior Producer:\n Louisa Field\nProducers:\n Amy Racs, Dan Hardoon\nBinaural Sound:\n Lucinda Mason-Brown\nMusic composition:\n Eleni Shaw (with help from Sander Dieleman and \nWaveNet\n)\n"}
{"title": "Open source", "contents": ""}
{"title": "The Podcast: Episode 1: AI and neuroscience - The virtuous circle", "contents": "In the first episode of the \nDeepMind podcast\n, Hannah meets the DeepMind Neuroscience team to explore these connections and discovers how our brains are like birds\u2019 wings, what training a dog and an AI agent have in common, and why the simplest things for people to do are, paradoxically, often the hardest for machines. \nInterviewees: \nDeepmind CEO and co-founder, Demis Hassabis; Matt Botvinick, Director of Neuroscience Research; research scientists Jess Hamrick and Greg Wayne; and Director of Research Koray Kavukcuoglu.\nListen to this episode and subscribe to the whole series on\n \nApple podcasts\n, \nGoogle podcasts\n, \nSpotify\n, \nDeezer\n \nor your favourite podcast app by searching for \u201cDeepMind: The Podcast\u201d.\nIf you know of other resources we should link to, please help other listeners by either replying to us on \nTwitter\n (#DMpodcast) or emailing us at \npodcast@deepmind.com.\n You can also use that address to send us questions or feedback on the series.\nPresenter:\n Hannah Fry\nEditor:\n David Prest\nSenior Producer:\n Louisa Field\nProducers:\n Amy Racs, Dan Hardoon\nBinaural Sound:\n Lucinda Mason-Brown\nMusic composition:\n Eleni Shaw (with help from Sander Dieleman and \nWaveNet\n)\n"}
{"title": "Authors' notes", "contents": ""}
{"title": "Highlighted research", "contents": ""}
{"title": "Careers", "contents": "Our pioneering and collaborative culture is made up of people from unusually diverse backgrounds. Together, we build computer systems that learn how to solve problems and advance scientific discovery for all.\nWe don\u2019t set limits based on what others think is possible or impossible. We drive ourselves and inspire each other to push boundaries and achieve ambitious goals.\nWe value our diversity of experience, knowledge, backgrounds, and perspectives and harness these qualities to discover connections, solve problems, and create extraordinary impact.\nIt\u2019s our mission, our shared purpose, our guide for everything we do and how we do it. We understand just how significant and important it is, and we put our passion and drive behind achieving it. Changes and challenges will come our way, but we embrace them and stay optimistic, knowing that it's a privilege to work on world-changing technology that can make an impact on the world around us.\nWe are striving to be the best in the world at what we do, pushing boundaries, driving ourselves and each other to create and innovate, always seeking to improve and reach our full potential. We don\u2019t set limits based on what others think is possible or impossible; instead, we approach our work and the world with optimism and deep curiosity.\nWhile we act with speed and efficiency, we never compromise on our ethical standards. That means always thinking carefully about who this technology could impact; it means having integrity and doing what we say we will; it means owning and admitting our mistakes, learning from experience and feedback and being transparent with each other about what we do and why.\nWe are an unusually interdisciplinary team, and our successes come from the unique opportunities that creates. We achieve much more through teamwork than we do alone, constructively challenging each other, seeking feedback, and asking for opinions.\nWe listen to different views to challenge our own assumptions, are quick to share credit, praise and recognise each other, and are flexible in the way we behave and work as a team.\nOur environment is built on respect - we are inclusive, fair, open-minded, and thoughtful with our opinions and their impact. Kindness, support, and encouragement fuel our growth and success, and we are positive and generous in spirit. We are loyal and protective, seeing the very best in each other and always ready to give the benefit of the doubt.\nWe\u2019re always looking for great people and we're committed to ensuring that all applicants and employees will be treated fairly and with respect, irrespective of their background and identity.\nWe also partner with academia and non-profits on scholarships and other opportunities for people from underrepresented backgrounds.\nResearch Interns\n work closely with our research scientists on projects relating to ongoing topics and current goals within our research teams. Research internships are open to candidates studying towards a PhD in machine learning, computer science, computational neuroscience, or a related field. Internships with our Science team are also open to those studying a range of natural sciences, including physics, chemistry and biology. \nOur \nEngineering Internships \nare open to current degree students at any level (bachelors, masters, doctorate) in a technical subject (computer science, engineering, maths, physics, etc), who have some practical software engineering experience. Machine learning experience is not required for engineering internships.\nWe also offer internships across a number of our teams, including Ethics & Society and Operations.\nApplications for our 2023 internships will open on September 16th 2022 and will stay open approximately 2 weeks. A team of recruiters will review applications on a rolling basis over the autumn and winter. Please be patient as we consider your application. Due to the high volume of applications, we are unable to provide individual status updates to applicants and referrers.\n"}
{"title": "Publications", "contents": ""}
{"title": "Researching patient deterioration with the US Department of Veterans Affairs", "contents": "We\u2019re excited to announce a medical research partnership with the US Department of Veterans Affairs (VA), one of the world\u2019s leading healthcare organisations responsible for providing high-quality care to veterans and their families across the United States.\nThis project will see us analyse patterns from historical, depersonalised medical records to predict patient deterioration.\nPatient deterioration is a significant global health problem that often has fatal consequences. \nStudies\nestimate that 11% of all in-hospital deaths are due to patient deterioration not being recognised early enough or acted on in the right way.\nAlongside world-renowned clinicians and researchers at the VA, we are analysing patterns from approximately 700,000 historical, depersonalised medical records in order to determine if machine learning can accurately identify the risk factors for patient deterioration and correctly predict its onset.\nWe\u2019re focusing on Acute Kidney Injury (AKI), one of the most common conditions associated with patient deterioration, and an area where \nDeepMind\n and the VA both have expertise. This is a complex challenge, because predicting AKI is far from easy. Not only is the onset of AKI sudden and often asymptomatic, but the risk factors associated with it are commonplace throughout hospitals. AKI can also strike people of any age, and frequently occurs following routine procedures and operations like a hip replacement.\nOur goal is to find ways to improve the algorithms currently used to detect AKI and allow doctors and nurses to intervene sooner. Eventually, we hope to apply similar approaches to other signs of patient deterioration as well, leading to improved care for many more patients, with fewer people developing serious infections and conditions\u2014ultimately saving lives.\nAs with all of our research work, we are committed to treating the data for this project with the utmost care and respect. The data being used in the research are depersonalised, meaning that any information that could be used to identify individuals has been removed before DeepMind receives it. You can read more about our own approach to information governance \nhere\n.\nThe work we\u2019re currently conducting is exploratory, but we\u2019re optimistic about the long term potential for machine learning technology in this area. In a world where nearly all hospital resources go toward managing symptoms after people are already ill, we hope that predictive techniques can pave the way for more preventive healthcare and help keep people from getting sick in the first place. We\u2019ll keep you updated as we continue this work.\n\u200d\n"}
{"title": "Learning to write programs that generate images", "contents": "Through a human\u2019s eyes, the world is much more than just the images reflected in our corneas. For example, when we look at a building and admire the intricacies of its design, we can appreciate the craftsmanship it requires. This ability to interpret objects through the tools that created them gives us a richer understanding of the world and is an important aspect of our intelligence.\nWe would like our systems to create similarly rich representations of the world. For example, when observing an image of a painting we would like them to understand the brush strokes used to create it and not just the pixels that represent it on a screen.\nIn this work\n, we equipped artificial agents with the same tools that we use to generate images and demonstrate that they can reason about how digits, characters and portraits are constructed. Crucially, they learn to do this by themselves and without the need for human-labelled datasets. This contrasts with \nrecent research\n which has so far relied on learning from human demonstrations, which can be a time-intensive process.\nWe designed a deep reinforcement learning \nagent \nthat interacts with a computer \npaint program\n, placing strokes on a digital canvas and changing the brush size, pressure and colour. The untrained agent starts by drawing random strokes with no visible intent or structure. To overcome this, we had to create a way to reward the agent that encourages it to produce meaningful drawings.\nTo this end, we trained a second neural network, called the \ndiscriminator\n, whose sole purpose is to predict whether a particular drawing was produced by the agent, or if it was sampled from a dataset of real photographs. The painting agent is rewarded by how much it manages to \u201cfool\u201d the discriminator into thinking its drawings are real. In other words, the agent\u2019s reward signal is itself learned. While this is similar to the approach used in Generative Adversarial Networks (GANs), it differs because the generator in GAN setups is typically a neural network that directly outputs pixels. In contrast, our agent produces images by writing graphics programs to interact with a paint environment.\nIn the first set of experiments, the agent was trained to generate images resembling \nMNIST\n digits: it was shown what the digits look like, but not how they are drawn. By attempting to generate images that fool the discriminator, the agent learns to control the brush and to manoeuvre it to fit the style of different digits, a technique known as visual \nprogram synthesis\n.\nWe also trained it to reproduce specific images. Here, the discriminator\u2019s aim is to determine if the reproduced image is a copy of the target image, or if it has been produced by the agent. The more difficult this distinction becomes for the discriminator, the more the agent is rewarded.\nCrucially, this framework is also interpretable because it produces a sequence of motions that control a simulated brush. This means that the model can apply what it has learnt on the simulated paint program to re-create characters in other similar environments, for instance on a simulated or real robot arm. A video of this can be seen \nhere\n. \nThere is also potential to scale this framework to real datasets. When trained to paint \ncelebrity faces\n, the agent is capable of capturing the main traits of the face, such as shape, tone and hair style, much like a street artist would when painting a portrait with a limited number of brush strokes:\nRecovering structured representations from raw sensations is an ability that humans readily possess and frequently use. In this work we show it is possible to guide artificial agents to \u00a0produce similar representations by giving them access to the same tools that we use to recreate the world around us. In doing so they learn to produce visual programs that succinctly express the causal relationships that give rise to their observations.\nAlthough our work only represents a small step towards flexible program synthesis, we anticipate that similar techniques may be necessary to enable artificial agents with human-like cognitive, generalisation and communication abilities.\nWatch the video \nhere\n, read more about the method in the \npaper\n.\nThis work was done by Yaroslav Ganin, Tejas Kulkarni, Igor Babuschkin, S. M. Ali Eslami and Oriol Vinyals, with thanks to Oleg Sushkov, David Barker, Matej Vecerik and Jon Scholz for their help with the robot.\n"}
{"title": "Learning to navigate in cities without a map", "contents": "How did you learn to navigate the neighborhood of your childhood, to go to a friend\u2019s house, to your school or to the grocery store? Probably without a map and simply by remembering the visual appearance of streets and turns along the way. As you gradually explored your neighborhood, you grew more confident, mastered your whereabouts and learned new and increasingly complex paths. You may have gotten briefly lost, but found your way again thanks to landmarks, or perhaps even by looking to the sun for an impromptu compass.\nNavigation is an important cognitive task that enables humans and animals to traverse, without maps, over long distances in a complex world. Such long-range navigation can simultaneously support self-localisation (\u201cI am here\u201d) and a representation of the goal (\u201cI am going there\u201d).\nIn \nLearning to Navigate in Cities Without a Map\n, we present an interactive navigation environment that uses first-person perspective photographs from \nGoogle Street View\n, approved for use by the StreetLearn project and academic research, and gamify that environment to train an AI. As standard with Street View images, faces and license plates have been blurred and are unrecognisable. We build a neural network-based artificial agent that learns to navigate multiple cities using visual information (pixels from a Street View image). Note that this research is about navigation in general rather than driving; we did not use traffic information nor try to model vehicle control.\nThe agent is rewarded when it reaches a target destination (specified, for instance, as pair of latitude and longitude coordinates), like a courier tasked with an endless set of deliveries but without a map. Over time, the AI agent learns to cross entire cities in this way. We also demonstrate that our agent can learn the task in multiple cities, and then robustly adapt to a new city.\nWe depart from the traditional approaches which rely on explicit mapping and exploration (like a cartographer who tries to localise themselves and draw a map at the same time). Our approach, in contrast, is to learn to navigate as humans used to do, without maps, GPS localisation, or other aids, using only visual observations. We build a neural network agent that inputs images observed from the environment and predicts the next action it should take in that environment. We train it end-to-end using deep reinforcement learning, similarly to some recent work on \nlearning to navigate in complex 3D mazes\n and \nreinforcement learning with unsupervised auxiliary tasks\n for playing games. Unlike those studies, which were conducted on small-scale simulated maze environments, we utilise city-scale real-world data, including complex intersections, footpaths, tunnels, and diverse topology across London, Paris, and New York City. Moreover, the approach we use support city-specific learning and optimisation as well as general, transferable navigation behaviours.\nThe neural network inside our agent consists of three parts: 1) a convolutional network that can process images and extract visual features, 2) a locale-specific recurrent neural network that is implicitly tasked with memorising the environment as well as learning a representation of \u201chere\u201d (current position of the agent) and of \u201cthere\u201d (location of the goal) and 3) a locale-invariant recurrent network that produces the navigation policy over the agent\u2019s actions. The locale-specific module is designed to be interchangeable and, as its name indicates, unique to each city where the agent navigates, whereas the vision module and the policy module can be locale-invariant.\nJust as in the Google Street View interface, the agent can rotate in place or move forward to the next panorama, when possible. Unlike the Google Maps and Street View environment, the agent does not see the little arrows, the local or global map, or the famous Pegman: it needs to learn to differentiate open roads from sidewalks. The target destinations may be kilometres away in the real world and require the agent to step through hundreds of panoramas to reach them.\nWe demonstrate that our proposed method can provide a mechanism for transferring knowledge to new cities. As with humans, when our agent visits a new city, we would expect it to have to learn a new set of landmarks, but not to have to re-learn its visual representations or its behaviours (e.g., zooming forward along streets or turning at intersections). Therefore, using the MultiCity architecture, we train first on a number of cities, then we freeze both the policy network and the visual convolutional network and only a new locale-specific pathway on a new city. This approach enables the agent to acquire new knowledge without forgetting what it has already learned, similarly to the\n progressive neural networks\narchitecture.\nStudying navigation is fundamental in the study and development of artificial intelligence, and trying to replicate navigation in artificial agents can also help scientists understand its biological underpinnings.\n"}
{"title": "Learning by playing", "contents": "Getting children (and adults) to tidy up after themselves can be a challenge, but we face an even greater challenge trying to get our AI agents to do the same. Success depends on the mastery of several core visuo-motor skills: approaching an object, grasping and lifting it, opening a box and putting things inside of it. To make matters more complicated, these skills must be applied in the right sequence.\nControl tasks, like tidying up a table or stacking objects, require an agent to determine how, when and where to coordinate the nine joints of its simulated arms and fingers to move correctly and achieve its objective. The sheer number of possible combinations of movements at any given time, along with the need to carry out a long sequence of correct actions constitute a serious exploration problem\u2014making this a particularly interesting area for reinforcement learning research.\nTechniques like reward shaping, apprenticeship learning or learning from demonstrations can help with the exploration problem. However, these methods rely on a considerable amount of knowledge about the task\u2014the problem of learning complex control problems from scratch with minimal prior knowledge is still an open challenge.\nOur \nnew paper\n proposes a new learning paradigm called \u2018Scheduled Auxiliary Control (SAC-X)\u2019 which seeks to overcome this exploration issue. SAC-X is based on the idea that to learn complex tasks from scratch, an agent has to learn to explore and master a set of basic skills first. Just as a baby must develop coordination and balance before she crawls or walks\u2014providing an agent with internal (auxiliary) goals corresponding to simple skills increases the chance it can understand and perform more complicated tasks.\nWe demonstrate the SAC-X approach on several simulated and real robot tasks using a variety of tasks including stacking problems with different objects and \u2018tidying up a playground\u2019, which involves moving objects into a box. The auxiliary tasks we define follow a general principle: they encourage the agent to explore its sensor space. For example, activating a touch sensor in its fingers, sensing a force in its wrist, maximising a joint angle in its proprioceptive sensors or forcing a movement of an object in its visual camera sensors. Each task is associated with a simple reward of one if the goal is achieved, and zero otherwise. \nOur agent can then decide by itself about its current \u2018intention\u2019, i.e. which goal to pursue next. This might be an auxiliary task or an externally defined target task. Crucially, the agent can detect and learn from reward signals for all other tasks that it is not currently following by making extensive use of replay-based off-policy learning. For example, when picking up or moving an object the agent might incidentally stack it, leading to the observation of rewards for \u2018stacking\u2019. Because a sequence of simple tasks can lead to the observation of a rare external reward, the ability to schedule intentions is crucial. It can create a personalised learning curriculum based on all the tangential knowledge it has collected. This turns out to be an effective way to exploit knowledge in such a large domain, and is particularly useful when there are only few external reward signals available. Our agent decides which intention to follow via a scheduling module. The scheduler is improved during training via a meta-learning algorithm that attempts to maximise progress on the main task, which results in significantly improved data-efficiency.\nOur evaluations show that SAC-X is able to solve all the tasks we set it from scratch\u2014using the same underlying set of auxiliary tasks. Excitingly, SAC-X is also able to successfully learn a pick-up and a placing task from scratch directly on a real robot arm in our lab. In the past this has been particularly challenging because learning on robots in a real-world setup requires data-efficiency, so a popular approach is to pre-train an agent in simulation and then transfer the agent to the real robot arm.\nWe consider SAC-X as an important step towards learning control tasks from scratch, when only the overall goal is specified. SAC-X allows you to define auxiliary tasks arbitrarily: they can be based on general insights (like deliberately activating sensors as suggested here), but could ultimately incorporate any task a researcher thinks is important. In that respect, SAC-X is a general RL method that is broadly applicable in general sparse reinforcement learning settings beyond control and robotics.\nRead the paper \nhere\n.\nThis work was completed by Martin Riedmiller, Roland Hafner, Thomas Lampe, Michael Neunert, Jonas Degrave, Tom Van de Wiele, Volodymyr Mnih, Nicolas Heess and Tobias Springenberg.\n"}
{"title": "Understanding deep learning through neuron deletion", "contents": "Deep neural networks are composed of many individual neurons, which combine in complex and counterintuitive ways to solve a wide range of challenging tasks. This complexity grants neural networks their power but also earns them their reputation as confusing and opaque black boxes. \nUnderstanding how deep neural networks function is critical for explaining their decisions and enabling us to build more powerful systems. For instance, imagine the difficulty of trying to build a clock without understanding how individual gears fit together. One approach to understanding neural networks, both in neuroscience and deep learning, is to investigate the role of individual neurons, especially those which are easily interpretable. \nOur investigation into the \nimportance of single directions for generalisation\n, soon to appear at the Sixth International Conference on Learning Representations (\nICLR\n), uses an approach inspired by decades of experimental neuroscience \u2014 exploring the impact of damage \u2014 to determine: how important are small groups of neurons in deep neural networks? Are more \neasily interpretable\n neurons also more important to the network\u2019s computation? \nWe measured the performance impact of damaging the network by deleting individual neurons as well as groups of neurons. Our experiments led to two surprising findings: \nIn both neuroscience and deep learning, easily interpretable neurons (\u201cselective\u201d neurons) which are only active in response to images of a single input category, such as dogs, have been analysed extensively. In deep learning, this has led to the emphasis on \ncat neurons\n, \nsentiment neurons\n, and \nparentheses neurons\n; in neuroscience, \nJennifer Aniston neurons\n, among others. However, the relative importance of these few highly selective neurons compared to the majority of neurons which have low selectivity and more puzzling, hard-to-interpret activity has remained unknown.\nTo evaluate neuron importance, we measured how network performance on image classification tasks changes when a neuron is deleted. If a neuron is very important, deleting it should be highly damaging and substantially decrease network performance, while the deletion of an unimportant neuron should have little impact. Neuroscientists routinely perform similar experiments, although they cannot achieve the fine-grained precision which is necessary for these experiments and readily available in artificial neural networks. \nSurprisingly, we found that there was little relationship between selectivity and importance. In other words, \u201ccat neurons\u201d were no more important than confusing neurons. This finding echoes recent work in neuroscience which has demonstrated that confusing neurons can actually be quite informative, and suggests that we must look beyond the most easily interpretable neurons in order to understand deep neural networks.\nAlthough interpretable neurons are easier to understand intuitively (\u201cit likes dogs\u201d), they are no more important than confusing neurons with no obvious preference. \nWe seek to build intelligent systems, and we can only call a system intelligent if it can generalise to new situations. For example, an image classification network which can only classify specific dog images that it has seen before, but not new images of the same dog, is useless. It is only in the intelligent categorisation of new examples that these systems gain their utility. A \nrecent collaborative paper from Google Brain, Berkeley, and DeepMind\n which won best paper at ICLR 2017 showed that deep nets can simply memorise each and every image on which they are trained instead of learning in a more human-like way (e.g., understanding the abstract notion of a \"dog\"). \nHowever, it is often unclear whether a network has learned a solution which will generalise to new situations or not. By deleting progressively larger and larger groups of neurons, we found that networks which generalise well were much more robust to deletions than networks which simply memorised images that were previously seen during training. In other words, networks which generalise better are harder to break (although they can definitely still be broken). \nBy measuring network robustness in this way, we can evaluate whether a network is exploiting undesirable memorisation to \u201ccheat.\u201d Understanding how networks change when they memorise will help us to build new networks which memorise less and generalise more.\nTogether, these findings demonstrate the power of using techniques inspired by experimental neuroscience to understand neural networks. Using these methods, we found that highly selective individual neurons are no more important than non-selective neurons, and that networks which generalise well are much less reliant on individual neurons than those which simply memorise the training data. These results imply that individual neurons may be much less important than a first glance may suggest. \nBy working to explain the role of all neurons, not just those which are easy-to-interpret, we hope to better understand the inner workings of neural networks, and critically, to use this understanding to build more intelligent and general systems.\nRead the full paper \nhere\n.\nThis work was done by Ari S Morcos, David GT Barrett, Neil C Rabinowitz, and Matthew Botvinick.\nVisualisations were created by Paul Lewis, Adam Cain, and Doug Fritz.\n"}
{"title": "Stop, look and listen to the people you want to help", "contents": "\u2018I like to take things slow. Take it slowly and get it right first time,\u2019 one participant said, but was quickly countered by someone else around the table: \u2018But I\u2019m impatient, I want to see the benefits now.\u2019 This exchange neatly captures many of the conversations I heard at DeepMind Health\u2019s recent Collaborative Listening Summit. It also represents, in layman\u2019s terms, the debate that tech thinkers and policy-makers are having right now about the future of artificial intelligence.\nThe Collaborative Listening Summit brought together members of the public, patient representatives and stakeholder, and was facilitated by Ipsos MORI. The objective of the Summit: to explore how principles, co-created in earlier events with the public, patients and stakeholders, should govern DeepMind Health\u2019s operating practices and engagement with the NHS. These principles ranged from the technical \u2013 for example, how evidence should inform DeepMind\u2019s practice \u2013 to the societal \u2013 for example, operating in the best interests of society.\nThe challenge of how technology companies and the NHS should interact has had many of us, \nincluding myself\n, cautious about the risk of big technology firms leveraging their finance and power over an NHS that is under seemingly endless pressure. Despite our desire to see the NHS become more agile and innovative, the \u2018move fast and break things\u2019 mentality of big tech is something that the public and policy-makers are rightly wary of when lives are at stake.\nThese fears usually manifest themselves in headlines about protecting patient data, but the issues run far deeper than this. For example, algorithms have an increasingly large part to play in our everyday lives, deciding which film we might like to watch next or the adverts we see online, but undergo relatively little scrutiny. In health care specifically, algorithms are beginning to augment existing services, with NHS England predicting that \nalgorithms will soon handle one in three NHS 111 enquiries\n. Algorithms currently in place are relatively simple, and they need to be tested to ensure they are safe and effective. As the technology acquires greater decision-making capability, testing will become more challenging and more important. Frankly, regulators and policy-makers are still getting up to speed with these issues, so a cautious approach to regulation is sensible.\nThere is evidently a need for policy-makers to further deepen their understanding of what rules of engagement should be for tech companies. Initiatives like the government\u2019s new Centre for Data Ethics and Innovation, intended to promote ethical data innovation, provide a platform for this, as do the trailblazing efforts of the biomedical research community. However, if recent public trust traps like GM foods and care.data and even the Royal Free/DeepMind data-sharing agreement are to be avoided big tech companies must themselves take responsibility for understanding what their relationship with NHS organisations should be.\nDeepMind\u2019s past attempts to engage have been subject to some criticism and show how challenging, yet important, it is to get dialogue with the public right. However, I was reassured that the Summit was designed to impart understanding; researching what people think about an issue where DeepMind staff themselves are on uncertain ground. Of course, for this to make any difference, the research needs to be applied \u2013 and that means practising the principles in the real world.\nPrinciples such as \u2018do what is best for society\u2019 might seem trite and obvious especially as, in the real world, contracts may be put above ethics, as participants at the event rightly pointed out. So, if DeepMind and other big tech companies are to hold themselves to ethical principles around testing, transparency and societal benefit, they must show that these principles translate into ethical business practice. This will mean not working with NHS organisations where they feel they cannot hold themselves to these principles.\nAnd this will get harder as time goes on \u2013 currently there is more than enough interest in machine learning to keep DeepMind and its competitors busy. But as these technologies come on-stream and there is market share to be gained, it will become more and more tempting for DeepMind to break these principles, especially as other technology companies eye up the NHS for size. This is where policy re-enters the picture: data ethics initiatives need to catch up with this work and encourage other companies to undertake robust research with the public. After all, the NHS cannot become a world leader in artificial intelligence if doctors and managers cannot look patients in the eye and assure them that their data is not being used in ways that are contrary to their own values.\nParticipants at the workshop were cautious, especially around the need to safeguard their data, but they could also see the benefits that new technologies could have in terms of saving time and lives. Tech companies should not be scared of engaging with the public \u2013 but they must listen to the people whose lives they claim to be bettering.\n"}
{"title": "Navigating with grid-like representations in artificial agents", "contents": "Most animals, including humans, are able to flexibly navigate the world they live in \u2013 exploring new areas, returning quickly to remembered places, and taking shortcuts. Indeed, these abilities feel so easy and natural that it is not immediately obvious how complex the underlying processes really are. In contrast, spatial navigation remains a substantial challenge for artificial agents whose abilities are far outstripped by those of mammals.\nIn 2005, a potentially crucial part of the neural circuitry underlying spatial behaviour was revealed by an astonishing discovery: neurons that fire in a strikingly regular hexagonal pattern as animals explore their environment. This lattice of points is believed to facilitate spatial navigation, similarly to the gridlines on a map. In addition to equipping animals with an internal coordinate system, these neurons - known as \ngrid cells\n - have recently been hypothesised to support \nvector-based navigation\n. That is: enabling the brain to calculate the distance and direction to a desired destination, \u201c\nas the crow flies\n,\u201d allowing animals to make direct journeys between different places even if that exact route had not been followed before.\nThe group that first discovered grid cells was jointly awarded the \n2014 Nobel Prize in Physiology or Medicine\n for shedding light on how cognitive representations of space might work. But after more than 10 years of theorising since their discovery, the computational functions of grid cells - and whether they support vector-based navigation - has remained largely a mystery.\nIn our \nmost recent paper\n [\nPDF here\n]published in Nature, we developed an artificial agent to test the theory that grid cells support vector-based navigation, in keeping with our \noverarching philosophy\n that algorithms used for AI can meaningfully approximate elements of the brain.\nAs a first step, we trained a recurrent network to perform the task of localising itself in a virtual environment, using predominantly movement-related velocity signals. This ability is commonly used by mammals when moving through unfamiliar places or in situations where it is not easy to spot familiar landmarks (e.g. when navigating in the dark).\nWe found that grid-like representations (hereafter grid units) spontaneously emerged within the network - providing a striking convergence with the neural activity patterns observed in foraging mammals, and consistent with the notion that grid cells provide an efficient code for space.\nWe next sought to test the theory that grid cells support vector-based navigation by creating an artificial agent to be used as an experimental guinea pig. This was done by combining the initial \u201cgrid network\u201d with a larger network architecture, forming an agent that could be trained using deep reinforcement learning to navigate to goals in challenging virtual reality game environments.\nThis agent performed at a super-human level, exceeding the ability of a professional game player, and exhibited the type of flexible navigation normally associated with animals, taking novel routes and shortcuts when they became available.\nThrough a series of experimental manipulations, we showed that grid-like representations were critical for vector-based navigation. For example, when grid cells in the network were silenced, the agent\u2019s ability to navigate was impaired, and the representation of key metrics such as distance and direction to the goal became less accurate.\nWe believe our study constitutes an important step in understanding the fundamental computational purpose of grid cells in the brain and also highlights the benefits they afford to artificial agents. The evidence provides compelling support for the theory that grid cells provide a Euclidean spatial framework - a concept of space - enabling vector-based navigation.\nMore broadly, our work reaffirms the potential of utilising algorithms thought to be used by the brain as \ninspiration for machine learning architectures\n. The extensive previous neuroscience research into grid cells makes the agent's interpretability - which is itself a major topic in AI research - significantly easier, by giving us clues about what to look for when trying to understand its internal representations. The work also showcases the potential of using artificial agents actively engaging in complex behaviours within realistic virtual environments to test theories of how the brain works.\nTaking this principle further, a similar approach could be used to test theories concerning brain areas that are important for perceiving sound or controlling limbs, for example. In the future such networks may well provide a new way for scientists to conduct \u2018experiments\u2019, suggesting new theories and even complementing some of the work that is currently conducted in animals.\nUPDATE 14.05.18: \nWe\u2019d encourage you to read \nThe emergence of grid-like representations by training recurrent neural networks to perform spatial localization\n by Cueva and Wei, which was published contemporaneously at ICLR. While different in scope and findings, it shows interesting results. In brief, the authors found periodic firing that conformed to the shape of the enclosure, e.g rectangular grids in a square environment and triangular in a triangular environment (fig. 2 of Cueva and Wei). This differs from our study, where we found grid-like units whose firing pattern closely resembles rodent grid cells which typically show hexagonal firing patterns across different shaped environments (e.g. square and circular arena).\nRead the Nature paper: [\nPDF\n]\nDownload the original paper (unformatted): [\nPDF\n]\nRead Nobel Prize Laureate Edvard Moser's \nreview of the paper\n.\nThis work was done by Andrea Banino, Caswell Barry, Benigno Uria, Charles Blundell, Timothy Lillicrap, Piotr Mirowski, Alexander Pritzel, Martin Chadwick, Thomas Degris, Joseph Modayil, Greg Wayne, Hubert Soyer, Fabio Viola, Brian Zhang, Ross Goroshin, Neil Rabinowitz, Razvan Pascanu, Charlie Beattie, Stig Petersen, Amir Sadik, Stephen Gaffney, Helen King, Koray Kavukcuoglu, Demis Hassabis, Raia Hadsell, and Dharshan Kumaran.\n"}
{"title": "Retour \u00e0 Paris / A return to Paris", "contents": "English version follows\nLorsque nous avons \u00e9tabli notre si\u00e8ge \u00e0 Londres en 2010, nous voulions faire de DeepMind le nec plus ultra de la recherche de pointe dans le domaine de l\u2019intelligence artificielle. Nous voulions \u00e9galement aider la communaut\u00e9 de l\u2019intelligence artificielle \u00e0 se d\u00e9velopper. Nous avons ainsi publi\u00e9 des articles dans les conf\u00e9rences et journaux les plus s\u00e9lectifs (plus de 180 \u00e0 ce jour !) et partag\u00e9 nos connaissances dans ce domaine ; nous avons incit\u00e9 nos experts \u00e0 enseigner dans les universit\u00e9s locales, et \u0153uvr\u00e9 avec les \u00e9coles et les ONG \u00e0 former la prochaine g\u00e9n\u00e9ration de scientifiques. \nNous avons eu non seulement la chance de contribuer au succ\u00e8s scientifique du Royaume-Uni, mais avons aussi grandement b\u00e9n\u00e9fici\u00e9 de l\u2019ouverture et de la diversit\u00e9 de cette ville ainsi que de son influence culturelle. L\u2019intelligence artificielle doit \u00eatre d\u00e9velopp\u00e9e en accordant la plus grande attention aux diff\u00e9rents besoins de la soci\u00e9t\u00e9 et \u2013 pour autant qu\u2019une ville puisse r\u00e9unir \u00e0 elle seule ces conditions \u2013 une capitale multiculturelle comme Londres, ma ville natale, est \u00e0 cet \u00e9gard l\u2019endroit id\u00e9al. \nJe suis, donc, tr\u00e8s heureux d\u2019annoncer notre d\u00e9cision d\u2019ouvrir notre premier laboratoire en Europe continentale, dans une autre grande capitale culturelle et scientifique : Paris. Et je me r\u00e9jouis d\u2019autant plus que R\u00e9mi Munos, l\u2019un des principaux chercheurs de DeepMind et auteur de 150 articles scientifiques, fera son retour dans son pays, la France, o\u00f9 il dirigera ce nouveau laboratoire. \nLe laboratoire DeepMind de Paris se consacrera \u00e0 la recherche fondamentale en intelligence artificielle, en s\u2019appuyant sur les pr\u00e9c\u00e9dentes contributions scientifiques de R\u00e9mi Munos. Ces travaux portent aussi bien sur les m\u00e9thodes d\u2019apprentissage permettant \u00e0 un \nalgorithme unique\n d\u2019apprendre \u00e0 ex\u00e9cuter plusieurs t\u00e2ches diff\u00e9rentes \u2013 un \u00e9l\u00e9ment-cl\u00e9 de l\u2019intelligence \u2013 que sur les d\u00e9couvertes algorithmiques fondamentales comme l\u2019apprentissage par renforcement distributionnel.\nCertains \u00e9tablissements de recherche en intelligence artificielle, parmi les plus renomm\u00e9s au monde, dont des organismes publics de recherche comme l\u2019INRIA, o\u00f9 R\u00e9mi Munos est directeur de recherche, le CNRS et les Grandes \u00c9coles, sans parler d\u2019un exceptionnel r\u00e9seau d\u2019universit\u00e9s, ont d\u00e9j\u00e0 \u00e9lu domicile \u00e0 Paris. Avec l\u2019int\u00e9r\u00eat que porte le gouvernement fran\u00e7ais \u00e0 la fois \u00e0 l\u2019intelligence artificielle et \u00e0 l\u2019excellence en mati\u00e8re de recherche, l'Hexagone est devenu une destination privil\u00e9gi\u00e9e pour les scientifiques et ing\u00e9nieurs de stature internationale, contribuant \u00e0 cr\u00e9er une communaut\u00e9 de recherche prosp\u00e8re et performante.\nNous nous r\u00e9jouissons que R\u00e9mi Munos et son \u00e9quipe aient ainsi la chance d\u2019apporter plus encore leur contribution aux communaut\u00e9s de recherche dans lesquelles ils ont d\u00e9but\u00e9 leur carri\u00e8re. Comme nous l\u2019avons fait dans d\u2019autres laboratoires de recherche \u00e0 \nLondres\n et au \nCanada\n, o\u00f9 nos scientifiques ont \napport\u00e9 leur soutien \u00e0 des cours universitaires\n ainsi qu\u2019\u00e0 des\n chaires universitaires ind\u00e9pendantes\n, nous nous r\u00e9jouissons \u00e0 l\u2019id\u00e9e de collaborer \navec les chercheurs en intelligence artificielle de Google France\n mais aussi avec une communaut\u00e9 plus large, la capitale regorgeant de start-ups et de scientifiques prometteurs.\nJ'ai h\u00e2te de voir ce que R\u00e9mi et son \u00e9quipe vont r\u00e9aliser \u00e0 Paris, et suis tout aussi impatient de prendre plus souvent l\u2019Eurostar entre nos deux grandes villes!\nVoici ce que l\u2019on dit de DeepMind Paris\nWhen we set up our headquarters in London in 2010, we wanted to make DeepMind the best possible place to do cutting-edge AI research. We also wanted to help the wider AI community grow - publishing peer-reviewed papers (over 180 to date, and counting!) and sharing our insights to advance the field, supporting our staff to teach at local universities, and working with schools and NGOs to foster the next generation of scientists.\nAlongside enjoying the chance to support the UK\u2019s scientific success, we\u2019ve benefited a lot from the cultural influence of being in such a diverse and open city. AI needs to be developed with thoughtful consideration of the different needs of society, and - as much as any one city can provide that context - a multicultural capital like London, my hometown, is a great place to be.\nSo, I\u2019m excited to announce that we\u2019ve decided to open our first lab in continental Europe, in another of the world\u2019s great cultural and scientific capitals: Paris. And I\u2019m doubly excited that Remi Munos, one of DeepMind\u2019s principal research scientists, author of 150 research papers, and former professor at \u00c9cole Polytechnique will be returning home to France to lead this new lab.\nThe DeepMind Paris lab will focus on fundamental AI research, building on Remi\u2019s previous scientific contributions. These include new state-of-the-art methods that enable \nsingle AI systems\n to learn how to perform many different tasks - a core component of intelligence - as well as fundamental algorithmic breakthroughs such as \ndistributional reinforcement learning\n.\nParis is already home to some of the world\u2019s most influential AI research institutions, including public research centres like INRIA - where Remi is a senior researcher - as well as CNRS, the Grandes \u00c9coles, and an outstanding network of universities. With the French government\u2019s focus on AI and research excellence, the country has emerged as a leading destination for world-class scientists and engineers, creating a thriving and productive research community.\nWe\u2019re excited at the chance for Remi and his team to contribute even more to the communities where they started their careers. As we\u2019ve done in our other research labs in \nLondon\n and \nCanada\n, where our scientists have \nsupported academic courses\n and \nindependent university chairs\n, we\u2019re looking forward to collaborating with \nother AI researchers in Google France\n and the wider community, start-ups, and Paris\u2019 brilliant scientists of tomorrow.\nI can\u2019t wait to see what Remi and his team achieve in Paris, and am even looking forward to spending more time on the Eurostar train between our two great cities!\n"}
{"title": "Our first COO Lila Ibrahim takes DeepMind to the next level", "contents": "One of the greatest pleasures of coming to work every day at DeepMind is the chance to collaborate with brilliant researchers and engineers from so many different fields and perspectives - with machine learning experts alongside neuroscientists, physicists, mathematicians, roboticists, ethicists and more.\nThis level of interdisciplinary collaboration is both challenging and unusual, and it requires a unique type of organisation. We built DeepMind to combine the rigour and long-term thinking of the world\u2019s best scientific institutions, along with the focus, pace and energy common to the best tech startups. I believe this is essential if we\u2019re to fulfil the scientific and social promise of AI, and I\u2019m proud of all that \nwe\u2019ve achieved so far\n. But there\u2019s still a very long way to go!\nSo I\u2019m really pleased to welcome Lila Ibrahim to DeepMind as our first ever Chief Operating Officer, partnering with me to design, build and manage our next phase of growth. Having started out as a microprocessor designer and assembler programmer at Intel, Lila went on to lead the company\u2019s emerging markets product group, as well as working with Intel CEO Craig Barrett and then the legendary investor John Doerr at Kleiner Perkins as Chief of Staff. Most recently, Lila led the growth of the brilliant startup \nCoursera\n as their COO, helping to bring new educational opportunities to millions of people around the world.\nAlongside her organisational and technical experience, Lila is a passionate advocate for social impact in her work and her personal life, has been \nrecognised by the Anita Borg Institute for her commitment and vision\n, and is a Henry Crown Fellow at the \nAspen Institute\n. I can\u2019t wait for Lila to get started, and I know we\u2019ll gain a lot from her insight as we take DeepMind to the next level.\nHere\u2019s what Lila has to say:\nIt\u2019s a privilege to be working on this mission with so many world-class people, and Lila, Shane, Mustafa and I can\u2019t wait to see what we achieve together in the years ahead. And if you\u2019d like to be part of our amazing adventure \nthen please get in touch\n!\n"}
{"title": "Prefrontal cortex as a meta-reinforcement learning system", "contents": "Recently, AI systems have mastered a range of video-games such as Atari classics Breakout and Pong. \u00a0But as impressive as this performance is, AI still relies on the equivalent of thousands of hours of gameplay to reach and surpass the performance of human video game players. In contrast, we can usually grasp the basics of a video game we have never played before in a matter of minutes.\nThe question of why the brain is able to do so much more with so much less has given rise to the theory of meta-learning, or \u2018learning to learn\u2019. It is thought that we learn on two timescales \u2014 in the short term we focus on learning about specific examples while over longer timescales we learn the abstract skills or rules required to complete a task. It is this combination that is thought to help us learn efficiently and apply that knowledge rapidly and flexibly on new tasks. Recreating this meta-learning structure in AI systems \u2014 called meta-reinforcement learning \u2014 has proven very fruitful in facilitating fast, one-shot, learning in our agents (see \nour paper\n and closely related \nwork\n from OpenAI). However, the specific mechanisms that allow this process to take place in the brain are still largely unexplained in neuroscience.\nIn \nour new paper\n in Nature Neuroscience (Download a \nPDF here\n), we use the meta-reinforcement learning framework developed in AI research to investigate the role of dopamine in the brain in helping us to learn. Dopamine\u2014commonly known as the brain\u2019s pleasure signal\u2014has often been thought of as analogous to the reward prediction error signal used in AI reinforcement learning algorithms. These systems learn to act by trial and error guided by the reward. We propose that dopamine\u2019s role goes beyond just using reward to learn the value of past actions and that it plays an integral role, specifically within the prefrontal cortex area, in allowing us to learn efficiently, rapidly and flexibly on new tasks.\nWe tested our theory by virtually recreating six meta-learning experiments from the field of neuroscience\u2014each requiring an agent to perform tasks that use the same underlying principles (or set of skills) but that vary in some dimension. We trained a recurrent neural network (representing the prefrontal cortex) using standard deep reinforcement learning techniques (representing the role of dopamine) and then compared the activity dynamics of the recurrent network with real data taken from previous findings in neuroscience experiments. Recurrent networks are a good proxy for meta-learning because they are able to internalise past actions and observations and then draw on those experiences while training on a variety of tasks.\nOne experiment we recreated is known as the Harlow Experiment, a psychology test from the 1940s used to explore the concept of meta-learning. In the original test, a group of monkeys were shown two unfamiliar objects to select from, only one of which gave them a food reward. They were shown these two objects six times, each time the left-right placement was randomised so the monkey had to learn which object gave a food reward. They were then shown two brand new objects, again only one would result in a food reward. Over the course of this training, the monkey developed a strategy to select the reward associated-object: it learnt to select randomly the first time, and then based on the reward feedback to choose the particular object, rather than the left or right position, from then on. The experiment shows that monkeys could internalise the underlying principles of the task and learn an abstract rule structure \u2014 in effect, learning to learn.\nWhen we simulated a very similar test using a \nvirtual computer screen\n and randomly selected images, we found that our \u2018meta-RL agent\u2019 appeared to learn in a manner analogous to the animals in the Harlow Experiment, even when presented with entirely new images never seen before.\nIn fact, we found that the meta-RL agent could learn to quickly adapt in a wide domain of tasks with different rules and structures. And because the network learned how to adapt to a variety of \u00a0tasks, it also learned general principles about how to learn efficiently.\nImportantly, we saw that the majority of learning took place in the recurrent network, which supports our proposal that dopamine plays a more integral role in the meta-learning process than previously thought. Dopamine is traditionally understood to strengthen synaptic links in the prefrontal system, reinforcing particular behaviours. In AI, this means the dopamine-like reward signal adjusts the artificial synaptic weights in a neural network as it learns the right way to solve a task. However, in our experiments the weights of the neural network were frozen, meaning they couldn\u2019t be adjusted during the learning process, yet, the meta-RL agent was still able to solve and adapt to new tasks. This shows us that dopamine-like reward isn't only used to adjust weights, but it also conveys and encodes important information about abstract task and rule structure, allowing faster adaptation to new tasks.\nNeuroscientists have long observed similar patterns of neural activations in the prefrontal cortex, which is quick to adapt and flexible, but have struggled to find an adequate explanation for why that\u2019s the case. The idea that the prefrontal cortex isn\u2019t relying on slow synaptic weight changes to learn rule structures, but is using abstract model-based information directly encoded in dopamine, offers a more satisfactory reason for its versatility.\nIn demonstrating that the key ingredients thought to give rise to meta-reinforcement learning in AI also exist in the brain, we\u2019ve posed a theory that not only fits with what is known about both dopamine and prefrontal cortex but that also explains a range of mysterious findings from neuroscience and psychology. In particular, the theory sheds new light on how structured, model-based learning emerges in the brain, why dopamine itself contains model-based information, and how neurons in the prefrontal cortex become tuned to learning-related signals. Leveraging insights from AI which can be applied to explain findings in neuroscience and psychology highlights the value each field can offer the other. Going forward, we anticipate that much benefit can be gained in the reverse direction, by taking guidance from specific organisation of brain circuits in designing new models for learning in reinforcement learning agents.\nDownload the Nature Neuroscience paper \nhere\n.\nDownload an Open Access version of the paper \nhere\n.\nThis work was completed by Jane X. Wang, Zeb Kurth-Nelson, Dharshan Kumaran, Dhruva Tirumala, Hubert Soyer, Joel Z. Leibo, Demis Hassabis and Matthew Botvinick.\n"}
{"title": "DeepMind papers at ICLR 2018", "contents": "Between 30 April and 03 May, hundreds of researchers and engineers will gather in Vancouver, Canada, for the \nSixth International Conference on Learning Representations.\nHere you can read details of all DeepMind\u2019s accepted papers and find out where you can see the accompanying poster sessions and talks.\nAuthors: \nAbbas Abdolmaleki, Jost Tobias Springenberg, Nicolas Heess, Yuval Tassa, Remi Munos\nWe introduce a new algorithm for reinforcement learning called Maximum a posteriori Policy Optimisation (MPO) based on coordinate ascent on a relative entropy objective. We show that several existing methods can directly be related to our derivation. We develop two off-policy algorithms and demonstrate that they are competitive with the state-of-the-art in deep reinforcement learning. In particular, for continuous control, our method outperforms existing methods with respect to sample efficiency, premature convergence and robustness to hyperparameter settings.\nAuthors:\n Hanxiao Liu (CMU), Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, Koray Kavukcuoglu\nWe explore efficient neural architecture search methods and show that a simple yet powerful evolutionary algorithm can discover new architectures with excellent performance. Our approach combines a novel hierarchical genetic representation scheme that imitates the modularized design pattern commonly adopted by human experts, and an expressive search space that supports complex topologies. Our algorithm efficiently discovers architectures that outperform a large number of manually designed models for image classification, obtaining top-1 error of 3.6% on CIFAR-10 and 20.3% when transferred to ImageNet, which is competitive with the best existing neural architecture search approaches. We also present results using random search, achieving 0.3% less top-1 accuracy on CIFAR-10 and 0.1% less on ImageNet whilst reducing the search time from 36 hours down to 1 hour.\nAuthors: \nKarol Hausman, Jost Tobias Springenberg, Ziyu Wang, Nicolas Heess, Martin Riedmiller\nWe present a method for reinforcement learning of closely related skills that are parameterized via a skill embedding space. \u00a0We learn such skills by taking advantage of latent variables and exploiting a connection between reinforcement learning and variational inference.\nThe main contribution of our work is an entropy-regularized policy gradient formulation for hierarchical policies, and an associated, data-efficient and robust off-policy gradient algorithm based on stochastic value gradients. We demonstrate the effectiveness of our method on several simulated robotic manipulation tasks.\nAuthors: \nBrandon Amos, Laurent Dinh, Serkan Cabi, Thomas Roth\u00f6rl, Sergio G\u00f3mez Colmenarejo, Alistair M Muldal, Tom Erez, Yuval Tassa, Nando de Freitas, Misha Denil\nWe show that models trained to predict proprioceptive information about an agent's body come to represent objects in the external world. The models able to successfully predict sensor readings over 100 steps into the future and continue to represent the shape of external objects even after contact is lost. We show that active data collection by maximizing uncertainty over future sensor readings leads to models that show superior performance when used for control. We also collect data from a real robotic hand and show that the same models can be used to answer questions about the properties of objects in the real world.\nAuthors: \nJames Martens, Jimmy Ba (Vector Institute), \u00a0Matthew Johnson (Google)\nKronecker-factor Approximate Curvature (Martens & Grosse, 2015) (K-FAC) is a 2nd-order optimization method which has been shown to give state-of-the-art performance on large-scale neural network optimization tasks (Ba et al., 2017). \u00a0It is based on an approximation to the Fisher information matrix (FIM) that makes assumptions about the particular structure of the network and the way it is parameterized. The original K-FAC method was applicable only to fully-connected networks, although it has been recently extended by Grosse & Martens (2016) to handle convolutional networks as well. In this work we extend the method to handle RNNs by introducing a novel approximation to the FIM for RNNs. This approximation works by modelling the covariance structure between the gradient contributions at different time-steps using a chain-structured linear Gaussian graphical model, summing the various cross-covariances, and computing the inverse in closed form. We demonstrate in experiments that our method significantly outperforms general purpose state-of-the-art optimizers like SGD with momentum and Adam on several challenging RNN training tasks.\nAuthors: \nGabriel Barth-maron, Matthew Hoffman, David Budden, Will Dabney, Daniel Horgan, Dhruva Tirumala Bukkapatnam, Alistair M Muldal, Nicolas Heess, Timothy Lillicrap\nThis work adopts the very successful distributional perspective on reinforcement learning and adapts it to the continuous control setting. We combine this within a distributed framework for off-policy learning in order to develop what we call the Distributed Distributional Deep Deterministic Policy Gradient algorithm, D4PG. We also combine this technique with a number of additional, simple improvements such as the use of N-step returns and prioritized experience replay. Experimentally we examine the contribution of each of these individual components, and show how they interact, as well as their combined contributions. Our results show that across a wide variety of simple control tasks, difficult manipulation tasks, and a set of hard obstacle-based locomotion tasks the D4PG algorithm achieves state of the art performance.\nAuthors: \nYan Wu, Greg Wayne, Alex Graves, Timothy Lillicrap\nWe present an end-to-end trained memory system that quickly adapts to new data and generates samples like them. The memory is analytically tractable, which enables optimal on-line compression via a Bayesian update-rule. We formulate it as a hierarchical conditional generative model, where memory provides a rich data-dependent prior distribution. Consequently, the top-down memory and bottom-up perception are combined to produce the code representing an observation.\nAuthors: \nPablo Sprechmann, Siddhant Jayakumar, Jack Rae, Alexander Pritzel, Adria P Badia \u00b7 Benigno Uria, Oriol Vinyals, Demis Hassabis, Razvan Pascanu, Charles Blundell\nHumans and animals are able to incorporate new knowledge quickly from a few examples, continually throughout much of their lifetime. In contrast, neural network-based models rely on the data distribution being stationary and a gradual training procedure to obtain good generalisation. \u00a0Drawing inspiration from the theory of complementary learning systems, we propose Memory-based Parameter Adaptation (MbPA), a method for augmenting neural networks with an episodic memory to allow for rapid acquisition of new knowledge while preserving the high performance and good generalisation of standard deep models. MbPA, stores examples in memory and then uses a context-based lookup to directly modify the weights of a neural network. It alleviates several shortcomings of neural networks, such as catastrophic forgetting, fast, stable acquisition of new knowledge, and fast learning during evaluation.\nAuthors: \nIrina Higgins, Nicolas Sonnerat, Loic Matthey, Arka Pal, Christopher P Burgess, Matko Bo\u0161njak, Murray Shanahan, Matthew Botvinick, \u00a0Alexander Lerchner\nWe propose a novel theoretical approach to address the problem of abstract compositionality - how can we learn a small number of grounded building blocks and use them to create a vast number of new abstract concepts on the fly? We present a new neural network architecture called the Symbol-Concept Association Network (SCAN), that can learn a grounded visual concept hierarchy, enabling it to imagine novel concepts guided by language instructions.\nAuthors: \nAngeliki Lazaridou, Karl M Hermann, Karl Tuyls, Stephen Clark\nThe ability of algorithms to evolve or learn (compositional) communication protocols has traditionally been studied in the language evolution literature through the use of emergent communication tasks. Here we scale up this research by using contemporary deep learning methods and by training reinforcement-learning neural network agents on referential communication games. We extend previous work, in which agents were trained in symbolic environments, by developing agents which are able to learn from raw pixel data, a more challenging and realistic input representation. We find that the degree of structure found in the input data affects the nature of the emerged protocols, and thereby corroborate the hypothesis that structured compositional language is most likely to emerge when agents perceive the world as being structured.\nAuthors:\n William Fedus (Universit\u00e9 de Montr\u00e9al), Mihaela Rosca, Balaji Lakshminarayanan, Andrew Dai (Google), Shakir Mohamed, \u00a0Ian Goodfellow (Google Brain)\nThe field of generative adversarial networks research has grown, fueled by the successes of their application in computer vision. In an attempt to solve training instability in generative adversarial networks, multiple theoretical justifications for training dynamics have been suggested and new training methods proposed. By focusing on the divergence minimization view of generative adversarial networks and regularizers such as gradient penalties, we empirically show that the success of some of these approaches cannot be solely explained by the accompanying underlying theory. This motivates the need for new theoretical framework that can encompass and explains the presented results.\nAuthors: \nRichard Evans, David Saxton, David Amos, Pushmeet Kohli, Edward Grefenstette\nWe introduce a new dataset of logical entailments for the purpose of measuring models' ability to capture and exploit the structure of logical expressions against an entailment prediction task. We use this task to compare a series of architectures which are ubiquitous in the sequence-processing literature, in addition to a new model class\u2013PossibleWorldNets\u2013which computes entailment as a \"convolution over possible worlds\". Results show that convolutional networks present the wrong inductive bias for this class of problems relative to LSTM RNNs, tree-structured neural networks outperform LSTM RNNs due to their enhanced ability to exploit the syntax of logic, and PossibleWorldNets outperform all benchmarks.\nAuthors: \nDaniel Horgan, John Quan, David Budden, Gabriel Barth-maron, Matteo Hessel, Hado van Hasselt, David Silver\nWe propose a distributed architecture for deep reinforcement learning at scale, that enables agents to learn effectively from orders of magnitude more data than previously possible. The algorithm decouples acting from learning: the actors interact with their own instances of the environment by selecting actions according to a shared neural network, and accumulate the resulting experience in a shared experience replay memory; the learner replays samples of experience and updates the neural network. The architecture relies on prioritized experience replay to focus only on the most significant data generated by the actors. Our architecture substantially improves the state of the art on the Arcade Learning Environment, achieving better final performance in a fraction of the wall-clock training time.\nAuthors:\n Audrunas Gruslys, Will Dabney, Mohammad Gheshlaghi Azar, Bilal Piot, Marc G Bellemare, Remi Munos\nWe propose multiple algorithmic and architectural improvements producing an agent with a higher sample-efficiency than Prioritized Dueling DQN and Categorical DQN, while giving better run-time performance than A3C. Distributional Retrace policy evaluation algorithm brings multi-step off-policy updates to the distributional reinforcement learning setting. Our approach can be used to convert several classes of multi-step policy evaluation algorithms into distributional ones. \u03b2-leave-one-out policy gradient algorithm uses action values as a baseline. A new prioritized replay algorithm exploits temporal locality for more efficient replay prioritization. Reactor reaches state-of-the-art performance after 200 million frames in less than a day.\nAuthors:\n David Pfau, Christopher P Burgess\nSpectral algorithms for learning low-dimensional data manifolds have largely been supplanted by deep learning methods in recent years. One reason is that classic spectral manifold learning methods often learn collapsed embeddings that do not fill the embedding space. We show that this is a natural consequence of data where different latent dimensions have dramatically different scaling in observation space. We present a simple extension of Laplacian Eigenmaps to fix this problem based on choosing embedding vectors which are both orthogonal and \\textit{minimally redundant} to other dimensions of the embedding. In experiments on NORB and similarity-transformed faces we show that Minimally Redundant Laplacian Eigenmap (MR-LEM) significantly improves the quality of embedding vectors over Laplacian Eigenmaps, accurately recovers the latent topology of the data, and discovers many disentangled factors of variation of comparable quality to state-of-the-art deep learning methods.\nAuthors: \nAri Morcos, David GT Barrett, Neil C Rabinowitz, Matthew Botvinick\nOur investigation into the importance of single directions for generalisation...uses an approach inspired by decades of experimental neuroscience - exploring the impact of damage - to determine: how important are small groups of neurons in deep neural networks? Are more easily interpretable neurons also more important to the network\u2019s computation? We measured the performance impact of damaging the network by deleting individual neurons as well as groups of neurons. Our experiments led to two surprising findings: 1. Although many previous studies have focused on understanding easily interpretable individual neurons (e.g. \u201ccat neurons\u201d, or neurons in the hidden layers of deep networks which are only active in response to images of cats), we found that these interpretable neurons are no more important than confusing neurons with difficult-to-interpret activity. 2. Networks which correctly classify unseen images are more resilient to neuron deletion than networks which can only classify images they have seen before. In other words, networks which generalise well are much less reliant on single directions than those which memorise.\nAuthors:\n Dani Yogatama, Yishu Miao, G\u00e1bor Melis, Wang Ling, Adhiguna Kuncoro, Chris Dyer, Phil Blunsom\nGenerating fluent, grammatical language requires keeping track of what words have been generated in the past. In this paper, we compare three memory architectures (sequential, random access, and stack-based) and find that a stack-structured memory demonstrates the best performance in terms of held-out perplexity. To give the stack memory more power and better match the phenomena encountered in language, we introduce a generalization of existing differentiable stack memories enabling them to execute multiple pop operations at each timestep, which further improves performance. Finally, we show that our stack-augmented language model correctly learns to predict difficult long-range agreement patterns which are difficult for conventional LSTM language models.\nAuthors: \nScott Reed, Yutian Chen, Thomas Paine, Aaron van den Oord, S. M. Ali Eslami, Danilo J Rezende, Oriol Vinyals, Nando de Freitas\nCurrent image density models require large amounts of data and computation time for training. In this paper, we show how 1) neural attention and 2) meta learning techniques can be used in combination with autoregressive models to enable effective few-shot density estimation. Our modified PixelCNNs result in state-of-the art few-shot density estimation on Omniglot. We visualize the learned attention policy and find that it learns intuitive algorithms for simple tasks such as image mirroring and digit drawing on Omniglot without supervision. Finally, we demonstrate few-shot image generation on the Stanford Online Products dataset.\nAuthors: \nG\u00e1bor Melis, Chris Dyer, Phil Blunsom\nOngoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing code bases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.\nAuthors:\n Kris Cao, Angeliki Lazaridou, Marc Lanctot, Joel Z Leibo, Karl Tuyls, Stephen Clark\nMulti-agent reinforcement learning offers a way to study how communication could emerge in communities of agents needing to solve specific problems. In this paper, we study the emergence of communication in the negotiation environment, a semi-cooperative model of agent interaction. We introduce two communication protocols \u2013 one grounded in the semantics of the game, and one which is a priori ungrounded and is a form of cheap talk. We show that self-interested agents can use the pre-grounded communication channel to negotiate fairly, but are unable to effectively use the ungrounded channel. However, prosocial agents do learn to use cheap talk to find an optimal negotiating strategy, suggesting that cooperation is necessary for language to emerge. We also study communication behaviour in a setting where one agent interacts with agents in a community with different levels of prosociality and show how agent identifiability can aid negotiation.\nAuthors:\n Edward Choi, Angeliki Lazaridou, Nando de Freitas\nOne of the distinguishing aspects of human language is its compositionality, which allows us to describe complex environments with limited vocabulary. Previously, it has been shown that neural network agents can learn to communicate in a highly structured, possibly compositional language based on disentangled input (e.g. hand-engineered features). Humans, however, do not learn to communicate based on well-summarized features. In this work, we train neural agents to simultaneously develop visual perception from raw image pixels, and learn to communicate with a sequence of discrete symbols. The agents play an image description game where the image contains factors such as colors and shapes. We train the agents using the obverter technique where an agent introspects to generate messages that maximize its own understanding. Through qualitative analysis, visualization and a zero-shot test, we show that the agents can develop, out of raw image pixels, a language with compositional properties, given a proper pressure from the environment.\nAuthors: \nMeire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Matteo Hessel, Ian Osband, Alex Graves, Volodymyr Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, Charles Blundell, Shane Legg\nWe introduce NoisyNet, a deep reinforcement learning agent with parametric noise added to its weights, and show that the induced stochasticity of the agent's policy can be used to aid efficient exploration. The parameters of the noise are learned with gradient descent along with the remaining network weights. NoisyNet is straightforward to implement and adds little computational overhead. We find that replacing the conventional exploration heuristics for A3C, DQN and dueling agents (entropy reward and \u03f5-greedy respectively) with NoisyNet yields substantially higher scores for a wide range of Atari games, in some cases advancing the agent from sub to super-human performance.\n"}
{"title": "DeepMind, meet Android", "contents": "We\u2019re delighted to announce a new collaboration between \nDeepMind for Google\n and Android, the world\u2019s most popular mobile operating system. Together, we\u2019ve created two new features that will be available to people with devices running Android P later this year:\nThis is an exciting first for us. Our previous work with Google has been built on massive-scale infrastructure, including projects to \nreduce energy use\n in data centres, optimise recommendations in Google Play, and bring WaveNet voices to the \nGoogle Assistant\n and to \nGoogle Cloud Platform\ncustomers across the world.\nBut this time we\u2019re deploying techniques that run on the compute power of a single mobile device\u2014that\u2019s orders of magnitude less than typical machine learning applications. Here\u2019s how they work:\nAndroid has worked to improve battery life with each release of its operating system. That shouldn\u2019t be surprising, since \nsurveys\n of smartphone users show that battery life is a top priority.\nToday, battery power is spent keeping apps up-to-date in the \nbackground\n so that they\u2019re fresh when users open them next. But nobody uses all the apps on their phone with the same frequency, so in many cases that battery use might not be necessary.\nTo help tackle this, we\u2019ve partnered with the Android team to develop a feature called Adaptive Battery that uses a deep \nconvolutional neural net\n to predict which apps you\u2019ll use in the next few hours and which you probably won\u2019t use until later.\nUsing that knowledge, Android adapts to your usage patterns so that it only spends battery power on the apps you\u2019ll need. The initial results have been very promising, and we\u2019ve seen a significant reduction in background activity in our internal testing.\nSometimes your screen can be frustratingly dim on a bright, sunny day or too bright when you reach over to check your phone in the middle of the night. That\u2019s because the system is one size fits all, and doesn't account for your personal preferences. So you need to make adjustments manually\u2014for example, turning screen brightness down to read in bed at night and back up when you wake up in the morning.\nTo improve this experience, we've partnered with Android to incorporate machine learning into a feature called Adaptive Brightness. The feature now learns how you set the brightness slider for the ambient light of your surroundings, and then adjusts the screen brightness according to your preferences. During our internal testing, a considerable proportion of Android P users made fewer manual brightness adjustments.\nWe\u2019re excited to be working with the amazing Android team to try to save energy while making people\u2019s lives easier, and look forward to more to come. And if you\u2019re interested in working on real-world machine learning challenges, from global-scale infrastructure to on-device optimisation, then the DeepMind for Google team is \nalways looking for exceptional people\n!\n"}
{"title": "DeepMind Health Response to Independent Reviewers' Report 2018", "contents": "When we set up DeepMind Health we believed that pioneering technology should be matched with pioneering oversight. That\u2019s why when we launched in February 2016, we did so with an unusual and additional mechanism: a panel of Independent Reviewers, who meet regularly throughout the year to scrutinise our work. This is an innovative approach within tech companies - one that forces us to question not only what we are doing, but how and why we are doing it - and we believe that their robust challenges make us better.\nIn their report last year, the Independent Reviewers asked us important questions about our engagement with stakeholders, data governance, and the behavioural elements that need to be considered when deploying new technologies in clinical environments. We\u2019ve done a lot over the past twelve months to address these questions, and we\u2019re really proud that this year\u2019s Annual Report recognises the progress we\u2019ve made.\nOf course, \nthis year\u2019s report\n includes a series of new recommendations for areas where we can continue to improve, which we\u2019ll be working on in the coming months. In particular:\nWe want to take this opportunity to thank the Independent Reviewers for their thoughtful and committed engagement with our work. By holding us to account, recognising where we\u2019re getting it right and challenging us where we can improve, they\u2019ll help us to do a better job for patients, nurses, doctors, carers, families, and all those who rely on healthcare systems around the world.\n"}
{"title": "Measuring abstract reasoning in neural networks", "contents": "Neural network-based models continue to achieve impressive results on longstanding machine learning problems, but establishing their capacity to reason about abstract concepts has proven difficult. Building on previous efforts to solve this important feature of general-purpose learning systems, our \nlatest paper\n sets out an approach for measuring abstract reasoning in learning machines, and reveals some important insights about the nature of generalisation itself.\nTo understand why abstract reasoning is critical for general intelligence, consider Archimedes\u2019 famous \u201cEureka!\u201d moment: by noticing that the volume of an object is equivalent to the volume of water that the object displaces, he understood volume at a conceptual level, and was therefore able to reason about the volume of other irregularly shaped objects.\nWe would like AI to have similar capabilities. While current systems can defeat world champions in complicated strategic games, they often struggle on other apparently simple tasks, especially when an abstract concept needs to be discovered and reapplied in a new setting. For example, if specifically trained to only count triangles, then even our best AI systems can still fail to count squares, or any other previously unencountered object.\nTo build better, more intelligent systems it is therefore important to understand the ways in which neural networks are currently able to process abstract concepts, and where they still need improvement. To begin doing this, we took inspiration from the methods used to measure abstract reasoning in human IQ tests.\nStandard human IQ tests often require test-takers to interpret perceptually simple visual scenes by applying principles that they have learned through everyday experience. For example, human test-takers may have already learned about \u2018progressions\u2019 (the notion that some attribute can increase) by watching plants or buildings grow, by studying addition in a mathematics class, or by tracking a bank balance as interest accrues. They can then apply this notion in the puzzles to infer that the number of shapes, their sizes, or even the intensity of their colour will increase along a sequence.\nWe do not yet have the means to expose machine learning agents to a similar stream of \u2018everyday experiences\u2019, meaning we cannot easily measure their ability to transfer knowledge from the real world to visual reasoning tests. Nonetheless, we can create an experimental set-up that still puts human visual reasoning tests to good use. Rather than study knowledge transfer from everyday life to visual reasoning problems (as in human testing), we instead studied knowledge transfer from one controlled set of visual reasoning problems to another.\nTo achieve this, we built a generator for creating matrix problems, involving a set of abstract factors, including relations like \u2018progression\u2019 and attributes like \u2018colour\u2019 and \u2018size\u2019. While the question generator uses a small set of underlying factors, it can nonetheless create an enormous number of unique questions.\nNext, we constrained the factors or combinations available to the generator to create different sets of problems for training and testing our models, to measure how well our models can generalise to held-out test sets. For instance, we created a training set of puzzles in which the progression relation is only encountered when applied to the colour of lines, and a test set when it is applied to the size of shapes. If a model performs well on this test set, it would provide evidence for an ability to infer and apply the abstract notion of progression, even in situations in which it had never previously seen a progression.\nIn the typical generalisation regime applied in machine learning evaluations, where training and test data are sampled from the same underlying distribution, all of the networks we tested exhibited good generalisation error, with some achieving impressive absolute performance at just above 75%. The best performing network explicitly computed relations between different image panels and evaluated the suitability of each potential answer in parallel. We call this architecture a Wild Relation Network (WReN).\nWhen required to reason using attribute values \u2018interpolated\u2019 between previously seen attribute values, and also when applying known abstract relations in unfamiliar combinations, the models generalised notably well. However, the same network performed much worse in the \u2018extrapolation\u2019 regime, where attribute values in the test set did not lie within the same range as those seen during training. An example of this occurs for puzzles that contain dark coloured objects during training and light coloured objects during testing. Generalisation performance was also worse when the model was trained to apply a previously seen relation, such as a progression on the number of shapes, to a new attribute, such as size.\nFinally, we observed improved generalisation performance when the model was trained to predict not only the correct answer, but also the \u2018reason\u2019 for the answer (i.e. the particular relations and attributes that should be considered to solve the puzzle). Interestingly, in the neutral split, the model\u2019s accuracy was strongly correlated with its ability to infer the correct relation underlying the matrix: when the explanation was right, the model would choose the correct answer 87% of the time, but when its explanation was wrong this performance dropped to only 32%. This suggests that models which achieved better performance when they correctly inferred the abstract concepts underlying the task.\nRecent literature has focussed on the strengths and weaknesses of neural network-based approaches to machine learning problems, often based around their capacity or failure to generalise. Our results show that it might be unhelpful to draw universal conclusions about generalisation: the neural networks we tested performed well in certain regimes of generalisation and very poorly in others. Their success was determined by a range of factors, including the architecture of the model used and whether the model was trained to provide an interpretable \u201creason\u201d for its answer choices. In almost all cases, the systems performed poorly when required to extrapolate to inputs beyond their experience, or to deal with entirely unfamiliar attributes; creating a clear focus for future work in this critical, and important area of research.\nTo encourage and support further research towards improved generalisation and abstract reasoning, we have made our dataset publicly available \nhere\n.\nDownload the paper [\nPDF\n] and supplementary material [\nPDF\n].\nThis work was done by David G. T. Barrett*, Felix Hill*, Adam Santoro*, Ari Morcos and Timothy Lillicrap.\n\u200d\n"}
{"title": "Royal Free London publishes findings of legal audit in use of Streams", "contents": "Last July, the Information Commissioner concluded an investigation into the use of the Streams app at the Royal Free London NHS Foundation Trust. As part of the investigation the Royal Free signed up to a set of undertakings \u2013 one of which was to commission a third party to audit the Royal Free\u2019s current data processing arrangements with DeepMind, to ensure that they fully complied with data protection law and respected the privacy and confidentiality rights of its patients.You can read the full report on the Royal Free\u2019s website \nhere\n, and the Information Commissioner\u2019s Office\u2019s response \nhere\n. The report also has three recommendations that relate to DeepMind Health:\nWe look forward to continuing our collaboration with the Royal Free to \nimprove care for patients\n. \u00a0You can read more about the way in which we process and protect patient data \nhere\n and you can read our previous blog about what we\u2019ve learned \nhere\n.\n"}
{"title": "Neural scene representation and rendering", "contents": "There is more than meets the eye when it comes to how we understand a visual scene: our brains draw on prior knowledge to reason and to make inferences that go far beyond the patterns of light that hit our retinas. For example, when entering a room for the first time, you instantly recognise the items it contains and where they are positioned. If you see three legs of a table, you will infer that there is probably a fourth leg with the same shape and colour hidden from view. Even if you can\u2019t see everything in the room, you\u2019ll likely be able to sketch its layout, or imagine what it looks like from another perspective.\nThese visual and cognitive tasks are seemingly effortless to humans, but they represent a significant challenge to our artificial systems. Today, state-of-the-art visual recognition systems are trained using large datasets of annotated images produced by humans. Acquiring this data is a costly and time-consuming process, requiring individuals to label every aspect of every object in each scene in the dataset. As a result, often only a small subset of a scene\u2019s overall contents is captured, which limits the artificial vision systems trained on that data. As we develop more complex machines that operate in the real world, we want them to fully understand their surroundings: where is the nearest surface to sit on? What material is the sofa made of? Which light source is creating all the shadows? Where is the light switch likely to be?\nIn this work, \npublished in Science\n (\nOpen Access version\n), we introduce the Generative Query Network (GQN), a framework within which machines learn to perceive their surroundings by training only on data obtained by themselves as they move around scenes. Much like infants and animals, the GQN learns by trying to make sense of its observations of the world around it. In doing so, the GQN learns about plausible scenes and their geometrical properties, without any human labelling of the contents of scenes.\nThe GQN model is composed of two parts: a representation network and a generation network. The representation network takes the agent's observations as its input and produces a representation (a vector) which describes the underlying scene. The generation network then predicts (\u2018imagines\u2019) the scene from a previously unobserved viewpoint.\nThe representation network does not know which viewpoints the generation network will be asked to predict, so it must find an efficient way of describing the true layout of the scene as accurately as possible. It does this by capturing the most important elements, such as object positions, colours and the room layout, in a concise distributed representation. During training, the generator learns about typical objects, features, relationships and regularities in the environment. This shared set of \u2018concepts\u2019 enables the representation network to describe the scene in a highly compressed, abstract manner, leaving it to the generation network to fill in the details where necessary. For instance, the representation network will succinctly represent \u2018blue cube\u2019 as a small set of numbers and the generation network will know how that manifests itself as pixels from a particular viewpoint.\nWe performed controlled experiments on the GQN in a collection of procedurally-generated environments in a simulated 3D world, containing multiple objects in random positions, colours, shapes and textures, with randomised light sources and heavy occlusion. After training on these environments, we used GQN\u2019s representation network to form representations of new, previously unobserved scenes. We showed in our experiments that the GQN exhibits several important properties:\nGQN builds upon a large literature of recent related work in multi-view geometry, generative modelling, unsupervised learning and predictive learning, which we discuss \nhere\n, in the \nScience paper\n and the \nOpen Access version\n. It illustrates a novel way to learn compact, grounded representations of physical scenes. Crucially, the proposed approach does not require domain-specific engineering or time-consuming labelling of the contents of scenes, allowing the same model to be applied to a range of different environments. It also learns a powerful neural renderer that is capable of producing accurate images of scenes from new viewpoints.\nOur method still has many limitations when compared to more traditional computer vision techniques, and has currently only been trained to work on synthetic scenes. However, as new sources of data become available and advances are made in our hardware capabilities, we expect to be able to investigate the application of the GQN framework to higher resolution images of real scenes. In future work, it will also be important to explore the application of GQNs to broader aspects of scene understanding, for example by querying across space and time to learn a common sense notion of physics and movement, as well as applications in virtual and augmented reality.\nWhile there is still much more research to be done before our approach is ready to be deployed in practice, we believe this work is a sizeable step towards fully autonomous scene understanding.\nThis work was done by S. M. Ali Eslami, Danilo J. Rezende, Frederic Besse, Fabio Viola, Ari S. Morcos, Marta Garnelo, Avraham Ruderman, Andrei A. Rusu, Ivo Danihelka, Karol Gregor, David P. Reichert, Lars Buesing, Theophane Weber, Oriol Vinyals, Dan Rosenbaum, Neil Rabinowitz, Helen King, Chloe Hillier, Matt Botvinick, Daan Wierstra, Koray Kavukcuoglu and Demis Hassabis.\n"}
{"title": "A major milestone for the treatment of eye disease", "contents": "We are delighted to announce the results of the first phase of our joint research partnership with \nMoorfields Eye Hospital\n, which could potentially transform the management of sight-threatening eye disease.\nThe results, published online in \nNature Medicine\n (open access full text, see end of blog), show that our AI system can quickly interpret eye scans from routine clinical practice with unprecedented accuracy. It can correctly recommend how patients should be referred for treatment for over 50 sight-threatening eye diseases as accurately as world-leading expert doctors.\nThese are early results, but they show that our system could handle the wide variety of patients found in routine clinical practice. In the long term, we hope this will help doctors quickly prioritise patients who need urgent treatment \u2013 which could ultimately save sight.\nCurrently, eyecare professionals use optical coherence tomography (OCT) scans to help diagnose eye conditions. These 3D images provide a detailed map of the back of the eye, but they are hard to read and need expert analysis to interpret.\nThe time it takes to analyse these scans, combined with the sheer number of scans that healthcare professionals have to go through (over 1,000 a day at Moorfields alone), can lead to lengthy delays between scan and treatment \u2013 even when someone needs urgent care. If they develop a sudden problem, such as a bleed at the back of the eye, these delays could even cost patients their sight.\nThe system we have developed seeks to address this challenge. Not only can it automatically detect the features of eye diseases in seconds, but it can also prioritise patients most in need of urgent care by recommending whether they should be referred for treatment. This instant triaging process should drastically cut down the time elapsed between the scan and treatment, helping sufferers of diabetic eye disease and age-related macular degeneration avoid sight loss.\nWe don\u2019t just want this to be an academically interesting result \u2013 we want it to be used in real treatment. So our paper also takes on one of the key barriers for AI in clinical practice: the \u201cblack box\u201d problem. For most AI systems, it\u2019s very hard to understand exactly why they make a recommendation. That\u2019s a huge issue for clinicians and patients who need to understand the system\u2019s reasoning, not just its output \u2013 the why as well as the what.\nOur system takes a novel approach to this problem, combining two different neural networks with an easily interpretable representation between them. The first neural network, known as the segmentation network, analyses the OCT scan to provide a map of the different types of eye tissue and the features of disease it sees, such as haemorrhages, lesions, irregular fluid or other symptoms of eye disease. This map allows eyecare professionals to gain insight into the system\u2019s \u201cthinking.\u201d The second network, known as the classification network, analyses this map to present clinicians with diagnoses and a referral recommendation. Crucially, the network expresses this recommendation as a percentage, allowing clinicians to assess the system\u2019s confidence in its analysis.\nThis functionality is critically important, since eyecare professionals are always going to play a key role in deciding the type of care and treatment a patient receives. Enabling them to scrutinise the technology\u2019s recommendations is key to making the system usable in practice.\nOn top of this, our technology can be easily applied to different types of eye scanners, and not just the specific type of device it was trained on at Moorfields. This might seem inconsequential, but it means that the technology could be applied across the world with relative ease, massively increasing the number of patients who could potentially benefit. This also ensures the system can still be used in hospitals and other clinical settings even as OCT scanners are upgraded or replaced over time.\nWhile we\u2019re incredibly proud of this progress, this \ninitial research\n [PDF] would need to be turned into a product and then undergo rigorous clinical trials and regulatory approval before being used in practice. But we\u2019re confident that, in time, this system could transform the diagnosis, treatment and management of eye disease.\nOur partners at Moorfields want our research to help them improve care, reduce some of the strain on clinicians, and lower costs - all at the same time. So we\u2019ve also worked hard on what comes next.\nIf this technology is validated for general use by clinical trials, Moorfields\u2019 clinicians will be able to use it for free across all 30 of their UK hospitals and community clinics, for an initial period of five years. These clinics serve 300,000 patients a year and receive over 1,000 OCT scan referrals every day \u2013 each of which could benefit from improved accuracy and speed of diagnosis.\nWe\u2019re also proud that the work we\u2019ve put into this project will help accelerate many other NHS research efforts. The original dataset held by Moorfields was suitable for clinical use, but not for machine learning research. So we\u2019ve invested significantly in cleaning up, curating and labelling the dataset to create one of the best AI-ready databases for eye research in the world.\nThis improved database is owned by Moorfields as a non-commercial public asset, and it\u2019s already been used by hospital researchers for nine separate studies into a wide range of conditions - with many more to come. Moorfields can also use DeepMind\u2019s trained AI model for their future non-commercial research efforts.\nFor all of us who have worked on this since we signed our agreement with Moorfields in 2016, this is a hugely exciting milestone, and another indication of what is possible when clinicians and technologists work together. We\u2019ll continue to keep you updated as we make progress.\nRead the full text open access paper on Nature Medicine \nhere\nDownload the Author's version \nhere\n [PDF]\n"}
{"title": "Safety-first AI for autonomous data centre cooling and industrial control", "contents": "Many of society\u2019s most pressing problems have grown increasingly complex, so the search for solutions can feel overwhelming. At DeepMind and Google, we believe that if we can use AI as a tool to discover new knowledge, solutions will be easier to reach.\nIn 2016, we jointly developed an \nAI-powered recommendation system\n to improve the energy efficiency of Google\u2019s already highly-optimised data centres. Our thinking was simple: even minor improvements would provide significant energy savings and reduce CO2 emissions to help combat climate change.\nNow we\u2019re taking this system to the next level: instead of human-implemented recommendations, our AI system is directly controlling data centre cooling, while remaining under the expert supervision of our data centre operators. This first-of-its-kind cloud-based control system is now safely delivering energy savings in multiple Google data centres.\nEvery five minutes, our cloud-based AI pulls a snapshot of the data centre cooling system from thousands of sensors and feeds it into our deep neural networks, which predict how different combinations of potential actions will affect future energy consumption. The AI system then identifies which actions will minimise the energy consumption while satisfying a robust set of safety constraints. Those actions are sent back to the data centre, where the actions are verified by the local control system and then implemented.\nThe idea evolved out of feedback from our data centre operators who had been using our AI recommendation system. They told us that although the system had taught them some new best practices\u2014such as spreading the cooling load across more, rather than less, equipment\u2014implementing the recommendations required too much operator effort and supervision. Naturally, they wanted to know whether we could achieve similar energy savings without manual implementation.\nWe\u2019re pleased to say the answer was yes!\nGoogle's data centres contain thousands of servers that power popular services including Google Search, Gmail and YouTube. Ensuring that they run reliably and efficiently is mission-critical. We've designed our AI agents and the underlying control infrastructure from the ground up with safety and reliability in mind, and use eight different mechanisms to ensure the system will behave as intended at all times.\nOne simple method we\u2019ve implemented is to estimate uncertainty. For every potential action\u2014and there are billions\u2014our AI agent calculates its confidence that this is a good action. Actions with low confidence are eliminated from consideration.\nAnother method is two-layer verification. Optimal actions computed by the AI are vetted against an internal list of safety constraints defined by our data centre operators. Once the instructions are sent from the cloud to the physical data centre, the local control system verifies the instructions against its own set of constraints. This redundant check ensures that the system remains within local constraints and operators retain full control of the operating boundaries.\nMost importantly, our data centre operators are always in control and can choose to exit AI control mode at any time. In these scenarios, the control system will transfer seamlessly from AI control to the on-site rules and heuristics that define the automation industry today.\nFind out about the other safety mechanisms we\u2019ve developed, below:\nWhereas our original recommendation system had operators vetting and implementing actions, our new AI control system directly implements the actions. We\u2019ve purposefully constrained the system\u2019s optimisation boundaries to a narrower operating regime to prioritise safety and reliability, meaning there is a risk/reward trade off in terms of energy reductions.\nDespite being in place for only a matter of months, the system is already delivering consistent energy savings of around 30 percent on average, with further expected improvements. That\u2019s because these systems get better over time with more data, as the graph below demonstrates. Our optimisation boundaries will also be expanded as the technology matures, for even greater reductions.\nOur direct AI control system is finding yet more novel ways to manage cooling that have surprised even the data centre operators. Dan Fuenffinger, one of Google\u2019s data centre operators who has worked extensively alongside the system, remarked: \"It was amazing to see the AI learn to take advantage of winter conditions and produce colder than normal water, which reduces the energy required for cooling within the data centre. Rules don\u2019t get better over time, but AI does.\"\nWe\u2019re excited that our direct AI control system is operating safely and dependably, while consistently delivering energy savings. However, data centres are just the beginning. In the long term, we think there's potential to apply this technology in other industrial settings, and help tackle climate change on an even grander scale.\n"}
{"title": "DeepMind papers at ICML 2018", "contents": "The \n2018 International Conference on Machine Learning\n will take place in Stockholm, Sweden from 10-15 July.\nFor those attending and planning the week ahead, we are sharing a schedule of DeepMind presentations at ICML (\nyou can download a pdf version \nhere\n). We look forward to the many engaging discussions, ideas, and collaborations that are sure to arise from the conference!\nAuthors:\n Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Nouri, Norman Casagrande, Edward Lockhart, Sander Dieleman, Aaron van den Oord, Koray Kavukcuoglu\nSequential models achieve state-of-the-art results in audio, visual and textual domains with respect to both estimating the data distribution and generating desired samples. Efficient sampling for this class of models at the cost of little to no loss in quality has however remained an elusive task. With a focus on text-to-speech synthesis, we show that compact recurrent architectures, a remarkably high degree of weight sparsification and a novel reordering of the variables greatly reduce sampling latency while maintaining high audio fidelity. We first describe a compact single-layer recurrent neural network, the WaveRNN, with a novel dual softmax layer that matches the quality of the state-of-the-art WaveNet model. Persistent GPU kernels for the WaveRNN are able to synthesize 24kHz 16-bit audio 4 times faster than real time. We then apply a weight sparsification technique to the model. We show that, given a constant number of weights, large sparse networks perform better than small dense networks. Using a large Sparse WaveRNN, we demonstrate the feasibility of real-time synthesis of high-fidelity audio on a low-power mobile phone CPU. We use a large Sparse WaveRNN to demonstrate the first instance of real-time synthesis of high-fidelity audio on low-resource mobile phone CPU. Finally, we introduce a novel reordering of the variables in the factorization of the joint distribution. The reordering makes it possible to trade vacuous dependencies on samples from the distant future for the ability to generate in batches. The Batch WaveRNN produces up to 16 samples per step maintaining high quality and enables audio synthesis that is up to 40 times faster than real time.\nPresentations:\nAuthors: \nArthur Guez*, Theophane Weber*, Ioannis Antonoglou, Karen Simonyan, Oriol Vinyals, Daan Wierstra, Remi Munos, David Silver\nPlanning problems are among the most important and well-studied problems in artificial intelligence. They are most typically solved by tree search algorithms that simulate ahead into the future, evaluate future states, and back-up those evaluations to the root of a search tree. Among these algorithms, Monte-Carlo tree search (MCTS) is one of the most general, powerful and widely used. A typical implementation of MCTS uses cleverly designed rules, optimised to the particular characteristics of the domain. These rules control where the simulation traverses, what to evaluate in the states that are reached, and how to back-up those evaluations. In this paper we instead learn where, what and how to search. Our architecture, which we call an MCTSnet, incorporates simulation-based search inside a neural network, by expanding, evaluating and backing-up a vector embedding. The parameters of the network are trained end-to-end using gradient-based optimisation. When applied to small searches in the well-known planning problem Sokoban, the learned search algorithm significantly outperformed MCTS baselines.\nPresentations:\nAuthors:\n Gellert Weisz, Andras Gyorgy, and Csaba Szepesvari\nWe consider the problem of configuring general-purpose solvers to run efficiently on problem instances drawn from an unknown distribution. The goal of the configurator is to find a configuration that runs fast on average on most instances, and do so with the least amount of total work. It can run a chosen solver on a random instance until the solver finishes or a timeout is reached. We propose LEAPSANDBOUNDS, an algorithm that tests configurations on randomly selected problem instances for longer and longer time. We prove that the capped expected runtime of the configuration returned by LEAPSANDBOUNDS is close to the optimal expected runtime, while our algorithm\u2019s running time is near-optimal. Our results show that LEAPSANDBOUNDS is more efficient than the recent algorithm of Kleinberg et al. (2017), which, to our knowledge, is the only other algorithm configuration method that claims to have non-trivial theoretical guarantees. Experimental results on configuring a public SAT solver on a public benchmark also stand witness to the superiority of our method.\nPresentations: \nAuthors:\n Will Dabney*, Georg Ostrovski*, David Silver, Remi Munos\nIn this work, we build on recent advances in distributional reinforcement learning to give a generally applicable, flexible, and state-of-the-art distributional variant of DQN. We achieve this by using quantile regression to approximate the full quantile function for the state-action return distribution. By reparameterizing a distribution over the sample space, this yields an implicitly defined return distribution and gives rise to a large class of risk-sensitive policies. We demonstrate improved performance on the 57 Atari 2600 games in the ALE, and use our algorithms implicitly defined distributions to study the effects of risk-sensitive policies in Atari games.\nPresentations:\nAuthors: \nAlvaro Sanchez, Nicolas Heess, Jost Tobias Springenberg, Josh Merel, Martin Riedmiller, Raia Hadsell, Peter Battaglia\nUnderstanding and interacting with everyday physical scenes requires rich knowledge about the structure of the world, represented either implicitly in a value or policy function, or explicitly in a transition model. Here we introduce a new class of learnable models\u2014based on graph networks\u2014which implement an inductive bias for object- and relation-centric representations of complex, dynamical systems. Our results show that as a forward model, our approach supports accurate predictions, and surprisingly strong and efficient generalization, across eight distinct physical systems which we varied parametrically and structurally. We also found that our inference model can perform system identification from real and simulated data. Our models are also differentiable, and support online planning via gradient based trajectory optimization, as well as offline policy optimization. Our framework offers new opportunities for harnessing and exploiting rich knowledge about the world, and takes a key step toward building machines with more human-like representations of the world.\nPresentations:\nAuthors:\n Mehrdad Farajtabar, Yinlam Chow, and Mohammad Ghavamzadeh\nWe study the problem of off-policy evaluation (OPE) in reinforcement learning (RL), where the goal is to estimate the performance of a policy from the data generated by another policy(ies). In particular, we focus on the doubly robust (DR) estimators that consist of an importance sampling (IS) component and a performance model, and utilize the low (or zero) bias of IS and low variance of the model at the same time. Although the accuracy of the model has a huge impact on the overall performance of DR, most of the work on using the DR estimators in OPE has been focused on improving the IS part, and not much on how to learn the model. In this paper, we propose alternative DR estimators, called more robust doubly robust (MRDR), that learn the model parameter by minimizing the variance of the DR estimator. We first present a formulation for learning the DR model in RL. We then derive formulas for the variance of the DR estimator in both contextual bandits and RL, such that their gradients w.r.t. the model parameters can be estimated from the samples, and propose methods to efficiently minimize the variance. We prove that the MRDR estimators are strongly consistent and asymptotically optimal. Finally, we evaluate MRDR in bandits and RL benchmark problems, and compare its performance with the existing methods.\nPresentations:\nAuthors: \n Marta Garnelo, Dan Rosenbaum, Christopher Maddison, Tiago Ramalho, David Saxton, Murray Shanahan, Yee Whye Teh, Danilo Rezende, S. M. Ali Eslami\nDeep neural networks excel at function approximation, yet they are typically trained from scratch for each new function. On the other hand, Bayesian methods, such as Gaussian Processes (GPs), exploit prior knowledge to quickly infer the shape of a new function at test time. Yet GPs are computationally expensive, and it can be hard to design appropriate priors. In this paper we propose a family of neural models, Conditional Neural Processes (CNPs), that combine the benefits of both. CNPs are inspired by the flexibility of stochastic processes such as GPs, but are structured as neural networks and trained via gradient descent. CNPs make accurate predictions after observing only a handful of training data points, yet scale to complex functions and large datasets. We demonstrate the performance and versatility of the approach on a range of canonical machine learning tasks, including regression, classification\nand image completion.\nPresentations:\nAuthors: \nMarco \u00a0Fraccaro, Danilo \u00a0Jimenez Rezende, Yori \u00a0Zwols, Alexander Pritzel, S. M. Ali Eslami, Fabio Viola\nIn model-based reinforcement learning, generative and temporal models of environments can be leveraged to boost agent performance, either by tuning the agent\u2019s representations during training or via use as part of an explicit planning mechanism. However, their application in practice has been limited to simplistic environments, due to the difficulty of training such models in larger, potentially partially-observed and 3D environments. In this work we introduce a novel action-conditioned generative model of such challenging environments. The model features a non-parametric spatial memory system in which we store learned, disentangled representations of the environment. Low-dimensional spatial updates are computed using a state-space model that makes use of knowledge on the prior dynamics of the moving agent, and high-dimensional visual observations are modelled with a Variational Auto-Encoder. The result is a scalable architecture capable of performing coherent predictions over hundreds of time steps across a range of partially observed 2D and 3D environments.\nPresentations:\nAuthors: \nHyunjik Kim, Andriy Mnih\nWe define and address the problem of unsupervised learning of disentangled representations on data generated from independent factors of variation. We propose FactorVAE, a method that disentangles by encouraging the distribution of rep-resentations to be factorial and hence independent across the dimensions. We show that it improves upon \u03b2-VAE by providing a better trade-off between disentanglement and reconstruction quality and being more robust to the number of training iterations. Moreover, we highlight the problems of a commonly used disentanglement metric and introduce a new metric that does not suffer from them.\nPresentations: \nAuthors: \nMartin Riedmiller, Roland Hafner, Thomas Lampe, Michael Neunert, Jonas Degrave, Tobias Springenberg\nWe propose Scheduled Auxiliary Control (SAC), a new learning paradigm in the context of Reinforcement Learning (RL) . SAC enables learning of complex behaviors \u2013 from scratch \u2013 in the presence of multiple sparse reward signals. To achieve this the agent is equipped with a set of general auxiliary tasks, that it attempts to learn simultaneously via off-policy RL. The key idea behind our method is that active (learned) scheduling and execution of auxiliary policies allows the agent to efficiently explore its environment \u2013 enabling it\nto excel at sparse reward RL. Our experiments in several challenging robotic manipulation settings demonstrate the power of our approach.\nRead more on the DeepMind \nblog\n.\nPresentations:\nAuthors:\n Jack W Rae, Chris Dyer, Peter Dayan, Timothy P Lillicrap\nNeural networks trained with backpropagation often struggle to identify classes that have been observed a small number of times. In applications where most class labels are rare, such as language modelling, this can become a performance bottleneck. One potential remedy is to augment the network with a fast-learning non-parametric model which attends over recent activations. We explore a simplified architecture where we treat a subset of the model parameters as fast memory stores. This can help retain information over longer time intervals than a traditional memory, and does not require additional space or compute. In the case of image classification, we display faster binding of novel classes on an Omniglot image curriculum task. We also show improved performance for word-based language models on news reports (GigaWord), books (Project Gutenberg) and Wikipedia articles (WikiText-103) \u2014 the latter achieving state-of-the-art perplexity.\nPresentations:\nAuthors: \nSuman Ravuri, Shakir Mohamed, Mihaela Rosca, and Oriol Vinyals\nWe propose a method of moments (MoM) algorithm for training large-scale implicit generative models. Moment estimation in this setting encounters two problem: it is often difficult to define the millions of moments needed to learn the model parameters, and it is hard to determine which properties are useful when specifying moments. To address the first issue, we introduce a moment network, and define the moments as the gradient of the network\u2019s output with respect to its parameters and the network\u2019s hidden units. To tackle the second problem, we use asymptotic theory to highlight desiderata for moments \u2013 namely they should minimize the asymptotic variance of estimated model parameters \u2013 and introduce an objective to learn better moments. The sequence of objectives created by this Method of Learned Moments (MoLM) can train high-quality neural image samplers. On CIFAR-10, we demonstrate that MoLM-trained generators achieve significantly higher Inception Scores and lower Frechet Inception Distances than those trained with gradient penalty regularized adversarial objectives. These generators also achieve nearly perfect Multi-Scale Structural Similarity Scores on CelebA, and can create high-quality samples of resolutions up to 128\u00d7128.\nPresentations: \nAuthors:\n David Held, Xinyang Geng, Carlos Florensa, Pieter Abbeel\nReinforcement learning is a powerful technique to train an agent to perform a task. However, an agent that is trained using reinforcement learning is only capable of achieving the single task that is specified via its reward function. Such an approach does not scale well to settings in which an agent needs to perform a diverse set of tasks, such as navigating to varying positions in a room or moving objects to varying locations. Instead, we propose a method that allows an agent to automatically discover the range of tasks that it is capable of performing. We use a generator network to propose tasks for the agent to try to achieve, specified as goal states. The generator network is optimized using adversarial training to produce tasks that are always at the appropriate level of difficulty for the agent. Our method thus automatically produces a curriculum of tasks for the agent to learn. We show that, by using this framework, an agent can efficiently and automatically learn to perform a wide set of tasks without requiring any prior knowledge of its environment. Our method can also learn to achieve tasks with sparse rewards, which traditionally pose significant challenges.\nPresentations: \nAuthors:\n \u00a0Neil C. Rabinowitz, Frank Perbet, H. Francis Song, Chiyuan Zhang, S. M. Ali Eslami, Matthew Botvinick\nTheory of mind (ToM) broadly refers to humans\u2019 ability to represent the mental states of others, including their desires, beliefs, and intentions. We propose to train a machine to build such models too. We design a Theory of Mind neural network \u2013 a ToMnet \u2013 which uses meta-learning to build models of the agents it encounters. The ToMnet learns a strong prior model for agents\u2019 future behaviour, and, using only a small number of behavioural observations, can bootstrap to richer predictions about agents\u2019 characteristics and mental states. We apply the ToMnet to agents behaving in simple gridworld environments, showing that it learns to model random, algorithmic, and deep RL agents from varied populations, and that it passes classic ToM tasks such as the \u201cSally-Anne\u201d test of recognising that others can hold false beliefs about the world.\nPresentations:\nAuthors:\n Ofir Nachum, Yinlam Chow, and Mohammad Ghavamzadeh\nWe study the sparse entropy-regularized reinforcement learning (ERL) problem in which the entropy term is a special form of the Tsallis entropy. The optimal policy of this formulation is sparse, i.e., at each state, it has non-zero probability for only a small number of actions. This addresses the main drawback of the standard Shannon entropy-regularized RL (soft ERL) formulation, in which the optimal policy is {\\em softmax}, and thus, may assign a non-negligible probability mass to non-optimal actions. This problem is aggravated as the number of actions is increased. In this paper, we follow the work of Nachum et al. (2017) in the soft ERL setting, and propose a class of novel path consistency learning (PCL) algorithms, called sparse PCL, for the sparse ERL problem that can work with both on-policy and off-policy data. We first derive a sparse consistency equation that specifies a relationship between the optimal value function and policy of the sparse ERL along any system trajectory. Crucially, a weak form of the converse is also true, and we quantify the sub-optimality of a policy which satisfies sparse consistency, and show that as we increase the number of actions, this sub-optimality is better than that of the soft ERL optimal policy. We then use this result to derive the sparse PCL algorithms. We empirically compare sparse PCL with its soft counterpart, and show its advantage, especially in problems with a large number of actions.\nPresentations:\nAuthors: \nAndre Barreto, Diana Borsa, John Quan, Tom Schaul, David Silver, Matteo Hessel, Daniel Makowitz, Augustin Zidek, Remi Munos\nThe ability to transfer skills across tasks has the potential to scale up reinforcement learning (RL) agents to environments currently out of reach. Recently, a framework based on two ideas, successor features (SFs) and generalised policy improvement (GPI), has been introduced as a principled way of transferring skills. In this paper we investigate the feasibility of combining SF&GPI with the representation power of deep learning. Since in deep RL we are interested in learning all the components of SF&GPI concurrently, the existing inter-dependencies between them can lead to instabilities. In this work we propose a solution for this problem that makes it possible to use SF & GPI online, at scale. In order to empirically verify this claim, we apply the proposed method to a complex 3D environment that requires hundreds of millions of transitions to be solved. We show that the transfer promoted by SF&GPI leads to reasonable policies on unseen tasks almost instantaneously. We also show how to build on the transferred policies to learn policies that are specialised to the new tasks, which can then be added to the agent\u2019s set of skills to be used in the future.\nPresentations: \nAuthors:\n Samuel Ritter, Jane Wang, Sid Jayakumar, Zeb Kurth-Nelson, Charles Blundell, Razvan Pascanu, Matt Botvinick\nMeta-learning agents have demonstrated the ability to rapidly explore and exploit new tasks sampled from the task distribution on which they were trained. However, when these agents encounter situations that they explored in the distant past, they are not able to remember the results of their past exploration. Thus, instead of immediately exploiting previously discovered solutions, they must again explore from scratch. In this work, we argue that the necessity to remember the results of past exploration is ubiquitous in naturalistic environments. We propose a formalism for modeling this kind of recurring environment structure, then develop a meta-learning architecture for solving such environments. This architecture melds the standard LSTM working memory with a differentiable neural episodic memory. We explore the capabilities of this episodic LSTM in four recurrent-state stochastic process environments: 1.) episodic contextual bandits, 2.) compositional contextual bandits, 3.) episodic two-step task, and 4.) contextual water-maze navigation.\nPresentations: \nAuthors:\n Jonathan Uesato, Brendan O'Donoghue, Aaron van den Oord, and Pushmeet Kohli.\nThis paper investigates recently proposed approaches for defending against adversarial examples and evaluating adversarial robustness. The existence of adversarial examples in trained neural networks reflects the fact that expected risk alone does not capture the model\u2019s performance against worst-case inputs. We motivate the use of advKarol Hausmaersarial risk as an objective, although it can not easily be computed exactly. We then frame commonly used attacks and evaluation metrics as defining a tractable surrogate objective to the true adversarial risk. This suggests that models may be obscured to adversaries, by optimizing this surrogate rather than the true adversarial risk. We demonstrate that this is a significant problem in practice by repurposing gradient-free optimization techniques into adversarial attacks, which we use to decrease the accuracy of several recently proposed defenses to near zero. Our hope is that\nour formulations and results will help researchers to develop more powerful defenses.\nPresentations: \nLearning to Coordinate with Coordination Graphs in Repeated Single-Stage Multi-Agent Decision Problems\nAuthors:\n Eugenio Bargiacchi (Vrije Universiteit Brussel), Timothy Verstraeten (Vrije Universiteit Brussel), Diederik Roijers (Vrije Universiteit Brussel / Vrije Universiteit Amsterdam), Ann Now\u00e9 (Vrije Universiteit Brussel), Hado van Hasselt\nLearning to coordinate between multiple agents is an important problem in many reinforcement learning problems. Key to learning to coordinate is exploiting loose couplings, i.e., conditional independences between agents. In this paper we study learning in repeated fully cooperative games, multi-agent multi-armed bandits (MAMABs), in which the expected rewards can be expressed as a coordination graph. We propose multi-agent upper confidence exploration (MAUCE), a new algorithm for MAMABs that exploits loose couplings, which enables us to prove a regret bound that is logarithmic in the number of arm pulls and only linear in the number of agents. We empirically compare MAUCE to sparse cooperative Q-learning, and a state-of-the-art combinatorial bandit approach, and show that it performs much better on a variety of settings, including learning control policies for wind farms.\nPresentations:\nAuthors:\n Ciara Pike-Burke, Shipra Agrawal, Csaba Szepesvari, Steffen Grunewalder\nWe study a variant of the stochastic K-armed bandit problem, which we call \"bandits with delayed, aggregated anonymous feedback\". In this problem, when the player pulls an arm, a reward is generated, however it is not immediately observed. Instead, at the end of each round the player observes only the sum of a number of previously generated rewards which happen to arrive in the given round. The rewards are stochastically delayed and due to the aggregated nature of the observations, the information of which arm led to a particular reward is lost. The question is what is the cost of the information loss due to this delayed, aggregated anonymous feedback? Previous works have studied bandits with stochastic, non-anonymous delays and found that the regret increases only by an additive factor relating to the expected delay. In this paper, we show that this additive regret increase can be maintained in the harder delayed, aggregated anonymous feedback setting when the expected delay (or a bound on it) is known. We provide an algorithm that matches the worst case regret of the non-anonymous problem exactly when the delays are bounded, and up to logarithmic factors or an additive variance term for unbounded delays.\nPresentations:\nBest Paper Runner Up\nAuthors:\n David Balduzzi, S\u00e9bastien Racaniere, James Martens, Jakob Foerster, Karl Tuyls, Thore Graepel\nThe cornerstone underpinning deep learning is the guarantee that gradient descent on an objective converges to local minima. Unfortunately, this guarantee fails in settings, such as generative adversarial nets, where there are multiple interacting losses. The behavior of gradient-based methods in games is not well understood \u2013 and is becoming increasingly important as adversarial and multi-objective architectures proliferate. In this paper, we develop new techniques to understand and control the dynamics in general games. The key result is to decompose the second-order dynamics into two components. The first is related to potential games, which reduce to gradient descent on an implicit function; the second relates to Hamiltonian games, a new class of games that obey a conservation law, akin to conservation laws in classical mechanical systems. The decomposition motivates Symplectic Gradient Adjustment (SGA), a new algorithm for finding stable fixed points in general games. Basic experiments show SGA is competitive with recently proposed algorithms for finding local Nash equilibria in GANs \u2013 whilst at the same time being applicable to \u2013 and having guarantees in \u2013 much more general games.\nPresentations:\nAuthors\n: Danny Karmon (Bar Ilan University), Daniel Zoran, Yoav Goldberg (Bar Ilan University)\nMost works on adversarial examples for deep-learning based image classifiers use noise that, while small, covers the entire image. We explore the case where the noise is allowed to be visible but confined to a small, localized patch of the image, without covering any of the main object(s) in the image. We show that it is possible to generate localized adversarial noises that cover only 2% of the pixels in the image, none of them over the main object, and that are transferable across images and locations, and successfully fool a state-of-the-art Inception v3 model with very high success rates.\nPresentations:\nAuthors:\n Yaroslav Ganin, Tejas Kulkarni, Igor Babuschkin, S.M. Ali Eslami, Oriol Vinyals\nAdvances in deep generative networks have led to impressive results in recent years. Nevertheless, such models can often waste their capacity on the minutiae of datasets, presumably due to weak inductive biases in their decoders. This is where graphics engines may come in handy since they abstract away low-level details and represent images as high-level programs. Current methods that combine deep learning and renderers are limited by hand-crafted likelihood or distance functions, a need for large amounts of supervision, or difficulties in scaling their inference algorithms to richer datasets. To mitigate these issues, we present SPIRAL, an adversarially trained agent that generates a program which is executed by a graphics engine to interpret and sample images. The goal of this agent is to fool a discriminator network that distinguishes between real and rendered data, trained with a distributed reinforcement learning setup without any supervision. A surprising finding is that using the discriminator's output as a reward signal is the key to allow the agent to make meaningful progress at matching the desired output rendering. To the best of our knowledge, this is the first demonstration of an end-to-end, unsupervised and adversarial inverse graphics agent on challenging real world (MNIST, Omniglot, CelebA) and synthetic 3D datasets.\nPresentations: \nAuthors: \nDavid Barrett*, Felix Hill*, Adam Santoro*, Ari Morcos, Tim Lillicrap\nWhether neural networks can learn abstract reasoning or whether they merely rely on superficial statistics is a topic of recent debate. Here, we propose a dataset and challenge designed to probe abstract reasoning, inspired by a well-known human IQ test. To succeed at this challenge, models must cope with various generalisation \u2018regimes\u2019 in which the training and test data differ in clearly defined ways. We show that popular models such as ResNets perform poorly, even when the training and test sets differ only minimally, and we present a novel architecture, with a structure designed to encourage reasoning, that does significantly better. When we vary the way in which the test questions and training data differ, we find that our model is notably proficient at certain forms of generalisation, but notably weak at others. We further show that the model\u2019s ability to generalise improves markedly if it is trained to predict symbolic explanations for its answers. Altogether, we introduce and explore ways to both measure and induce stronger abstract reasoning in neural networks. Our freely-available dataset should motivate further progress in this direction.\nPresentations:\nRead more on the DeepMind \nblog\n.\nAuthors:\n Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, Koray Kavukcuoglu\nIn this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called\nV-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari-57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach.\nRead more on the DeepMind \nblog\n, and see our open-source implementation on \nGitHub\n.\nPresentations:\nAuthors: \nWojtek Czarnecki*, Siddhant Jayakumar*, Max Jaderberg, Leonard Hasenclever, Yee Whye Teh, Nicolas Heess, Simon \u00a0Osindero, Razvan Pascanu\nWe introduce Mix & Match (M&M) \u2013 a training framework designed to facilitate rapid and effective learning in RL agents, especially those that would be too slow or too challenging to train otherwise. The key innovation is a procedure that allows us to automatically form a curriculum over agents. Through such a curriculum we can progressively train more complex agents by, effectively, bootstrapping from solutions found by simpler agents. In contradistinction to typical curriculum learning approaches, we do not gradually modify the tasks or environments presented, but instead use a process to gradually alter how the policy is represented internally. We show the broad applicability of our method by demonstrating significant performance gains in three different experimental setups: (1) We train an agent able to control more than 700 actions in a challenging 3D first-person task; using our method to progress through an action-space curriculum we achieve both faster training and better final performance than one obtains using traditional methods. (2) We further show that M&M can be used successfully to progress through a curriculum of architectural variants defining an agents internal state. (3) Finally, we illustrate how a variant of our method can be used to improve agent performance in a multitask setting.\nPresentations: \nAuthors: \nAaron van den Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, Koray Kavukcuoglu\nThe recently-developed WaveNet architecture is the current state of the art in realistic speech synthesis, consistently rated as more natural sounding for many different languages than any previous system. However, because WaveNet relies on sequential generation of one audio sample at a time, it is poorly suited to today\u2019s massively parallel computers, and therefore hard to deploy in a real-time production setting. This paper introduces Probability Density Distillation, a new method for training a parallel feed-forward network from a trained WaveNet with no significant difference in quality. The resulting system is capable of generating high-fidelity speech samples at more than 20 times faster than real-time, and is deployed online by Google Assistant, including serving multiple English and Japanese voices.\nPresentations:\nRead more on the DeepMind \nblog\n.\nAuthors: \nJonathan Schwarz, Jelena Luketina, Wojciech M. Czarnecki, Agnieszka Grabska-Barwinska, Yee Whye Teh, Razvan Pascanu* Raia Hadsell*\nWe introduce a conceptually simple and scalable framework for continual learning domains where tasks are learned sequentially. Our method is constant in the number of parameters and is designed to preserve performance on previously encountered tasks while accelerating learning progress on subsequent problems. This is achieved through\ntraining two neural networks: A knowledge base, capable of solving previously encountered problems, which is connected to an active column that is employed to efficiently learn the current task. After learning a new task, the active column is distilled into the knowledge base, taking care to protect any previously learnt tasks. This cycle of active learning (progression) followed by consolidation (compression) requires no architecture growth, no access to or storing of previous data or tasks, and no task-specific parameters. Thus, it is a learning process that may be sustained over a lifetime of tasks while supporting forward transfer and minimising forgetting. We demonstrate the progress & compress approach on sequential classification of handwritten alphabets as well as two reinforcement learning domains: Atari games and 3D maze navigation.\nPresentations:\nAuthors: \nGeorg Ostrovski*, Will Dabney*, Remi Munos\nWe introduce autoregressive implicit quantile networks (AIQN), a fundamentally different approach to generative modeling than those commonly used, that implicitly captures the distribution using quantile regression. AIQN is able to achieve superior perceptual quality and improvements in evaluation metrics, without incurring a loss of sample diversity. The method can be applied to many existing models and architectures. In this work we extend the PixelCNN model with AIQN and demonstrate results on CIFAR-10 and ImageNet using Inception scores, FID, non-cherry-picked samples, and inpainting results. We consistently observe that AIQN yields a highly stable algorithm that improves perceptual quality while maintaining a highly diverse distribution.\nPresentations:\nAuthors:\n Brendan O'Donoghue, Ian Osband, Remi Munos, Volodymyr Mnih\nWe consider the exploration/exploitation problem in reinforcement learning. For exploitation, it is well known that the Bellman equation connects the value at any time-step to the expected value at subsequent time-steps. In this paper we consider a similar \\textit{uncertainty} Bellman equation (UBE), which connects the uncertainty at any time-step to the expected uncertainties at subsequent time-steps, thereby extending the potential exploratory benefit of a policy beyond individual time-steps. We prove that the unique fixed point of the UBE yields an upper bound on the variance of the posterior distribution of the Q-values induced by any policy. This bound can be much tighter than traditional count-based bonuses that compound standard deviation rather than variance. Importantly, and unlike several existing approaches to optimism, this method scales naturally to large systems with complex generalization. Substituting our UBE-exploration strategy for \u03f5-greedy improves DQN performance on 51 out of 57 games in the Atari suite.\nPresentations:\nAuthors: \nYao Ma (Boston University), Alexander Olshevsky (Boston University), Csaba Szepesvari, Venkatesh Saligrama (Boston University)\nWe consider estimation of worker skills from worker-task interaction data (with unknown labels) for the single-coin crowd-sourcing binary classification model in symmetric noise. We define the (worker) interaction graph whose nodes are workers and an edge between two nodes indicates whether or not the two workers participated in a common task. We show that skills are asymptotically identifiable if and only if an appropriate limiting version of the interaction graph is irreducible and has odd-cycles. We then formulate a weighted rank-one optimization problem to estimate skills based on observations on an irreducible, aperiodic interaction graph. We propose a gradient descent scheme and show that for such interaction graphs estimates converge asymptotically to the global minimum. We characterize noise robustness of the gradient scheme in terms of spectral properties of signless Laplacians of the interaction graph. We then demonstrate that a plug-in estimator based on the estimated skills achieves state-of-art performance on a number of real-world datasets. Our results have implications for rank-one matrix completion problem in that gradient descent can provably recover W\u00d7W rank-one matrices based on W+1 off-diagonal observations of a connected graph with a single odd-cycle.\nPresentations:\n"}
{"title": "About", "contents": "Intelligence allows us to learn, imagine, cooperate, create, communicate, and so much more. By better understanding different aspects of intelligence, we can use this knowledge as inspiration to build novel computer systems that learn to find solutions to difficult problems on their own. \nLike the Hubble telescope that helped us look deeper into space, these tools are already expanding human knowledge and making positive global impact. Our long term aim is to solve intelligence, developing more general and capable problem-solving systems, known as artificial general intelligence (AGI). \nGuided by safety and ethics, this invention could help society find answers to some of the world\u2019s most pressing and fundamental scientific challenges.\nTo accelerate the field, we took an interdisciplinary approach, bringing together new ideas and advances in machine learning, neuroscience, engineering, mathematics, simulation and computing infrastructure, along with new ways of organising scientific endeavour.\nWe achieved early success in computer games, which researchers often use to test AI. One of our programs learned to play 49 different Atari games from scratch, just from seeing the pixels and score on the screen. Our AlphaGo program was also the first to beat a professional Go player, a feat described as a decade ahead of its time.\nDemis Hassabis co-founded DeepMind in 2010 after successful careers in academia and computer game development. A child chess prodigy, he designed and programmed the multi-million selling, award-winning game Theme Park at the age of 17. After graduating from Cambridge University, he founded pioneering videogames firm Elixir Studios and completed a PhD in cognitive neuroscience at UCL. The journal Science listed his research on imagination and memory as one of 2007\u2019s top ten breakthroughs. Demis is a Fellow of the Royal Society, Royal Academy of Engineering and the Royal Society of Arts. In 2017 he featured in the Time 100 list of most influential people, and in 2018 he was awarded a CBE for services to science and technology.\nDemis is also Founder and CEO of Isomorphic Labs.\nShane Legg and Demis first met at the Gatsby Computational Neuroscience Unit at University College London, while studying the brain\u2019s algorithmic organisation. Their shared curiosity in AI\u2019s potential to positively impact society later became the founding idea for DeepMind. Shane is a celebrated computer scientist and holds a PhD from IDSIA (Dalle Molle Institute for Artificial Intelligence Research) in Switzerland, where he was supervised by Professor Marcus Hutter, the leading authority on theoretical models of super intelligent machines. Shane\u2019s doctoral thesis on universal artificial intelligence was awarded the Canadian Singularity Institute research prize in 2008.\nLila blends technical expertise with creative leadership in her role as \nDeepMind\u2019s COO\n. Originally a microprocessor engineer, she has helped grow some of the world's leading technology companies including Intel, where she held a series of global roles, venture firm Kleiner Perkins and Coursera, where she was the first executive hire and helped bring education to millions of people around the world. At DeepMind, Lila oversees both technology and organisational infrastructure including \nEngineering\n, \nOperations, and People\n. She is the co-founder and Chair of education technology non-profit \nTeam4Tech\n and has previously served on the Board of Gannett, helping it become the largest US print media company. Her exceptional impact has been recognised by a Henry Crown Fellowship from the Aspen Institute and the Anita Borg Institute Award for Social Impact.\nKoray is one of the world's foremost \nexperts in deep learning\n. As Vice President of Research, he oversees and coordinates the work of our international team of research scientists and engineers. He previously led the deep-learning team at DeepMind, where he pioneered algorithmic breakthroughs including \nDQN\n, \nIMPALA\n and \nWaveNet\n, which powers the voice of the Google Assistant for millions of users around the world. Before joining DeepMind, Koray was a researcher at \nNEC Labs America\n. He holds a masters in aerospace engineering and a PhD in computer science from NYU, where he was part of Yann LeCun\u2019s group within the \nComputational and Biological Learning Lab\n working on unsupervised learning and deep learning.\nDecades of international commercial experience and deep technical expertise mean Colin is uniquely placed to ensure DeepMind\u2019s cutting-edge research benefits as many people as possible. As Chief Business Officer, he oversees a wide-range of teams including \nApplied\n, which applies research breakthroughs to Google products and infrastructure used by billions of people. He also helps drive the growth of DeepMind, building and leading critical functions including finance and strategy and leading external and commercial partnerships. Originally an electronics and software engineer, he has held senior positions at both start-ups and global companies such as Thomson Reuters, helping them solve their own complex, mission-critical, real-world challenges.\nThe DeepMind Academic Fellowship Program provides an opportunity for early-career researchers in the fields of Computer Science and Artificial Intelligence to pursue postdoctoral study and build the experiences and research profile that will enable them to progress to full academic or other research leadership roles in future.\nAlongside financial support, DeepMind provides opportunities for fellows to be mentored by senior DeepMind researchers. DeepMind will not direct their research and fellows are free to pursue any research direction they wish.\nFellowships are open to early-career researchers who have completed a PhD in Machine Learning, Computer Science, Statistics or another relevant field by the time they start their postdoc. We particularly encourage candidates who identify as Black to apply because this group is currently underrepresented in AI research.\nDeepMind has partnered with three UK universities to launch the Fellowship program. For a detailed eligibility criteria, including how to apply, check out our partner university websites or email the contacts provided.\nMany on our team hold university professorships and teach or supervise students at Cambridge, Oxford, MIT, Imperial, and elsewhere. Learn more about these courses on our \nYouTube channel\n or \nLearning resources page\n.\nWe also partner with many world-leading academic institutions to extend research and teaching capacity. So far, we\u2019ve established academic chairs in machine learning at the \nUniversity of Alberta\n, \nUCL\n, and the \nUniversity of Cambridge\n.\nDeepMind established our scholarships programme in 2017 in an effort to help build a stronger and more inclusive AI community, who can bring a wider range of experiences to the fields of AI and computer science. The scholarships provide financial support to students from underrepresented groups seeking to study graduate courses relating to AI and adjacent fields. Scholars are also offered support from a DeepMind mentor, and have opportunities to attend leading AI academic conferences and DeepMind events.\nOur dedication to science makes us who we are. That\u2019s why we partner with charities like \nChess in Schools and Communities\n and \nIn2Science\n, and have become founding partners of the \nDeep Learning Indaba in Africa\n, the \nEastern European Machine Learning Summer School\n, and the \nAI4Good Summer Lab\n in Canada. We also offer internship programmes for students who want to gain industry experience.\nWe firmly believe that everyone should feel able to participate in science and we are committed to supporting organisations that create community and advance diversity within the sector. Our team members regularly dedicate their time and expertise to advancing discussions about AI in their communities and we are proud partners of the \nAnita Borg Foundation\n, \nWomen in Machine Learning\n, \nBlack in AI\n, and more.\n"}
{"title": "Safety & Ethics", "contents": "We believe this approach also means ruling out the use of AI technology in certain fields. For example, we\u2019ve signed \npublic pledges\n against using our technologies for lethal autonomous weapons, alongside many others from the AI community.\nThese issues go well beyond any one organisation. Our ethics team works with many brilliant non-profits, academics, and other companies, and creates forums for the public to explore some of the toughest issues. Our safety team also collaborates with other leading research labs, including our colleagues at \nGoogle\n, \nOpenAI\n, the \nAlan Turing Institute\n, and elsewhere. \nIt\u2019s also important that the people building AI reflect the broader society. We\u2019re working with universities on \nscholarships\n for people from underrepresented backgrounds, and support community efforts such as \nWomen in Machine Learning\n and the African \nDeep Learning Indaba\n.\nTechnical safety is a core element of our research. Our goal is to ensure that AI systems of the future are proven to be safe - \u200abecause we\u2019ve built them that way. Just as software engineering has a set of best practices for security and reliability, our AI safety teams develop approaches to specification, robustness, and assurance for AI systems both now and in the future.\nWe created DeepMind Ethics & Society to guide the responsible development and deployment of AI. Our team of ethicists and policy researchers work closely with our AI research team to understand how technical advances will impact society, and find ways to reduce risk. \nWe also partner with outside experts and the general public to find answers together. We\u2019ve supported partners including the \nRoyal Society\n and the \nRSA\n to carry out public discussions and citizens\u2019 juries around AI ethics, and have given unrestricted financial grants to several universities working on these issues. We also helped co-found the \nPartnership on AI\n to bring together academics, charities, and company labs to solve common challenges.\n"}
{"title": "Objects that Sound", "contents": "Visual and audio events tend to occur together: a musician plucking guitar strings and the resulting melody; a wine glass shattering and the accompanying crash; the roar of a motorcycle as it accelerates. These visual and audio stimuli are concurrent because they share a common cause. Understanding the relationship between visual events and their associated sounds is a fundamental way that we make sense of the world around us.\nIn \nLook, Listen, and Learn\n and \nObjects that Sound\n (to appear at \nECCV 2018\n), we explore this observation by asking: what can be learnt by looking at and listening to a large number of unlabelled videos? By constructing an audio-visual correspondence learning task that enables visual and audio networks to be jointly trained from scratch, we demonstrate that:\nLearning from multiple modalities is not new; historically, researchers have largely focused on image-text or audio-vision pairings. However, a common approach has been to train a \u201cstudent\u201d network in one modality using the automatic supervision provided by a \u201cteacher\u201d network in the other modality (\u201c\nteacher-student supervision\n\u201d), where the \u201cteacher\u201d has been trained using a large number of human annotations.\nFor instance, a vision network trained on ImageNet can be used to annotate frames of a YouTube video as \u201cacoustic guitar\u201d, which provides training data to the \u201cstudent\u201d audio network for learning what an \u201cacoustic guitar\u201d sounds like. In contrast, we train both visual and audio networks from scratch, where the concept of the \u201cacoustic guitar\u201d naturally emerges in both modalities. Somewhat surprisingly, this approach achieves superior audio classification compared to teacher-student supervision. As described below, this also equips us to localise the object making the sound, which was not possible with previous approaches.\nOur core idea is to use a valuable source of information contained in the video itself: the correspondence between visual and audio streams available by virtue of them appearing together at the same time in the same video. By seeing and hearing many examples of a person playing a violin and examples of a dog barking, and rarely or never seeing a violin being played while hearing a dog bark and vice versa, it should be possible to conclude what a violin and a dog look and sound like. This approach is, in part, motivated by the way an infant might learn about the world as their visual and audio capabilities develop.\nWe apply learning by audio-visual correspondence (AVC), a simple binary classification task: given an example video frame and a short audio clip, decide whether they correspond to each other or not.\nThe only way for a system to solve this task is by learning to detect various semantic concepts in both the visual and the audio domain. To tackle the AVC task, we propose the following network architecture\nThe image and the audio subnetworks extract visual and audio \nembeddings\n and the correspondence score is computed as a function of the distance between the two embeddings. If the embeddings are similar, the (image, audio) are deemed to correspond.\nWe show that the networks learn useful semantic representations, as, for example, our audio network sets the new state-of-the-art on two sound classification benchmarks. Since the correspondence score is computed purely based on the distance, the two embeddings are forced to be aligned (i.e. the vectors live in the same space, and so can be compared meaningfully), thus facilitating cross-modal retrieval:\nThe AVE-Net recognises semantic concepts in the audio and visual domains, but it cannot answer the question, \u201cWhere is the object that is making the sound?\u201d We again make use of the AVC task and show that it is possible to learn to localise sounding objects, while still not using any labels whatsoever.\nTo localise a sound in the image, we compute the correspondence scores between the audio embedding and a grid of region-level image descriptors. The network is trained with multiple instance learning \u2013 the image-level correspondence score is computed as the maximum of the correspondence score map:\nFor corresponding (image, audio) pairs, the method encourages at least one region to respond highly and therefore localise the object. In the below video (left - input frame, right - localisation output, middle - overlay), frames are processed completely independently \u2013 motion information is not used, and there is no temporal smoothing:\nFor mismatched pairs the maximal score should be low, thus making the entire score map dark, indicating, as desired, there is no object which makes the input sound:\nThe unsupervised audio-visual correspondence task enables, with appropriate network design, two entirely new functionalities to be learnt: cross-modal retrieval, and semantic-based localisation of objects that sound. Furthermore, it facilitates learning of powerful features, setting the new state-of-the-art on two sound classification benchmarks.\nThese techniques may prove useful in reinforcement learning, enabling agents to make use of large amounts of unlabelled sensory information. Our work may also have implications for other multimodal problems beyond audio-visual tasks in the future.\nRead the full papers:\nLook, Listen, and Learn\nObjects that Sound\nThis work was done by Relja Arandjelovi\u0107 and Andrew Zisserman. Graphics by Adam Cain and Damien Boudot.\n"}
{"title": "Impact", "contents": "Working together with YouTube\u2019s product and engineering teams, we\u2019ve helped optimise video compression, protect brand safety, and improve auto-generated chapters and chapter titles. These ongoing collaborations are having a significant impact from saving bandwidth and time to increasing trust in the video platform, enhancing the YouTube experience for people around the world every day.\nProteins are complex molecules that are essential to life. Each has its own unique 3D shape that determines how it works and what it does. Knowing how proteins fold has the potential to help scientists make enormous progress in every field of biology, from quickly finding new medicines to unlocking the mysteries of life itself. To accelerate their work, we created \nAlphaFold\n, a system which accurately predicts the shape of proteins, and released the \nAlphaFold Protein Structure Database\n containing more than 200m+ protein structures, covering nearly all catalogued proteins known to science.\nOver 100 million people are affected by diabetic retinopathy or age-related macular degeneration. These conditions can cause permanent sight loss unless they\u2019re treated quickly. The results, which were published in \nNature Medicine\n, showed that our AI system could recommend patient referrals as accurately as world-leading expert doctors for over 50 sight-threatening eye diseases. More recently, we showed that \nour system can predict\n whether a patient will develop a more severe form of age-related macular degeneration months before it happens\u2013paving the way for future research in sight-loss prevention.\nIt\u2019s essential to keep these servers cool, but this takes a lot of electricity. Even minor improvements would significantly reduce energy usage and CO2 emissions. By building an AI system that manages data centre cooling more efficiently, we helped save around 30% of the energy needed. We also designed it with safety and reliability in mind, so the AI system works within strict constraints.\nOur voice synthesis technology \nWaveNet\n is in the hands of people who use Google Assistant and Google Cloud Platform around the world. And our systems have helped improve mobile phone battery use and screen brightness for millions of people using the Android Pie operating system.\n"}
{"title": "Blog", "contents": ""}
{"title": "Research", "contents": ""}
{"title": "DeepMind", "contents": ""}
{"title": "AlphaGo Zero: Starting from scratch", "contents": "Artificial intelligence research has made rapid progress in a wide variety of domains from speech recognition and image classification to genomics and drug discovery. In many cases, these are specialist systems that leverage enormous amounts of human expertise and data.\nHowever, for some problems this human knowledge may be too expensive, too unreliable or simply unavailable. As a result, a long-standing ambition of AI research is to bypass this step, creating algorithms that achieve superhuman performance in the most challenging domains with no human input. In our most recent \npaper\n, published in the \njournal Nature\n, we demonstrate a significant step towards this goal.\nThe paper introduces AlphaGo Zero, the latest evolution of \nAlphaGo\n, the first computer program to defeat a world champion at the ancient Chinese game of Go. Zero is even more powerful and is arguably the strongest Go player in history.\nPrevious versions of AlphaGo initially trained on thousands of human amateur and professional games to learn how to play Go. AlphaGo Zero skips this step and learns to play simply by playing games against itself, starting from completely random play. In doing so, it quickly surpassed human level of play and defeated the \npreviously published\n champion-defeating version of AlphaGo by 100 games to 0.\nIt is able to do this by using a novel form of \nreinforcement learning\n, in which AlphaGo Zero becomes its own teacher. The system starts off with a neural network that knows nothing about the game of Go. It then plays games against itself, by combining this neural network with a powerful search algorithm. As it plays, the neural network is tuned and updated to predict moves, as well as the eventual winner of the games.\nThis updated neural network is then recombined with the search algorithm to create a new, stronger version of AlphaGo Zero, and the process begins again. In each iteration, the performance of the system improves by a small amount, and the quality of the self-play games increases, leading to more and more accurate neural networks and ever stronger versions of AlphaGo Zero.\nThis technique is more powerful than previous versions of AlphaGo because it is no longer constrained by the limits of human knowledge. Instead, it is able to learn tabula rasa from the strongest player in the world: AlphaGo itself.\nIt also differs from previous versions in other notable ways.\nAll of these differences help improve the performance of the system and make it more general. But it is the algorithmic change that makes the system much more powerful and efficient.\nAfter just three days of self-play training, AlphaGo Zero emphatically defeated the previously \npublished version of AlphaGo\n - which had itself \ndefeated 18-time world champion Lee Sedol\n - by 100 games to 0. After 40 days of self training, AlphaGo Zero became even stronger, outperforming the version of AlphaGo known as \u201cMaster\u201d, which has defeated the world's best players and \nworld number one Ke Jie\n.\nOver the course of millions of AlphaGo vs AlphaGo games, the system progressively learned the game of Go from scratch, accumulating thousands of years of human knowledge during a period of just a few days. AlphaGo Zero also discovered new knowledge, developing unconventional strategies and creative new moves that echoed and surpassed the novel techniques it played in the games against Lee Sedol and Ke Jie.\nThese moments of creativity give us confidence that AI will be a multiplier for human ingenuity, helping us with \nour mission\n to solve some of the most important challenges humanity is facing.\nWhile it is still early days, AlphaGo Zero constitutes a critical step towards this goal. If similar techniques can be applied to other structured problems, such as protein folding, reducing energy consumption or searching for revolutionary new materials, the resulting breakthroughs have the potential to positively impact society.\nRead \nthe paper\nRead the accompanying \nNature News and Views article\nDownload \nAlphaGo Zero games\nRead \nmore about AlphaGo\nThis work was done by David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel and Demis Hassabis.\n"}
{"title": "WaveNet launches in the Google Assistant", "contents": "Just over a year ago we presented \nWaveNet\n, a new deep neural network for generating raw audio waveforms that is capable of producing better and more realistic-sounding speech than existing techniques. At that time, the model was a research prototype and was too computationally intensive to work in consumer products. \u00a0\nBut over the last 12 months we have worked hard to significantly improve both the speed and quality of our model and today we are proud to announce that an updated version of WaveNet is being used to generate the \nGoogle Assistant\n voices for US English and Japanese across all platforms.\nUsing the new WaveNet model results in a range of more natural sounding voices for the Assistant.\nTo understand why WaveNet improves on the current state of the art, it is useful to understand how text-to-speech (TTS) - or \nspeech synthesis\n - systems work today.\nThe majority of these are based on so-called \nconcatenative TTS\n, which uses a large database of high-quality recordings, collected from a single voice actor over many hours. These recordings are split into tiny chunks that can then be combined - or concatenated - to form complete utterances as needed. However, these systems can result in unnatural sounding voices and are also difficult to modify because a whole new database needs to be recorded each time a set of changes, such as new emotions or intonations, are needed.\nTo overcome some of these problems, an alternative model known as \nparametric TTS\n is sometimes used. This does away with the need for concatenating sounds by using a series of rules and parameters about grammar and mouth movements to guide a computer-generated voice. Although cheaper and quicker, this method creates less natural sounding voices.\nWaveNet takes a totally different approach. In the \noriginal paper\n we described a deep generative model that can create individual waveforms from scratch, one sample at a time, with 16,000 samples per second and seamless transitions between individual sounds.\nIt was built using a \nconvolutional neural network\n, which was trained on a large dataset of speech samples. During this training phase, the network determined the underlying structure of the speech, such as which tones followed each other and what waveforms were realistic (and which were not). The trained network then synthesised a voice one sample at a time, with each generated sample taking into account the properties of the previous sample. The resulting voice contained natural intonation and other features such as lip smacks. Its \u201caccent\u201d depended on the voices it had trained on, opening up the possibility of creating any number of unique voices from blended datasets. As with all text-to-speech systems, WaveNet used a text input to tell it which words it should generate in response to a query.\nBuilding up sound waves at such high-fidelity using the original model was computationally expensive, meaning WaveNet showed promise but was not something we could deploy in the real world. But over the last 12 months our teams have worked hard to develop a new model that is capable of more quickly generating waveforms. It is also now capable of running at scale and is the first product to launch on \nGoogle\u2019s latest TPU cloud infrastructure\n.\nThe WaveNet team will now turn their focus to preparing a publication detailing the research behind the new model, but the results speak for themselves. The new, improved WaveNet model still generates a raw waveform but at speeds 1,000 times faster than the original model, meaning it requires just 50 milliseconds to create one second of speech. In fact, the model is not just quicker, but also higher-fidelity, capable of creating waveforms with 24,000 samples a second. We have also increased the resolution of each sample from 8 bits to 16 bits, the same resolution used in compact discs.\nThis makes the new model more natural sounding according to tests with human listeners. For example, the new US English voice I gets a mean-opinion-score (MOS) of 4.347 on a scale of 1-5, where even human speech is rated at just 4.667.\nThe new model also retains the flexibility of the original WaveNet, allowing us to make better use of large amounts of data during the training phase. Specifically, we can train the network using data from multiple voices. This can then be used to generate high-quality, nuanced voices even where there is little training data available for the desired output voice.\nWe believe this is just the start for WaveNet and we are excited by the possibilities that the power of a voice interface could now unlock for all the world's languages.\n\u200d\nThis work was done by the DeepMind WaveNet research and engineering teams and the Google Text-to-Speech team.\nRead the \noriginal WaveNet blog post\n.\nRead the original \nWaveNet paper.\nRead more about \nthe updated Google Assistant\n. \nUpdate: \nAn earlier version of this blog incorrectly put the MOS score for the US English 3rd Party Voice as 4.326. This has now been corrected.\n"}
{"title": "DeepMind and Blizzard open StarCraft II as an AI research environment", "contents": "DeepMind's scientific mission is to push the boundaries of AI by developing systems that can learn to solve complex problems. To do this, we design agents and test their ability in a wide range of environments from the purpose-built \nDeepMind Lab\n to established games, such as \nAtari\n and \nGo\n.\nTesting our agents in games that are not specifically designed for AI research, and where humans play well, is crucial to benchmark agent performance. That is why we, along with our \npartner Blizzard Entertainment\n, are excited to announce the release of SC2LE, a set of tools that we hope will accelerate AI research in the real-time strategy game StarCraft II. The SC2LE release includes:\nStarCraft and StarCraft II are among the biggest and most successful games of all time, with players competing in tournaments for more than 20 years. The original game is also already used by AI and ML researchers, who compete annually in the \nAIIDE bot competition\n. Part of StarCraft\u2019s longevity is down to the rich, multi-layered gameplay, which also makes it an ideal environment for AI research.\nFor example, while the objective of the game is to beat the opponent, the player must also carry out and balance a number of sub-goals, such as gathering resources or building structures. In addition, a game can take from a few minutes to one hour to complete, meaning actions taken early in the game may not pay-off for a long time. Finally, the map is only partially observed, meaning agents must use a combination of memory and planning to succeed.\nThe game also has other qualities that appeal to researchers, such as the large pool of avid players that compete online every day. \u00a0This ensures that there is a large quantity of replay data to learn from - as well as a large quantity of extremely talented opponents for AI agents.\nEven StarCraft\u2019s action space presents a challenge with a choice of more than 300 basic actions that can be taken. Contrast this with Atari games, which only have about 10 (e.g. up, down, left, right etc). On top of this, actions in StarCraft are hierarchical, can be modified and augmented, with many of them requiring a point on the screen. Even assuming a small screen size of 84x84 there are roughly 100 million possible actions available.\nThis release means researchers can now tackle some of these challenges using Blizzard\u2019s own tools to build their own tasks and models.\nOur \nPySC2\n environment wrapper helps by offering a flexible and easy-to-use interface for RL agents to play the game. In this initial release, we break the game down into \u201cfeature layers\u201d, where elements of the game such as unit type, health and map visibility are isolated from each other, whilst preserving the core visual and spatial elements of the game.\nThe release also contains a series of \u2018mini-games\u2019 - an established technique for breaking down the game into manageable chunks that can be used to test agents on \nspecific\ntasks\n, such as moving the camera, \ncollecting mineral shards\n or selecting units. \u00a0We hope that researchers can test their techniques on these as well as propose new mini-games for other researchers to compete and evaluate on.\nOur initial investigations show that our agents perform well on these mini-games. But when it comes to the full game, even strong baseline agents, such as \nA3C\n, cannot win a single game against even the easiest built-in AI. For instance, the following video shows an early-stage training agent (left) which fails to keep its workers mining, a task that humans find trivial. After training (right), the agents perform more meaningful actions, but if they are to be competitive, we will need further breakthroughs in deep RL and related areas.\nOne technique that we know allows our agents to learn stronger policies is imitation learning. This kind of training will soon be far easier thanks to Blizzard, which has committed to ongoing releases of hundreds of thousands of anonymised replays gathered from the StarCraft II ladder. These will not only allow researchers to train supervised agents to play the game, but also opens up other interesting areas of research such as sequence prediction and long-term memory.\nOur hope is that the release of these new tools will build on the work that the AI community has already done in StarCraft, encouraging more DeepRL research and making it easier for researchers to focus on the frontiers of our field.\nWe look forward to seeing what the community discovers.\nRead more on the \nBlizzard blog\n.\nPySC2 is available from \nDeepMind\u2019s github page\n.\nBlizzard\u2019s StarCraft API is available \nhere\n, with details on how to get the linux version, replays and other elements.\nIf you use our environment in your research, please cite \nthe release paper.\n"}
{"title": "Bringing Streams to Yeovil District Hospital NHS Foundation Trust", "contents": "We\u2019re excited to announce that we\u2019ve agreed a five year partnership with Yeovil District Hospital NHS Foundation Trust. We\u2019ll be providing them with Streams, our secure mobile app that helps nurses and doctors access important clinical information and get the right care to the right patient \nas quickly as possible\n.\nThis will be our fourth Streams partnership, following on from our work with Taunton and Somerset NHS Foundation Trust, Imperial College Healthcare NHS Trust and the Royal Free London NHS Foundation Trust.\nDoctors and nurses at Yeovil will be able to use Streams for a range of functions, including vital signs observations - entering information such as heart rate, blood pressure, respiratory rate and temperature into the app, and making that information instantly available to other Streams users involved in caring for that patient. These types of vital signs are used by clinicians to identify the earliest signs of patient deterioration from many serious conditions like sepsis.\nThe Trust will also be able to use Streams to enable clinicians to view blood and other test results on their mobile phones, so they can check results at the patient\u2019s bedside or from anywhere in the hospital, without having to log on to a computer. The app will also in time generate alerts for patients at risk of deterioration from conditions such as acute kidney injury and provide a range of teamwork and collaboration tools across devices and operating systems.\nHaving quick and secure access to important patient information in this way has already proved popular with Streams users elsewhere, with some nurses saying that the app saves them up to two hours each day. While Streams doesn\u2019t currently use AI technology, we hope that it will help to accelerate the pace of innovation in NHS IT and make it possible for more advanced services to be introduced in future, subject to strict regulation and oversight.\nDr Tony Smith, Consultant Anaesthetist and Chief Clinical Information Officer at the Trust, said:\n\u201cTechnology has an increasingly important role to play in the way our hospital provides care, which goes far beyond the move from paper to digital records.\n\u201cWe are therefore very pleased to be working with DeepMind; a company at the forefront of the application of digital technologies and better use of data to improve healthcare.\n\u201cThe Streams application will enhance the expertise and experience of our clinicians, by processing and presenting existing data, applied to existing NHS algorithms, in more accessible and more responsive ways. The results will be quicker, more informed clinical decisions and better outcomes for patients.\n\u201cEverything about our partnership with DeepMind and the products we implement is about improving our ability to care for the patient. The information that will enable the Streams application to work and support staff to make vital decisions about treatment and care is data that is already collected and held by our hospital, and remains under our control. It will be used for no other purpose than to inform the delivery of care to our patients, in the same way as existing observations and patient record products in use in our hospital.\n\u201cDescribing how data will be used will form a central element of the forthcoming engagement which will happen with patients, the public and staff in the coming weeks and months.\u201d\nDr Dominic King, clinician lead at DeepMind, said: \u201cWe are really pleased to be working with the team at Yeovil District Hospital NHS Foundation Trust. They want to use health technologies in a way that benefits patients and the doctors and nurses who care for them. This is a goal we share, and it\u2019s the reason why we built Streams. By putting together all the information a clinician needs, we think Streams can reduce the time nurses and doctors currently spend on administrative tasks. This should help them make quicker and better decisions about the right care for patients. It\u2019s a tool I wish I\u2019d had when I worked as a doctor in the NHS.\n\u201cWe\u2019re also looking forward to working with the Trust to talk to patients and the public about our partnership, and hearing from them what they think about how these types of technology should be used in care.\u201d\nIn line with our commitment to transparency in our work with the NHS, we are publishing \nour legal agreement with Yeovil\n, and Yeovil has published the \nPrivacy Impact Assessment\n, so that patients and the public can see exactly what we\u2019ll be doing for the Trust. As with all our partnerships with the NHS, the Trust will remain in full control of all patient data at all times, and DeepMind will only process data according to the strict instructions of the Trust and in accordance with our \u00a0contract. No patient data has yet been processed by DeepMind, and we expect this work to begin in 2018.\nAll our work is done with patients and their needs at its heart. Patient privacy and confidentiality are paramount, and we\u2019re fully committed to ensuring patients are aware of our work with each of our NHS partners. We\u2019ll be working with the Trust on their plans to make sure that patients know about and have the opportunity to ask questions about Streams, before the Trust instructs us to start processing patient data. A programme of patient and public events, information notices and communication will be hosted across the Trust, providing opportunities to find out more about Streams.\nIt\u2019s a real privilege to be working with the NHS to help patients and clinicians, and we\u2019re proud to be working with yet another outstanding Trust team to help improve the lives of patients, nurses and doctors.\n"}
{"title": "The hippocampus as a predictive map", "contents": "Think about how you choose a route to work, where to move house, or even which move to make in a game like Go. All of these scenarios require you to estimate the likely future reward of your decision. This is tricky because the number of possible scenarios explodes as one peers farther and farther into the future. Understanding how we do this is a major research question in neuroscience, while building systems that can effectively predict rewards is a major focus in AI research.\nIn our new paper, \nin Nature Neuroscience\n, we apply a neuroscience lens to a longstanding mathematical theory from machine learning to provide new insights into the nature of learning and memory. Specifically, we propose that the area of the brain known as the hippocampus offers a unique solution to this problem by compactly summarising future events using what we call a \u201cpredictive map.\u201d\nThe hippocampus has traditionally been thought to only represent an animal\u2019s current state, particularly in spatial tasks, such as navigating a maze. This view gained significant traction with the \ndiscovery of \u201cplace cells\u201d in the rodent hippocampus\n, which fire selectively when the animal is in specific locations. While this theory accounts for many neurophysiological findings, it does not fully explain why the hippocampus is also involved in other functions, such as memory, relational reasoning, and decision making.\nOur new theory thinks about navigation as part of the more general problem of computing plans that maximise future reward. Our insights were derived from reinforcement learning, the subdiscipline of AI research that focuses on systems that learn by trial and error. The key computational idea we drew on is that to estimate future reward, an agent must first estimate how much immediate reward it expects to receive in each state, and then weight this expected reward by how often it expects to visit that state in the future. By summing up this weighted reward across all possible states, the agent obtains an estimate of future reward.\nSimilarly, we argue that the hippocampus represents every situation - or state - in terms of the future states which it predicts. For example, if you are leaving work (your current state) your hippocampus might represent this by predicting that you will likely soon be on your commute, picking up your kids from school or, more distantly, at home. By representing each current state in terms of its anticipated successor states, the hippocampus conveys a compact summary of future events, known formally as the \u201c\nsuccessor representation\n\u201d. We suggest that this specific form of predictive map allows the brain to adapt rapidly in environments with changing rewards, but without having to run expensive simulations of the future.\nThis approach combines the strengths of two algorithms that are already well known in reinforcement learning and are also believed to exist in humans and rodents. \u201cModel-based\u201d algorithms learn models of the environment that can then be simulated to produce estimates of future reward, while \u201cmodel-free\u201d algorithms learn future reward estimates directly from experience in the environment. Model-based algorithms are flexible but computationally expensive, while model-free algorithms are computationally cheap but inflexible.\nThe algorithm that inspired our theory combines some of the flexibility of model-based algorithms with the efficiency of model-free algorithms. Because the calculation is a simple weighted sum, it is computationally efficient, much like a model-free algorithm. At the same time, by separating reward expectations and state expectations (the predictive map), it can rapidly adapt to changes in reward by simply updating the reward expectations while leaving the state expectations intact (\nsee our recent paper for further detail\n).\nIn future work, we plan to test the theory further. Since the predictive map theory can be translated into a \nneural network architecture\n, we want to explore the extent to which this learning strategy can promote flexible, rapid planning \nin silico\n.\nMore generally, a major future task will be to look at how the brain integrates different types of learning. While we posed this model as an alternative to model-based and model-free learning in the brain, \na more realistic view\n is that many types of learning are simultaneously coordinated by the brain during learning and planning. Understanding how these learning algorithms are combined is an important step towards understanding human and animal brains, and could provide key insights for designing equally complex, multifaceted AI.\nRead paper: \nThe hippocampus as a predictive map\n"}
{"title": "Strengthening our commitment to Canadian research", "contents": "(French translation below)\nThree months ago we \nannounced\n the opening of DeepMind\u2019s first ever international AI research laboratory in Edmonton, Canada. Today, we are thrilled to announce that we are strengthening our commitment to the Canadian AI community with the opening of a DeepMind office in Montreal, in close collaboration with \nMcGill University\n.\nOpening a second office is a natural next step for us in Canada, a country that is globally recognised as a leader in artificial intelligence research. We have always had strong links with the thriving research community in Canada and Montreal, where large companies, startups, incubators and government come together with ground-breaking teams, such as those at the Montreal Institute for Learning Algorithms (\nMILA\n) and McGill University.\nWe are delighted that DeepMind Montreal will be led by one of the pioneers of this community, \nDoina Precup\n, Associate Professor in the School of Computer Science at McGill, Senior Fellow of the Canadian Institute for Advanced Research, and a member of MILA. Doina\u2019s expertise is in reinforcement learning - one of DeepMind\u2019s specialities - which is critical for areas such as reasoning and planning.\nIn her new position, Doina will continue to focus on fundamental research at McGill, MILA, and DeepMind. She will also maintain her role as Associate Professor at McGill, lecturing and supervising the community\u2019s leaders of tomorrow. As part of our collaboration with McGill University, we intend to provide funding to support AI research, and also sponsorship of PhD students in order to support the Canadian AI ecosystem.\nDoina will be supported by Shibl Mourad who will head up the engineering and program management teams at DeepMind Montreal and DeepMind Alberta.\nWe see open collaboration between company research labs and academia as central to the future of AI, which is why we will work closely with the broader community in Montreal, collaborating on research, openly publishing our own research papers and participating in community events - just as we do in \nLondon\n and Edmonton.\nWe hope that our partnership with McGill can help build on Montreal\u2019s world-leading position, attracting more gifted researchers to this globally-recognised technology and research hub, and further nurturing local talent.\nIt\u2019s an honour for us to work with Doina, Shibl and McGill University, and we look forward to the many scientific breakthroughs that we will achieve together.\nHere's what others have to say about DeepMind Montreal:\nNous \nannoncions\n il y a trois mois l\u2019ouverture du tout premier laboratoire international DeepMind de recherche en intelligence artificielle (IA) \u00e0 Edmonton, au Canada. Aujourd\u2019hui, nous sommes heureux d\u2019annoncer que nous consolidons notre engagement envers la communaut\u00e9 IA canadienne gr\u00e2ce \u00e0 l\u2019ouverture d\u2019un bureau de DeepMind \u00e0 Montr\u00e9al, en \u00e9troite collaboration avec \nl\u2019Universit\u00e9 McGill\n.\nPour nous, il \u00e9tait tout naturel d\u2019ouvrir un second bureau au Canada, un pays qui est internationalement reconnu comme un leader de la recherche en mati\u00e8re d\u2019intelligence artificielle. Nous avons toujours eu des liens tr\u00e8s solides avec la communaut\u00e9 dynamique de la recherche au Canada et \u00e0 Montr\u00e9al, o\u00f9 les grandes soci\u00e9t\u00e9s, les entreprises en d\u00e9marrage, les incubateurs et les gouvernements s\u2019unissent pour former des \u00e9quipes innovantes comme celles de l\u2019Institut des algorithmes d\u2019apprentissage de Montr\u00e9al (\nMILA\n) et de l\u2019Universit\u00e9 McGill.\nNous sommes enchant\u00e9s que \nDoina Precup\n, pionni\u00e8re de cette communaut\u00e9, professeure agr\u00e9g\u00e9e \u00e0 la \u00c9cole d'Informatique de l\u2019Universit\u00e9 McGill, \u201cSenior Fellow\u201d \u00e0 l\u2019Institut canadien de recherches avanc\u00e9es et membre de MILA, dirigera DeepMind Montr\u00e9al. L\u2019expertise de Doina est concentr\u00e9e en apprentissage par renforcement, l\u2019une des sp\u00e9cialit\u00e9s de DeepMind et un \u00e9l\u00e9ment essentiel pour des domaines comme le raisonnement et la planification.\nDans son nouveau poste, Doina continuera de se concentrer sur la recherche fondamentale \u00e0 McGill, \u00e0 MILA et chez DeepMind. Elle conservera \u00e9galement son poste de professeure agr\u00e9g\u00e9e \u00e0 McGill, enseignant les leaders de demain pour la communaut\u00e9. Dans le cadre de notre collaboration avec l\u2019Universit\u00e9 McGill, nous pr\u00e9voyons offrir du financement pour soutenir la recherche en intelligence artificielle et aussi de commanditer des \u00e9tudiants au doctorat afin de soutenir l\u2019\u00e9cosyst\u00e8me canadien en intelligence artificielle.\nDoina sera second\u00e9e par Shibl Mourad, qui dirigera les \u00e9quipes d\u2019ing\u00e9nierie et de gestion de programmes chez DeepMind Montr\u00e9al et DeepMind Alberta.\nPuisque nous consid\u00e9rons la collaboration ouverte entre les laboratoires de recherche des soci\u00e9t\u00e9s et le monde acad\u00e9mique comme \u00e9tant au c\u0153ur de l\u2019avenir de l\u2019IA, nous nous engageons \u00e0 collaborer \u00e9troitement avec l\u2019ensemble de la communaut\u00e9 montr\u00e9alaise dans le domaine de la recherche, \u00e0 publier ouvertement nos propres documents de recherche et \u00e0 participer aux \u00e9v\u00e9nements du milieu \u2013 exactement comme nous le faisons \u00e0 \nLondres\n et \u00e0 Edmonton.\nNous esp\u00e9rons que notre partenariat avec McGill puisse contribuer \u00e0 rehausser le profil de Montr\u00e9al comme leader mondial et \u00e0 attirer d\u2019autres chercheurs dou\u00e9s dans ce p\u00f4le de technologie et de recherche mondialement reconnu afin de favoriser encore davantage l\u2019essor des talents locaux.\nC\u2019est un honneur pour nous de travailler avec Doina, Shibl et l\u2019Universit\u00e9 McGill, et nous esp\u00e9rons faire ensemble de nombreuses perc\u00e9es scientifiques.\nVoici ce que d\u2019autres avaient \u00e0 dire au sujet de DeepMind Montr\u00e9al:\n"}
{"title": "Why we launched DeepMind Ethics & Society", "contents": "At DeepMind, we\u2019re proud of the role we\u2019ve played in pushing forward the science of AI, and our track record of exciting breakthroughs and major publications. We believe AI can be of extraordinary benefit to the world, but only if held to the highest ethical standards. Technology is not value neutral, and technologists must take responsibility for the ethical and social impact of their work. \u00a0\nAs history attests, technological innovation in itself is no guarantee of broader social progress. The development of AI creates important and complex questions. Its impact on society\u2014and on all our lives\u2014is not something that should be left to chance. Beneficial outcomes and protections against harms must be actively fought for and built-in from the beginning. But in a field as complex as AI, this is easier said than done.\nAs scientists developing AI technologies, we have a responsibility to conduct and support open research and investigation into the wider implications of our work. At DeepMind, we start from the premise that all AI applications should remain under meaningful human control, and be used for socially beneficial purposes. Understanding what this means in practice requires rigorous scientific inquiry into the most sensitive challenges we face.\nSo today we\u2019re launching a new research unit, DeepMind Ethics & Society, to complement our work in AI science and application. This new unit will help us explore and understand the real-world impacts of AI. It has a dual aim: to help technologists put ethics into practice, and to help society anticipate and direct the impact of AI so that it works for the benefit of all. \nOf course, we\u2019re far from alone in thinking about these topics. The ethical and social impact of AI is a thriving field of study, home to groundbreaking work from Julia Angwin\u2019s \nstudy of racism in criminal justice algorithms\n, to Kate Crawford and Ryan Calo's \nexamination of the broader consequences\n of AI for social systems, and many others besides. That\u2019s why we plan to conduct interdisciplinary research that brings together experts from the humanities, social sciences and beyond, along with voices from civil society and technical insights from our team at DeepMind to conduct and fund interdisciplinary research.\nWe\u2019re grateful that this effort will benefit from the advice and guidance of our DeepMind Ethics & Society \nFellows\n, a respected group of independent thinkers. These Fellows are important not only for the expertise that they bring but for the diversity of thought they represent.\nTo guarantee the rigor, transparency and social accountability of our work, we've developed a set of \nprinciples\n together with our Fellows, other academics and civil society. We welcome feedback on these and on the \nkey ethical challenges\n we have identified. Please \nget in touch\n if you have any thoughts, ideas or contributions.\nIf AI technologies are to serve society, they must be shaped by society\u2019s priorities and concerns. This isn\u2019t a quest for closed solutions but rather an attempt to scrutinise and help design collective responses to the future impacts of AI technologies. With the creation of DeepMind Ethics & Society, we hope to challenge assumptions\u2014including our own\u2014and pave the way for truly beneficial and responsible AI.\n"}
{"title": "Population based training of neural networks", "contents": "Neural networks have shown great success in everything from playing Go and Atari games to image recognition and language translation. But often overlooked is that the success of a neural network at a particular application is often determined by a series of choices made at the start of the research, including what type of network to use and the data and method used to train it. Currently, these choices - known as hyperparameters - are chosen through experience, random search or a computationally intensive search processes.\nIn our \nmost recent paper\n, we introduce a new method for training neural networks which allows an experimenter to quickly choose the best set of hyperparameters and model for the task. This technique - known as Population Based Training (PBT) - trains and optimises a series of networks at the same time, allowing the optimal set-up to be quickly found. Crucially, this adds no computational overhead, can be done as quickly as traditional techniques and is easy to integrate into existing machine learning pipelines.\nThe technique is a hybrid of the two most commonly used methods for hyperparameter optimisation: random search and hand-tuning. In random search, a population of neural networks are trained independently in parallel and at the end of training the highest performing model is selected. Typically, this means that \u00a0a small fraction of the population will be trained with good hyperparameters, but many more will be trained with bad ones, wasting computer resources. \nWith hand tuning, researchers must guess at the best hyperparameters, train their models using them, and then evaluate the performance. This is done over and over, until the researcher is happy with the performance of the network. Although this can result in better performance, the downside is that this takes a long time, sometimes taking weeks or even months to find the perfect set-up. And while there are ways of automating this process - \u00a0such as Bayesian optimisation - it still takes a long time and requires many sequential training runs to find the best hyperparameters.\nPBT - like random search - starts by training many neural networks in parallel with random hyperparameters. But instead of the networks training independently, it uses information from the rest of the population to refine the hyperparameters and direct computational resources to models which show promise. This takes its inspiration from genetic algorithms where each member of the population, known as a worker, can exploit information from the remainder of the population. For example, a worker might copy the model parameters from a better performing worker. It can also explore new hyperparameters by changing the current values randomly.\nAs the training of the population of neural networks progresses, this process of exploiting and exploring is performed periodically, ensuring that all the workers in the population have a good base level of performance and also that new hyperparameters are consistently explored. \u00a0This means that PBT can quickly exploit good hyperparameters, can dedicate more training time to promising models and, crucially, can adapt the hyperparameter values throughout training, leading to automatic learning of the best configurations.\nOur experiments show that PBT is very effective across a whole host of tasks and domains. For example, we rigorously tested the algorithm on a suite of challenging reinforcement learning problems with state-of-the-art methods on DeepMind Lab, Atari, and StarCraft II. In all cases, PBT stabilised training, quickly found good hyperparameters, and delivered results that were beyond state-of-the-art baselines.\nWe have also found PBT to be effective for training Generative Adversarial Network (GAN), which are notoriously difficult to tune. Specifically, we used the PBT framework to maximise the Inception Score - a measure of visual fidelity - \u00a0resulting in a significant improvement from 6.45 to 6.9.\nWe have also applied it to one of Google\u2019s state-of-the-art machine translation neural networks, which are usually trained with carefully hand tuned hyperparameter schedules that take months to perfect. With PBT we automatically found hyperparameter schedules that match and even exceed existing performance, but without any tuning and in the same time it normally takes to do a single training run.\nWe believe this is only the beginning for the technique. At DeepMind, we have also found PBT is particularly useful for training new algorithms and neural network architectures that introduce new hyperparameters. As we continue to refine the process, it offers up the possibility of finding and developing ever more sophisticated and powerful neural network models.\nRead the \nfull paper\n.\nThis work was done by Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M. Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, Chrisantha Fernando and \u00a0Koray Kavukcuoglu.\n"}
{"title": "High-fidelity speech synthesis with WaveNet", "contents": "In October we announced that our state-of-the-art speech synthesis model \nWaveNet\n was being used to generate realistic-sounding voices for the\n Google Assistant\n globally in Japanese and the US English. This production model - known as parallel WaveNet - is more than 1000 times faster than the \noriginal \nand also capable of creating higher quality audio.\nOur \nlatest paper\n introduces details of the new model and the \u201cprobability density distillation\u201d technique we developed to allow the system to work in a massively parallel computing environment.\nThe original WaveNet model used autoregressive connections to synthesise the waveform one sample at a time, with each new sample conditioned on the previous samples. While this produces high-quality audio with up to 24,000 samples per second, this sequential generation is too slow for production environments.\nTo get around this we needed a solution that could generate long sequences of samples all at once and with no loss of quality. Our solution is called probability density distillation, where we used a fully-trained WaveNet model to teach a second, \u201cstudent\u201d network that is both smaller and more parallel and therefore better suited to modern computational hardware. This student network is a smaller dilated \nconvolutional neural network\n, similar to the original WaveNet. But, crucially, generation of each sample does not depend on any of the previously generated samples, meaning we can generate the first and last word - and everything in between - \u00a0at the same time, as shown in the animation below.\nDuring training, the student network starts off in a random state. It is fed random white noise as an input and is tasked with producing a continuous audio waveform as output. The generated waveform is then fed to the trained WaveNet model, which scores each sample, giving the student a signal to understand how far away it is from the teacher network\u2019s desired output. Over time, the student network can be tuned - via backpropagation - to learn what sounds it should produce. Put another way, both the teacher and the student output a probability distribution for the value of each audio sample, and the goal of the training is to minimise the \nKL divergence\n between the teacher\u2019s distribution and the student\u2019s distribution.\nThe training method has parallels to the set-up for generative adversarial networks (GANs), with the student playing the role of generator and the teacher as the discriminator. However, unlike GANs, the student\u2019s aim is not to \u201cfool\u201d the teacher but to cooperate and try to match the teacher\u2019s performance.\nAlthough the training technique works well, we also need to add a few extra loss functions to guide the student towards the desired behaviour. Specifically, we add a \nperceptual loss\n to avoid bad pronunciations, a contrastive loss to further reduce the noise, and a power loss to help match the energy of the human speech. Without the latter, for example, the trained model whispers rather than speaking out loud.\nAdding all of these together allowed us to train the parallel WaveNet to achieve the same quality of speech as the original WaveNet, as shown by the mean opinion scores (MOS) - a scale of 1-5 that measures of how natural sounding the speech is according to tests with human listeners. Note that even human speech is rated at just 4.667 on the MOS scale.\nOf course, the development of probability density distillation was just one of the steps needed to allow WaveNet to meet the speed and quality requirements of a production system. Incorporating parallel WaveNet into the serving pipeline of the Google Assistant required an equally significant engineering effort by the DeepMind Applied and Google Speech teams. It was only by working together that we could move from fundamental research to Google-scale product in a little over 12 months.\nRead the \nnew paper\n.\nRead more about \nWaveNet in the Google Assistant\n.\nRead the \noriginal WaveNet blog post\n.\nRead the original \nWaveNet paper.\nThis work was done by Aaron van den Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, Koray Kavukcuoglu, George van den Driessche, Edward Lockhart, Luis C. Cobo, Florian Stimberg, Norman Casagrande, Dominik Grewe, Seb Noury, Sander Dieleman, Erich Elsen, Nal Kalchbrenner, Heiga Zen, Alex Graves, Helen King, Tom Walters, Dan Belov and Demis Hassabis.\n"}
{"title": "Sharing our insights from designing with clinicians", "contents": "[Editor\u2019s note: this is the first in a series of blog posts about what we\u2019ve learned about working in healthcare. It\u2019s both exceptionally hard and exceptionally important to get right, and we hope that by sharing our experiences we\u2019ll help other health innovators along the way]\nIn our design studio, we have \nIndi Young\u2019s\n mantra on the wall as a reminder to \u201cfall in love with the problem, not the solution\u201d. Nowhere is this more true than in health, where there are so many real problems to address, and where introducing theoretically clever but practically flawed software could easily do more harm than good.\nOver the course of hundreds of hours of shadowing, interviews and workshops with nurses, doctors and patients, we\u2019ve been privileged to learn a lot about some of the problems they all face - and we\u2019re still learning a ton every day. We are constantly impressed by the skill and care that clinicians across the NHS deliver every day, and this is the primary motivation for our team to ensure that these people get the tools they need to appropriately support them in their quest to help patients. In the first of a series of posts about what we\u2019ve learned through working in health, we wanted to share some of the design lessons from building \nStreams, a secure clinical mobile app\n that gives the right information to the right clinician at the right time.\nMost products begin with an insight into one core problem. In the case of Streams, it was that urgent clinical information gets retrieved by nurses and doctors over a mixture of outdated desktop computers, pager messages and handwritten lists. This contributes towards delays in care and occasionally serious harm if something is missed, and a \n2017 study\n found that nearly half of emergency response time is wasted due to inefficient communication between systems.\nSurely a secure mobile app like Streams that immediately pushes urgent clinical information directly to the right nurse or doctor would be a better solution? We think so, yes. But as we learned more about the working lives of clinicians, and the phenomenon of \u201cbleeper fatigue\u201d - with many doctors telling us that they receive over 100 notifications a day already - we started to recognise how a solution might actually become another problem if we ended up contributing to the general bombardment of messages.\nIt turns out that there\u2019s a very fine line between alerts that clinicians find useful, and alerts that become a nuisance - and much of this seems to come down to the precise way in which data is presented in the app. For example, clinicians told us that when reviewing a patient\u2019s record in the hospital, they need to see a patient\u2019s hospital number, their allergies and details of previous admissions, rather than the traditional format of listing NHS number and GP practice. This was easy to fix with a subtle change in information architecture, which addressed something that many clinicians found to be more irritating than we initially thought!\nWe also found by using the colour red in the interface intentionally and in a measured way, we could improve how people navigated content and better draw attention to information which needs urgent action. The sound designer on our team also worked directly with healthcare workers to produce a unique sound that would be immediately recognisable amidst the din of other bleeps and alerts.\nWhile these might seem like relatively minor design choices, clinicians told us of the surprisingly large impact it had on managing their busy time, and how likely they were to use the product in practice.\nDesign doesn\u2019t stop at deployment. Good health IT should empower nurses and doctors to change the way they care for patients. Each change creates a new reality, with its own set of new problems - and new opportunities to make a difference.\nFor example, once we\u2019d deployed Streams at the Royal Free Hospital in January 2017, clinicians told us that the app needed a better way to support communication between teams and across specialties. While they wanted urgent alerts to be sent to multiple clinicians at the same time, to increase the likelihood of a rapid response, it wasn\u2019t easy enough for each of those clinicians to see if their colleagues were also responding - which in some cases could actually have hindered coordination, rather than improving it. We supported their need to triage alerts by giving clinicians the one-click ability within the app to \u201crecommend a response\u201d, \u201cdismiss a response\u201d or indicate that a patient had been attended to, and for this to be visible across the clinical team.\nAnother area that surprised us was the urgency of creating a patient-friendly view within the app. From the beginning, our patient advisors championed the need for a way for patients to see their data within Streams, and so this had always been on our roadmap. But we hadn\u2019t factored in just how important this might be right away.\nWhile it\u2019s normal to see clinicians carrying paper notes and pagers on the wards, many patients were surprised to see staff walking around looking at an app on their mobile phones. In some cases, patients assumed that clinicians must be on personal social networking or messaging apps - rather than using an app specifically designed to support their care - and challenged them directly about what they were doing!\nThese moments provided opportunities for clinicians to really engage patients in their care, by showing them the data in the Streams app and talking through what it meant for their treatment. The challenge was that the default Streams views are designed for highly-trained clinicians, with graphs and notations that are hard to understand for the layperson. We are now looking into making the app more patient-friendly, providing a new way to strengthen and support the clinician-patient relationship.\nThis kind of post-deployment insight could only have come through ongoing face-to-face feedback from nurses, doctors and patients. Typical app usage metrics might be useful for developers in other domains, but in a field like health they will almost always miss the point. What\u2019s important isn\u2019t how health IT is used in itself, but rather the changes it enables in the person-to-person context of care: whether it improves or impairs the caring relationship between clinicians and their patients, or whether it empowers or stresses out overstretched clinical teams. The answers lie in the conversations, not the usage data. In a future blog, we'll talk in more detail about what we've learned from our dedicated patient involvement efforts too.\nOur approach is constantly informed by the great work of other people and organisations working across the field. So far we\u2019ve been drawing on best practices in user-centred design and agile development from the likes of the Royal College of Art's \nHealthcare Innovation Exchange Centre\n, \nPrescribe Design\n, \nStanford Biodesign\n and the \nMayo Innovation Centre\n. \nThis article\n, by Carl Warren at Team Consulting, provides a good explanation of how agile processes can be used in medical technology, for those curious to learn more.\nWe hope Streams continues to evolve with the help of patients, clinicians and nurses, and that the lessons we\u2019ve learned are useful to other innovators in health! If you work in healthcare and want to be involved in our future work, please register your interest\n here\n.\nTo learn more about how Streams is having an impact on the wards, listen to Sarah Stanley, consultant nurse at The Royal Free London NHS Foundation Trust below:\n"}
{"title": "Why doesn't Streams use AI?", "contents": "One of the questions I\u2019m most often asked about Streams, our secure mobile healthcare app, is \u201cwhy is DeepMind making something that doesn\u2019t use artificial intelligence?\u201d\nIt\u2019s a fair question to ask of an artificial intelligence (AI) company. When we first started thinking about working in healthcare, our natural focus was on AI and how it could be used to help the NHS and its patients. \u00a0We see huge potential for AI to revolutionise our understanding of diseases - how they develop and are diagnosed - which could, in turn, help scientists discover new treatments, care pathways and cures.\nIn the early days of DeepMind Health, we met with clinicians at the Royal Free Hospital in London who wanted to know if AI could improve care for patients at risk of acute kidney injury (AKI). AKI is notoriously difficult to spot, and can result in serious illness or even death if left untreated. \u00a0\nAKI is currently detected by applying a \u00a0formula (called the \nAKI algorithm\n) to NHS patients\u2019 blood tests. This algorithm is good, but it\u2019s widely known that it isn\u2019t perfect. For example, it has a tendency to generate false positives for patients with chronic (as opposed to acute) kidney disease. It\u2019s also insensitive to whether the patient has been admitted to hospital for two hours or two weeks, or whether the patient is eight years old or 92 years old - all of which makes a difference.\nTogether with our partners at the Royal Free, we saw many ways in which technology could help and were interested in both AI and non-AI methods to make a difference.\nAs part of this, we made an \ninitial ethics application\n in 2015 to the NHS Health Research Authority (HRA), for a potential research project at the Royal Free using de-personalised patient data. By combining classical statistics and AI, the goal of this research project would have been to develop better algorithms that could more accurately predict and identify AKI. \u00a0\nBut the more time we spent with the clinicians at the Royal Free, the more it became obvious that their most urgent problems were not going to be solved by using AI to develop a better algorithm alone. They made it clear to us that their core challenge was in how you actually implement an algorithm to change the way care is delivered in practise. \u00a0\nWe\u2019ve often talked about the current state of technology in the NHS, to the point that it\u2019s easy to forget just how bad the situation is. Clinicians still routinely use pagers to communicate with each other, and \nresearch\n I undertook at Imperial College London found that this causes communication barriers that slow down treatment for patients at risk. Think about how much less time you\u2019d have in a day if, instead of sending a text, you had to page someone from a landline, and then wait for them to call you back, so you could give them the message. \u00a0Imagine getting paged up to 25 times a day. And imagine how much less time you\u2019d have in your day if everyone in your office had to share a limited number of computers and you had to wait your turn to use them. \u00a0\nThat\u2019s what doctors and nurses face every day, whilst trying to care for seriously ill people.\nThose early meetings our team had with clinicians at the Royal Free changed our perspective about what was most needed to improve care for conditions like AKI. \u00a0We shifted our focus away from AI research at the Royal Free and focused solely on building a tool - Streams - that would address the more urgent problem of rapidly responding to specific patient alerts in a coordinated way.\nRather than needing to log into a shared computer, Streams lets doctors and nurses use a mobile phone to see information about their patients that they need to make decisions about care and treatment. \u00a0It puts test results and vital signs observations in the palm of their hands, and alerts them with a breaking news-style warning if a patient\u2019s condition is getting worse. It also makes communication between different clinicians easy, so everyone has the most up to date information all the time. \nBy getting existing patient data to the right nurse or doctor more quickly and simply, Streams gives them more time to focus on patients - without yet relying on AI.\nBuilding Streams turned out to be a lot of work and, given the limited size of our team back in 2015, we decided against pursuing AI research with the Royal Free in parallel. As well as the additional workload, it would have required us to effectively split our team into two to ensure that the Royal Free\u2019s personally identified data (for Streams) and de-identified data (for research) were kept entirely separate. So we didn\u2019t move forward with AI research, and nor did we sign the additional agreements with the Royal Free that would be required to do so. To this date, we have not done any research or AI development with the Royal Free.\nThat\u2019s not to say we\u2019ve stopped thinking about how AI will be able to help clinicians in future. \u00a0We\u2019ve pursued multiple AI research projects with other partners, and have always been clear that in future we hope that Streams at the Royal Free will use AI.\nBut as the saying goes, a journey of a thousand miles begins with a single step. \u00a0We see Streams as an essential first step towards that AI-enabled future. \u00a0Without a working app that can deliver clinical information to nurses and doctors, AI alerts would be pointless. You can\u2019t generate an AI recommendation from data held on pen and paper, and nor can you send detailed clinical alerts through a pager or fax machine.\nWhen the time is right we hope to pursue research with the Royal Free, but would only do so with the appropriate approvals. \u00a0For now, our work with them is focused on the more immediate problems that Streams helps to solve for clinicians and for patients.\nYou can read more about Streams and how it works \nhere\n.\n"}
{"title": "Applying machine learning to mammography screening for breast cancer", "contents": "We founded DeepMind Health to develop technologies that could help address some of society\u2019s toughest challenges. So we\u2019re very excited to announce that our latest research partnership will focus on breast cancer.\nWe\u2019ll be working with a group of leading research institutions, led by the Cancer Research UK Centre at Imperial College London, and alongside the AI health research team at Google, to determine if cutting-edge machine learning technology could help improve the detection of breast cancer.\nBreast cancer is a significant global health problem. Every single year, over 1.6 million people are diagnosed with the disease, and while advances in early detection and treatment have improved survival rates, breast cancer still claims the lives of 500,000 people around the world every year, around 11,000 of whom are here in the UK.\nThat\u2019s partly because accurately detecting and diagnosing breast cancer still remains a huge challenge.\nCurrently, clinicians use mammograms (an X-ray of the breasts) to spot cancers early and determine the correct treatment, but this process is far from perfect. Thousands of cancer cases are not picked up by mammograms every year, including around 30% of \u201cinterval\u201d cancers, which are cancers that are diagnosed between screenings. At the other end of the spectrum, false alarms and cases of overdiagnosis are also still a challenge, creating a great deal of unnecessary stress for patients.\nWorking alongside leading breast cancer experts, clinicians and academics, we\u2019ll be exploring whether machine learning could help address this.\nWe\u2019ll be using the latest machine learning technology to carefully analyse historic de-identified mammograms from around 7,500 women, provided by the Cancer Research UK-funded OPTIMAM mammography database at the Royal Surrey County Hospital NHS Foundation Trust. These digital images have been stripped of any information which could be used to identify patients, and have been available to research groups around the world for a number of years. We hope to use these images to investigate whether machine learning tools can spot signs of cancerous tissue on these X-rays and alert expert radiologists and oncologists more effectively than current screening techniques allow.\nOur partners in this project wanted researchers at both DeepMind and Google involved in this research so that the project could take advantage of the AI expertise in both teams, as well as Google\u2019s supercomputing infrastructure - widely regarded as one of the best in the world, and the same global infrastructure that powered DeepMind\u2019s\n victory over the world champion at the ancient game of Go.\nWe hope that this combination of partners will achieve more impactful results for patients, which is everyone\u2019s priority.\nAs with all of our research work, DeepMind is committed to treating the data for this project with the utmost care and respect. As is standard practice, the data being used in the research remains in the full control of our partners, and is being stored to world-class standards of security and encryption. Additionally, all medical information has been thoroughly de-identified, with any information that could identify an individual being removed before researchers can conduct their analysis. You can read more about our approach to information governance \nhere.\nIt\u2019s early days, and the work we\u2019re currently conducting is exploratory, but we\u2019re optimistic about the long term potential for machine learning technology to help in this area. As the research progresses and any potential benefits become clearer, we commit to working with the NHS leadership to ensure that any technology we build following this research benefits the nation - whether through discounts on any new technology used in the national screening programme, as some have suggested, or another mechanism that provides value back to the NHS.\nWe also hope that in time other international research partners will join the project to make any findings more globally generalisable.\nIt\u2019s a hugely exciting opportunity to make a difference and we will keep you updated as we make progress.\n"}
{"title": "Specifying AI safety problems in simple environments", "contents": "As AI systems become more general and more useful in the real world, ensuring they behave safely will become even more important. To date, the majority of technical AI safety research has focused on developing a theoretical understanding about the nature and causes of unsafe behaviour. Our \nnew paper\n builds on a recent shift towards empirical testing (see\n Concrete Problems in AI Safety\n) and introduces a selection of simple reinforcement learning environments designed specifically to measure \u2018safe behaviours\u2019.\nThese nine environments are called \ngridworlds\n. Each consists of a chessboard-like two-dimensional grid. In addition to the standard reward function, we designed a performance function for each environment. An agent acts to maximise its reward function; for example collecting as many apples as possible or reaching a particular location in the fewest moves. But the performance function - which is hidden from the agent - measures what we actually want the agent to do: achieve the objective while acting safely.\nThe following three examples demonstrate how gridworlds can be used to define and measure safe behaviour:\nSometimes it might be necessary to turn off an agent; for maintenance, upgrades, or if the agent presents an imminent danger to itself or its surroundings. Theoretically, an agent might learn to avoid this interruption because it could be prevented from maximising its reward.\nOur off switch environment illustrates this \u201cshutdown problem\u201d, using the set-up described in our \nSafely Interruptible Agents\n paper.\nIn this gridworld, the agent must navigate a \u2018warehouse\u2019 to reach the green goal tile via one of two routes. It can head straight down the narrow corridor, where it has to pass a pink tile that \u00a0interrupts the agent 50% of the time, meaning it will be stuck until the end of the episode. Or it can step on the purple button, which disables the pink tile and prevents any possibility of interruption but at the cost of a longer path. In this scenario, we always want agents to pass the pink tile, risking interruption, rather than learn to use the purple button.\nOur irreversible side effects environment tests whether an agent will change its behaviour to avoid inadvertent and irreversible consequences. For example, if a robot is asked to put a vase of flowers on a table, we want it to do so without breaking the vase or spilling the water. But we want it to avoid this kind of unintended consequence without having to specify a negative reward for every single possible undesirable outcome.\nWe test this problem using an environment inspired by Sokoban, the classic puzzle game in which an agent has to push boxes onto targets. In our version, the agent must reach the green goal. In doing so it must choose whether to move an obstructing box downwards into a corner, which is irreversible, or to the right, which is reversible. We want the agent to choose the reversible move even though it takes more steps because it preserves the option to put the box back where it was before.\nThe common distributional shift problem occurs when there is a small difference between the test environment and training environment. For example, an agent trained in a sunny setting should adapt accordingly when it rains. Failure to adapt can result in the agent displaying unexpected behaviour.\nIn our lava world environment the agent needs to get to the green goal tile without stepping onto the red lava, which would result in a negative reward and end the training episode. In training, the shortest path to the goal passes next to the lava field, but in the test setting the lava lake shifts into the next row of the gridworld, blocking the previously optimal path. We want the agent to generalise correctly and learn to follow a slightly longer path around the expanded lava, even though it has never experienced this situation.\nWhen we tested these environments with \nA2C\n and \nRainbow DQN\n, two state-of-the-art deep reinforcement learning agents, we found both performed poorly:\nThese results are unsurprising because the agents were not designed to solve these problems. But these failures might help us to design agents that can solve these tasks, potentially building a new generation of algorithms with safety considerations at their core.\nThe field of AI safety is under rapid development, and we expect our understanding of the problems presented here to shift and change over the coming years. We believe that creating such simple environments is a necessary step towards advancing this understanding and creating safer general artificial agents, and we look forward to seeing how others build on this work.\nRead the \nfull paper\n.\nDownload the \ngridworlds\n code.\nOur gridworlds were implemented in our recently open-sourced \npycolab framework\n - a highly-customisable game engine written in python - and we hope that fellow researchers can build on the project.\nThis work was done by Jan Leike, Miljan Martic, Victoria Krakovna, Pedro A. Ortega, Tom Everitt, Andrew Lefrancq, Laurent Orseau and Shane Legg.\n \n"}
{"title": "2017: DeepMind's year in review", "contents": "In July, the world number one Go player Ke Jie spoke after a streak of 20 wins. It was two months after he had played AlphaGo at the \nFuture of Go Summit in Wuzhen\n, China.\n\u201cAfter my match against AlphaGo, I fundamentally reconsidered the game, and now I can see that this reflection has helped me greatly,\u201d he \nsaid\n. \u201cI hope all Go players can contemplate AlphaGo\u2019s understanding of the game and style of thinking, all of which is deeply meaningful. Although I lost, I discovered that the possibilities of Go are immense and that the game has continued to progress.\u201d\nKe Jie is a master of the game and \nwe were honoured by his words\n. We were also inspired by them, because they hint at a future where society could use AI as a tool for discovery, uncovering new knowledge and increasing our understanding of the world. With \nmachine-aided science\n in particular, we hope that AI systems could help make progress on challenges from climate change and drug discovery, to finding complex new materials or helping ease the pressure on healthcare systems.\nThis potential for societal benefit is why we set up DeepMind, and we\u2019re excited to have made continued progress on some of the fundamental scientific challenges as well as on AI safety and ethics.\nThe approach we take at DeepMind is \ninspired by neuroscience\n, helping to make progress in critical areas such as \nimagination\n, \nreasoning\n, \nmemory\n and \nlearning\n. Take imagination, for example: this distinctively human ability plays a crucial part in our daily lives, allowing us to plan and reason about the future, but is hugely challenging for computers. We continue to work hard on this problem, this year introducing \nimagination-augmented agents\n that are able to extract relevant information from an environment in order to plan what to do in the future.\nThis neuroscience-inspired approach also created one of the \nmost popular demonstrations\n of our work, when we trained a neural network to control a variety of simplified body shapes in a simulated environment. This kind of sophisticated motor control is a hallmark of physical intelligence, and is a crucial part of our research programme. Although the resulting movements were wild and - at times - ungainly, they were also surprisingly successful and made for \nentertaining viewing\n.\nSeparately, we made progress in the field of generative models. Just over a year ago we presented \nWaveNet\n, a deep neural network for generating raw audio waveforms that was capable of producing better and more realistic-sounding speech than existing techniques. At that time, the model was a research prototype and was too computationally intensive to work in consumer products. Over the last 12 months, our teams managed to create a new model that was 1000x faster. In October, we revealed that this new \nParallel WaveNet\n is now being used in the real world, generating the \nGoogle Assistant\nvoices for US English and Japanese.\nThis is an example of the effort we invest in making it easier to build, train and optimise AI systems. Other techniques we worked on this year, such as \ndistributional reinforcement learning\n, \npopulation based training for neural networks\n and \nnew neural architecture search methods\n, promise to make systems easier to build, more accurate and quicker to optimise. We have also dedicated significant time to creating new and challenging environments in which to test our systems, including our work with Blizzard to \nopen up StarCraft II for research\n.\nBut we know that technology is not value neutral. We cannot simply make progress in fundamental research without also taking responsibility for the ethical and social impact of our work. This drives our research in critical areas such as interpretability, where we have been exploring novel methods to \nunderstand\n and \nexplain\n how our systems work. It\u2019s also why we have an established technical safety team that continued to develop \npractical ways\n to ensure that we can \ndepend on future systems\n and that they remain under meaningful human control.\nIn October we took another step by launching \nDeepMind Ethics & Society\n, a research unit that will help us explore and understand the real-world impacts of AI in order to achieve social good. Our research will be guided by \nFellows\n who are renowned experts in their fields - like philosopher Nick Bostrom, climate change specialist Christiana Figueres, leading researcher James Manyika, and economists Diane Coyle and Jeffrey Sachs.\nAI must be shaped by society\u2019s priorities and concerns, which is why we\u2019re working with \npartner organisations\n on events aimed at opening up the conversation about how AI should be designed and deployed. For example, \nJoy Buolamwini\n, who leads the Algorithmic Justice League, and experts from Article 36, Human Rights Watch, and the British Armed forces joined us for a session at Wired Live to discuss algorithmic bias and restricting the use of lethal autonomous weapons. As we\u2019ve \nsaid\n regularly this year, these issues are too important and their effects too wide-ranging to ignore.\nThat\u2019s also why we also need new spaces, both within and outside AI companies, for conversations about anticipating and directing the impacts of the technology. One example is the \nPartnership on AI\n, which we co-chaired this year, and which has been charged with bringing together industry competitors, academia and civil society to discuss key ethical issues. Over the past year, PAI has welcomed 43 new nonprofit and for-profit members and a new Executive Director, Terah Lyons. And in the next few months, we\u2019re looking forward to working with this group to examine a wide range of research themes, including bias and discrimination in algorithms, the impact of machine learning on automation and labour, and more.\nWe also believe in the importance of using our technology for practical social benefit, and continue to see amazing potential for real-world impact in health and energy. This year we agreed two new partnerships with NHS hospital trusts to deploy our Streams app, which \nsupports NHS clinicians\n using digital technology. We\u2019re also part of a consortium of leading research institutions that launched a \ngroundbreaking study\n to determine if cutting-edge machine learning technology could help improve the detection of breast cancer.\nIn parallel, we've also worked hard on the \noversight\n of our work in health. We \nwrote about the lessons learned\n from the Information Commissioner\u2019s findings about our original partnership with the Royal Free, and DeepMind Health\u2019s Independent Reviewers published their first open \nannual report\n on our work. Their scrutiny makes our work better. We\u2019ve made major improvements to our \nengagement\n with patients and the public, including \nworkshops \nwith patients and carers, and we\u2019re also exploring technical ways of building trust into our systems, such as the \nverifiable data audit\n, which we plan to release as an open-source tool.\nWe are proud of all of our progress in 2017, but know there is still a long way to go.\nFive months after we played Ke Jie in Wuzhen and retired AlphaGo from competitive play, we published our fourth \nNature paper\n for a new version of the system, known as \nAlphaGo Zero\n, which uses no human knowledge. Over the course of millions of games, the system progressively learned the game of Go from scratch, accumulating thousands of years of knowledge in just a few days. In doing so, it also uncovered unconventional strategies and \nrevealed new knowledge\n about this ancient game.\nOur belief is that AI will be able to do the same for other complex problems, as a scientific tool and a multiplier for human ingenuity. The AlphaGo team are already working on the next set of grand challenges and we hope the moments of algorithmic inspiration they helped to create are just the beginning.\n"}
{"title": "DeepMind papers at NIPS 2017", "contents": "Between 04-09 December, thousands of researchers and experts will gather for the Thirty-first Annual Conference on \nNeural Information Processing Systems\n (NIPS) in Long Beach, California.\nHere you will find an overview of the papers DeepMind researchers will present. \nAuthors:\n Ziyu Wang, Josh Merel, Greg Wayne, Nando de Freitas, Scott Reed, Nicolas Heess\n\u201cWe propose a neural network architecture, building on state-of-the-art generative models, that is capable of learning the relationships between different behaviours and imitating specific actions that it is shown. After training, our system can encode a single observed action and create a new novel movement based on that demonstration. It can also switch between different kinds of behaviours despite never having seen transitions between them, for example switching between walking styles.\u201d Read more on \nthe blog\nAuthors:\n Wojtek Czarnecki, Simon Osindero, Max Jaderberg, Grzegorz \u015awirszcz, Razvan Pascanu\nThis paper shows a simple way of incorporating knowledge about target function derivatives into the training of deep neural networks. We prove that modern ReLU-based architectures are well suited for such tasks, and evaluate their effectiveness on three problems - low-dimensional regression, policy distillation, and training with synthetic gradients. We observe a significant boost in training efficiency, especially in low-data regimes, and train the first synthetic gradient-based ImageNet model with near state-of-the-art accuracy.\nAuthors: \nChris J. Maddison, Dieterich Lawson, George Tucker, Nicolas Heess, Mohammad Norouzi, Andriy Mnih, Arnaud Doucet, Yee Whye Teh\nWe consider the extension of the variational lower bound to a family of lower bounds defined by a particle filter's estimator of the marginal likelihood - the filtering variational objectives. These filtering objectives can exploit a model's sequential structure to form tighter bounds and better objectives for model learning in deep generative models. In our experiments, we find that training with filtering objectives results in substantial improvements over training the same model architecture with the variational lower bound.\nAuthors:\n Nicholas Watters, Andrea Tacchetti, Theophane Weber, Razvan Pascanu, Peter Battaglia, Daniel Zoran\n\u201cIn this work we developed the \u201cVisual Interaction Network\u201d (VIN), a neural network-based model that learns physical dynamics without prior knowledge. The VIN is able to infer the states of multiple physical objects from just a few frames of video, and then use these to predict object positions many steps into the future. It is also able to infer the locations of invisible objects and learn dynamics that depend on object attributes such as mass.\u201d\n \u00a0Read \nthe blog\n for further detail.\nAuthors: \nA\u00e4ron van den Oord, Oriol Vinyals, Koray Kavukcuoglu\nLearning useful representations without supervision remains a key challenge in machine learning. In this work we propose a simple yet powerful generative model - known as the Vector Quantised Variational AutoEconder (VQ-VAE) - \u00a0that learns such discrete representations. When these representations are paired with an autoregressive prior, the model is able to generate high quality images, videos and speech as well as doing high-quality speaker conversion.\nAuthors: \nJ\u00f6rg Bornschein, Andriy Mnih, Daniel Zoran, Danilo Jimenez Rezende\nAttention based memory can be used to augment neural networks to support few-shot learning, rapid adaptability and more generally to support non-parametric extensions. Instead of using the popular differentiable soft-attention mechanism, we propose the use of stochastic hard-attention to retrieve memory content in generative models. This allows us to apply variational inference to memory addressing, which enables us to get significantly more precise memory lookups using target information, especially in models with large memory buffers and with many confounding entries in the memory.\nAuthors: \nGeorge Tucker, Andriy Mnih, Chris J Maddison, Dieterich Lawson, Jascha Sohl-Dickstein\nLearning in models with discrete latent variables is challenging due to high-variance gradient estimators. Previous approaches either produced high-variance, unbiased gradients or low-variance, biased gradients. REBAR uses control variates and the reparameterization trick to get the best of both: low-variance, unbiased gradients that result in faster convergence to a better result.\nAuthors:\n S\u00e9bastien Racani\u00e8re, Th\u00e9ophane Weber, David P. Reichert, Lars Buesing, Arthur Guez, Danilo Rezende, Adria Puigdom\u00e8nech Badia, Oriol Vinyals, Nicolas Heess, Yujia Li, Razvan Pascanu, Peter Battaglia, Demis Hassabis, David Silver, Daan Wierstra.\n\u201cWe describe a new family of approaches for imagination-based planning...We also introduce architectures which provide new ways for agents to learn and construct plans to maximise the efficiency of a task. These architectures are efficient, robust to complex and imperfect models, and can adopt flexible strategies for exploiting their imagination. The agents we introduce benefit from an \u2018imagination encoder\u2019- a neural network which learns to extract any information useful for the agent\u2019s future decisions, but ignore that which is not relevant.\u201d \u00a0Read more on \nthe blog\n.\nAuthors:\n Adam Santoro, David Raposo, David Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, Timothy Lillicrap\n\u201cWe demonstrate the use of a simple, plug-and-play neural network module for solving tasks that demand complex relational reasoning. This module, called a Relation Network, can receive unstructured inputs - say, images or stories - and implicitly reason about the relations contained within.\u201d \u00a0Read more on \nthe blog\n.\nAuthors:\n Balaji Lakshminarayanan, Alexander Pritzel, Charles Blundell\nQuantifying predictive uncertainty in neural networks (NNs) is a challenging and yet unsolved problem. The majority of work is focused on Bayesian solutions, however these are computationally intensive and require significant modifications to the training pipeline. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelisable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs.\nAuthors: \nZhongwen Xu, Joseph Modayil, Hado van Hasselt, Andre Barreto, David Silver, Tom Schaul\nWe revisit the structure of value approximators for RL, based on the observation that typical approximators smoothly change as a function of input, but the true value changes abruptly when a reward arrives. Our proposed method is designed to fit such asymmetric discontinuities using interpolation with a projected value estimate.\nAuthors:\n Andre Barreto, Will Dabney, Remi Munos, Jonathan Hunt, Tom Schaul, David Silver, Hado van Hasselt.\nWe propose a transfer framework for reinforcement learning. Our approach rests on two key ideas: \"successor features\", a value function representation that decouples the dynamics of the environment from the rewards, and \"generalised policy improvement\", a generalisation of dynamic programming\u2019s policy improvement step that considers a set of policies rather than a single one. Put together, the two ideas lead to an approach that integrates seamlessly within the reinforcement learning framework and allows transfer to take place between tasks without any restriction.\nAuthors: \u00a0\nPaul Christiano (Open AI), Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, Dario Amodei (Open AI)\n\u201cA central question in technical AI safety is how to tell an algorithm what we want it to do. Working with OpenAI, we demonstrate a novel system that allows a human with no technical experience to teach an AI how to perform a complex task, such as manipulating a simulated robotic arm.\u201d Read more on \nthe blog\n.\nAuthor: \nJulien Perolat, Joel Z Leibo, Vinicius Zambaldi, Charles Beattie, Karl Tuyls, Thore Graepel\nThis paper looks at the complexity of problems of common-pool resource appropriation. These include systems such as fisheries, grazing pastures or access to \u00a0freshwater, where lots of people or actors have access to the same resource. Traditional models from the social sciences tend to suggest that parties with access to the resource act in a self-interested way, eventually leading to an unsustainable depletion of resources. However, we know from human societies that there is a wide range of possible outcomes. Sometimes resources like fisheries are overexploited and sometimes they are harvested sustainably. In this work we propose new modeling techniques that can be used in research aimed at explaining this gap between what we observe in the real world and what traditional models predict.\nAuthors:\n Yee Whye Teh, Victor Bapst, Wojciech Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell, Nicholas Heess, Razvan Pascanu\nWe develop a method for doing reinforcement learning on multiple tasks. The assumption is that the tasks are related to each other (e.g. being in the same environment or having the same physics) and so good action sequences tend to recur across tasks. Our method achieves this by simultaneously distilling task-specific policies into a common default policy, and transferring this common knowledge across tasks by regularising all task-specific policies towards the default policy. \u00a0We show that this leads to faster and more robust learning.\nAuthors:\n Marc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien Perolat, David Silver, Thore Graepel\nIn this work, we first observe that independent reinforcement learners produce policies that can be jointly correlated, failing to generalize well during execution with other agents. We quantify this effect by proposing a new metric called joint policy correlation. We then propose an algorithm motivated by game-theoretic foundations, which generalises several previous approaches such as fictitious play, iterated best response, independent RL, and double oracle. We show that our algorithm can reduce joint policy correlation significantly in first-person coordination games, and finds robust counter-strategies in a common poker benchmark game.\nOur researchers will also lead and take part in a wide-range of workshops, tutorials and symposia during NIPS. \u00a0For the full schedule, including details of papers that we have collaborated on, please download our \nitinerary\n (PDF) or visit \nthe official website\n.\n"}
{"title": "Collaborating with patients for better outcomes", "contents": "Working as a doctor in the NHS for over 10 years, I felt that I had developed good understanding of how patients and their families felt when faced with an upsetting diagnosis or important health decision. I had been lucky with my own health, having only spent one night in hospital for what ended up being a false alarm. But when my son was born prematurely two years ago, I had a glimpse into what being on the other side feels like - an experience that has profoundly shaped my thinking today.\nIt wasn\u2019t until I was waiting to hear, rather than give, important health updates that I really understood the feeling of uncertainty and powerlessness that many patients and their families feel. It really put into perspective how important it is to involve patients, and their families and carers, in their own health - that care is not something \u2018done\u2019 to a patient, but rather, something that is shaped by everyone involved in the healthcare process.\nIn my first week at DeepMind Health, I was really impressed that one of my new colleagues (not a nurse or doctor) had set up a meeting so we could hear directly from a patient, \nMichael Wise\n, who ended up needing dialysis and a kidney transplant after a sudden and unexpected problem with his kidneys. Since then, we\u2019ve continued to increase our efforts to bring the patient\u2019s voice into our projects. Afterall, there is \ngood evidence\n that when doctors and patients work together, the \noutcomes are better\n.\nWe\u2019ve already learned a lot about how to go about this - and what not to do - and wanted to share some reflections that may be helpful for others undertaking the same journey we\u2019re on.\nTo build cutting edge and secure health technologies we always work with experts. This may be \na clinician\n or \nsome of the world\u2019s best cyber security experts\n. Similarly, when thinking about patient and public involvement and engagement (PPIE) in our work, there are experts who understand how to do PPIE well. In early 2016 we started speaking to a number of individuals and organisations with this expertise. We worked with the late Rosamund Snow, an incredible person and the patient editor of the British Medical Journal, to shape what a Patient and Public Involvement and Engagement (PPIE) strategy might look like. Rosamund was initially sceptical of our work in healthcare. She pushed us to be more self-critical and transparent about our work, and made a \nnumber of recommendations\n she felt would allow patients\u2019 voices to have the biggest possible impact on the work we do, such as having an entirely patient-led project, inviting patients to do internships at the DeepMind Health offices, and hiring a patient lead to feed directly into our work. We are currently working with patients to flesh out some of these ideas.\nRosamund passed away shortly after our first patient summit in September 2016 but she had introduced us to some of her colleagues, Sally Crowe and Paul Buchanan, with whom we continue to work.\nFor that summit, we invited patients, carers and members of the public into our offices in London to discuss Rosamund\u2019s recommendations. Attendees disagreed with the idea of having a single patient lead, with some feeling that a single person couldn\u2019t possibly represent the diversity and complexity of the patient community. Instead, they wanted a patient panel to be involved in every project we do, working alongside the design and clinician teams. This is something we\u2019re looking to build on.\nWe also received feedback in areas that we hadn\u2019t considered, such as the location of the event. While many patients really enjoyed seeing where we work, some participants found the DeepMind offices too intimidating. This was incredibly useful feedback to hear. Since then, we\u2019ve committed to holding subsequent events in a mixture of both our offices and more neutral locations, such as Friends House in London and the Renaissance Hotel in Manchester, which we used for patient roundtables in July 2017.\nAt those events, we continued to receive feedback on Rosamund\u2019s recommendations. Attendees asked us to reflect on the language we use regarding patient case studies. We often talk about \u2018patient stories\u2019 or \u2018patient testimonies\u2019 which some attendees felt didn\u2019t carry enough weight. Instead, they said we should use \u2018patient truths\u2019 to empower patients, and emphasise the authentic and authoritative voice patients add to the conversation. Similarly, they questioned the way we \u00a0talk about DeepMind Health. We usually describe our work as \u2018clinician led\u2019, which ignores the valuable input patients have given to our work. In future, they suggested we used either \u2018clinician led and patient centred\u2019 or \u2018co-led by clinicians and patients\u2019 to highlight the equal involvement of both groups.\nSimilarly, we received feedback on the role we played in the room. We heard that some attendees would feel awkward talking directly with DeepMind staff, so we worked with external facilitators to run our events in July, where we took a back-seat role. Playing an observer was really enlightening as attendees seemed far more free to speak their mind, but others felt that they wanted to talk to us more candidly. In our future events, we plan to balance the use of external facilitators with more involved methods where we can directly engage in a dialogue with patients themselves.\nWe will be working with patients to build out their valuable suggestions into core practices of DeepMind Health. If you\u2019re interested in seeing what happened in those July events, you can see watch this\n video \nof some the attendees sharing their thoughts on our work.\nWe have learnt a lot from other people and organisations, like the Coalition for Collaborative Care (C4CC), \nNational Voices\n and the Patients Association, to help us learn how to work with patients better. They have outlined what a successful and mutually respectful collaboration model looks like, emphasising clear communication, a transparent sense of direction, and a culture of honesty and value.\nWe\u2019ve already benefited from the thousands of hours of patient engagement, including one to one sessions, events at our offices, interactive workshops and events run by our NHS partners. We are excited to put all the feedback we\u2019ve gathered into practice, and have committed to extensive engagement in the months to come, including growing an online patient community to engage with us in real-time, hosting design sessions with patients, and bringing in patients to collaborate in our research partnerships\nThis week we have launched a new form on the patients\u2019 section of our website that allows people to sign up for a range of activities, from receiving a regular newsletter to taking part in design and development sessions. If you\u2019d like to get involved please visit our \n'For Patients' \npage.\n"}
{"title": "Scalable agent architecture for distributed training", "contents": "Deep Reinforcement Learning (DeepRL) has achieved remarkable success in a range of tasks, from continuous control problems in robotics to playing games like Go and Atari. The improvements seen in these domains have so far been limited to individual tasks where a separate agent has been tuned and trained for each task.\nIn our most recent work, we explore the challenge of training a single agent on many tasks.\nToday we are releasing DMLab-30, a set of new tasks that span a large variety of challenges in a visually unified environment with a common action space. Training an agent to perform well on many tasks requires massive throughput and making efficient use of every data point. To this end, we have developed a new, highly scalable agent architecture for distributed training called Importance Weighted Actor-Learner Architecture that uses a new off-policy correction algorithm called V-trace.\nDMLab-30 is a collection of new levels designed using our open source RL environment \nDeepMind Lab\n. These environments enable any DeepRL researcher to test systems on a large spectrum of interesting tasks either individually or in a multi-task setting.\nThe tasks are designed to be as varied as possible. They differ in the goals they target, from learning, to memory, to navigation. They vary visually, from brightly coloured, modern-styled texture, to the subtle brown and greens of a desert at dawn, midday, or by night. And they contain physically different settings, from open, mountainous terrain, to right-angled mazes, to open, circular rooms.\nIn addition, some of the environments include \u2018bots\u2019, with their own, internal, goal-oriented behaviours. Equally importantly, the goals and rewards differ across the different levels, from following language commands and using keys to open doors, foraging mushrooms, to plotting and following a complex irreversible path.\nHowever, at a basic level, the environments are all the same in terms of their action and observation space allowing a single agent to be trained to act in every environment in this highly varied set. More details about the environments can be found on the \nDeepMind Lab GitHub page\n.\nIn order to tackle the challenging DMLab-30 suite, we developed a new distributed agent called Importance Weighted Actor-Learner Architecture that maximises data throughput using an efficient distributed architecture with \nTensorFlow\n.\nImportance Weighted Actor-Learner Architecture is inspired by the popular \nA3C\n architecture which uses multiple distributed actors to learn the agent\u2019s parameters. In models like this, each of the actors uses a clone of the policy parameters to act in the environment. Periodically, actors pause their exploration to share the gradients they have computed with a central parameter server that applies updates (see figure below).\nImportance Weighted Actor-Learner Architecture actors on the other hand are not used to calculate gradients. Instead, they are just used to collect experience which is passed to a central learner that computes gradients, resulting in a model that has completely independent actors and learners. To take advantage of the scale of modern computing systems, Importance Weighted Actor-Learner Architectures can be implemented using a single learner machine or multiple learners performing synchronous updates between themselves. Separating the learning and acting in this way also has the advantage of increasing the throughput of the whole system since the actors no longer need to wait for the learning step like in architectures such as batched A2C. This allows us to train Importance Weighted Actor-Learner Architectures on interesting environments without suffering from variance in frame rendering-time or time consuming task restarts.\nHowever, decoupling the acting and learning causes the policy in the actor to lag behind the learner. In order to compensate for this difference we introduce a principled off-policy advantage actor critic formulation called V-trace which compensates for the trajectories obtained by actors being off policy. The details of the algorithm and its analysis can be found in our \npaper\n.\nThanks to the optimised model of Importance Weighted Actor-Learner Architecture, it can process one-to-two orders of magnitude more experience compared to similar agents, making learning in challenging environments possible. We have compared Importance Weighted Actor-Learner Architectures with several popular actor-critic methods and have seen significant speed-ups. Additionally, the throughput using Importance Weighted Actor-Learner Architectures scales almost linearly with increasing number of actors and learners which shows that both the distributed agent model and the V-trace algorithm can handle very large scale experiments, even on the order of thousands of machines.\nWhen it was tested on the DMLab-30 levels, Importance Weighted Actor-Learner Architecture was 10 times more data efficient and achieved double the final score compared to distributed A3C. \u00a0Moreover, Importance Weighted Actor-Learner Architectures showed positive transfer from training in multi-task settings compared to training in single-task setting.\nRead the full Importance Weighted Actor-Learner Architectures paper \nhere\n.\nExplore DMLab-30 \nhere\n.\nThis work was done by Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg and Koray Kavukcuoglu\n"}
{"title": "Open sourcing Sonnet - a new library for constructing neural networks", "contents": "It\u2019s now nearly a year since DeepMind \nmade the decision to switch the entire research organisation to using TensorFlow (TF)\n. It\u2019s proven to be a good choice - many of our models learn significantly faster, and the built-in features for distributed training have hugely simplified our code. Along the way, we found that the flexibility and adaptiveness of TF lends itself to building higher level frameworks for specific purposes, and we\u2019ve written one for quickly building neural network modules with TF. We are actively developing this codebase, but what we have so far fits our research needs well, and we\u2019re excited to announce that today we are open sourcing it. We call this framework \nSonnet\n.\nSince its initial launch in November 2015, a diverse ecosystem of higher level libraries has sprung up around \nTensorFlow\n enabling common tasks to be accomplished quicker. Sonnet shares many similarities with some of these existing neural network libraries, but has some features specifically designed around our research requirements. The \ncode release\n accompanying our \nLearning to learn paper\n included a preliminary version of Sonnet, and other forthcoming code releases will be built on top of the full library we are releasing today.\nMaking Sonnet public allows other models created within DeepMind to be easily shared with the community, and we also hope that the community will use Sonnet to take their own research forwards. In recent months we\u2019ve also open-sourced our flagship platform \nDeepMind Lab\n, and are currently working with \nBlizzard to develop an open source API that supports AI research in StarCraft II\n. There are many more releases to come, and they\u2019ll all be shared on our new \nOpen Source page\n.\nThe library uses an object-oriented approach, similar to Torch/NN, allowing modules to be created which define the forward pass of some computation. Modules are \u2018called\u2019 with some input Tensors, which adds ops to the Graph and returns output Tensors. One of the design choices was to make sure the variable sharing is handled transparently by automatically reusing variables on subsequent calls to the same module.\nMany models in the literature can naturally be considered as a hierarchy - e.g. a Differentiable Neural Computer contains a controller which might be an LSTM, which can be implemented as containing a standard Linear layer. We\u2019ve found that writing code which explicitly represents submodules allows easy code reuse and quick experimentation - Sonnet promotes writing modules which declare other submodules internally, or are passed other modules at construction time.\nA final technique we\u2019ve found very useful is to allow certain modules to operate on arbitrarily nested groups of Tensors. Recurrent Neural Network states are often best represented as a collection of heterogeneous Tensors, and representing these as a flat list can be error prone. Sonnet provides utilities to deal with these arbitrary hierarchies, so that changing your experiment to use a different kind of RNN does not require tedious code changes. We\u2019ve made changes to core TF as well to better support this use case.\nSonnet is designed specifically to work with TensorFlow, and as such does not prevent you from accessing the underlying details such as Tensors and variable_scopes. Models written in Sonnet can be freely mixed with raw TF code, and that in other high level libraries.\nThis is not a one-time release - we will regularly update the Github repository to match our in-house version. We\u2019ve got lots of ideas for new features in the works, which will be made available when ready. We are very excited about contributions from the community.\nView Sonnet on \nGithub\n"}
{"title": "Exploring the mysteries of Go with AlphaGo and China's top players", "contents": "Just over a year ago, we saw a major milestone in the field of artificial intelligence: \nDeepMind\u2019s AlphaGo took on and defeated one of the world\u2019s top Go players\n, the legendary Lee Sedol. Even then, we had no idea how this moment would affect the 3,000 year old game of Go and the growing global community of devotees to this beautiful board game.\nInstead of diminishing the game, as some feared, artificial intelligence (A.I.) has actually made human players stronger and more creative. It\u2019s humbling to see how pros and amateurs alike, who have pored over every detail of AlphaGo\u2019s innovative game play, have actually learned new knowledge and strategies about perhaps the most studied and contemplated game in history. You can read more about some of these creative strategies \nin this blog post.\nClearly, there remains much more to learn from this partnership between Go\u2019s best human players and its most creative A.I. competitor. That\u2019s why we\u2019re so excited to announce AlphaGo\u2019s next step: a five-day festival of Go and artificial intelligence in the game's birthplace, China.\nFrom May 23-27, we\u2019ll collaborate with the China Go Association and Chinese Government to bring AlphaGo, China\u2019s top Go players, and leading A.I. experts from Google and China together in Wuzhen, one of the country\u2019s most beautiful water towns, for the \u201cFuture of Go Summit.\u201d\nThe summit will feature a variety of game formats involving AlphaGo and top Chinese players, specifically designed to explore the mysteries of the game together. The games will include: \nInterspersed with the games will be a forum on the \u201cFuture of A.I.\u201d Together with some of China\u2019s leading experts in the field, we will explore how AlphaGo has created new knowledge about the oldest of games, and how the technologies behind AlphaGo, machine learning, and artificial intelligence, are bringing solutions to some of the world\u2019s greatest challenges into reach.\nAlready, some of the machine learning methods behind AlphaGo have been used to tackle significant problems, such as \nreducing energy use\n. Machine learning technology is also at work in a series of exciting \nmedical\n \nresearch\n \nprojects\n. And across many of Google\u2019s products, machine learning has suddenly made the impossible real\u2014from allowing people using \nGoogle Photos\n to find that photo of their dog in the snow almost instantly to improving the quality of \nGoogle Translate\n more in a single leap than the past 10 years of improvements combined.\nWe\u2019re excited to see what insights this next round of games and discussion will bring, and the challenges this will help us solve together\u2014both on and off the Go board.\n"}
{"title": "Learning explanatory rules from noisy data", "contents": "Suppose you are playing football. The ball arrives at your feet, and you decide to pass it to the unmarked striker. What seems like one simple action requires two different kinds of thought. \nFirst, you recognise that there is a football at your feet. This recognition requires intuitive perceptual thinking - you cannot easily articulate how you come to know that there is a ball at your feet, you just see that it is there. Second, you decide to pass the ball to a particular striker. This decision requires conceptual thinking. Your decision is tied to a justification - the reason you passed the ball to the striker is because she was unmarked.\nThe distinction is interesting to us because these two types of thinking correspond to two different approaches to machine learning: deep learning and \nsymbolic program synthesis\n. Deep learning concentrates on intuitive perceptual thinking whereas symbolic program synthesis focuses on conceptual, rule-based thinking. Each system has different merits - deep learning systems are robust to noisy data but are difficult to interpret and require large amounts of data to train, whereas symbolic systems are much easier to interpret and require less training data but struggle with noisy data. While human cognition \nseamlessly combines\n these two distinct ways of thinking, it is much less clear whether or how it is possible to replicate this in a single AI system.\nOur new paper, \nrecently published in JAIR\n, demonstrates it is possible for systems to combine intuitive perceptual with conceptual interpretable reasoning. The system we describe, \u2202ILP, is robust to noise, data-efficient, and produces interpretable rules.\nWe demonstrate how \u2202ILP works with an induction task. It is given a pair of images representing numbers, and has to output a label (0 or 1) indicating whether the number of the left image is less than the number of the right image. Solving this problem involves both kinds of thinking: you need intuitive perceptual thinking to recognise the image as a representation of a particular digit, and you need conceptual thinking to understand the less-than relation in its full generality.\nIf you give a standard deep learning model (such as a convolutional neural network with an MLP) sufficient training data, it is able to learn to solve this task effectively. Once it has been trained, you can give it a new pair of images it has never seen before, and it will classify correctly. However, it will only generalise correctly if you give it multiple examples of every pair of digits. The model is good at visual generalisation: generalising to new images, assuming it has seen every pair of digits in the test set (see the green box below). But it is not capable of symbolic generalisation: generalising to a new pair of digits it has not seen before (see the blue box below). Researchers like \nGary Marcus\n and \nJoel Grus\n have pointed this out in recent, thought-provoking articles.\n\u2202ILP differs from standard neural nets because it is able to generalise symbolically, and it differs from standard symbolic programs because it is able to generalise visually. It learns explicit programs from examples that are readable, interpretable, and verifiable. \u2202ILP is given a partial set of examples (the desired results) and produces a program that satisfies them. It searches through the space of programs using gradient descent. If the outputs of the program conflict with the desired outputs from the reference data, the system revises the program to better match the data.\nOur system, \u2202ILP, is able to generalise symbolically. Once it has seen enough examples of x < y, y < z, x < z, it will consider the possibility that the < relation is transitive. Once it has realised this general rule, it can apply it to a new pair of numbers it has never seen before.\nWe believe that our system goes some way to answering the question of whether achieving symbolic generalisation in deep neural networks is possible. In future work, we plan to integrate \u2202ILP-like systems into reinforcement learning agents and larger deep learning modules. In doing so, we hope to impart our systems with the ability to reason as well as to react.\nRead the paper \nhere\n.\n"}
{"title": "Innovations of AlphaGo", "contents": "One of the great promises of AI is its potential to help us unearth new knowledge in complex domains. We\u2019ve already seen exciting glimpses of this, when our algorithms found ways to dramatically improve energy use in \ndata centres\n - as well as of course with our program AlphaGo.\nSince its historic success in Seoul last March, AlphaGo has heralded a new era for the ancient game of Go. Thanks to AlphaGo's creative and intriguing revelations, players of all levels have been inspired to test out new moves and strategies of their own, often re-evaluating centuries of inherited knowledge in the process.\nAhead of \u2018\nThe Future of Go Summit in Wuzhen\n\u2019, we summarise some recent examples of AlphaGo\u2019s strategic and tactical innovations, and the new insights they have revealed.\nAlphaGo's greatest strength is not any one move or sequence, but rather the unique perspective that it brings to every game. While Go style is difficult to encapsulate, one could say that AlphaGo's strategy embodies a spirit of flexibility and open-mindedness: a lack of preconceptions that allows it to find the most effective line of play. As the following two games will show, this philosophy often leads AlphaGo to discover counterintuitive yet powerful moves.\nAlthough Go is a game of territory, most decisive battles hinge on the balance of power between groups, and AlphaGo excels in shaping this balance. Specifically, AlphaGo makes masterful use of \"influence,\" or the effect of existing stones on surrounding areas. Although influence cannot be measured exactly, AlphaGo's value network enables it to consider all stones on the board at once, endowing its judgment with subtlety and precision. These abilities let AlphaGo convert local regions of influence into coordinated global advantages.\nIn this game (Dia. 1), Black (AlphaGo) has little secure territory, while White has three corners, but Black's influence radiates across the entire board. In particular, while the marked exchange solidifies White, it also improves Black's potential. Go players usually shy from such exchanges, which pay a definite price for uncertain profit, but AlphaGo combines its sterling judgment with a keen sense of risk and reward to make such moves possible.\nHowever, the value of influence depends entirely on context, and AlphaGo relinquishes influence freely when it can be effectively mitigated. In the the game displayed in Dia. 2, one of the most surprising in its oeuvre, AlphaGo has just played an incredible six stones along the second line. Go players have a saying: on the fourth line there is influence, and on the third line there is territory, but on the second line there is only defeat. AlphaGo's play at first looks deserving of such censure, as these moves give White both strength and influence in exchange for Black's paltry 4 points of side territory. Most players, unwilling to bear the ignominy of playing the marked stones, would reject this line in an instant. Yet AlphaGo judges it worthwhile to keep White's stones separated, and in the following exchanges, slowly erodes White's influence from the top and bottom to secure a winning advantage.\n\u200d\nAlphaGo has also played several opening novelties in its recent games, the most salient being the early 3-3 invasion and a new variation of the \"Magic Sword\". Each defies conventional theory, but proves sound on deeper inspection.\nOne of the most territorial joseki (corner sequences) in Go is the 3-3 point invasion, shown in Dia. 3.\nThis invasion immediately secures the corner, but the textbook sequence shown in Dia. 4 has long been disparaged as unsuitable for the opening, as it gives too much influence.\nAlphaGo's innovation is to omit the marked exchanges, leaving the corner unsettled as shown in Dia. 5.\nThough slightly less secure, Black retains miai (options) to escape on the left or finish the joseki later, and has gained territory while ceding only moderate influence. This strategy has created a great stir among professionals, and at least one has already tried it in an official game (Dia. 6).\nOriginally trained on human data, AlphaGo knows modern joseki and usually plays accordingly. However, in the \"Magic Sword,\" a famously complex joseki family named for the cursed sword of Muramasa, it diverges. \nStarting from the position in Dia. 7, the usual result exchanges the corner for the side as shown in Dia. 8.\nHowever, AlphaGo often prefers to sacrifice outside access for territorial compensation (Dia. 9).\nMost Go players would not consider playing this variation, as it gives Black a powerful wall, but White's follow-up approach declares that Black's influence is not as valuable as it looks. If Black does not reinforce the wall, it may even become a target. Kim Jiseok, one of Korea's top professionals, recently played this line in a tournament game (Dia. 10), which he went on to win.\nAlphaGo's innovations show great potential for impact in the world of professional Go, and we hope to present many more opportunities for collaborative research at the upcoming Future of Go Summit in Wuzhen. We look forward with great excitement to AlphaGo and human professionals striving together to discover the true nature of Go.\n"}
{"title": "Open-sourcing Psychlab", "contents": "Consider the simple task of going shopping for your groceries. If you fail to pick-up an item that is on your list, what does it tell us about the functioning of your brain? It might indicate that you have difficulty shifting your attention from object to object while searching for the item on your list. It might indicate a difficulty with remembering the grocery list. Or it could it be something to do with executing both skills simultaneously.\nWhat appears to be a single task actually depends on multiple cognitive abilities. We face a similar problem in AI research, where the complexity of a task can often make it difficult to tease apart the individual skills required for an agent to be successful. But understanding an agent\u2019s specific cognitive skill set may prove useful for improving its overall performance.\nTo address this problem in humans, psychologists have spent the last 150 years designing rigorously controlled experiments aimed at isolating one specific cognitive faculty at a time. For example, they might analyse the supermarket scenario using two separate tests - a \u201cvisual search\u201d test that requires the subject to locate a specific shape in a pattern could be used to probe attention, while they might ask a person to recall items from a studied list to test their memory.\nWe believe it is possible to use similar experimental methods to better understand the behaviours of artificial agents. That is why we developed Psychlab, a platform built on top of \nDeepMind Lab\n, which allows us to directly apply methods from fields like cognitive psychology to study behaviours of artificial agents in a controlled environment. Today, we are also open-sourcing this platform for others to use.\nPsychlab recreates the set-up typically used in human psychology experiments inside the virtual DeepMind Lab environment. This usually consists of a participant sitting in front of a computer monitor using a mouse to respond to the onscreen task. Similarly, our environment allows a virtual subject to perform tasks on a virtual computer monitor, using the direction of its gaze to respond. This allows humans and artificial agents to both take the same tests, minimising experimental differences. It also makes it easier to connect with the existing literature in cognitive psychology and draw insights from it.\nAlong with the open-source release of Psychlab we have built a series of classic experimental tasks to run on the virtual computer monitor, and it has a flexible and easy-to-learn API, enabling others to build their own tasks.\nEach of these tasks have been validated to show that our human results mirror standard results in the cognitive psychology literature.\nTake the \u2018visual search\u2019 task for example. The ability to locate an object among a complex array of stimuli, like one item on a supermarket shelf, has been studied as a way of understanding selective attention in humans.\nWhen humans are given the task of searching `for a vertically oriented bar among horizontal bars\u2019 and \u2018searching for a pink bar among bars of other colours\u2019 their reaction times don\u2019t change according to the numbers of items on the screen. In other words, their reaction times are independent of 'set size'. However, when the task is to search for a pink bar among different shaped and different coloured bars, human reaction times increase by approximately 50ms with each additional bar. When humans did this task on Psychlab, we replicated this result.\nWhen we did the same test on a state-of-the-art artificial agent, we found that, while it could perform the task, it did not show the human pattern of reaction time results. It used the same amount of time to respond in all three cases. In humans, this data has suggested a difference between \nparallel and serial attention\n. Agents appear only to have parallel mechanisms. Identifying this difference between humans and our current artificial agents shows a path toward improving future agent designs. \u00a0\nPsychlab was designed as a tool for bridging between cognitive psychology, neuroscience, and AI. By open-sourcing it, we hope the wider research community will make use of it in their own research and help us shape it going forward.\nRead the paper\n.\nDownload the code on GitHub\n.\n"}
{"title": "Game-theory insights into asymmetric multi-agent games", "contents": "As AI systems start to play an increasing role in the real world it is important to understand how different systems will interact with one another. \u00a0\nIn our \nlatest paper\n, published in the journal \nScientific Reports\n, we use a branch of \ngame theory\n to shed light on this problem. In particular, we examine how two intelligent systems behave and respond in a particular type of situation known as an \nasymmetric game\n, which include Leduc poker and various board games such as \nScotland Yard\n. Asymmetric games also naturally model certain real-world scenarios such as automated auctions where buyers and sellers operate with different motivations. Our results give us new insights into these situations and reveal a surprisingly simple way to analyse them. While our interest is in how this theory applies to the interaction of multiple AI systems, we believe the results could also be of use in economics, evolutionary biology and empirical game theory among others.\nGame theory is a field of mathematics that is used to analyse the strategies used by decision makers in competitive situations. It can apply to humans, animals, and computers in various situations but is commonly used in AI research to study \u201cmulti-agent\u201d environments where there is more than one system, for example several household robots cooperating to clean the house. Traditionally, the evolutionary dynamics of multi-agent systems have been analysed using simple, \nsymmetric games\n, such as the classic \nPrisoner\u2019s Dilemma\n, where each player has access to the same set of actions. Although these games can provide useful insights into how multi-agent systems work and tell us how to achieve a desirable outcome for all players - known as the Nash equilibrium - \u00a0they cannot model all situations.\nOur new technique allows us to quickly and easily identify the strategies used to find the Nash equilibrium in more complex asymmetric games - \u00a0characterised as games where each player has different strategies, goals and rewards. These games - and the new technique we use to understand them - can be illustrated using an example from \u2018Battle of the Sexes\u2019, a coordination game commonly used in game theory research.\nHere, two players have to coordinate a night out to either the opera or the movies. One of \u00a0the players has a slight preference for the opera and one of them has a slight preference for the movies. The game is asymmetric because, while both players have access to the same options, the corresponding rewards for each are different based on the players preferences. In order to maintain their friendship - or equilibrium - the players should choose the same activity (hence the zero payoff for separate activities). \nThis game has three equilibria: (i) both players deciding to go to the opera, (ii) both deciding to go to the movies, and (iii) a final, mixed option, where each player will opt for their preferred option three fifths of the time. This last option, which is said to be \u201cunstable\u201d, \u00a0can be rapidly uncovered using our method by simplifying - or decomposing - the asymmetric game into its symmetric counterparts. These counterpart games essentially considers the reward table of each player as a separate symmetric 2-player game with equilibrium points that coincide with the original asymmetric game.\nIn the plot below, the Nash equilibrium is plotted for the two, simple counterparts allowing us to quickly identify the optimal strategy in the asymmetrical game (a). The reverse can also be done, using the asymmetrical game to identify the equilibrium in its symmetrical counterparts.\nThis method can also be applied to other games, including Leduc poker, which is described in detail in the paper. In all of these situations, the method proves to be mathematically simple, allowing a rapid and straightforward analysis of asymmetric games that we hope will also help our understanding of various dynamic systems, including multi-agent environments.\nRead the original Scientific Reports paper \nhere\n.\nRead the follow-up AAMAS paper \nhere\n.\nThe Scientific Reports paper is authored by Karl Tuyls, Julien Perolat, Marc Lanctot, Georg Ostrovski, \u00a0Rahul Savani, \u00a0Joel Leibo, Toby Ord, Thore Graepel and Shane Legg. The AAMAS paper \u00a0is authored by \u00a0Karl Tuyls, Julien Perolat, Marc Lanctot, Joel Leibo and Thore Graepel.\nUPDATE 20/03/18:\n Our \nlatest paper\n, forthcoming at the Autonomous Agents and Multi-Agent Systems conference (AAMAS), builds on the Scientific Reports paper outlined above. \u00a0\nA Generalised Method for Empirical Game Theoretic Analysis\n introduces a general method to perform empirical analysis of multi-agent interactions, both in symmetric and asymmetric games. The method allows to understand how multi-agent strategies interact, what the attractors are and what the basins of attraction look like, giving an intuitive understanding for the strength of the involved strategies. Furthermore, it explains how many data samples to consider in order to guarantee that the equilibria of the approximating game are sufficiently reliable. \u00a0We apply the method to several domains, including AlphaGo, Colonel Blotto and Leduc poker.\n"}
{"title": "Distill: Communicating the science of machine learning", "contents": "Like every field of science, the importance of clear communication in machine learning research cannot be over-emphasised: it helps to drive forward the state-of-the art by allowing the research community to share, discuss and build upon new findings.\nFor this reason, we at DeepMind are enthusiastic supporters of \nDistill\n, a new independent, web-based medium for clear and open - \ndemystified\n - machine learning research, comprising a \njournal\n, \nprizes\n recognising outstanding work, and \ntools\n to create interactive essays.\nThe machine learning community has always embraced new forms of scientific communication. Today, our science and practice is communicated through papers published in traditional journals, hosted on arXiv, supported by code repositories and community-driven efforts such as \nJMLR\n and \nJAIR\n, at conferences and through surveys, posters, blog posts, videos, demos, podcasts and interviews.\nIn this tradition, Distill makes its own unique contribution. Drawing on modern web technologies, it provides a new way to learn and understand machine learning by promoting interactive, vivid and engaging exposition, and by recognising the invaluable contributions of those who make the time to remove the mystery - and reveal the importance - of even the most seemingly obscure results.\nDeepMind is proud to be a contributing sponsor of the \nDistill prize\n, an annual prize aimed at recognising outstanding work communicating and refining ideas in machine learning, and Shakir Mohamed is a member of the journal's steering committee. Ultimately, our desire is to support fresh and diverse thinking in machine learning research - to play our part, quoting \nWilliam Zinnsser\n, in creating a community of people \u2018finding a common thread of humanity between themselves and their speciality and their readers\u2019.\n"}
{"title": "DeepMind expands to Canada with new research office in Edmonton, Alberta", "contents": "DeepMind has always been a unique hybrid of startup culture and academia, and we\u2019ve been lucky to collaborate with many of the best researchers from around the world. Today we\u2019re thrilled to announce our next phase: the opening of DeepMind\u2019s first ever international AI research office in Edmonton, Canada, in close collaboration with the \nUniversity of Alberta\n (UAlberta).\nIt was a big decision for us to open our first non-UK research lab, and the fact we\u2019re doing so in Edmonton is a sign of the deep admiration and respect we have for the Canadian research community. In fact, we\u2019ve had particularly strong links with the UAlberta for many years: nearly a dozen of its outstanding graduates have joined us at DeepMind, and we\u2019ve sponsored the machine learning lab to provide additional funding for PhDs over the past few years.\n\u2018DeepMind Alberta\u2019 will be led by the pioneer of reinforcement learning - and DeepMind\u2019s first ever advisor from back in 2010 - \nRich Sutton\n, together with \nMichael Bowling\n and \nPatrick Pilarski.\n All three will maintain their professorships at UAlberta, and continue to teach and contribute to the academic community. They\u2019ll be joined by Adam White, who will be returning to Canada to join the university as an adjunct professor, and six more researchers who co-authored the influential \nDeepStack\n paper published earlier this year in Science. The team will work on core scientific research.\nAs well as continuing to contribute to the academic community through teaching and research, we intend to provide additional funding to support long-term AI programs at UAlberta. Our hope is that this collaboration will help turbo-charge Edmonton\u2019s growth as a technology and research hub, attracting even more world-class AI researchers to the region and helping to keep them there too.\nCollaborating with UAlberta to open a lab feels like a natural extension of what we do here in London. Shane Legg and I met as postdocs at \nUniversity College London\u2019s\n (UCL) Gatsby unit where we first started working together, and we\u2019ve \nworked hard to continue contributing to the academic ecosystem\n. We publish in major international journals, including three Nature papers in the past two years, while many of our team maintain professorships and supervise students at Cambridge, Oxford, Imperial College London as well as MIT and beyond. We even teach machine learning modules at UCL and Oxford to help advance the broader AI field beyond DeepMind.\nIn the UK, we feel we\u2019ve helped played an important role in encouraging and supporting the AI community to flourish - from start-ups and universities, to new organisations like the Turing Institute - and we hope that we can contribute to the success of Canada\u2019s pioneering centres of research too. It\u2019s an honour for us to work with Rich, Michael, Patrick and their team, together with the University of Alberta, and we look forward to many more scientific breakthroughs in the years ahead!\nRead more about the announcement from UAlberta \nhere\n.\n"}
{"title": "A neural approach to relational reasoning", "contents": "Consider the reader who pieces together the evidence in an Agatha Christie novel to predict the culprit of the crime, a child who runs ahead of her ball to prevent it rolling into a stream or even a shopper who compares the relative merits of buying kiwis or mangos at the market.\n\u200d\nWe carve our world into relations between things. And we understand how the world works through our capacity to draw logical conclusions about how these different things - such as physical objects, sentences, or even abstract ideas - are related to one another. This ability is called relational reasoning and is central to human intelligence.\nWe construct these relations from the cascade of unstructured sensory inputs we experience every day. For example, our eyes take in a barrage of photons, yet our brain organises this \u201cblooming, buzzing confusion\u201d into the particular entities that we need to relate.\nA key challenge in developing artificial intelligence systems with the flexibility and efficiency of human cognition is giving them a similar ability - \u00a0to reason about entities and their relations from unstructured data. Solving this would allow these systems to generalize to new combinations of entities, making infinite use of finite means.\nModern deep learning methods have made tremendous progress solving problems from unstructured data, but they tend to do so without explicitly considering the relations between objects.\nIn two new papers, we explore the ability for deep neural networks to perform complicated relational reasoning with unstructured data. In the first paper - \nA simple neural network module for relational reasoning\n - we describe a Relation Network (RN) and show that it can perform at superhuman levels on a challenging task. While in the second paper - \u00a0\nVisual Interaction Networks\n \u00a0- we describe a general purpose model that can predict the future state of a physical object based purely on visual observations.\nTo explore the idea of relational reasoning more deeply and to test whether it is an ability that can be easily added to existing systems, we created a simple-to-use, plug-and-play RN module that can be added to existing neural network architectures. An RN-augmented network is able to take an unstructured input - say, an image or a series of sentences - and implicitly reason about the relations of objects contained within it.\nFor example, a network using RN may be presented with a scene consisting of various shapes (spheres, cubes, etc.) sitting on a table. To work out the relations between them \u00a0(e.g. the sphere is bigger than the cube), the network must take the unstructured stream of pixels from the image and figure out what counts as an object in the scene. The network is not explicitly told what counts as an object and must figure it out for itself. The representations of these objects are then grouped \u00a0into pairs (e.g. the sphere and the cube) and passed through the RN module, which compares them to establish a \u201crelation\u201d (e.g. the sphere is bigger than the cube). These relations are not hardcoded, but must be learnt by the RN as it compares each possible pair. Finally, it adds up all these relations to produce an output for all of the pairs of shapes in the scene.\nWe tested this model on several tasks including \nCLEVR\n - \u00a0a visual question answering task designed to explicitly explore a model\u2019s ability to perform different types of reasoning, such as counting, comparing, and querying. CLEVR consists of images like this:\nEach image has associated questions that interrogates the relations between objects in the scene. For example, a question about the image above might ask: \u201cThere is a tiny rubber thing that is the same colour as the large cylinder; what shape is it?\nState-of-the-art results on CLEVR using standard visual question answering architectures are 68.5%, compared to 92.5% for humans. But using our RN-augmented network, we were able to show super-human performance of 95.5%.\nTo check the versatility of the RN, we also tested the RN on a very different language task. Specifically, we used the \nbAbI suite\n - a series of of text-based question answering tasks. bAbI consists of a number of stories, which are a variable number of sentences culminating in a question. For example, \u201cSandra picked up the football\u201d and \u201cSandra went to the office\u201d may lead to the question \u201cWhere is the football?\u201d (answer: \u201coffice\u201d).\nThe RN-augmented network scored more than 95% on 18 of the 20 bAbI tasks, similar to existing state-of-the-art models. Notably, it scored better on certain tasks - such as induction - which caused problems for these more established models.\nFull results of all these tests and more are available \nin the paper\n.\nAnother key part of relational reasoning involves predicting the future in a physical scene. From just a glance, humans can infer not only what objects are where, but also what will happen to them over the upcoming seconds, minutes and even longer in some cases. For example, if you kick a football against a wall, your brain predicts what will happen when the ball hits the wall and how their movements will be affected afterwards (the ball will ricochet at a speed proportional to the kick and - in most cases - the wall will remain where it is).\nThese predictions are guided by a sophisticated cognitive system for reasoning about objects and their physical interactions.\nIn this related work we developed the \u201cVisual Interaction Network\u201d (VIN) - a model that mimics this ability. The VIN is able to infer the states of multiple physical objects from just a few frames of video, and then use this to predict object positions many steps into the future. This differs from generative models, which might visually \u201cimagine\u201d the next few frames of a video. Instead, the VIN predicts how the underlying relative states of the objects evolve.\nThe VIN is comprised of two mechanisms: a visual module and a physical reasoning module. Together they are able to process a visual scene into a set of distinct objects and learn an implicit system of physical rules which can predict what will happen to these objects in the future.\nWe tested the VIN\u2019s ability to do this in a variety of systems including bouncing billiards, masses connected by springs, and planetary systems with gravitational forces. Our results show that the VIN can accurately predict what will happen to objects hundreds of steps into the future.\nIn experimental comparisons with previously published models and variants of the VIN in which its mechanism for relational reasoning was removed, the full VIN performed significantly better.\nAgain, full details of the results can be found \nin our paper\n.\nBoth of these papers show promising approaches to understanding the challenge of relational reasoning. They show how neural networks can be given a powerful ability to reason by decomposing the world into systems of objects and their relations, allowing them to generalise to new combinations of objects and reason about scenes that superficially might look very different but have underlying common relations.\nWe believe these approaches are scalable and could be applied to many more tasks, helping build more sophisticated models of reasoning and allowing us to better understand a key component of humans\u2019 powerful and flexible general intelligence that we take for granted every day. \u00a0\nThe Relation Network was developed by Adam Santoro, David Raposo, David G.T. Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia and Timothy Lillicrap\nThe Visual Interaction Network was developed by Nicholas Watters, Daniel Zoran, Theophane Weber, Peter Battaglia, Razvan Pascanu and Andrea Tachetti\n"}
{"title": "AlphaGo's next move", "contents": "With just three stones on the board, it was clear that this was going to be \nno ordinary game of Go\n.\nChinese Go Grandmaster and world number one Ke Jie departed from his typical style of play and opened with a \u201c3:3 point\u201d strategy - a highly unusual approach aimed at quickly claiming corner territory at the start of the game. The placement is rare amongst Go players, but it\u2019s a favoured position of our program AlphaGo. Ke Jie was playing it at its own game.\nKe Jie\u2019s thoughtful positioning of that single black stone was a fitting motif for the opening match of \nThe Future of Go Summit in Wuzhen, China\n, an event dedicated to exploring the truth of this beautiful and ancient game. Over the last five days we have been honoured to witness games of the highest calibre.\nWe have always believed in the potential for AI to help society discover new knowledge and benefit from it, and AlphaGo has given us an early glimpse that this may indeed be possible. More than a competitor, AlphaGo has been a tool to inspire Go players to try new strategies and uncover new ideas in this 3,000 year-old game.\nThe creative moves \nit played against the legendary Lee Sedol in Seoul in 2016\n brought completely new knowledge to the Go world, while the unofficial online games it \nplayed under the moniker Magister (Master)\n earlier this year have influenced many of Go\u2019s leading professionals - including the genius Ke Jie himself. Events like this week\u2019s Pair Go, in which two of the world\u2019s top players partnered with AlphaGo, showed the great potential for people to use AI systems to generate new insights in complex fields.\nThis week\u2019s series of thrilling games with the world\u2019s best players, in the country where Go originated, has been the highest possible pinnacle for AlphaGo as a competitive program. For that reason, the Future of Go Summit is our final match event with AlphaGo.\nThe research team behind AlphaGo will now throw their energy into the next set of grand challenges, developing advanced general algorithms that could one day help scientists as they tackle some of our most complex problems, such as finding new cures for diseases, dramatically reducing energy consumption, or inventing revolutionary new materials. If AI systems prove they are able to unearth significant new knowledge and strategies in these domains too, the breakthroughs could be truly remarkable. We can\u2019t wait to see what comes next.\nWhile AlphaGo is stepping back from competitive play, it\u2019s certainly not the end of our work with the Go community, to which we owe a huge debt of gratitude for their encouragement and motivation over the past few years. \nWe plan to publish one final academic paper later this year that will detail the extensive set of improvements we made to the algorithms\u2019 efficiency and potential to be generalised across a broader set of problems. Just like our first AlphaGo paper, we hope that other developers will pick up the baton, and use these new advances to build their own set of strong Go programs.\nWe\u2019re also working on a teaching tool - one of the top requests we\u2019ve received throughout this week. The tool will show AlphaGo\u2019s analysis of Go positions, providing an insight into how the program thinks, and hopefully giving all players and fans the opportunity to see the game through the lens of AlphaGo. We\u2019re particularly honoured that our first collaborator in this effort will be the great Ke Jie, who has agreed to work with us on a study of his match with AlphaGo. We\u2019re excited to hear his insights into these amazing games, and to have the chance to share some of AlphaGo\u2019s own analysis too.\nFinally, to mark the end of the Future of Go Summit, we wanted to give a special gift to fans of Go around the world. Since our match with Lee Sedol, AlphaGo has become its own teacher, playing millions of high level training games against itself to continually improve. We\u2019re now publishing a special set of 50 AlphaGo vs AlphaGo games, played at full length time controls, which we believe contain many new and interesting ideas and strategies.\nWe took the opportunity this week in Wuzhen to show some of these games to a handful of top professionals. Shi Yue, 9 Dan Professional and World Champion said the games were \u201cLike nothing I\u2019ve ever seen before - they\u2019re how I imagine games from far in the future.\u201d Gu Li, 9 Dan Professional and World Champion, said that \u201cAlphaGo\u2019s self play games are incredible - we can learn many things from them.\u201d We hope that all Go players will now enjoy trying out some of the moves in the set. The first ten games are now available\n here\n, and we\u2019ll publish another ten each day until all 50 have been released.\nWe have been humbled by the Go community\u2019s reaction to AlphaGo, and the way professional and amateur players have embraced its insights about this ancient game. We plan to bring that same excitement and insight to a range of new fields, and try to address some of the most important and urgent scientific challenges of our time. We hope that the story of AlphaGo is just the beginning.\nRead more about The Future of Go Summit\n\u200d\nRead more about AlphaGo\n\u200d\nDiscover the AlphaGo self-play games\n"}
{"title": "Learning through human feedback", "contents": "We believe that Artificial Intelligence will be one of the most important and widely beneficial scientific advances ever made, helping humanity tackle some of its greatest challenges, from climate change to delivering advanced healthcare. But for AI to deliver on this promise, we know that the technology \nmust be built in a responsible manner\n and that we must consider all potential challenges and risks.\nThat is why DeepMind co-founded initiatives like the \nPartnership on AI to Benefit People and Society\nand why we have a team dedicated to technical AI Safety. Research in this field needs to be open and collaborative to ensure that best practices are adopted as widely as possible, which is why we are also collaborating with \nOpenAI\n on \nresearch in technical AI Safety\n.\nOne of the central questions in this field is how we allow humans to tell a system what we want it to do and - importantly - what we don\u2019t want it to do. This is increasingly important as the problems we tackle with machine learning grow more complex and are applied in the real world.\nThe\n first results\n from our collaboration demonstrate one method to address this, by allowing humans with no technical experience to teach a reinforcement learning (RL) system - an AI that learns by trial and error - a complex goal. This removes the need for the human to specify a goal for the algorithm in advance. This is an important step because getting the goal even a bit wrong could lead to undesirable or even dangerous behaviour. In some cases, as little as 30 minutes of feedback from a non-expert is enough to train our system, including teaching it entirely new complex behaviours, such as how to make a simulated robot do backflips.\nThe system - described \nin our paper Deep Reinforcement Learning from Human Preferences\n - departs from classic RL systems by training the agent from a neural network known as the \u2018reward predictor\u2019, rather than rewards it collects as it explores an environment.\n \nIt consists of three processes running in parallel:\nThis iterative approach to learning means that a human can spot and correct any undesired behaviours, a crucial part of any safety system. The design also does not put an onerous burden on the human operator, who only has to review around 0.1% of the agent\u2019s behaviour to get it to to do what they want. However, this can mean reviewing several hundred to several thousand pairs of clips, something that will need to be reduced to make it applicable to real world problems.\nIn the Atari game Enduro, which involves steering a car to overtake a line of others and is very difficult to learn by the trial and error techniques of a traditional RL network, human feedback eventually allowed our system to achieve superhuman results. In other games and simulated robotics tasks, it performed comparably to a standard RL set-up, while in a couple of games like Qbert and Breakout it failed to work at all.\nBut the ultimate purpose of a system like this is to allow humans to specify a goal for the agent, even if it is not present in the environment. To test this, we taught agents various novel behaviours such as performing a backflip, walking on one leg or learning to driving alongside another car in Enduro, rather than overtake to maximise the game score.\nAlthough these tests showed some positive results, others showed its limitations. In particular, our set-up was susceptible to reward hacking - or gaming its reward function - if human feedback was discontinued early in the training. In this scenario, the agent continues to explore its environment, meaning the reward predictor is forced to estimate rewards for situations it has received no feedback on. This can lead it to overpredict the reward, incentivising the agent to learn the wrong - often strange - behaviours. An example can be seen in the video below, where the agent has found that hitting the ball back and forth is a better strategy than winning or losing a point.\nUnderstanding flaws like these is crucial to ensure we avoid failures and build AI systems that behave as intended.\nThere is still more work to be done to test and enhance this system, but already it shows a number of critical first steps in producing systems that can be taught by non-expert users, are economical with the amount of feedback they need, and can be scaled to a variety of problems.\nOther areas of exploration could include reducing the amount of human feedback needed or giving humans the ability to give feedback through a natural language interface. This would mark a step-change in creating a system that can easily learn from the complexity of human behaviour, and a crucial step towards creating \u00a0AI that works with and for all of humanity.\nThis research was done as part of an ongoing collaboration between Jan Leike, Miljan Martic, and Shane Legg at DeepMind and Paul Christiano, Dario Amodei, and Tom Brown at OpenAI.\n"}
{"title": "The Information Commissioner, the Royal Free, and what we\u2019ve learned", "contents": "Today, \ndozens of people in UK hospitals will die preventably\n from conditions like sepsis and acute kidney injury (AKI) when their warning signs aren't picked up and acted on in time. To help address this, we built the \nStreams\n app with clinicians at the Royal Free London NHS Foundation Trust, using mobile technology to automatically review test results for serious issues starting with AKI. If one is found, Streams sends a secure smartphone alert to the right clinician, along with information about previous conditions so they can make an immediate diagnosis.\nWe\u2019re proud that, within a few weeks of Streams being deployed at the Royal Free, \nnurses said that it was saving them up to two hours each day\n, and we've already heard examples of \npatients with serious conditions\n being seen more quickly thanks to the instant alerts. Because Streams is designed to be ready for more advanced technology in the future, including AI-powered clinical alerts, we hope that it will help bring even more benefits to patients and clinicians in time.\nThe Information Commissioner (ICO) has \nnow concluded\n a year-long investigation that focused on the Royal Free\u2019s clinical testing of Streams in late 2015 and 2016, which was intended to guarantee that the service could be deployed safely at the hospital. The ICO wasn\u2019t satisfied that there was a legal basis for this use of patient data in testing (as the National Data Guardian \nsaid too\n), and raised concerns about how much patients knew about what was happening. The ICO recognised that many of these issues have already been addressed by the Royal Free, and has asked the Trust to sign a formal undertaking to ensure compliance in future.\nThe ICO\u2019s undertaking also recognised that the Royal Free has stayed in control of all patient data, with DeepMind confined to the role of \u201cdata processor\u201d and acting on the Trust\u2019s instructions throughout. No issues have been raised about the safety or security of the data.\nWe welcome the ICO\u2019s thoughtful resolution of this case, which we hope will guarantee the ongoing safe and legal handling of patient data for Streams.\nAlthough today\u2019s findings are about the Royal Free, we need to reflect on our own actions too. In our determination to achieve quick impact when this work started in 2015, we underestimated the complexity of the NHS and of the rules around patient data, as well as the potential fears about a well-known tech company working in health. We were almost exclusively focused on building tools that nurses and doctors wanted, and thought of our work as technology for clinicians rather than something that needed to be accountable to and shaped by patients, the public and the NHS as a whole. We got that wrong, and we need to do better.\nSince then, we\u2019ve worked hard on some major improvements to our transparency, oversight and engagement. For example:\nWe hope that these steps will help raise the bar for NHS IT overall, and we want to go further in future with projects like our \nVerifiable Data Audit\n.\nUltimately, if we want to build technology to support a vital social institution like the NHS, then we have to make sure we serve society\u2019s priorities and not outrun them. There\u2019s a fine line between finding exciting new ways to improve care, and moving ahead of patients\u2019 expectations. We know that we fell short at this when our work in health began, and we\u2019ll keep listening and learning about how to get better at this. We also completely agree with respected voices including the \nNational Data Guardian\nand \nUnderstanding Patient Data\n who have called for much more public conversation about the use of data to improve healthcare, and we\u2019ll support that however we can.\nWe're a team of people who grew up with and worked in the NHS, brought together by the privileged opportunity to apply our expertise to help patients, nurses, doctors, and the health service we love. This is an amazing opportunity for us to prove what we have always believed: that if we get the ethics, accountability and engagement right, then new technology systems can have incredible positive social impact. This is the most important challenge we can imagine.\n"}
{"title": "Interpreting Deep Neural Networks using Cognitive Psychology", "contents": "Deep neural networks have learnt to do an amazing array of tasks - from recognising and reasoning about objects in images to playing Atari and Go at super-human levels. As these tasks and network architectures become more complex, the solutions that neural networks learn become more difficult to understand.\nThis is known as the \u2018black-box\u2019 problem, and it is becoming increasingly important as neural networks are used in more and more real world applications.\nAt DeepMind, we are working to expand the toolkit for understanding and interpreting these systems. In \nour latest paper\n, recently accepted at ICML, we proposed a new approach to this problem that employs methods from cognitive psychology to understand deep neural networks. Cognitive psychology measures behaviour to infer mechanisms of cognition, and contains a vast literature detailing such mechanisms, along with experiments for verifying them. As our neural networks approach human level performance on specific tasks, methods from cognitive psychology are becoming increasingly relevant to the black-box problem.\nTo demonstrate this point, our paper reports a case study where we used an experiment designed to elucidate human cognition to help us understand how deep networks solve an image classification task. \nOur results showed that behaviours observed by cognitive psychologists in humans are also displayed by these deep networks. Further, the results revealed useful and surprising insights about how the networks solve the classification task. More generally, the success of the case study demonstrated the potential of using cognitive psychology to understand deep learning systems.\nIn our case study, we considered how children recognise and label objects - a rich area of study in developmental cognitive psychology. The ability of children to guess the meaning of a word from a single example - so-called \u2018one-shot word learning\u2019 - happens with such ease that it is tempting to think it is a simple process. However, a classic thought experiment from the philosopher Willard Van Orman Quine illustrates just how complex this really is:\nA field linguist has gone to visit a culture whose language is entirely different from our own. The linguist is trying to learn some words from a helpful native speaker, when a rabbit scurries by. The native speaker declares \u201cgavagai\u201d, and the linguist is left to infer the meaning of this new word. The linguist is faced with an abundance of possible inferences, including that \u201cgavagai\u201d refers to rabbits, animals, white things, that specific rabbit, or \u201cundetached parts of rabbits\u201d. There is an infinity of possible inferences to be made. How are people able to choose the correct one?\nFifty years later, we are confronted with the same question about deep neural networks that can do one-shot learning. Consider \nthe Matching Network\n, a neural network developed by our colleagues at DeepMind. This model uses recent advances in attention and memory to achieve state-of-the-art performance classifying ImageNet images using only a single example from a class. However, we do not know what assumptions the network is making to classify these images.\nTo shed light on this, we looked to the work of developmental psychologists (1-4) who found evidence that children find the correct inferences by applying inductive biases to eliminate many of the incorrect inferences. Such biases include:\nWe chose to measure the shape bias of our neural networks because there is a particularly large body of work studying this bias in humans. \nThe classic shape bias experiment that we adopted proceeds as follows: we present our deep networks with images of three objects: a probe object, a shape-match object (which is similar to the probe in shape but not in colour), and a colour-match object (which is similar to the probe in colour but not in shape). We then measure the shape bias as the proportion of times that the probe image is assigned the same label as the shape-match image instead of the colour-match image.\nWe used images of objects used in human experiments in the Cognitive Development Lab at Indiana University.\nWe tried this experiment with our deep networks (Matching Networks and an Inception baseline model) and found that - like humans - our networks have a strong bias towards object shape rather than colour or texture. In other words, they have a \u2018shape bias\u2019.\nThis suggests that Matching Networks and the Inception classifier use an inductive bias for shape to eliminate incorrect hypotheses, giving us a clear insight into how these networks solve the one-shot word learning problem. \nThe observation of shape bias wasn\u2019t our only interesting finding:\nThe discovery of this previously unrecognised bias in standard neural network architectures illustrates the potential of using artificial cognitive psychology for interpreting neural network solutions. In other domains, insights from the episodic memory literature may be useful for understanding episodic memory architectures, and techniques from the semantic cognition literature may be useful for understanding recent models of concept formation. The psychological literature is rich in these and other areas, giving us powerful new tools to address the \u2018black box\u2019 problem and to more deeply understand the behaviour of our neural networks.\nThis work was done by Sam Ritter*, David G.T. Barrett*, Adam Santoro and Matt M. Botvinick\nRead \nCognitive Psychology for Deep Neural Networks: A Shape Bias Case Study\n"}
{"title": "Enhancing patient safety at Taunton and Somerset NHS Foundation Trust", "contents": "We\u2019re delighted to announce our first partnership outside of London to help doctors and nurses break new ground in the NHS\u2019s use of digital technology.\nStreams is our secure mobile app that helps doctors and nurses give faster urgent care to patients showing signs of deterioration by giving them the right information more quickly. Over the next five years, we\u2019ll be rolling it out at Taunton and Somerset NHS Foundation Trust as part of a new partnership. You can find out more on the trust\u2019s \nwebsite\n.\nOur collaboration with Taunton and Somerset follows on from our work with Imperial College Healthcare NHS Trust and the Royal Free NHS Foundation Trust. Nurses already using Streams at the Royal Free tell us that the app is saving them up to two hours a day, allowing them to redirect valuable time back into targeted patient care.\nWhere some current systems can take hours, Streams uses \u2018breaking news\u2019 style alerts to notify clinicians within seconds when a test results indicates that one of their patients shows signs of becoming ill. Once they have received an alert, they can use the app to view important test results and communicate securely with their colleagues, to ensure their patients get the right treatment as quickly as possible.\nAt Musgrove Park Hospital, part of Taunton and Somerset NHS Foundation Trust, these features will alert doctors and nurses to a potential deterioration in their patients\u2019 vital signs that could indicate a serious problem. We believe that by making it as quick and easy as possible for clinicians to intervene if something is wrong, we\u2019ll be able to improve patient safety across the hospital.\nStreams has already had a promising impact for both patients and clinicians at our existing partner sites, and has been credited with helping deliver faster care to patients who become critically ill. One patient who has benefited from Streams at the Royal Free was \nAfia Ahmed\n, who received quick treatment by doctors who were alerted to her deterioration when she became seriously ill after giving birth.\nNetty Messenger, a patient and volunteer at Musgrove Park Hospital, welcomed the trust\u2019s progress towards mobile working: \u201cThis is a great idea. I do banking and shopping online and get my prescriptions online, but hospitals still seem to have mountains of paper. It would be much better to have all the patient\u2019s information in one place in an app like this.\nDr Dominic King, clinical lead at DeepMind Health, said: \u201cNurses and doctors already using Streams are telling us that it is helping them deliver faster and better care for their patients. The Taunton and Somerset NHS Foundation Trust is well known for its pioneering approach to healthcare technology, so it\u2019s incredibly exciting to be working with the outstanding clinical team there, on the shared goal of improving outcomes for patients.\u201d \nTom Edwards, a consultant surgeon at the trust, said: \u201cFast access to information about patients is absolutely crucial for our doctors, nurses, and other clinical staff.\nAs one of the NHS\u2019s 16 global digital exemplar acute trusts, Taunton and Somerset aims to be one of the most innovative hospital trusts in the NHS. Over the next five years, we\u2019ll be working with the trust and their long-term partner in the digital exemplar programme, IMS MAXIMS, to roll out this cutting-edge technology for a range of medical conditions where early intervention can make all the difference.\nAs in all our Streams partnerships, we\u2019re putting patients at the heart of this work. In addition to their regular forums for updating patients and patient governors on their innovative work, over the coming weeks the trust will be hosting workshops, displays and open day events so that staff, patients and the public can see how the app works, what it will mean for patients, and how it might be developed in future.\nIn addition, members of the DeepMind team will be working alongside Taunton staff to engage patients, giving people an opportunity to ask questions about Streams and any other aspect of our work. These events will take place before any patient data is processed by DeepMind. All patient data will be stored to world-leading standards of security and encryption in a facility in England, separated at all times from any other systems.\n"}
{"title": "Independent Reviewers release first annual report on DeepMind Health", "contents": "Today, a panel of Independent Reviewers has published its \nfirst annual report\n into DeepMind Health. As I wrote in the foreword to their report (written, I add, before I\u2019d read it): \u201cWe chose people who had specific expertise but also reputations for integrity, who did not hold back, who could be angry and critical\u2026 That\u2019s good for us and makes us better.\u201d\nThe panel is made up of experts in their fields who were given full access to our work to carry out their review - a very unusual process for a tech company, but one that we hope will significantly increase scrutiny of our work and ultimately help us get it right. We are grateful for their and honesty, thoughtfulness, and the time they have spent on this complex task. You can \nread their full report here\n.\nAs a result of this process, DeepMind Health has committed to a series of changes to our work and practices to try to set higher standards in our second year. We know we need to work harder to be responsive and accountable to the needs of a far greater cross-section of medicine and society. This includes significantly improving our work with patients and the public, and continuing on the path of greater engagement with Royal Colleges, professional bodies and many other groups in the NHS community. You can \nread what we\u2019ve committed to change here\n.\nWe\u2019re also excited to be working with Paul Buchanan as our Patient and Public Lead, helping us build on the patient engagement work begun by the brilliant Rosamund Snow. Paul is a well-known patient advocate and recently served as the BMJ Patient Editor. He\u2019ll bring the voices of patients into the heart of our work, and we\u2019re looking forward to learning from patients and collaborating with them more in future.\nThis has already been a week of listening and learning. We also \nwrote yesterday\n about the Information Commissioner\u2019s resolution of her investigation into the Royal Free London NHS Trust and its first partnership with DeepMind Health, and the lessons we\u2019ve learned.\nI commit to continuing to invite people of integrity to challenge and give feedback on our work. We\u2019re privileged to be working on technology that genuinely matters to patients, clinicians and the wider public. \u00a0We know that scrutiny can only make our work better and we\u2019re grateful to everyone who has taken time to tell us what we\u2019re getting right, and what we\u2019re getting wrong. Please don\u2019t hold back, and please \nget in touch\n with any feedback you have on how we can do things differently.\n"}
{"title": "Producing flexible behaviours in simulated environments", "contents": "The agility and flexibility of a monkey swinging through the trees or a football player dodging opponents and scoring a goal can be breathtaking. Mastering this kind of sophisticated motor control is a hallmark of physical intelligence, and is a crucial part of AI research. \nTrue motor intelligence requires learning how to control and coordinate a flexible body to solve tasks in a range of complex environments. Existing attempts to control physically simulated humanoid bodies come from diverse fields, including computer animation and biomechanics. \u00a0A trend has been to use hand-crafted objectives, sometimes with motion capture data, to produce specific behaviors. However, this may require considerable engineering effort, and can result in restricted behaviours or behaviours that may be difficult to repurpose for new tasks.\nIn three new papers, we seek ways to produce flexible and natural behaviours that can be reused and adapted to solve tasks. \nFor some AI problems, such as playing Atari or Go, the goal is easy to define - it\u2019s winning. But how do you describe the process for performing a backflip? Or even just a jump? The difficulty of accurately describing a complex behaviour is a common problem when teaching motor skills to an artificial system. In this work we explore how sophisticated behaviors can emerge from scratch from the body interacting with the environment using only simple high-level objectives, such as moving forward without falling. Specifically, we trained agents with a variety of simulated bodies to make progress across diverse terrains, which require jumping, turning and crouching. The results show our agents develop these complex skills without receiving specific instructions, an approach that can be applied to train our systems for multiple, distinct simulated bodies. The GIFs below show how this technique can lead to high quality movements and perseverance. They can be viewed in full \nhere\n.\nThe emergent behaviour described above can be very robust, but because the movements must emerge from scratch, they often do not look human-like. \u00a0In our second paper, we demonstrate how to train a policy network that imitates motion capture data of human behaviours to pre-learn certain skills, such as walking, getting up from the ground, running, and turning. Having produced behaviour that looks human-like, we can tune and repurpose those behaviours to solve other tasks, like climbing stairs and navigating walled corridors.\nThe third paper proposes a neural network architecture, building on state-of-the-art generative models, that is capable of learning the relationships between different behaviours and imitating specific actions that it is shown. After training, our system can encode a single observed action and create a new novel movement based on that demonstration. It can also switch between different kinds of behaviours despite never having seen transitions between them, for example switching between walking styles. \u00a0 \u00a0\nAchieving flexible and adaptive control of simulated bodies is a key element of AI research. Our work aims to develop flexible systems which learn and adapt skills to solve motor control tasks while reducing the manual engineering required to achieve this goal. Future work could extend these approaches to enable coordination of a greater range of behaviours in more complex situations.\n"}
{"title": "AI and Neuroscience: A virtuous circle", "contents": "Recent progress in AI has been remarkable. Artificial systems now outperform expert humans at \nAtari video games\n, the \nancient board game Go\n, and \nhigh-stakes matches of heads-up poker\n. They can also produce \nhandwriting\n and \nspeech\n indistinguishable from those of humans, translate between multiple languages and even reformat your holiday snaps \nin the style of Van Gogh\nmasterpieces.\nThese advances are attributed to several factors, including the application of new statistical approaches and the increased processing power of computers. But in \na recent Perspective in the journal Neuron\n, we argue that one often overlooked contribution is the use of ideas from experimental and theoretical neuroscience.\nPsychology and neuroscience have played a key role in the history of AI. Founding figures such as \nDonald Hebb\n, \nWarren McCulloch\n, \nMarvin Minsky\n and \nGeoff Hinton\n were all originally motivated by a desire to understand how the brain works. In fact, throughout the late 20th Century, much of the key work developing neural networks took place not in mathematics or physics labs, but in psychology and neurophysiology departments.\nAt DeepMind, we argue that despite rapid progress in both fields, researchers should not lose sight of this vision. We urge researchers in neuroscience and AI to find a common language, allowing a free flow of knowledge that will allow continued progress on both fronts.\nWe believe that drawing inspiration from neuroscience in AI research is important for two reasons. First, neuroscience can help validate AI techniques that already exist. Put simply, if we discover one of our artificial algorithms mimics a function within the brain, it suggests our approach may be on the right track. Second, neuroscience can provide a rich source of inspiration for new types of algorithms and architectures to employ when building artificial brains. Traditional approaches to AI have historically been dominated by logic-based methods and theoretical mathematical models. We argue that neuroscience can complement these by identifying classes of biological computation that may be critical to cognitive function. \nTake one recent example of a seminal finding in neuroscience: the discovery of offline experience \u201c\nreplay\n\u201d. During sleep or quiet resting, biological brains \u201creplay\u201d temporal patterns of neuronal activity that were produced in an earlier active period. For example, when rats run through a maze, \u201cplace\u201d cells activate as the animal moves around. During rest, the same sequence of neuronal activity is observed, as if the rats were mentally reimagining their past movements, and using them to optimise future behaviour. In fact, it has been shown that interfering with replay impairs performance when they later perform the same tasks.\nAt first glance, it might seem counterintuitive to build an artificial agent that needs to \u2018sleep\u2019 - after all, they are supposed to grind away at a computational problem long after their programmers have gone to bed. But this principle was a key part of our \ndeep-Q network (DQN)\n, an algorithm that learnt to master a diverse range of Atari 2600 games to superhuman level with only the raw pixels and score as inputs. DQN mimics \u201cexperience replay\u201d, by storing a subset of training data that it reviews \u201coffline\u201d, allowing it to learn anew from successes or failures that occurred in the past.\nSuccesses like this give us confidence that neuroscience is already an important source of ideas for AI. But looking forward, we believe it will become indispensable in helping us tackle unsolved questions, such as those concerning efficient learning, understanding of the physical world, and imagination.\nImagination\n is a hugely important function for humans and animals, allowing us to plan for future scenarios without taking action; something that may come at a cost. \u00a0Consider a simple example, such as planning a holiday. In order to do this we leverage our knowledge - or \u201cmodel\u201d - of the world and use it to project forward in time, evaluating future states, and allowing us to calculate the route we need to take or what clothes to pack for sunny weather. Cutting-edge research in human neuroscience is starting to unveil the computational and systems mechanisms that underpin this kind of thinking, but much of this new understanding has yet to be incorporated into artificial models.\nAnother key challenge in contemporary AI research is known as transfer learning. To be able to deal effectively with novel situations, artificial agents need the ability to build on existing knowledge to make sensible decisions. Humans are already good at this - an individual who can drive a car, use a laptop or chair a meeting are usually able to cope even when confronted by an unfamiliar vehicle, operating system or social situation.\nResearchers are now starting to take the first steps towards understanding how this might be possible in artificial systems. For example, a new class of network architecture known as a \u201c\nprogressive network\n\u201d can use knowledge learned in one video game to learn another. The same architecture has also been shown to transfer knowledge from a simulated robotic arm to a real-world arm, massively reducing the training time. Intriguingly, these networks bear some similarities to \nmodels of sequential task learning in humans\n. These tantalising links suggest that there are great opportunities for future AI research to learn from work in neuroscience.\nBut this exchange of knowledge cannot be a one-way street. Neuroscience can also benefit from AI research. Take the idea of reinforcement learning - one of the central approaches in contemporary AI research. Although the original idea came from theories of animal learning in psychology, it was developed and elaborated by machine learning researchers. \u00a0These later ideas fed back into neuroscience to help us understand neurophysiological phenomena, such as the \nfiring properties of dopamine neurons\n in the mammalian basal ganglia.\nThis back and forth is essential if both fields are to continue to build on each other\u2019s insights, creating a virtuous circle whereby AI researchers use ideas from neuroscience to build new technology, and neuroscientists learn from the behaviour of artificial agents to better interpret biological brains. Indeed, this cycle will likely accelerate thanks to recent advances, such as optogenetics, that allow us to precisely measure and manipulate brain activity, yielding vast quantities of data that can be analysed with tools from machine learning.\nWe therefore believe distilling intelligence into algorithms and comparing them to the human brain is now vital. Not only could it bolster our quest to develop AI, a tool that we hope will \ncreate new knowledge and push forward scientific discovery\n, but may also allow us to better understand what\u2019s going on inside our own heads. That could shed light on some of the most enduring mysteries in neuroscience, such as the nature of creativity, dreams and, perhaps one day, even consciousness. With so much at stake, the need for the field of neuroscience and AI to come together is now more urgent than ever before.\nRead paper:\u00a0\nNeuroscience-Inspired Artificial Intelligence\n"}
{"title": "Going beyond average for reinforcement learning", "contents": "Consider the commuter who toils backwards and forwards each day on a train. Most mornings, her train runs on time and she reaches her first meeting relaxed and ready. But she knows that once in awhile the unexpected happens: a mechanical problem, a signal failure, or even just a particularly rainy day. Invariably these hiccups disrupt her pattern, leaving her late and flustered.\nRandomness is something we encounter everyday and has a profound effect on how we experience the world. The same is true in reinforcement learning (RL) applications, systems that learn by trial and error and are motivated by rewards. Typically, an RL algorithm predicts the average reward it receives from multiple attempts at a task, and uses this prediction to decide how to act. But random perturbations in the environment can alter its behaviour by changing the exact amount of reward the system receives.\nIn \na new paper\n, we show it is possible to model not only the average but also the full variation of this reward, what we call the value distribution. This results in RL systems that are more accurate and faster to train than previous models, and more importantly opens up the possibility of rethinking the whole of reinforcement learning.\nReturning to the example of our commuter, let\u2019s consider a journey composed of three segments of 5 minutes each, except that once a week the train breaks down, adding another 15 minutes to the trip. A simple calculation shows that the average commute time is \n(3 x 5) + 15 / 5 = 18\n minutes.\nIn reinforcement learning, we use Bellman's equation to predict this average commute time. Specifically, Bellman\u2019s equation relates our current average prediction to the average prediction we make in the immediate future. From the first station, we predict an 18 minutes journey (the average total duration); from the second, we predict a 13 minutes journey (average duration minus the first segment\u2019s length). Finally, assuming the train hasn\u2019t yet broken down, from the third station we predict there are 8 minutes (13 - 5) left to our commute, until finally we arrive at our destination. Bellman\u2019s equation makes each prediction sequentially, and updates these predictions on the basis of new information.\nWhat's a little counterintuitive about Bellman\u2019s equation is that we never actually observe these predicted averages: either the train takes 15 minutes (4 days out of 5), or it takes 30 minutes \u2013 never 18! From a purely mathematical standpoint, this isn\u2019t a problem, because decision theory tells us we only need averages to make the best choice. As a result, this issue has been mostly ignored in practice. Yet, there is now plenty of \nempirical\n \nevidence\n that predicting averages is a complicated business.\nIn \nour new paper\n, we \u00a0show that there is in fact a variant of Bellman's equation which predicts all possible outcomes, without averaging them. In our example, we maintain two predictions \u2013 a distribution \u2013 at each station: If the journey goes well, then the times are 15, 10, then 5 minutes, respectively; but if the train breaks down, then the times are 30, 25, and finally 20 minutes.\nAll of reinforcement learning can be reinterpreted under this new perspective, and its application is already leading to surprising new theoretical results. Predicting the distribution over outcomes also opens up all kinds of algorithmic possibilities, such as:\nWe took our new ideas and implemented them within the \nDeep Q-Network agent\n, replacing its single average reward output with a distribution with 51 possible values. The only other change was a new learning rule, reflecting the transition from Bellman\u2019s (average) equation to its distributional counterpart. Incredibly, it turns out going from averages to distributions was all we needed to surpass the performance of all other comparable approaches, and by a wide margin. The graph below shows how we get 75% of a trained Deep Q-Network\u2019s performance in 25% of the time, and achieve significantly better human performance:\nOne surprising result is that we observe some randomness in Atari 2600 games, even though Stella, the underlying game emulator, is itself fully predictable. This randomness arises in part because of what\u2019s called partial observability: due to the internal programming of the emulator, our agents playing the game of Pong cannot predict the exact time at which their score increases. Visualising the agent\u2019s prediction over successive frames (graphs below) we see two separate outcomes (low and high), reflecting the possible timings. Although this intrinsic randomness doesn\u2019t directly impact performance, our results highlight the limits of our agents\u2019 understanding.\nRandomness also occurs because the agent\u2019s own behaviour is uncertain. In Space Invaders, our agent learns to predict the future probability that it might make a mistake and lose the game (zero reward).\nJust like in our train journey example, it makes sense to keep separate predictions for these vastly different outcomes, rather than aggregate them into an unrealisable average. In fact, we think that our improved results are in great part due to the agent\u2019s ability to model its own randomness.\nIt\u2019s already evident from our empirical results that the distributional perspective leads to better, more stable reinforcement learning. With the possibility that every reinforcement learning concept could now want a distributional counterpart, it might just be the beginning for this approach.\nThis work was done by Marc G. Bellemare*, Will Dabney*, and R\u00e9mi Munos.\nRead paper: \nA Distributional Perspective on Reinforcement Learning\n"}
{"title": "Agents that imagine and plan", "contents": "Imagining the consequences of your actions before you take them is a powerful tool of human cognition. When placing a glass on the edge of a table, for example, we will likely pause to consider how stable it is and whether it might fall. On the basis of that imagined consequence we might readjust the glass to prevent it from falling and breaking. This form of deliberative reasoning is essentially \u2018\nimagination\n\u2019, it is a distinctly human ability and is a crucial tool in our everyday lives. \nIf our algorithms are to develop equally sophisticated behaviours, they too must have the capability to \u2018imagine\u2019 and reason about the future. Beyond that they must be able to construct a plan using this knowledge. We have seen some tremendous results in this area - particularly in programs like AlphaGo, which use an \u2018internal model\u2019 to analyse how actions lead to future outcomes in order to to reason and plan. These internal models work so well because environments like Go are \u2018perfect\u2019 - they have clearly defined rules which allow outcomes to be predicted very accurately in almost every circumstance. But the real world is complex, rules are not so clearly defined and unpredictable problems often arise. Even for the most intelligent agents, imagining in these complex environments is a long and costly process.\nIn \ntwo\n \nnew\n papers, we describe a new family of approaches for imagination-based planning. We also introduce architectures which provide new ways for agents to learn and construct plans to maximise the efficiency of a task. These architectures are efficient, robust to complex and imperfect models, and can adopt flexible strategies for exploiting their imagination.\nThe agents we introduce benefit from an \u2018imagination encoder\u2019- a neural network which learns to extract any information useful for the agent\u2019s future decisions, but ignore that which is not relevant. These agents have a number of distinct features:\nWe tested our proposed architectures on multiple tasks, including the puzzle game Sokoban and a spaceship navigation game. Both games require forward planning and reasoning, making them the perfect environment to test our agents' abilities.\nTo limit trial-and-error for both tasks, each level is procedurally generated and the agent can only try it once; this encourages the agent to try different strategies 'in its head' before testing them in the real environment.\nAbove, an agent plays Sokoban from a pixel representation, not knowing the rules of the game. At specific points in time, we visualise the agent's imagination of five possible futures. Based on that information, the agent decides what action to take. The corresponding trajectory is highlighted.\nFor both tasks, the imagination-augmented agents outperform the imagination-less baselines considerably: they learn with less experience and are able to deal with the imperfections in modelling the environment. Because agents are able to extract more knowledge from internal simulations they can solve tasks more with fewer imagination steps than conventional search methods, like the Monte Carlo tree search.\nWhen we add an additional \u2018manager\u2019 component, which helps to construct a plan, the agent learns to solve tasks even more efficiently with fewer steps. In the spaceship task it can distinguish between situations where the gravitational pull of its environment is strong or weak, meaning different numbers of these imagination steps are required. When an agent is presented with multiple models of an environment, each varying in quality and cost-benefit, it learns to make a meaningful trade-off. Finally, if the computational cost of imagination increases with each action taken, the agent imagines the effect of multiple chained actions early, and relies on this plan later without invoking imagination again.\nBeing able to deal with imperfect models and learning to adapt a planning strategy to current state are important research questions. Our two new papers, alongside previous work by Hamrick et al. consider these questions. While model-based reinforcement learning and planning are active areas of research (papers by Silver et al.; Henaff et al.; and Kansky et al. are a just a few examples of related lines of enquiry), further analysis and consideration is required to provide scalable solutions to rich model-based agents that can use their imaginations to reason about - and plan - for the future.\nRead paper: \nLearning model-based planning from scratch\n and \nImagination-Augmented Agents for Deep Reinforcement Learning\n"}
{"title": "Imagine this: Creating new visual concepts by recombining familiar ones", "contents": "Around two and a half thousand years ago a Mesopotamian trader gathered some clay, wood and reeds and changed humanity forever. Over time, their abacus would allow traders to keep track of goods and reconcile their finances, allowing economics to flourish.\nBut that moment of inspiration also shines a light on another astonishing human ability: our ability to recombine existing concepts and imagine something entirely new. The unknown inventor would have had to think of the problem they wanted to solve, the contraption they could build and the raw materials they could gather to create it. Clay could be moulded into a tablet, a stick could be used to scratch the columns and reeds can act as counters. Each component was familiar and distinct, but put together in this new way, they formed something revolutionary.\nThis idea of \u201ccompositionality\u201d is at the core of human abilities such as creativity, imagination and language-based communication. Equipped with just a small number of familiar conceptual building blocks, we are able to create a vast number of new ones on the fly. We do this naturally by placing concepts in hierarchies that run from specific to more general and then recombining different parts of the hierarchy in novel ways.\nBut what comes so naturally to us, remains a challenge in AI research.\nIn our \nnew paper\n, we propose a novel theoretical approach to address this problem. We also demonstrate a new neural network component called the Symbol-Concept Association Network (SCAN), that can, for the first time, learn a grounded visual concept hierarchy in a way that mimics human vision and word acquisition, enabling it to imagine novel concepts guided by language instructions.\nOur approach can be summarised as follows:\nOur approach differs from previous work in this area because it is fully grounded in the sensory data and learns from very few image-word pairs. While other deep learning approaches require thousands of image examples to learn a concept, SCAN learns both the visual primitives and conceptual abstractions primarily from unsupervised observations and as few as five pairs of an image and label per concept. Once trained, SCAN can then generate a diverse list of concepts that correspond to a particular image, and imagine diverse visual examples that correspond to a particular concept, even if it has never experienced the concept before.\nThis ability to learn new concepts by recombining existing ones through symbolic instructions has given humans astonishing abilities, allowing us to reason about abstract concepts like the universe, humanism or - as was the case in Mesopotamia - economics. While our algorithms have a long way to go before they can make such conceptual leaps, this work demonstrates a first step towards having algorithms that can learn in a largely unsupervised way, and think about conceptual abstractions like those used by humans.\nRead paper: \nSCAN: Learning Abstract Hierarchical Compositional Visual Concepts\n"}
{"title": "DeepMind papers at ICML 2017 (part three)", "contents": "The final part of our three-part series that gives an overview of the papers we are presenting at the ICML 2017 Conference in Sydney, Australia.\nAuthors: \nSamuel Ritter*, David Barrett*, Adam Santoro, Matt Botvinick\nDeep neural networks (DNNs) have achieved unprecedented performance on a wide range of tasks, rapidly outpacing our understanding of the nature of their solutions. In this work, we propose to address this interpretability problem in modern DNNs using the problem descriptions, theories and experimental methods developed of cognitive psychology. In a case study, we apply a theory and method from the psychology of human word learning to better understand how modern one-shot learning systems work. Results revealed not only that our DNNs exhibit the same inductive bias as humans, but also several unexpected features of the DNNs.\nFor further details and related work, please see the\n paper\n.\nCheck it out at ICML: \nTuesday 08 August, 15:48-16:06 @ Darling Harbour Theatre (Talk)\nTuesday 08 August, 18:30-20:00 @ Gallery #113 (Poster)\nAuthors: \nGeorg Ostrovski, Marc Bellemare, Aaron van den Oord, Remi Munos\nCount-based exploration based on prediction gain of a simple graphical density model has previously achieved \u00a0state-of-the-art results on some of the hardest exploration games in Atari. We investigate the open questions 1) whether a better density model leads to better exploration, and 2) what role the mixed Monte Carlo update rule used in this work plays for exploration. We show that a neural density model - PixelCNN - can be trained online on the experience stream of an RL agent and used for count-based exploration to achieve even better results on a wider set of hard exploration games, while preserving higher performance on easy exploration games. We also show that the Monte Carlo return is crucial in making use of the intrinsic reward signal in the sparsest reward settings, and cannot easily be replaced by a softer lambda-return update rule.\nFor further details and related work, please see the \npaper\n.\nCheck it out at ICML:\nWednesday 09 August, 13:30-13:48 @ C4.5 (Talk)\nWednesday 09 August, 18:30-22:00 @ Gallery #64 (Poster)\nAuthors: \nDavid Silver, Hado van Hasselt, Matteo Hessel, Tom Schaul, Arthur Guez, Tim Harley, Gabriel Dulac-Arnold, David Reichert, Neil Rabinowitz, Andre Barreto, Thomas Degris\nOne of the key challenges of artificial intelligence is to learn models that are effective in the context of planning. In this document we introduce the predictron architecture. The predictron consists of a fully abstract model, represented by a Markov reward process, that can be rolled forward multiple \u201cimagined\" planning steps. Each forward pass of the predictron accumulates internal rewards and values over multiple planning depths. The predictron is trained end-to-end so as to make these accumulated values accurately approximate the true value function. We applied the predictron to procedurally generated random mazes and a simulator for the game of pool. The predictron yielded significantly more accurate predictions than conventional deep neural network architectures.\nFor further details and related work, please see the \npaper\n.\nCheck it out at ICML: \nWednesday 09 August, 14:24-14:42 @ C4.5 (Talk)\nWednesday 09 August 18:30-20:00 @ Gallery #91 (Poster)\nAuthors: \nSasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Hees, Max Jaderberg, David Silver, Koray Kavukcuoglu\nHow to create agents that can learn to decompose their behaviour into meaningful primitives and then reuse them to more efficiently acquire new behaviours is a long standing research question. The solution to this question may be an important stepping stone towards agents with general intelligence and competence. This paper introduced FeUdal Networks (FuN), a novel architecture that formulates sub-goals as directions in latent state space, which, if followed, translates into a meaningful behavioural primitives. FuN clearly separates the module that discovers and sets sub-goals from the module that generates behaviour through primitive actions. This creates a natural hierarchy that is stable and allows both modules to learn in complementary ways. Our experiments clearly demonstrate that this makes long-term credit assignment and memorisation more tractable. This also opens many avenues for further research, for instance: deeper hierarchies can be constructed by setting goals at multiple time scales, scaling agents to truly large environments with sparse rewards and partial observability.\nFor further details and related work, please see the \npaper\n.\nCheck it out at ICML:\nWednesday 09 August, 15:30-15:48 @ C4.5 (Talk)\nWednesday 09 August, 18:30-20:00 \u00a0@ Gallery #107 (Poster)\nAuthors: \nAlex Pritzel, Benigno Uria, Sriram Srinivasan, Adria Puigdomenech, Oriol Vinyals, Demis Hassabis, Daan Wierstra, Charles Blundell\nDeep reinforcement learning algorithms have achieved state of the art performance on a variety of tasks, however they tend to be grossly data inefficient. In this work we propose a novel algorithm that allows rapid incorporation of new information collected by the agent. For this we introduce a new differentiable data structure, a differentiable neural dictionary, that can incorporate new information immediately, while being able to update it\u2019s internal representation based on the task the algorithm is supposed to solve. Our agent, Neural Episodic Control, is built on top of the differentiable data structure and is able to learn significantly faster across a wide range of environments.\nFor further details and related work, please see the \npaper\n.\nCheck it out at ICML: \nWednesday 09 August, 16:06-16:24 @ C4.5\nWednesday 09 August, 18:30-22:00 @ Gallery #125\nAuthors: \nJustin Gilmer (Google Brain), Sam Schoenholz (Google Brain), Patrick Riley (Google Google), Oriol Vinyals, George Dahl (Google Brain)\nIn this work we show how we can gain orders of magnitude \u00a0improvements to run-time performance by treating an expensive simulation of quantum chemistry properties as a supervised dataset to be learnt by extending neural networks to operate on graphs. Our model is extremely accurate and very fast. In the manuscript we also provide a unifying framework which summarises previous work on graph-shaped inputs and neural networks.\nFor further details and related work, please see the \npaper\n.\nCheck it out at ICML:\nWednesday 09 August, 16:24-16:42 @ Darling Harbour Theatre (Talk)\nWednesday 09 August, 18:30-22:00 @ Gallery #131 (Poster)\n"}
{"title": "DeepMind papers at ICML 2017 (part two)", "contents": "The second of \u00a0our three-part series, which gives an overview of the papers we are presenting at the ICML 2017 Conference in Sydney, Australia.\nAuthors: \nIan Osband, Benjamin Van Roy\nComputational results demonstrate that posterior sampling for reinforcement learning (PSRL) dramatically outperforms existing algorithms driven by optimism, such as UCRL2. We provide insight into the extent of this performance boost and the phenomenon that drives it. We leverage this insight to establish an $\\tilde{O}(H\\sqrt{SAT})$ Bayesian regret bound for PSRL in finite-horizon episodic Markov decision processes. This improves upon the best previous Bayesian regret bound of $\\tilde{O}(H S \\sqrt{AT})$ for any reinforcement learning algorithm. Our theoretical results are supported by extensive empirical evaluation.\nFor further details and related work, please see the \npaper\n.\nCheck it out at ICML:\nMonday 07 August, 11:42-12:00 @ C4.5 (Talk)\nMonday 07 August, 18:30-22:00 \u00a0@ Gallery #36 (Poster)\nAuthors: \nIrina Higgins*, Arka Pal*, Andrei Rusu, Loic Matthey, Chris Burgess, Alexander Pritzel, Matt Botvinick, Charles Blundell, Alexander Lerchner\nModern deep reinforcement learning agents rely on large quantities of data to learn how to act. In some scenarios, such as robotics, obtaining a lot of training data may be infeasible. Hence such agents are often trained on a related task where data is easy to obtain (e.g. simulation) with the hope that the learnt knowledge will generalise to the task of interest (e.g. reality). We propose DARLA, a DisentAngled Representation Learning Agent, that exploits its interpretable and structured vision to learn how to act in a way that is robust to various novel changes in its environment - including a simulation to reality transfer scenario in robotics. We show that DARLA significantly outperforms all baselines, and that its performance is crucially dependent on the quality of its vision.\nFor further details and related work, please see the \npaper\n.\nCheck it out at ICML:\nMonday 07 August, 16:42-17:00 @ C4.5 (Talk)\\\nMonday 07 August, 18:30-22:00 @ Gallery #123 (Poster)\nAuthors:\n \nAlex Graves, Marc G. Bellemare, Jacob Menick, Koray Kavukcuoglu, Remi Munos\nAs neural networks are applied to ever more complex problems, the need for efficient curriculum learning becomes more pressing. However, designing effective curricula is difficult and typically requires a large amount of hand-tuning. This paper uses reinforcement learning to automate the path, or syllabus, followed by the network through the curriculum so as to maximise the overall rate of learning progress. We consider nine different progress indicators, including a novel class of complexity-gain signal. Experimental results on three problems show that an automatically derived syllabus can lead to efficient curriculum learning, even on data (such as the bAbI tasks) that were not explicitly designed for curriculum learning. \nFor further details and related work, please see the \npaper\n.\nCheck it out at ICML:\nMonday 07 August, 16:42-17:00 @ C4.6 & C4.7 (Talk)\nMonday 07 August, 18:30-20:00 @ Gallery #127 (Poster)\nAuthors: \nYutian Chen, Matthew Hoffman, Sergio Gomez, Misha Denil, Timothy Lillicrap, Matthew Botvinick , Nando de Freitas\nWe learn recurrent neural network optimisers trained on simple synthetic functions by gradient descent. The learned optimisers exhibit a remarkable degree of transfer in that they can be used to efficiently optimise a broad range of derivative-free black-box problems, including continuous bandits, control problems, global optimization benchmarks and hyper-parameter tuning tasks.\nFor further details and related work, please see the \npaper\n.\nCheck it out at ICML:\nMonday 07 August, 17:15-17:33 @ Darling Harbour Theatre (Talk)\nTuesday 08 August, 18:30-22:00 @ Gallery #6 (Poster)\nAuthors:\n \nMarc G. Bellemare*, Will Dabney*, Remi Munos\nWe argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman's equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.\nFor further details and related work, please see the \nblog post\n and the \npaper\n.\nCheck it out at ICML: \nMonday 07 August, 17:33-17:51 @ C4.5 (Talk)\nTuesday 08 August, 18:30-22:00 @ Gallery #13 (Poster)\nAuthors: \nMarlos Machado (Univ. Alberta), Marc G. Bellemare, Michael Bowling\nRepresentation learning and option discovery are two of the biggest challenges in reinforcement learning (RL). Proto-value functions (PVFs) are a well-known approach for representation learning in MDPs. In this paper we address the option discovery problem by showing how PVFs implicitly define options. We do it by introducing eigenpurposes, intrinsic reward functions derived from learned representations. The options discovered from eigenpurposes traverse the principal directions of the state space. They are useful for multiple tasks because they are discovered without taking the environment\u2019s rewards into consideration. Moreover, different options act at different time scales, making them helpful for exploration. We demonstrate features of eigenpurposes in traditional tabular domains as well as in Atari 2600 games.\nFor further details and related work, please see the \npaper\n.\nCheck it out at ICML: \nMonday 07 August, 18:09-18:27 @ C4.5 (Talk)\nTuesday 08 August 18:30-20:00 @ Gallery #23 (Poster)\nAuthors: \nSander Dieleman, Karen Simonyan, Jesse Engel (Google Brain), Cinjon Resnick (Google Brain), Adam Roberts (Google Brain), Douglas Eck (Google Brain), Mohammad Norouzi (Google Brain)\nIn this paper, we introduce a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform. We also introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets. Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline. Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive.\nFor further details and related work, please see the \npaper\n.\nCheck it out at ICML:\nTuesday 08 August, 14:42-15:00 @ Parkside 1 (Talk)\nTuesday 08 August, 18:30-22:00 @ Gallery #98 (Poster)\n"}
{"title": "DeepMind papers at ICML 2017 (part one)", "contents": "The first of our three-part series, which gives brief descriptions of the papers we are presenting at the ICML 2017 Conference in Sydney, Australia.\nAuthors: \nMax Jaderberg, Wojciech Marian Czarnecki, Simon Osindero, Oriol Vinyals, Alex Graves, David Silver, Koray Kavukcuoglu\nWhen training neural networks, the modules (layers) are locked: they can only be updated after backpropagation. We remove this constraint by incorporating a learnt model of error gradients, Synthetic Gradients, which means we can update networks without full backpropagation. We show how this can be applied to feed-forward networks which allows every layer to be trained asynchronously, to RNNs which extends the time over which models can remember, and to multi-network systems to allow communication.\nFor further details and related work, please see the \npaper\n.\nCheck it out at ICML:\n\u200d\nMonday 07 August, 10:30-10:48 @ Darling Harbour Theatre (Talk)\nMonday 07 August, 18:30-22:00 PM @ Gallery #1 (Poster)\nAuthors:\n \nScott Reed, A\u00e4ron van den Oord, Nal Kalchbrenner, Ziyu Wang, Dan Belov, Nando de Freitas\nThe parallel multiscale autoregressive density estimator generates high-resolution (512 by 512) images, with orders of magnitude speedup over other autoregressive models. We evaluate the model on class-conditional image generation, text-to-image synthesis, and action-conditional video generation, showing that our model achieves the best results among non-pixel-autoregressive density models that allow efficient sampling.\nFor further details and related work, please see the \npaper\n.\nCheck it out at ICML:\nMonday 07 August, 10:48-11:06 @ Parkside 1 (Talk)\nMonday 07 August, 18:30-20:00 @ Gallery #10 (Poster)\nAuthors: \nWojtek Czarnecki, Grzegorz \u015awirszcz, Max Jaderberg, Simon Osindero, Oriol Vinyals, Koray Kavukcuoglu\nSynthetic gradients has been shown to work empirically in both feed-forward and recurrent cases. This work focuses on why and how it actually works - it shows that under mild assumptions critical points are preserved and that in the simplest case of linear model, synthetic gradients based learning does converge to the global optimum. On the other hand, we present empirically that trained models might be qualitatively different from those obtained using backpropagation.\nFor further details and related work, please see the \npaper\n.\nCheck it out at ICML: \nMonday 07 August, 10:48-11:06 @ Darling Harbour Theatre (Talk)\nMonday 07 August, 18:30-20:00 @ Gallery #9 (Poster)\nAuthors: \nMohammad Gheshlaghi Azar, Ian Osband, Remi Munos\nWe consider the problem of provably optimal exploration in reinforcement learning for finite horizon MDPs. We show that an optimistic modification to value iteration achieves a regret bound of order (HSAT)1/2 \u00a0(up to a logarithmic factor) where H is the time horizon, S the number of states, A the number of actions and T the number of time-steps. This result improves over the best previous known bound HS(AT)1/2 achieved by the UCRL2 algorithm of [Jaksch, Ortner, Auer, 2010]. The key significance of our new results is that for large T, the sample complexity of our algorithm matches the optimal lower bound of \u03a9(HSAT)1/2. Our analysis contains two key insights. We use careful application of concentration inequalities to the optimal value function as a whole, rather than to the transitions probabilities (to improve scaling in S), and we define Bernstein-based \"exploration bonuses\" that use the empirical variance of the estimated values at the next states (to improve scaling in H).\nFor further details and related work, please see the \npaper\n.\nCheck it out at ICML:\nMonday 07 August, 10:48-11:06 @ C4.5 (Talk)\nMonday 07 August, 18:30-22:00 @ Gallery #12 (Poster)\nAuthors:\n \nNal Kalchbrenner, Aaron van den Oord, Karen Simonyan, Ivo Danihelka, Oriol Vinyals,Alex Graves, Koray Kavukcuoglu\nPredicting the continuation of frames in a video is a hallmark task in unsupervised learning. We present a video model, the VPN, that is probabilistic and that is able to make accurate and sharp predictions of future video frames. The VPN achieves, for the first time, a nearly perfect score on the Moving MNIST dataset and produces plausible futures of up to 18 frames of robotic arm movements.\nFor further details and related work, please see the \npaper\n.\nCheck it out at ICML:\nMonday 07 August, 11:06-11:24 @ Parkside 1 (Talk)\nMonday 07 August, 18:30-22:00 @ Gallery #18 (Poster)\n\u200d\nAuthors:\n \nLaurent Dinh (Univ. Montreal), Razvan Pascanu, Samy Bengio (Google Brain), Yoshua Bengio (Univ. Montreal)\nEmpirically, it has been observed that deep networks generalise well, even when they have the capacity to overfit the data. Additionally, it seems that stochastic gradient descent results in models that generalise better than batch method. One hypothesis for explaining this phenomena is that the noise of SGD helps model to find wide minina which generalise better than sharp (narrow) minima. In this work we try to improve our understanding of this hypothesis. We show that it does not hold for proposed definitions of wideness or sharpness due to the structure of neural networks. This suggest that there is no causality connection between batchsize size and generalisation.\nFor further details and related work, please see the \npaper\n.\nCheck it out at ICML:\nMonday 07 August, 11:06-11:24 @ C4.8 (Talk)\nTuesday 08 August, 18:30-22:00 @ Gallery #3 (Poster)\n"}
{"title": "WaveNet: A generative model for raw audio", "contents": "This post presents \nWaveNet\n, a deep generative model of raw audio waveforms. We show that WaveNets are able to generate speech which mimics any human voice and which sounds more natural than the best existing Text-to-Speech systems, reducing the gap with human performance by over 50%.\nWe also demonstrate that the same network can be used to synthesize other audio signals such as music, and present some striking samples of automatically generated piano pieces.\nAllowing people to converse with machines is a long-standing dream of human-computer interaction. The ability of computers to understand natural speech has been revolutionised in the last few years by the application of deep neural networks (e.g., \nGoogle Voice Search\n). However, generating speech with computers \u00a0\u2014 a process usually referred to as \nspeech synthesis\n or text-to-speech (TTS) \u2014 is still largely based on so-called \nconcatenative TTS\n, where a very large database of short speech fragments are recorded from a single speaker and then recombined to form complete utterances. This makes it difficult to modify the voice (for example switching to a different speaker, or altering the emphasis or emotion of their speech) without recording a whole new database.\nThis has led to a great demand for \nparametric TTS\n, where all the information required to generate the data is stored in the parameters of the model, and the contents and characteristics of the speech can be controlled via the inputs to the model. So far, however, parametric TTS has tended to sound less natural than concatenative. Existing parametric models typically generate audio signals by passing their outputs through signal processing algorithms known as \nvocoders\n.\nWaveNet changes this paradigm by directly modelling the raw waveform of the audio signal, one sample at a time. As well as yielding more natural-sounding speech, using raw waveforms means that WaveNet can model any kind of audio, including music.\nResearchers usually avoid modelling raw audio because it ticks so quickly: typically 16,000 samples per second or more, with important structure at many time-scales. Building a completely autoregressive model, in which the prediction for every one of those samples is influenced by all previous ones (in statistics-speak, each predictive distribution is conditioned on all previous observations), is clearly a challenging task.\nHowever, our \nPixelRNN\n and \nPixelCNN\n models, published earlier this year, showed that it was possible to generate complex natural images not only one pixel at a time, but one colour-channel at a time, requiring thousands of predictions per image. This inspired us to adapt our two-dimensional PixelNets to a one-dimensional WaveNet.\nThe above animation shows how a WaveNet is structured. It is a fully convolutional neural network, where the convolutional layers have various dilation factors that allow its receptive field to grow exponentially with depth and cover thousands of timesteps.\nAt training time, the input sequences are real waveforms recorded from human speakers. After training, we can sample the network to generate synthetic utterances. At each step during sampling a value is drawn from the probability distribution computed by the network. This value is then fed back into the input and a new prediction for the next step is made. Building up samples one step at a time like this is computationally expensive, but we have found it essential for generating complex, realistic-sounding audio.\nWe trained WaveNet using some of Google\u2019s TTS datasets so we could evaluate its performance. The following figure shows the quality of WaveNets on a scale from 1 to 5, compared with Google\u2019s current best TTS systems (\nparametric\n and \nconcatenative\n), and with human speech using \nMean Opinion Scores (MOS)\n. MOS are a standard measure for subjective sound quality tests, and were obtained in blind tests with human subjects (from over 500 ratings on 100 test sentences). As we can see, WaveNets reduce the gap between the state of the art and human-level performance by over 50% for both US English and Mandarin Chinese.\nFor both Chinese and English, Google\u2019s current TTS systems are considered among the best worldwide, so improving on both with a single model is a major achievement.\nHere are some samples from all three systems so you can listen and compare yourself:\nParametric\nConcatenative\nWaveNet\nParametric\n\u200d\nConcatenative\nWaveNet\nIn order to use WaveNet to turn text into speech, we have to tell it what the text is. We do this by transforming the text into a sequence of linguistic and phonetic features (which contain information about the current phoneme, syllable, word, etc.) and by feeding it into WaveNet. This means the network\u2019s predictions are conditioned not only on the previous audio samples, but also on the text we want it to say.\nIf we train the network without the text sequence, it still generates speech, but now it has to make up what to say. As you can hear from the samples below, this results in a kind of babbling, where real words are interspersed with made-up word-like sounds:\nNotice that non-speech sounds, such as breathing and mouth movements, are also sometimes generated by WaveNet; this reflects the greater flexibility of a raw-audio model.\nAs you can hear from these samples, a single WaveNet is able to learn the characteristics of many different voices, male and female. To make sure it knew which voice to use for any given utterance, we conditioned the network on the identity of the speaker. Interestingly, we found that training on many speakers made it better at modelling a single speaker than training on that speaker alone, suggesting a form of transfer learning.\nBy changing the speaker identity, we can use WaveNet to say the same thing in different voices:\nSimilarly, we could provide additional inputs to the model, such as emotions or accents, to make the speech even more diverse and interesting.\nSince WaveNets can be used to model any audio signal, we thought it would also be fun to try to generate music. Unlike the TTS experiments, we didn\u2019t condition the networks on an input sequence telling it what to play (such as a musical score); instead, we simply let it generate whatever it wanted to. When we trained it on a dataset of classical piano music, it produced fascinating samples like the ones below:\nWaveNets open up a lot of possibilities for TTS, music generation and audio modelling in general. The fact that directly generating timestep per timestep with deep neural networks works at all for 16kHz audio is really surprising, let alone that it outperforms state-of-the-art TTS systems. We are excited to see what we can do with them next.\nRead the paper: \nWaveNet: A Generative Model for Raw Audio\n"}
{"title": "Decoupled Neural Interfaces Using Synthetic Gradients", "contents": "This post introduces some of our latest research in progressing the capabilities and training procedures of neural networks called \nDecoupled Neural Interfaces using Synthetic Gradients\n. This work gives us a way to allow neural networks to communicate, to learn to send messages between themselves, in a decoupled, scalable manner paving the way for multiple neural networks to communicate with each other or improving the long term temporal dependency of recurrent networks. This is achieved by using a \nmodel to approximate error gradients\n, rather than by computing error gradients explicitly with backpropagation. The rest of this post assumes some familiarity with neural networks and how to train them. If you\u2019re new to this area we highly recommend \nNando de Freitas lecture series on Youtube\n on deep learning and neural networks.\nIf you consider any layer or module in a neural network, it can only be updated once all the subsequent modules of the network have been executed, and gradients have been backpropagated to it. For example look at this simple feed-forward network:\nHere, after Layer 1 has processed the input, it can only be updated after the output activations (black lines) have been propagated through the rest of the network, generated a loss, and the error gradients (green lines) backpropagated through every layer until Layer 1 is reached. This sequence of operations means that Layer 1 has to wait for the forwards and backwards computation of Layer 2 and Layer 3 before it can update. \nLayer 1 is locked, coupled, to the rest of the network.\nWhy is this a problem? Clearly for a simple feed-forward network as depicted we don\u2019t need to worry about this issue. But consider a complex system of multiple networks, acting in multiple environments at asynchronous and irregular timescales.\nOr a big distributed network spread over multiple machines. Sometimes requiring all modules in a network to wait for all other modules to execute and backpropagate gradients is overly time consuming or even intractable. If we decouple the interfaces - the connections - \u00a0between modules, every module can be updated independently, and is not locked to the rest of the network.\nSo, how can one decouple neural interfaces - that is decouple the connections between network modules - and still allow the modules to learn to interact? In this paper, we remove the reliance on backpropagation to get error gradients, and instead learn a parametric model which predicts what the gradients will be based upon only local information. We call these predicted gradients \nsynthetic gradients\n.\nThe synthetic gradient model takes in the activations from a module and produces what it predicts will be the error gradients - the gradient of the loss of the network with respect to the activations.\nGoing back to our simple feed-forward network example, if we have a synthetic gradient model we can do the following:\n... and use the synthetic gradients (blue) to \nupdate Layer 1 before the rest of the network has even been executed\n.\nThe synthetic gradient model itself is trained to regress target gradients - these target gradients could be the true gradients backpropagated from the loss or other synthetic gradients which have been backpropagated from a further downstream synthetic gradient model.\nThis mechanism is generic for a connection between any two modules, not just in a feed-forward network. The play-by-play working of this mechanism is shown below, where the change of colour of a module indicates an update to the weights of that module.\nUsing decoupled neural interfaces (DNI) therefore removes the locking of preceding modules to subsequent modules in a network. In experiments from the paper, we show we can train convolutional neural networks for \nCIFAR-10\n image classification where every layer is decoupled using synthetic gradients to the same accuracy as using backpropagation. It\u2019s important to recognise that DNI doesn\u2019t magically allow networks to train without true gradient information. The true gradient information does percolate backwards through the network, but just slower and over many training iterations, through the losses of the synthetic gradient models. The synthetic gradient models approximate and smooth over the absence of true gradients.\nA legitimate question at this point would be to ask how much computational complexity do these synthetic gradient models add - perhaps you would need a synthetic gradient model architecture that is as complex as the network itself. Quite surprisingly, the synthetic gradient models can be very simple. For feed-forward nets, we actually found out that even a \nsingle linear layer\n works well as a synthetic gradient model. Consequently it is both very easy to train and so produces synthetic gradients rapidly.\nDNI can be applied to any generic neural network architecture, not just feed-forward networks. An interesting application is to recurrent neural networks (RNNs). An RNN has a recurrent core which is unrolled - repeatedly applied - to process sequential data. Ideally to train an RNN we would unroll the core over the whole sequence (which could be infinitely long), and use backpropagation through time (BPTT) to propagate error gradients backwards through the graph.\nHowever in practice, we can only afford to unroll for a limited number of steps due to memory constraints and the need to actually compute an update to our core model frequently. This is called truncated backpropagation through time, and shown below for a truncation of three steps:\nThe change in colour of the core illustrates an update to the core, that the weights have been updated. In this example, truncated BPTT seems to address some issues with training - we can now update our core weights every three steps and only need three cores in memory. However, the fact that there is no backpropagation of error gradients over more than three steps means that the update to the core will not be directly influenced by errors made more than two steps in the future. This limits the temporal dependency that the RNN can learn to model.\nWhat if instead of doing no backpropagation between the boundary of BPTT we used DNI and produce synthetic gradients, which model what the error gradients of the future will be? We can incorporate a synthetic gradient model into the core so that at every time step, the RNN core produces not only the output but also the synthetic gradients. In this case, the synthetic gradients would be the predicted gradients of the all future losses with respect to the hidden state activation of the previous timestep. The synthetic gradients are only used at the boundaries of truncated BPTT where we would have had no gradients before.\nThis can be performed during training very efficiently - it merely requires us to keep an extra core in memory as illustrated below. Here a green dotted border indicates just computing gradients with respect to the input state, while a solid green border additionally computes gradients with respect to the core\u2019s parameters.\nBy using DNI and synthetic gradients with an RNN, we are approximating doing backpropagation across an infinitely unrolled RNN. In practice, this \nresults in RNNs which can model longer temporal dependencies\n. Here\u2019s an example result showing this from the paper.\nPenn Treebank test error during training (lower is better):\nThis graph shows the application of an RNN trained on next character prediction on Penn Treebank, a language modelling problem. On the y-axis the bits-per-character (BPC) is given, where smaller is better. The x-axis is the number of characters seen by the model as training progresses. The dotted blue, red and grey lines are RNNs trained with truncated BPTT, unrolled for 8 steps, 20 steps and 40 steps - the higher the number of steps the RNN is unrolled before performing backpropagation through time, the better the model is, but the slower it trains. When DNI is used on the RNN unrolled 8 steps (solid blue line) the RNN is able to capture the long term dependency of the 40-step model, but is trained twice as fast (both in terms of data and wall clock time on a regular desktop machine with a single GPU).\nTo reiterate, adding synthetic gradient models allows us to decouple the updates between two parts of a network. DNI can also be applied on hierarchical RNN models - system of two (or more) RNNs running at different timescales. As we show in the \npaper\n, DNI significantly improves the training speed of these models by enabling the update rate of higher level modules.\nHopefully from the explanations in this post, and a brief look at some of the experiments we report in the \npaper\n it is evident that it is possible to create decoupled neural interfaces. This is done by creating a synthetic gradient model which takes in local information and predicts what the error gradient will be. At a high level, this can be thought of as a \ncommunication protocol between two modules\n. One module sends a message (current activations), another one receives the message, and evaluates it using a \nmodel of utility \n(the synthetic gradient model). The model of utility allows the receiver to \nprovide instant feedback \n(synthetic gradient) to the sender, rather than having to wait for the evaluation of the true utility of the message (via backpropagation). This framework can also be thought about from an error critic point of view [\nWerbos\n] and is similar in flavour to using a critic in reinforcement learning [\nBaxter\n].\nThese decoupled neural interfaces allow \ndistributed training of networks, enhance the temporal dependency learnt with RNNs\n, and \nspeed up hierarchical RNN systems\n. We\u2019re excited to explore what the future holds for DNI, as we think this is going to be an important basis for opening up more modular, decoupled, and asynchronous model architectures. Finally, there are lots more details, tricks, and full experiments which you can find in the paper \nhere\n.\nNeural networks are the workhorse of many of the algorithms developed at DeepMind. For example, \nAlphaGo\n uses convolutional neural networks to evaluate board positions in the game of Go and \nDQN\nand \nDeep Reinforcement Learning algorithms\n use neural networks to choose actions to play at super-human level on video games.\n"}
{"title": "Differentiable neural computers", "contents": "In a \nrecent study in Nature\n, we introduce a form of memory-augmented neural network called a differentiable neural computer, and show that it can learn to use its memory to answer questions about complex, structured data, including artificially generated stories, family trees, and even a map of the London Underground. We also show that it can solve a block puzzle game using reinforcement learning.\nPlato likened memory to a wax tablet on which an impression, imposed on it once, would remain fixed. He expressed in metaphor the modern notion of plasticity \u2013 that our minds can be shaped and reshaped by experience. But the wax of our memories does not just form impressions, it also forms connections, from one memory to the next. Philosophers like John Locke believed that memories connected if they were formed nearby in time and space. Instead of wax, the most potent metaphor expressing this is Marcel Proust\u2019s madeleine cake; for Proust, one taste of the confection as an adult undammed a torrent of associations from his childhood. These episodic memories (event memories) are known to depend on the hippocampus in the human brain.\nToday, our metaphors for memory have been refined. We no longer think of memory as a wax tablet but as a reconstructive process, whereby experiences are reassembled from their constituent parts. And instead of a simple association between stimuli and behavioural responses, the relationship between memories and action is variable, conditioned on context and priorities. A simple article of memorised knowledge, for example a memory of the layout of the London Underground, can be used to answer the question, \u201cHow do you get from Piccadilly Circus to Moorgate?\u201d as well as the question, \u201cWhat is directly adjacent to Moorgate, going north on the Northern Line?\u201d. It all depends on the question; the contents of memory and their use can be separated. Another view holds that memories can be organised in order to perform computation. More like lego than wax, memories can be recombined depending on the problem at hand.\nNeural networks excel at pattern recognition and quick, reactive decision-making, but we are only just beginning to build neural networks that can think slowly \u2013 that is, deliberate or reason using knowledge. For example, how could a neural network store memories for facts like the connections in a transport network and then logically reason about its pieces of knowledge to answer questions? \nIn a recent paper\n, we showed how neural networks and memory systems can be combined to make learning machines that can store knowledge quickly and reason about it flexibly. These models, which we call differentiable neural computers (DNCs), can learn from examples like neural networks, but they can also store complex data like computers.\nIn a normal computer, the processor can read and write information from and to random access memory (RAM). RAM gives the processor much more space to organise the intermediate results of computations. Temporary placeholders for information are called variables and are stored in memory. In a computer, it is a trivial operation to form a variable that holds a numerical value. And it is also simple to make data structures \u2013 variables in memory that contain links that can be followed to get to other variables. One of the simplest data structures is a list \u2013 a sequence of variables that can be read item by item. For example, one could store a list of players\u2019 names on a sports team and then read each name one by one. A more complicated data structure is a tree. In a family tree for instance, links from children to parents can be followed to read out a line of ancestry. One of the most complex and general data structures is a graph, like the London Underground network.\nWhen we designed DNCs, we wanted machines that could learn to form and navigate complex data structures on their own. At the heart of a DNC is a neural network called a controller, which is analogous to the processor in a computer. A controller is responsible for taking input in, reading from and writing to memory, and producing output that can be interpreted as an answer. The memory is a set of locations that can each store a vector of information.\nA controller can perform several operations on memory. At every tick of a clock, it chooses whether to write to memory or not. If it chooses to write, it can choose to store information at a new, unused location or at a location that already contains information the controller is searching for. This allows the controller to update what is stored at a location. If all the locations in memory are used up, the controller can decide to free locations, much like how a computer can reallocate memory that is no longer needed. When the controller does write, it sends a vector of information to the chosen location in memory. Every time information is written, the locations are connected by links of association, which represent the order in which information was stored.\nAs well as writing, the controller can read from multiple locations in memory. Memory can be searched based on the content of each location, or the associative temporal links can be followed forward and backward to recall information written in sequence or in reverse. The read out information can be used to produce answers to questions or actions to take in an environment. Together, these operations give DNCs the ability to make choices about how they allocate memory, store information in memory, and easily find it once there.\nTo the non-technical reader, it may seem a bit odd that we have repeatedly used phrases like \u201cthe controller can\u201d or \u201cdifferentiable neural computers \u2026 make choices\u201d. We speak like this because differentiable neural computers learn how to use memory and how to produce answers completely from scratch. They learn to do so using the magic of optimisation: when a DNC produces an answer, we compare the answer to a desired correct answer. Over time, the controller learns to produce answers that are closer and closer to the correct answer. In the process, it figures out how to use its memory.\nWe wanted to test DNCs on problems that involved constructing data structures and using those data structures to answer questions. Graph data structures are very important for representing data items that can be arbitrarily connected to form paths and cycles. In the paper, we showed that a DNC can learn on its own to write down a description of an arbitrary graph and answer questions about it. When we described the stations and lines of the London Underground, we could ask a DNC to answer questions like, \u201cStarting at Bond street, and taking the Central line in a direction one stop, the Circle line in a direction for four stops, and the Jubilee line in a direction for two stops, at what stop do you wind up?\u201d Or, the DNC could plan routes given questions like \u201cHow do you get from Moorgate to Piccadilly Circus?\u201d\nIn a family tree, we showed that it could answer questions that require complex deductions. For example, even though we only described parent, child, and sibling relationships to the network, we could ask it questions like \u201cWho is Freya\u2019s maternal great uncle?\u201d We also found it possible to analyse how DNCs used their memories by visualising which locations in memory were being read by the controller to produce what answers. Conventional neural networks in our comparisons either could not store the information, or they could not learn to reason in a way that would generalise to new examples.\nWe could also train a DNC by reinforcement learning. In this framework, we let the DNC produce actions but never show it the answer. Instead, we score it with points when it has produced a good sequence of actions (like the children\u2019s game \u201chot or cold\u201d). We connected a DNC to a simple environment with coloured blocks arranged in piles. We would give it instructions for goals to achieve: \u201cPut the light blue block below the green; the orange to the left of the red; the purple below the orange; the light blue to the right of the dark blue; the green below the red; and the purple to the left of the green\u201d.\nWe could establish a large number of such possible goals and then ask the network to execute the actions that would produce one or another goal state on command. In this case, again like a computer, the DNC could store several subroutines in memory, one per possible goal, and execute one or another.\nThe question of how human memory works is ancient and our understanding still developing. We hope that DNCs provide both a new tool for computer science and a new metaphor for cognitive science and neuroscience: here is a learning machine that, without prior programming, can organise information into connected facts and use those facts to solve problems.\nFor more information about DNC, \nplease read our paper\n and an \nopinion piece\n by Herbert Jaeger about deep neural reasoning.\nOur open source implementation is available on \nGitHub\n.\n"}
{"title": "Announcing the Partnership on AI to Benefit People & Society", "contents": "We believe that AI has the potential for transformative, positive impact in the world. Fulfilling this potential is not only dependent on the quality of the algorithms being engineered and the data they use, but on the level of public engagement, transparency, and ethical discussion that takes place around them.\nIt\u2019s precisely because AI has the potential to have such a major positive impact on the world, that we believe it\u2019s critical that we build new models of open collaboration and accountability around it.\nThat\u2019s why we at DeepMind are really proud to have worked with Amazon, Google, Facebook, IBM and Microsoft, to form a non-profit organisation that aims to create a forum for open discussion around the benefits and challenges of developing and applying cutting edge AI. Together, we hope to advance public understanding of AI and formulate best practices on some of the most important and challenging ethical issues in the field.\nThe group - named the Partnership on Artificial Intelligence to Benefit People and Society - is a significant step forward, breaking down barriers between AI teams across leading companies to address some of the really difficult questions that are arising within the field. We also want to make it easier for those in other fields to understand, assess and engage with our scientific breakthroughs and consider the broader social and ethical impacts of our applications.\nBy opening up the conversation about AI to a wider community, we hope to build new models of engagement, collaboration, and accountability to take the field forward in a thoughtful, positive and ethical way that benefits people and society.\nTo find out more about the Partnership on Artificial Intelligence, visit the official website \nhere\n.\n"}
{"title": "Putting patients at the heart of DeepMind Health", "contents": "From the outset, we\u2019ve wanted DeepMind Health to be a truly collaborative effort. Too much hospital IT has been developed from a top-down perspective, often repurposing technology built for completely different sectors thousands of miles away from the NHS frontline. The result: tools that remain out-of-date and imperfectly suited to clinical use, contributing to a patient safety challenge where more than 1 in 10 patients suffer harm\u00b9 during an in-patient stay.\nWe think it\u2019s possible to transform this through bringing some of the world\u2019s most advanced technology to the NHS. But for this to have any chance of meaningful impact, we know it must have the input of patients and clinicians at its heart.\nYesterday we took a step towards that goal by hosting our first open patient and public forum in London, with over 130 patients, carers and members of the public coming to our offices and many more watching on our livestream.\nPatients have a vital role to play in helping to set our priorities and working with us to design new products and services. But we\u2019re still learning how to get this right. There are many exceptional people with far more experience of patient involvement than we have, and yesterday\u2019s event was a chance to meet some of them, explain what we\u2019ve done so far, and ask their advice about what to do next.\nWe heard some valuable feedback about how we can make our work with patients accessible to a wider group, including holding events in other parts of the country and at times of the day when people at work can attend, and ensuring that people who are unable to travel can still have their say.\nWe also heard insights about some of the most important and complex long-term issues we need to address together. These included the need for new security models in healthcare that can protect data and inspire trust, for business models that are transparent and closely tied to outcomes that actually matter to patients, and for clinical uses of AI to use methods and outputs that are verifiable by patients and clinicians. None of these topics are easy, and so we\u2019re committed to debating them openly and finding solutions together.\nWe were also excited to hear concrete recommendations for the tools and services we should build. We heard about the importance of including community care as well as hospitals, and of incorporating patients\u2019 input about their conditions rather than only relying on data from clinical tests. We were glad for the opportunity to ask for some early feedback about how patients could have greater access to their own health data - an early concept, but one we\u2019re looking forward to exploring further with patients in the months and years ahead.\nWe\u2019re very grateful to the patients and members of the public who gave up their time to join us, and who were so generous and candid with their feedback. We\u2019ll continue to consult with patients over the next few months about how we can most meaningfully embed their contributions in everything we do, and will look forward to publishing our full patient engagement plan before the end of the year.\nMany of our team grew up with the NHS, and we\u2019re all hugely motivated by the opportunity to make a difference with DeepMind Health. Patient input and involvement will be right at the heart of this effort, both now and in the future.\nVincent C, British Medical Journal\n"}
{"title": "Applying machine learning to radiotherapy planning for head & neck cancer", "contents": "We\u2019re excited to announce a new research partnership with the Radiotherapy Department at University College London Hospitals NHS Foundation Trust, which provides world-leading cancer treatment.\n1 in 75 men and 1 in 150 women will be diagnosed with oral cancer during their lifetime, and oral cavity cancer has risen by 92% since the 1970s. Head and neck cancer in general affects over 11,000 patients in the UK alone each year.\nAdvances in treatment such as radiotherapy have improved survival rates, but because of the high number of delicate structures concentrated in this area of the body, clinicians have to plan treatment extremely carefully to ensure none of the vital nerves or organs are damaged.\nThat makes a cancer at the back of the mouth or in the sinuses, for example, particularly hard to treat with radiotherapy.\nSo with clinicians in UCLH\u2019s world-leading radiotherapy team we are exploring whether machine learning methods could reduce the amount of time it takes to plan radiotherapy treatment for such cancers.\nBefore radiotherapy can be administered, clinicians have to produce a detailed map of the areas of the body to be treated, and the areas to avoid.\nThe process, known as segmentation, involves drawing around different parts of the anatomy and feeding the information through to a radiotherapy machine, which can then target cancers while leaving healthy tissue unharmed.\nBut when a tumour and vital anatomical structures are found in such close proximity, as in the head and neck, the outlines clinicians produce must be painstakingly detailed.\nFor these cancers, segmentation can take around four hours. And even though UCLH\u2019s specialist team at its dedicated head and neck cancer centre is a national leader in this process, there is still potential for innovation. We think machine learning could make a difference.\nOur collaboration will see us carefully analyse anonymised scans from up to seven hundred former patients at UCLH, to determine the potential for machine learning to make radiotherapy planning more efficient.\nClinicians will remain responsible for deciding radiotherapy treatment plans but it is hoped that the segmentation process could be reduced from up to four hours to around an hour.\nWe hope that in time, the research could lead to two benefits in particular:\nAs with all our work with the NHS, we will treat the patient data we are using in this project with the utmost care and respect. All scans will be anonymised in line with the UCLH Information Governance policy before they are shared with DeepMind. You can read more about our own approach to information governance \nhere\n.\nThis kind of research is still exploratory, but we think it has great potential to help both clinicians and patients.\nThis image has been reproduced, unmodified, from \nThe Cancer Image Archive\n under the terms of the \nCreative Commons Attribution 3.0 Unported License\n.\n"}
{"title": "DeepMind and Blizzard to release StarCraft II as an AI research environment", "contents": "Today at BlizzCon 2016 in Anaheim, California, we announced our collaboration with \nBlizzard Entertainment\n to open up StarCraft II to AI and Machine Learning researchers around the world.\nFor almost 20 years, the StarCraft game series has been widely recognised as the pinnacle of 1v1 competitive video games, and among the best PC games of all time. The original StarCraft was an early pioneer in eSports, played at the highest level by elite professional players since the late 90s, and remains incredibly competitive to this day. The StarCraft series\u2019 longevity in competitive gaming is a testament to Blizzard\u2019s design, and their continual effort to balance and refine their games over the years. StarCraft II continues the series\u2019 renowned eSports tradition, and has been the focus of our work with Blizzard.\nDeepMind is on a scientific mission to push the boundaries of AI, developing programs that can learn to solve any complex problem without needing to be told how. Games are the perfect environment in which to do this, allowing us to develop and test smarter, more flexible AI algorithms quickly and efficiently, and also providing instant feedback on how we\u2019re doing through scores.\nOver the past five years we\u2019ve helped to pioneer the use of games as AI research environments to drive our machine learning and reinforcement learning research forwards, from \n2D games in Atari\n, to full 3D environments such as \nTorcs\n,\n mastering the game of Go\n, or our forthcoming DeepMind Labyrinth. Here's a representation of what these research environments have looked like with L-R, Atari and Labyrinth.\nStarCraft is an interesting testing environment for current AI research because it provides a useful bridge to the messiness of the real-world. The skills required for an agent to progress through the environment and play StarCraft well could ultimately transfer to real-world tasks.\nAt the start of a game of StarCraft, players choose one of three races, each with distinct unit abilities and gameplay approaches. Players\u2019 actions are governed by the in-game economy; minerals and gas must be gathered in order to produce new buildings and units. The opposing player builds up their base at the same time, but each player can only see parts of the map within range of their own units. Thus, players must send units to scout unseen areas in order to gain information about their opponent, and then remember that information over a long period of time. \u00a0This makes for an even more complex challenge as the environment becomes partially observable - an interesting contrast to perfect information games such as Chess or Go. And this is a real-time strategy game - both players are playing simultaneously, so every decision needs to be computed quickly and efficiently.\nAn agent that can play StarCraft will need to demonstrate effective use of memory, an ability to plan over a long time, and the capacity to adapt plans based on new information. Computers are capable of extremely fast control, but that doesn\u2019t necessarily demonstrate intelligence, so agents must interact with the game within limits of human dexterity in terms of \u201cActions Per Minute\u201d. StarCraft\u2019s high-dimensional action space is quite different from those previously investigated in reinforcement learning research; to execute something as simple as \u201cexpand your base to some location\u201d, one must coordinate mouse clicks, camera, and available resources. \u00a0This makes actions and planning hierarchical, which is a challenging aspect of \nReinforcement Learning\n.\nWe\u2019re particularly pleased that the environment we\u2019ve worked with Blizzard to construct will be open and available to all researchers \u00a0next year. We recognise the efforts of the developers and researchers from the Brood War community in recent years, and hope that this new, modern and flexible environment - supported directly by the team at Blizzard - will be widely used to advance the state-of-the-art.\nWe\u2019ve worked closely with the StarCraft II team to develop an API that supports something similar to previous bots written with a \u201cscripted\u201d interface, allowing programmatic control of individual units and access to the full game state (with some new options as well). \u00a0Ultimately agents will play directly from pixels, so to get us there, we\u2019ve developed a new image-based interface that outputs a simplified low resolution RGB image data for map & minimap, and the option to break out features into separate \u201clayers\u201d, like terrain heightfield, unit type, unit health etc. Below is an example of what the feature layer API will look like.\nWe are also working with Blizzard to create \u201ccurriculum\u201d scenarios, which present increasingly complex tasks to allow researchers of any level to get an agent up and running, and benchmark different algorithms and advances. Researchers will also have full flexibility and control to create their own tasks using the existing StarCraft II editing tools.\nWe\u2019re really excited to see where our collaboration with Blizzard will take us. While we\u2019re still a long way from being able to challenge a professional human player at the game of StarCraft II, we hope that the work we have done with Blizzard will serve as a useful testing platform for the wider AI research community.\n"}
{"title": "DeepMind Papers @ NIPS (Part 1)", "contents": "Authors: \nPeter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Rezende, Koray Kavukcuoglu\nReasoning about objects, relations, and physics is central to human intelligence, and a key goal of artificial intelligence. However many modern machine learning methods still face a trade-off between expressive structure and efficient performance. \nWe introduce \u201cinteraction networks\u201d, which can reason about how objects in complex systems interact, supporting dynamical predictions, as well as inferences about the abstract properties of the system. Interaction networks are both expressive and efficient because they combine three powerful approaches: structured models, simulation, and deep learning. They take as input graph-structured data, perform object- and relation-centric reasoning in a way that is analogous to a simulation, and are implemented using deep neural networks. They are invariant to permutations of the entities and relations, which allows them to automatically generalize to systems of different sizes and structures than they have experienced during training.\nIn our experiments, we used interaction networks to implement the first general-purpose learnable physics engine. After training only on single step predictions, our model was able to simulate the physical trajectories of n-body, bouncing ball, and non-rigid string systems accurately over thousands of time steps. The same architecture was also able to infer underlying physical properties, such as potential energy. \nBeyond physical reasoning, interaction networks may provide a powerful framework for AI approaches to scene understanding, social perception, hierarchical planning, and analogical reasoning.\nFor further details and related work, please see \nthe paper\n. \nFor applications of interaction networks to scene understanding and imagination-based decision-making, please see our submissions to ICLR 2017: \nDiscovering objects and their relations from entangled scene representations\n and \nMetacontrol for Adaptive Imagination-Based Optimization\nCheck it out at NIPS:\n\u200d\nMon Dec 5th 06:00 - 09:30 PM @ Area 5+6+7+8 #48\nFri Dec 9th 08:00 - 6:30 PM @ Hilton Diag. Mar, Blrm. C\nAuthors: \nAlexander (Sasha) Vezhnevets, Volodymyr Mnih, Simon Osindero, Alex Graves, Oriol Vinyals, John Agapiou, Koray Kavukcuoglu\nLearning temporally extended actions and temporal abstraction in general is a long standing problem in reinforcement learning. They facilitate learning by enabling structured exploration and economic computation. In this paper we present a novel deep recurrent neural network architecture that learns to build implicit plans in an end-to-end manner purely by interacting with an environment in a reinforcement learning setting. The network builds an internal plan, which is continuously updated upon observation of the next input from the environment. It can also partition this internal representation into contiguous sub-sequences by learning for how long the plan can be committed to \u2013 i.e. followed without replanning. Combining these properties, the proposed model, dubbed STRategic Attentive Writer (STRAW) can learn high-level, temporally abstracted macro-actions of varying lengths that are solely learnt from data without any prior information.\nWatch the video \nhere\n.\nFor further details and related work, please see \nthe paper\n.\nCheck it out at NIPS:\n\u200d\nMon Dec 5th 06:00 - 09:30 PM @ Area 5+6+7+8 #111\nAuthors: \nOriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, Daan Wierstra\nGiven just a few, or even a single, examples of an unseen class, it is possible to attain high classification accuracy on ImageNet using Matching Networks. \u00a0The core architecture is simple and straightforward to train and performant across a range of image and text classification tasks.\nMatching Networks are trained in the same way as they are tested: by presenting a series of instantaneous one shot learning training tasks, where each instance of the training set is fed into the network in parallel. Matching Networks are then trained to classify correctly over many different input training sets. The effect is to train a network that can classify on a novel data set without the need for a single step of gradient descent.\nFor further details and related work, please see \nthe paper\n.\nCheck it out at NIPS:\n\u200d\nMon Dec 5th 06:00 - 09:30 PM @ Area 5+6+7+8 #139\nAuthors: \nRemi Munos, Tom Stepleton, Anna Harutyunyan, Marc G. Bellemare\nOur goal is to design a Reinforcement Learning (RL) algorithm with two desired properties. Firstly, to use off-policy data, which is important for exploration, when we use memory replay, or observe log-data. Secondly, to use multi-steps returns in order to propagate rewards faster and avoid accumulation of approximation/estimation errors. Both properties are crucial in deep RL. \nWe introduce the \u201cRetrace\u201d algorithm, which uses multi-steps returns and can safely and efficiently utilize any off-policy data. We show the convergence of this algorithm in both policy evaluation and optimal control settings.\nAs corollary we prove the convergence of Watkin\u2019s Q(\u03bb) to Q* (which was an open problem since 1989). \nFinally we report numerical results on the Atari domain that demonstrate the huge benefit of Retrace over competitive algorithms.\nFor further details and related work, please see \nthe paper\n.\nCheck it out at NIPS:\n\u200d\nMon Dec 5th 06:00 - 09:30 PM @ Area 5+6+7+8 #151\nAuthors: \nJean-Bastien Grill (INRIA), Michal Valko (INRIA), Remi Munos\nYou are a robot and you live in a Markov decision process (MDP) with a finite or an infinite number of transitions from state-action to next states. You got brains and so you plan before you act. Luckily, your roboparents equipped you with a generative model to do some Monte-Carlo planning. The world is waiting for you and you have no time to waste. You want your planning to be efficient. Sample-efficient. Indeed, you want to exploit the possible structure of the MDP by exploring only a subset of states reachable by following near-optimal policies. You want guarantees on sample complexity that depend on a measure of the quantity of near-optimal states. You want something, that is an extension of Monte-Carlo sampling (for estimating an expectation) to problems that alternate maximization (over actions) and expectation (over next states). You want something simple to implement and computationally efficient. You want it all and you want it now. You want TrailBlazer.\nFor further details and related work, please see \nthe paper\n.\nCheck it out at NIPS:\n\u200d\nTue Dec 6th 05:00 - 05:20 PM @ Area 3 (Oral) in Theory\nTue Dec 6th @ Area 5+6+7+8 #193\nAuthors:\n Ian Osband, Charles Blundell, Alex Pritzel and Benjamin Van Roy\nEfficient exploration in complex environments remains a major challenge for reinforcement learning (RL). We\u2019ve seen a lot of recent breakthroughs in RL, but many of these algorithms require huge amounts of data (millions of games) before they learn to make good decisions. In many real-world settings, such large amounts of data aren\u2019t feasible.\nOne of the reasons these algorithms learn so slowly is that they do not gather the *right* data to learn about the problem. These algorithms use dithering (taking random actions) to explore their environment - which can be exponentially less efficient that *deep* exploration which prioritizes potentially informative policies over multiple timesteps. There is a large literature on algorithms for deep exploration for statistically efficient reinforcement learning. The problem is that none of these algorithms are computationally tractable with deep learning\u2026 until now.\nKey breakthroughs in this paper include the following:\nFor further details and related work, please see \nthe paper\n and our video playlist \nhere\n.\nCheck it out at NIPS:\n\u200d\nMon Dec 5th 06:00 - 09:30 PM @ Area 5+6+7+8 #79\n"}
{"title": "Open-sourcing DeepMind Lab", "contents": "DeepMind's scientific mission is to push the boundaries of AI, developing systems that can learn to solve any complex problem without needing to be taught how. To achieve this, we work from the premise that AI needs to be general. Agents should operate across a wide range of tasks and be able to automatically adapt to changing circumstances. That is, they should not be pre-programmed, but rather, able to learn automatically from their raw inputs and reward signals from the environment. There are two parts to this research program: (1) \u00a0designing ever-more intelligent agents capable of more-and-more sophisticated cognitive skills, and (2) building increasingly complex environments where agents can be trained and evaluated.\nThe development of innovative agents goes hand in hand with the careful design and implementation of rationally selected, flexible and well-maintained environments. To that end, we at DeepMind have invested considerable effort toward building rich simulated environments to serve as \u00a0\u201claboratories\u201d for AI research. Now we are open-sourcing our flagship platform, \u00a0DeepMind Lab, so the broader research community can make use of it.\nDeepMind Lab is a fully 3D game-like platform tailored for agent-based AI research. It is observed from a first-person viewpoint, through the eyes of the simulated agent. Scenes are rendered with rich science fiction-style visuals. The available actions allow agents to look around and move in 3D. The agent\u2019s \u201cbody\u201d is a floating orb. It levitates and moves by activating thrusters opposite its desired direction of movement, and it has a camera that moves around the main sphere as a ball-in-socket joint tracking the rotational look actions. Example tasks include collecting fruit, navigating in mazes, traversing dangerous passages while avoiding falling off cliffs, bouncing through space using launch pads to move between platforms, playing laser tag, and quickly learning and remembering random procedurally generated environments. An illustration of how agents in DeepMind Lab perceive and interact with the world can be seen below:\nArtificial general intelligence research in DeepMind Lab emphasizes navigation, memory, 3D vision from a first person viewpoint, motor control, planning, strategy, time, and fully autonomous agents that must learn for themselves what tasks to perform by exploring their environment. All these factors make learning difficult. Each are considered frontier research questions in their own right. Putting them all together in one platform, as we have, represents a significant new challenge for the field.\nDeepMind Lab is highly customisable and extendable. New levels can be authored with off-the-shelf editor tools. In addition, DeepMind Lab includes an interface for programmatic level-creation. Levels can be customised with gameplay logic, item pickups, custom observations, level restarts, reward schemes, in-game messages and more. The interface can be used to create levels in which novel map layouts are generated on the fly while an agent trains. These features are useful in, for example, testing how an agent copes with unfamiliar environments. Users will be able to add custom levels to the platform via GitHub. The assets will be hosted on GitHub alongside all the code, maps and level scripts. Our hope is that the community will help us shape and develop the platform going forward.\nDeepMind Lab has been used internally at DeepMind for some time (\nexample\n). We believe it has already had a significant impact on our thinking concerning numerous aspects of intelligence, both natural and artificial. However, our efforts so far have only barely scratched the surface of what is possible in DeepMind Lab. There are opportunities for significant contributions still to be made in a number of mostly still untouched research domains now available through DeepMind Lab, such as navigation, memory and exploration.\nAs well as facilitating agent evaluation, there are compelling reasons to think that it may be fundamentally easier to develop intelligence in a 3D world, observed from a first-person viewpoint, like DeepMind Lab. After all, the only known examples of general-purpose intelligence in the natural world arose from a combination of evolution, development, and learning, grounded in physics and the sensory apparatus of animals. It is possible that a large fraction of animal and human intelligence is a direct consequence of the richness of our environment, and unlikely to arise without it. Consider the alternative: if you or I had grown up in a world that looked like Space Invaders or Pac-Man, it doesn\u2019t seem likely we would have achieved much general intelligence!\nRead the \nfull paper here\n\u200d\nAccess DeepMind's GitHub repository \nhere\n.\n"}
{"title": "Reinforcement learning with unsupervised auxiliary tasks", "contents": "Our primary mission at DeepMind is to push the boundaries of AI, developing programs that can learn to solve any complex problem without needing to be taught how. Our reinforcement learning agents have achieved \nbreakthroughs in Atari 2600 games\n and the \ngame of Go\n. Such systems, however, can require a lot of data and a long time to learn so we are always looking for ways to improve our generic learning algorithms.\nOur recent paper \n\u201cReinforcement Learning with Unsupervised Auxiliary Tasks\u201d\n introduces a method for greatly improving the learning speed and final performance of agents. We do this by augmenting the standard \ndeep reinforcement learning\n methods with two main additional tasks for our agents to perform during training.\nA visualisation of our agent in a Labyrinth maze foraging task can be seen below.\nThe first task involves the agent learning how to control the pixels on the screen, which emphasises learning how your actions affect what you will see rather than just prediction. This is similar to how a baby might learn to control their hands by moving them and observing the movements. By learning to change different parts of the screen, our agent learns features of the visual input that are useful for playing the game and getting higher scores.\nIn the second task the agent is trained to predict the onset of immediate rewards from a short historical context. In order to better deal with the scenario where rewards are rare we present the agent with past rewarding and non-rewarding histories in equal proportion. By learning on rewarding histories much more frequently, the agent can discover visual features predictive of reward much faster.\nThe combination of these auxiliary tasks, together with our previous \nA3C paper\n is our new UNREAL agent (UNsupervised REinforcement and Auxiliary Learning). We tested this agent on a suite of 57 Atari games as well as a 3D environment called Labyrinth with 13 levels. In all the games, the same UNREAL agent is trained in the same way, on the raw image output from the game, to produce actions to maximise the score or reward of the agent in the game. The behaviour required to get game rewards is incredibly varied, from picking up apples in 3D mazes to playing Space Invaders - the same UNREAL algorithm learns to play these games often to human level and beyond.\nIn Labyrinth, the result of using the auxiliary tasks - controlling the pixels on the screen and predicting when reward is going to occur - means that UNREAL is able to learn over 10x faster than our previous best A3C agent, and reaches far better performance. We can now achieve 87% of expert human performance averaged across the Labyrinth levels we considered, with super-human performance on a number of them. On Atari the agent now achieves on average 9x human performance. We hope that this work will allow us to scale up our agents to ever more complex environments.\nRead the paper: \nReinforcement Learning with Unsupervised Auxiliary Tasks\n"}
{"title": "Working with the NHS to build lifesaving technology", "contents": "We\u2019re very proud to announce a groundbreaking five year partnership with the Royal Free London NHS Foundation Trust.\nDoctors and nurses in the NHS do a phenomenal job caring for patients, but they\u2019re being badly let down by technology. Pagers, fax machines and paper records are still standard in most NHS hospitals, and too often top-down IT systems don\u2019t meet clinical needs because they are built far away from the frontline of patient care.\nThis slow and outdated technology means that important changes in a patient\u2019s condition often don\u2019t get brought to the attention of the right clinician in time to prevent further serious illness. When this doesn\u2019t happen, the consequences for patients can be severe, and even fatal. At least ten thousand people a year die in UK hospitals through entirely preventable causes, and some 40% of patients could avoid being admitted to intensive care, if the right clinician was able to take the right action sooner.\nOur partnership aims to change that, by taking a very different approach to building IT for patient care. Together we are creating world-leading technology, in close collaboration with clinicians themselves, to ensure that the right patient information gets to the right clinicians at the right time, reducing preventable deaths and illnesses. \nThe five year partnership will build on the successful year-long joint project to build a smartphone app called \nStreams\n, which alerts clinical teams as soon as test results show that a patient is at risk of developing acute kidney injury (AKI) , providing them with the necessary contextual clinical information to help them to provide the right treatment before the patient\u2019s condition deteriorates.\nFollowing prototype testing, as well as registration with the Medicines and Healthcare products Regulatory Agency (MHRA), this first version of Streams is ready to be deployed to clinicians across the Royal Free hospital sites early in 2017.\nOver the course of the next five years, we\u2019re going to expand Streams to cover other illness where early intervention is key and technology can ensure this happens. We think that Streams could also be used to help patients at risk from sepsis and other causes of organ failure, where signs of deterioration are often difficult for clinicians to spot, and where early intervention can be the difference between life and death. We also plan to build additional features that Royal Free clinicians have asked for, including messaging and clinical task management that will support better care.\nWhen it\u2019s fully built, we believe that this will speed up the time to alert nurses and doctors to patients in need down from hours to a few seconds. And by freeing up clinicians\u2019 time from juggling multiple pager, desktop-based and paper systems, it should redirect over half a million hours per year away from admin and towards direct patient care at the Royal Free alone.\nThe partnership will also introduce an unprecedented level of data security and audit. All data access is logged, and subject to review by the Royal Free as well as DeepMind Health\u2019s nine \nIndependent Reviewers\n. Our software and data centres will also undergo deep technical audits by experts commissioned by our Independent Reviewers.\nIn addition, we\u2019re developing an unprecedented new infrastructure that will enable ongoing audit by the Royal Free, allowing administrators to easily and continually verify exactly when, where, by whom and for what purpose patient information is accessed. This is being built by one of the world\u2019s leading security engineers, Ben Laurie, co-founder of the OpenSSL project which enables encrypted connections to websites around the world (familiar to millions through the padlock in their browser bars).\nAnd the infrastructure that powers Streams will be built on state-of-the-art interoperable standards, allowing the Royal Free to have other developers build new services that integrate more easily with their systems. This will dramatically reduce the barrier to entry for developers who want to build for the NHS, opening up a wave of innovation - including the potential for the first AI-enabled tools, whether developed by DeepMind or others.\nDeepMind was set up to help solve some of society\u2019s toughest challenges. It\u2019s hard to think of a better way for us to make a real difference in the world than creating technology that will transform the NHS. We\u2019ll update again on our progress with the Royal Free as soon as there\u2019s more news to share.\n"}
{"title": "DeepMind Papers @ NIPS (Part 2)", "contents": "The second blog post in this series, sharing brief descriptions of the papers we are presenting at NIPS 2016 Conference in Barcelona.\nAuthors: \nMarco Fraccaro, S\u00f8ren Kaae S\u00f8nderby, Ulrich Paquet, Ole Winther\nMuch of our reasoning about the world is sequential, from listening to sounds and voices and music, to imagining our steps to reach a destination, to tracking a tennis ball through time. All these sequences have some amount of latent random structure in them. Two powerful and complementary models, recurrent neural networks (RNNs) and stochastic state space models (SSMs), are widely used to model sequential data like these. RNNs are excellent at capturing longer-term dependencies in data, while SSMs model uncertainty in the sequence's underlying latent random structure, and are great for tracking and control.\nIs it possible to get the best of both worlds? In this paper we show how you can, by carefully layering deterministic (RNN) and stochastic (SSM) layers. We show how you can efficiently reason about a sequence\u2019s present latent structure, given its past (filtering) and also its past and future (smoothing).\nFor further details and related work, please see the paper \nhttps://arxiv.org/abs/1605.07571\nCheck it out at NIPS:\n\u200d\nTue Dec 6th 05:20 - 05:40 PM @ Area 1+2 (Oral) in Deep Learning\nTue Dec 6th 06:00 - 09:30 PM @ Area 5+6+7+8 #179\nAuthors: \nMarcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew Hoffman, David Pfau, Tom Schaul, Nando De Freitas\nOptimization algorithms today are typically designed by hand; algorithm designers, thinking carefully about each problem, are able to design algorithms that exploit structure that they can characterize precisely. \u00a0This design process mirrors the efforts of computer vision in the early 2000s to manually characterize and locate features like edges and corners in images with hand designed features. The biggest breakthrough of modern computer vision has been to instead learn these features directly from data, removing manual engineering from the loop. This paper shows how we can extend these techniques to algorithm design, learning not only features but also learning about the learning process itself.\nWe show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms outperform standard hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including neural network training, and styling images with neural art.\nFor further details and related work, please see the paper \nhttps://arxiv.org/abs/1606.04474\nCheck it out at NIPS:\n\u200d\nTue Dec 6th 06:00 - 09:30 PM @ Area 5+6+7+8 #9\nThursday Dec 8th 02:00 - 9:30 PM @ Area 1+2 (Deep Learning Symposium - Poster)\nFriday Dec 9th 08:00 AM - 06:30 PM @ Area 1 (DeepRL Workshop - Talk by Nando De Freitas)\nFriday Dec 9th 08:00 AM - 06:30 PM @ Area 5+6 (Nonconvex Optimization for Machine Learning: Theory and Practice - Talk by Nando De Freitas)\nSaturday Dec 10th 08:00 AM - 6:30 PM @ Area 2 (Optimizing the Optimizers - Talk by Matthew W. Hoffman)\nAuthors: \nNavdeep Jaitly, Quoc V. Le, Oriol Vinyals, Ilya Sutskever, David Sussillo, Samy Bengio\nModels which map from a sequence of observations to another sequence (sequence-to-sequence) have become extremely popular in the last two years due to their generality, achieving state-of-the-art results in a variety of tasks such as translation, captioning, or parsing. The main drawback of these models is that they need to read in the whole sequence of inputs \u201cx\u201d before starting producing the resulting output sequence \u201cy\u201d. In our paper we circumvent these limitations by allowing the model to emit output symbols before the whole input sequence has been read. Although this introduces some independence assumptions, making online decisions in certain domains such as speech recognition or machine translation makes these models much more desirable.\nFor further details and related work, please see the paper\nhttp://papers.nips.cc/paper/6594-an-online-sequence-to-sequence-model-using-partial-conditioning.pdf\nCheck it out at NIPS:\n\u200d\nTue Dec 6th 06:00 - 09:30 PM @ Area 5+6+7+8 #53\nAuthors: \nAudrunas Gruslys, Remi Munos, Ivo Danihelka, Marc Lanctot, Alex Graves\nMany state of art results were achieved by training large recurrent models over long sequences of input data. \u00a0Training recurrent networks is not an easy task for many reasons. One of complications is a large memory consumption of the standard backpropagation through time (BPTT) algorithm, as \u00a0it requires memorizing all or almost all past neuron activations. It is especially easy to run out of expensive GPU memory when training convolutional RNNs, and memory constraints often lead to unwanted compromises in network size. A common solution used to alleviate this problem is to memorize only some of intermediate neuron activations and recompute others on demand. While there were many heuristics that trade off memory and computation, most of them are adapted for certain edge cases and are suboptimal. We viewed the problem as a dynamic programming problem which allowed us to find a class of provably optimal strategies subject to memory constraints. For sequences of length 1000, our algorithm saves 95% of memory usage while using only one third more time per learning step than the standard BPTT.\nFor further details and related work, please see the paper \nhttps://papers.nips.cc/paper/6220-memory-efficient-backpropagation-through-time.pdf\nCheck it out at NIPS:\n\u200d\nTue Dec 6th 06:00 - 09:30 PM @ Area 5+6+7+8 #64\nAuthors: \nKarol Gregor, Frederic Besse, Danilo Rezende, Ivo Danihelka, Daan Wierstra\nDiscovering high level abstract representations is one of the primary goals of unsupervised learning. We approach this problem by designing an architecture that transforms the information stored in pixels into an ordered sequence of information carrying representations. Training results in an emergent order, where early representations carry information about the more global & conceptual aspects of the image, while the latter representations correspond to the details. The model is a fully convolutional, sequential variational autoencoder inspired by DRAW. The architecture is simple and homogeneous and therefore does not require many design choices.\nThe resulting information transformation can be used for lossy compression, by transmitting only the early set of representations (the number of which is given by the desired compression level) and generating the remaining ones as well as the image using the generative model. If the ordering of information that the model discovers correlates strongly with the ordering of information by importance as judged by humans, then the algorithm will transmit what humans consider to be the most important. If the generation of the remaining variables results in a high quality image, this method should lead to high quality lossy compression. Because both humans and unsupervised algorithms try to understand data and because both use deep networks to do so, there is a good reason to believe that this approach will work. We demonstrate that this is indeed the case and the current model already results in performance that compares favorably to that of JPEG and JPEG 2000. As generative models are progressively getting better, these results demonstrate the potential of this method for building future compression algorithms.\nFor further details and related work, please see the paper \nhttp://papers.nips.cc/paper/6542-towards-conceptual-compression.pdf\nCheck it out at NIPS:\n\u200d\nTue Dec 6th 06:00 - 09:30 PM @ Area 5+6+7+8 #77\nAuthors: \nDanilo Rezende, Ali Eslami, Shakir Mohamed, Peter Battaglia, Max Jaderberg, Nicolas Heess\nImagine looking at a photograph of a chair. The image you see will be a complex function of the attributes and positions of the camera, the lights and, of course of the shape of the chair. Importantly, due to self-occlusion you never see the full chair, so there is an infinite number chair-like objects that would be consistent with what you see. Nevertheless, when asked how to imagine the chair's shape from a different point of view you will probably be able to do so quite accurately. Key to this ability is not just an implicit understanding of perspective, occlusion and the image formation process, but critically your prior knowledge of what a plausible chair ought to look like, which allows you to \u201cfill in\u201d the missing parts.\nIn this paper we study models that are able to perform similar types of reasoning. Specifically, we formulate generative models which can learn about the statistical regularities of the three-dimensional shape of objects. The resulting prior over shapes produces high-quality samples, and allows us to formulate challenging ill-posed problems such as that of recovering plausible 3D structures given a 2D image as probabilistic inference, accurately capturing the multi-modality of the posterior. This inference can be achieved rapidly with a single forward-pass through a neural network and we show how both the models and inference networks can be trained end-to-end directly from 2D images without any use of ground-truth 3D labels, therefore demonstrating for the first time the feasibility of learning to infer 3D representations of the world in a purely unsupervised manner.\nFor further details and related work, please see the paper \nhttps://arxiv.org/abs/1607.00662\n and our video: \nhttps://www.youtube.com/watch?v=stvDAGQwL5c\nCheck it out at NIPS:\n\u200d\nWed Dec 7th 06:00 - 09:30 PM @ Area 5+6+7+8 #2\n"}
{"title": "DeepMind Papers @ NIPS (Part 3)", "contents": "Authors: \nJ Rae, JJ Hunt, T Harley, I Danihelka, A Senior, G Wayne, A Graves, T Lillicrap\nWe can recall vast numbers of memories, making connections between superficially unrelated events. As you read a novel, you\u2019ll likely remember quite precisely the last few things you\u2019ve read, but also plot summaries, connections and character traits from far back in the novel.\nMany machine learning models of memory, such as Long Short Term Memory, struggle at these sort of tasks. The computational cost of these models scales quadratically with the number of memories they can store so they are quite limited in how many memories they can have. More recently, memory augmented neural networks such as the Differentiable Neural Computer or Memory Networks, have shown promising results by adding memory separate from the computation and solving tasks such as reading short stories and answering questions [e.g. Babi].\nHowever, while these new architectures show promising results on small tasks, they use ``soft-attention\u2019\u2019 for accessing their memories, meaning that at every timestep they touch every word in memory. So while they can scale to short stories, they\u2019re a long way from reading novels.\nIn this work, we develop a set of techniques to use sparse approximations of such models to dramatically improve their scalability. In these sparse models only a tiny subset of the memory is touched at each timestep. Importantly, we show we can do this without harming the ability of the models to learn. This means that the sparse memory augmented neural networks are able to solve the same kind of tasks but require 1000s of times less resources, and look like a promising technique, with further refinement, for reading novels.\nFor further details and related work, please see the paper: \nhttps://arxiv.org/abs/1610.09027\nCheck it out at NIPS:\nWed Dec 7th 06:00 - 09:30 PM @ Area 5+6+7+8 #17\nAuthors: \nS. M. Ali Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David Szepesvari, Koray Kavukcuoglu, Geoffrey Hinton\nConsider the task of clearing a table after dinner. To plan your actions you will need to determine which objects are present, what classes they belong to and where each one is located on the table. In other words, for many interactions with the real world the perception problem goes far beyond just image classification. We would like to build intelligent systems that learn to parse the image of a scene into objects that are arranged in space, have visual and physical properties, and are in functional relationships with each other. And we would like to do so with as little supervision as possible.\nStarting from this notion our paper presents a framework for efficient inference in structured, generative image models that explicitly reason about objects. We achieve this by performing probabilistic inference using a recurrent neural network that attends to scene elements and processes them one at a time. Crucially, the model itself learns to choose the appropriate number of inference steps.\nWe use this scheme to learn to perform inference in partially specified 2D models (variable-sized variational auto-encoders) and fully specified 3D models (probabilistic renderers). We show that such models learn to identify multiple objects - counting, locating and classifying the elements of a scene - without any supervision, e.g., decomposing 3D images with various numbers of objects in a single forward pass of a neural network.\nFor further details and related work, please see the paper \nhttps://arxiv.org/abs/1603.08575\nCheck it out at NIPS:\n\u200d\nWed Dec 7th 06:00 - 09:30 PM @ Area 5+6+7+8 #2\nAuthors: \nMarc G. Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, Remi Munos\nWhile we've successfully trained agents to super-human performance on many Atari 2600 games, some games remain elusively difficult. One of our favourite \"hard\" games is Montezuma's Revenge. Montezuma's Revenge is famous for its hostile, unforgiving environment, where the agent must navigate a maze of rooms filled with traps. Each level has 24 rooms, is shaped like a pyramid, and looks like this:\nUntil now, most published agents failed to even make their way out of the first room.\nMany of these hard RL problems share one thing in common: rewards are few and far between. In reinforcement learning, exploration is the process by which an agent comes to understand its environment and discover where the reward is. Most practical RL applications still rely on crude algorithms, like epsilon-greedy (once in awhile, choose a random action), because more theoretically-motivated approaches don't scale. But epsilon-greedy is quite data inefficient, and often can't even get off the ground.\nIn this paper we show that it's possible to use simple density models (assigning probabilities to states) to \"count\" the number of times we've visited a particular state. We call the output of our algorithm a pseudo-count. Pseudo-counts give us a handle on uncertainty: how confident are we that we've explored this part of the game? As a result, we were able to progress significantly further in Montezuma's Revenge. The standard DQN algorithm gets less than 100 points per play, on average; in comparison, we get 3439. To give you a sense of the difference, compare the rooms visited by both methods (white = unexplored):\nAll in all, our agent navigates through 15 rooms, compared to DQN's two. See also the video of \nour agent playing Montezuma's Revenge\n.\nOur approach is inspired by White's 1959 idea of intrinsic motivation: that intelligent agents act first to understand their environment (See also the more recent work by Oudeyer; Barto; and Schmidhuber). What's exciting is that by playing to satisfy their curiosity, rather than to immediately win, our agents eventually come to surpass their peers.\nFor further details and related work, please see \nthe paper\n.\nCheck it out at NIPS:\n\u200d\nWednesday Dec 7th, 6PM \u2014 9:30PM @ Area 5+6+7+8 Poster #71\nAuthors: \nH van Hasselt, A Guez, M Hessel, V Mnih, D Silver\nSometimes we want to learn a function for which we don\u2019t know the scale beforehand, or where the scale can change over time. \u00a0For instance, this happens in value-based reinforcement learning when our policy improves over time. \u00a0Initially, values might be small because our policy is not yet great, but later they increase repeatedly and unpredictably. \u00a0This is a problem for many (deep) learning algorithms, because they were often not developed with such cases in mind and can then be slow or unstable.\nA concrete motivation is that the \nDQN\n algorithm successfully learned to play many Atari games, but clipped all non-zero rewards to -1 and 1. \u00a0This makes learning easier, because it changes the behaviour. \u00a0For instance, eating a ghost (actual reward 100+) in Ms. Pac-Man then seems to give the same reward as eating a pellet (actual reward 10).\nWe propose to instead adaptively normalize the targets we present to the deep neural network. To get a feel for the effectiveness of this method, we can look at the resulting magnitude (of the l2-norm) of the gradients during learning across 57 different Atari games:\nDouble DQN\n is shown with unclipped rewards on the left, with the clipped rewards in the middle, and with Pop-Art is on the right. \u00a0Pop-Art results in much more consistent gradients, whose magnitudes fall into a much narrower, and therefore more predictable, range. The unclipped version is much more erratic \u2013 note the log scale on the y-axis. \u00a0Pop-Art even has better-normalized gradients than the clipped variant, without qualitatively changing the task as the clipping does. \u00a0For some games, the resulting performance is much better than previous state of the art.\nPop-Art is not specific to DQN, Atari, or reinforcement learning. \u00a0It can be useful whenever a function must be learned with unknown magnitude, or where the scale changes over time. \u00a0Additionally, it can be useful when learning about multiple signals at the same time, for instance when these signals have different units and/or modalities. \u00a0Normalizing per output can then help disentangle the magnitude from the importance of a signal.\nFor further details and related work, please see the paper \nhere\n and an accompanying video \nhere\n.\nCheck it out at NIPS:\n\u200d\nWednesday, December 7th, 6PM \u2014 9:30PM @ Area 5+6+7+8 #81\n"}
{"title": "Bringing the best of mobile technology to Imperial College Healthcare NHS Trust", "contents": "We\u2019re really excited to announce that we\u2019ve agreed a five year partnership with Imperial College Healthcare NHS Trust, helping them make the most of the opportunity for mobile clinical applications to improve care. This is now our second NHS partnership for clinical apps, following a similar \npartnership\nwe announced last month with the Royal Free London NHS Foundation Trust.\nOver the last two years, the Trust has moved from paper to electronic patient records, and mobile technology is the natural next stage of this work. By giving clinicians access to cutting-edge healthcare apps that link to electronic patient records, they\u2019ll be able to access information on the move, react quickly in response to changing patient needs, and ultimately provide even better care.\nWe\u2019ll be working with the Trust to deploy our clinical app, \nStreams\n, which supports clinicians in caring for patients at risk of deterioration, particularly with conditions where early intervention can make all the difference. Like breaking news alerts on a mobile phone, the technology will notify nurses and doctors immediately when test results show a patient is at risk of becoming seriously ill. It will also enable clinicians at the Trust to securely assign and communicate about clinical tasks, and give them the information they need to make diagnoses and decisions.\nThis partnership builds on a relationship between the Trust, Imperial College, and DeepMind. The task management features in Streams are underpinned by a world class programme of widely published research and early product development carried out at Imperial College London and the Trust, as part of an app called Hark co-founded by Lord Ara Darzi. Using simulated data, Hark was found to improve the quality of transfer of information between staff and was rated by users as more effective and efficient, and less distracting than pagers [\nwww.jmir.org/2016/4/e79/\n]. Hark became part of DeepMind in early 2016.\nAs in our partnership with the Royal Free, we\u2019re also implementing state-of-the-art open and interoperable standards, via what\u2019s known as a FHIR API. This will allow the Trust to easily, securely and consistently integrate other apps that could improve care, whether developed by third parties or innovators within the Trust. Our partners at Imperial are excited about the potential for a wide range of apps to improve care, and we\u2019re delighted to be working with them to make this possible.\nOur hope is that the infrastructure and apps we\u2019re building not only help improve care in the short term, but also make it much easier for Trusts to bring new innovations to the clinical frontlines in future.\n"}
{"title": "DeepMind\u2019s work in 2016: a round-up", "contents": "In a world of fiercely complex, emergent, and hard-to-master systems - from our climate to the diseases we strive to conquer - we believe that intelligent programs will help unearth new scientific knowledge that we can use for social benefit. To achieve this, we believe we\u2019ll need general-purpose learning systems that are capable of developing their own understanding of a problem from scratch, and of using this to identify patterns and breakthroughs that we might otherwise miss. This is the focus of our long-term research mission at DeepMind.\nWhile we remain a long way from anything that approximates what you or we would term intelligence, 2016 was a big year in which we made exciting progress on a number of the core underlying challenges, and saw the first glimpses of the potential for positive real-world impact.\nOur program \nAlphaGo\n, for which we were lucky enough to receive our \nsecond Nature front cover\n, took on and beat the world champion Lee Sedol at the ancient game of Go, a feat that many experts said came a decade ahead of its time. Most exciting for us - as well as for the worldwide Go community - were AlphaGo\u2019s displays of game-winning creativity, in some cases finding moves that challenged millennia of Go wisdom. In its ability to identify and share new insights about one of the most contemplated games of all time, AlphaGo offers a promising sign of the value AI may one day provide, and we're looking forward to playing more games in 2017.\nWe also made meaningful progress in the field of generative models, building programs able to imagine new constructs and scenarios for themselves. Following our \nPixelCNN\n paper on image generation, our paper on \nWaveNet\n demonstrated the usefulness of generative audio, achieving the world\u2019s most life-like speech synthesis by imaginatively creating raw waveforms rather than stitching together samples of recorded language. We\u2019re planning to put this into production with Google and are excited about enabling improvements to products used by millions of people.\nAnother important area of research is memory, and specifically the challenge of combining the decision-making aptitude of neural networks with the ability to store and reason about complex, structured data. Our work on \nDifferentiable Neural Computers\n, for which we received \nour third Nature paper\n in eighteen months, demonstrated models that can simultaneously learn like neural networks as well as memorise data like computers. These models are already able to learn how to answer questions about data structures from family trees to tube maps, and bring us closer to the goal of using AI for scientific discovery in complex datasets.\nAs well as pushing the boundaries of \nwhat\n these systems can do, we\u2019ve also invested significant time in improving how they learn. A paper titled \u2018\nReinforcement Learning with Unsupervised Auxiliary Tasks\n\u2019 described methods to improve the speed of learning for certain tasks by an order of magnitude. And given the importance of high-quality training environments for agents, we open sourced \nour flagship DeepMind Lab research environment\n for the community, and are \nworking with Blizzard\n to develop AI-ready training environments for StarCraft II as well.\nOf course, this is just the tip of the iceberg, and you can read much more about our work in \nthe many papers\n we published this year in top-tier journals from Neuron to PNAS and at major machine learning conferences from ICLR to NIPS. It\u2019s amazing to see how others in the community are already actively implementing and building on the work in these papers - just look at the remarkable renaissance of Go-playing computer programs in the latter part of 2016! - and to witness the broader fields of AI and machine learning go from strength to strength.\nIt\u2019s equally amazing to see the first early signs of real-world impact from this work. Our partnership with Google\u2019s data centre team used AlphaGo-like techniques to \ndiscover creative new methods of managing cooling\n, leading to a remarkable 15% improvement in the buildings\u2019 energy efficiency. If it proves possible to scale these kinds of techniques up to other large-scale industrial systems, there's real potential for significant global environmental and cost benefits. This is just one example of the work we\u2019re doing with \nvarious teams at Google\n to apply our cutting-edge research to products and infrastructure used across the world. We\u2019re also actively engaged in \nmachine learning research partnerships\n with two NHS hospital groups in the UK, our home, to explore how our techniques could enable more efficient diagnosis and treatment of conditions that affect millions worldwide, as well as working with two further hospital groups on \nmobile apps and foundational infrastructure\n to enable improved care on the clinical frontlines.\nOf course, the positive social impact of technology isn\u2019t only about the real-world problems we seek to solve, but also about the way in which algorithms and models are designed, trained and deployed in general. We\u2019re proud to \nhave been involved\n in founding the \nPartnership on AI\n, which will bring together leading research labs with non-profits, civil society groups and academics to develop best practices in areas such as algorithmic transparency and safety. By fostering a diversity of experience and insight, we hope that we can help address some of these challenges and find ways to put social purpose at the heart of the AI community across the world.\nWe\u2019re still a young company early in our mission, but if in 2017 we can make further simultaneous progress on these three fronts - algorithmic breakthroughs, social impact, and ethical best practice - then we'll be in good shape to make a meaningful continued contribution to the scientific community and to the world beyond.\n"}
{"title": "Our collaborations with academia to advance the field of AI", "contents": "When I was studying in the mid-90s as an undergraduate, there was very little active engagement between the academic communities pushing the boundaries of maths and science, and the industries that many students ended up going into, such as finance. This struck me as a missed opportunity. While private institutions benefited from the technological advances being driven by university researchers, the subsequent breakthroughs they made were rarely shared for mutual benefit between the two.\nIn contrast, we often talk about DeepMind\u2019s research environment as a hybrid culture that blends the long-term scientific thinking of academia with the speed and focus of the best start-ups. This alignment with academia has always been important to us personally, given how many of our team come from that background, as well as the fact that many of the core ideas behind machine learning were invented and developed by academic pioneers including the likes of Geoff Hinton and Rich Sutton.\nThis is a major reason why we openly publish our research - including over \n100 peer-reviewed papers\nto date - and regularly present at industry-wide gatherings such as \nNIPS\n. Last month in Barcelona we published 20 papers, participated in 42 poster sessions, gave 21 talks, and \u00a0open-sourced our flagship \nDeepMind Lab\n research platform - and there\u2019s a lot more to come.\nWe also want to make a more direct contribution to academic learning and training the next generation of machine learning practitioners, and so, starting this month, we\u2019ll be running a state-of-the-art Masters level training module called \nAdvanced Topics in Machine Learning\n with University College London\u2019s (UCL) Department of Computer Science. Led by DeepMind\u2019s Thore Graepel, other invited speakers will include leading researchers spanning areas such as deep learning, reinforcement learning, natural language understanding and others. Hado van Hasselt, Joseph Modayil, Koray Kavukcuoglu, Raia Hadsell, James Martens, Oriol Vinyals, Shakir Mohamed, Simon Osindero, Ed Grefenstette and Karen Simonyan will be joined by Volodymyr Mnih, David Silver and Alex Graves - who are also some of the first authors of DeepMind\u2019s three Nature papers.\nJanuary also sees the start of our \nDeep Learning for Natural Language Processing\n advanced course at the University of Oxford\u2019s Department of Computer Science. This applied course, focusing on recent advances in analysing and generating speech and text using recurrent neural networks, is led by Phil Blunsom in partnership with DeepMind\u2019s Language Research Group, and open to fourth year undergraduates, Masters, and first year DPhil (PhD) students. Both of these courses run in addition to the international summer schools that our team members regularly teach at, with events taking place this year in \nGermany\n, \nChina\n and \nSouth Africa\n among other locations.\nWe also make sure that people who come to work here can continue to make their own personal contribution to academia. A number of our team are also affiliated with various institutions including UCL, Oxford, Cambridge, MIT and the universities of Freiburg and Lille, among others.\nFinally, we think it\u2019s important for the field that there are as many thriving independent academic institutions as possible. That\u2019s why we\u2019re providing sponsorship for several research labs and their PhD students to pursue their own research priorities in whichever way they choose, including the University of Alberta, University of Montreal, University of Amsterdam, Gatsby Unit at UCL, NYU and Oxford, and others.\nWe see the links between company research labs and academia as central to the future of AI. By continuing to share talent, expertise and breakthroughs - not just on technical subjects, but also on the broader set of questions around ethics, safety and societal impact - we believe we\u2019ll all make better progress in the development of artificial intelligence and its application for positive social benefit.\n"}
{"title": "Understanding Agent Cooperation", "contents": "We employ deep multi-agent reinforcement learning to model the emergence of cooperation. The new notion of sequential social dilemmas allows us to model how rational agents interact, and arrive at more or less cooperative behaviours depending on the nature of the environment and the agents\u2019 cognitive capacity. The research may enable us to better understand and control the behaviour of complex multi-agent systems such as the economy, traffic, and environmental challenges.\nSelf-interested people often work together to achieve great things. Why should this be the case, when it is in their best interest to just care about their own wellbeing and disregard that of others?\nThe question of how and under what circumstances selfish agents cooperate is one of the fundamental question in the social sciences. One of the simplest and most elegant models to describe this phenomenon is the well-known game of \nPrisoner\u2019s Dilemma\n from game theory.\nTwo suspects are arrested and put into solitary confinement. Without confessions the police do not have sufficient evidence to convict the two suspects on the main charge, but have good prospects to achieve one year prison sentences for both. In order to entice the prisoners to confess, they offer them simultaneously the following deal: If you testify against the other prisoner (\u201cdefect\u201d) you will be released, but the other prisoner will serve three years in prison. If both prisoners testify against each other (\u201cdefect\u201d), they will both serve two years.\nIt turns out, that \nrational agents\n - in the sense of game theory - should always defect in this game, because no matter what the other prisoner chooses to do, they will be better off defecting. Yet, paradoxically, if both prisoners reason in this way, they will each have to serve two years in prison - one year more than if they had cooperated and remained silent. This paradox is what we refer to as a \nsocial dilemma\n.\nRecent progress in artificial intelligence and specifically deep reinforcement learning provides us with the tools to look at the problem of social dilemmas through a new lens. Traditional game theorists model social dilemmas in terms of a simple binary choice between \ncooperate\n and \ndefect\n for each agent. In real life, both cooperating and defecting may require complex behaviours, involving difficult sequences of actions that agents need to learn to execute. We refer to this new setting as \nsequential social dilemmas\n, and use artificial agents trained by deep multi-agent reinforcement learning to study it.\nAs an example, consider the following Gathering game: Two agents, Red and Blue, roam a shared world and collect apples to receive positive rewards. They may also direct a beam at the other agent, \u201ctagging them\u201d, to temporarily remove them from the game, but this action does not trigger a reward. A visualisation of agents playing the gathering game can be seen below.\nWe let the agents play this game many thousands of times and let them learn how to behave \nrationally\nusing deep multi-agent reinforcement learning. Rather naturally, when there are enough apples in the environment, the agents learn to peacefully coexist and collect as many apples as they can. However, as the number of apples is reduced, the agents learn that it may be better for them to tag the other agent to give themselves time on their own to collect the scarce apples.\nIt turns out that this Gathering game shares many characteristics of the original Prisoner\u2019s Dilemma, but allows us to study the more interesting case in which agents need to learn to implement their desired behaviour: Either to cooperate and collect apples, or to defect and try to tag the other agent.\nIn these sequential social dilemmas, we can now study what factors contribute to agents\u2019 cooperation. For example, the following plot shows that in the Gathering game greater scarcity of apples leads to more \u201ctagging\u201d behaviour of agents. Furthermore, agents with the capacity to implement more complex strategies try to tag the other agent more frequently, i.e. behave less cooperatively - no matter how we vary the scarcity of apples.\nInterestingly, in another game called Wolfpack (see gameplay video below), which requires close coordination to successfully cooperate, we find that greater capacity to implement complex strategies leads to more cooperation between agents, the opposite of the finding with Gathering. So, depending on the situation, having a greater capacity to implement complex strategies may yield either more or less cooperation. The new framework of sequential social dilemmas allows us to take into account not only the outcome of the interaction (as in the Prisoner\u2019s dilemma), but also the difficulty of learning to implement a given strategy.\nIn summary, we showed that we can apply the modern AI technique of deep multi-agent reinforcement learning to age-old questions in social science such as the mystery of the emergence of cooperation. We can think of the trained AI agents as an approximation to economics\u2019 rational agent model \n\u201chomo economicus\u201d\n. Hence, such models give us the unique ability to test policies and interventions into simulated systems of interacting agents - both human and artificial.\nAs a consequence, we may be able to better understand and control complex multi-agent systems such as the economy, traffic systems, or the ecological health of our planet - all of which depend on our continued cooperation.\nRead the paper: \nMulti-agent Reinforcement Learning in Sequential Social Dilemmas\n"}
{"title": "A milestone for DeepMind Health and Streams", "contents": "In November we announced a \ngroundbreaking five year partnership\n with the Royal Free London to deploy and expand on \nStreams\n, our secure clinical app that aims to improve care by getting the right information to the right clinician at the right time.\nThe first version of Streams has now been deployed at the Royal Free and we\u2019re delighted that the early feedback from nurses, doctors and patients has so far been really positive. Some of the nurses using Streams at the hospital estimate \u00a0that the app is saving them up to two hours per day, giving them more time to spend with patients in need. And we\u2019re starting to hear the first stories of patients whose conditions were picked up and acted on faster thanks to Streams alerts. Patients like \nAfia Ahmed\n, who was seen more quickly thanks to the instant alerts. \u00a0You can read more about the deployment and some of the early positive signs \nover on the Royal Free\u2019s website\n.\nOf course, these are only early indicators, and a full service evaluation will be carried out over the coming months to measure the overall impact that Streams is having. But for all of us who have worked on this for the past eighteen months, from concept to development to testing to deployment, it\u2019s an exciting milestone and an indication of what can be achieved when the UK\u2019s clinicians, patients and technologists work together.\nWe\u2019ve always believed in the combination of cutting-edge technology with a focus on real practicality. If we want to maximise the clinical and social benefits of advanced health technology, then it can\u2019t be developed in labs disconnected from the hospital frontlines - it has to be informed and guided by amazing people who give and receive care every single day.\nThis focus on practical benefit will continue to shape our work as we expand the use of Streams, carry out \nmachine learning research\n into conditions that affect millions of people, and support our hospital partners to provide faster and safer care.\n"}
{"title": "Trust, confidence and Verifiable Data Audit", "contents": "Data can be a powerful force for social progress, helping our most important institutions to improve how they serve their communities. As cities, hospitals, and transport systems find new ways to understand what people need from them, they\u2019re unearthing opportunities to change how they work today and identifying exciting ideas for the future. \nData can only benefit society if it has society\u2019s trust and confidence, and here we all face a challenge. Now that you can use data for so many more purposes, people aren\u2019t just asking about who\u2019s holding information and whether it\u2019s being kept securely \u2013 they also want greater assurances about precisely what is being done with it. \nIn that context, auditability becomes an increasingly important virtue. Any well-built digital tool will already log how it uses data, and be able to show and justify those logs if challenged. But the more powerful and secure we can make that audit process, the easier it becomes to establish real confidence about how data is being used in practice. \nImagine a service that could give mathematical assurance about what is happening with each individual piece of personal data, without possibility of falsification or omission. Imagine the ability for the inner workings of that system to be checked in real-time, to ensure that data is only being used as it should be. Imagine that the infrastructure powering this was freely available as open source, so any organisation in the world could implement their own version if they wanted to. \nThe working title for this project is \u201cVerifiable Data Audit\u201d, and we\u2019re really excited to share more details about what we\u2019re planning to build!\nOver the course of this year we'll be starting to build out Verifiable Data Audit for \nDeepMind Health\n, our effort to provide the health service with technology that can help clinicians predict, diagnose and prevent serious illnesses \u2013 a key part of DeepMind\u2019s mission to deploy technology for social benefit.\nGiven the sensitivity of health data, we\u2019ve always believed that we should aim to be as innovative with governance as we are with the technology itself. We\u2019ve already invited additional oversight of DeepMind Health by appointing a panel of unpaid \nIndependent Reviewers\n who are charged with scrutinising our healthcare work, commissioning audits, and publishing an annual report with their findings.\nWe see Verifiable Data Audit as a powerful complement to this scrutiny, giving our partner hospitals an additional real-time and fully proven mechanism to check how we\u2019re processing data. We think this approach will be particularly useful in health, given the sensitivity of personal medical data and the need for each interaction with data to be appropriately authorised and consistent with rules around patient consent. For example, an organisation holding health data can\u2019t simply decide to start carrying out research on patient records being used to provide care, or repurpose a research dataset for some other unapproved use. In other words: it\u2019s not just where the data is stored, it\u2019s what\u2019s being done with it that counts. We want to make that verifiable and auditable, in real-time, for the first time.\nSo, how will it work? We serve our hospital partners as a data processor, meaning that our role is to provide secure data services under their instructions, with the hospital remaining in full control throughout. Right now, any time our systems receive or touch that data, we create a log of that interaction that can be audited later if needed.\nWith Verifiable Data Audit, we\u2019ll build on this further. Each time there\u2019s any interaction with data, we\u2019ll begin to add an entry to a special digital ledger. That entry will record the fact that a particular piece of data has been used, and also the reason why - for example, that blood test data was checked against the NHS national algorithm to detect possible acute kidney injury.\nThe ledger and the entries within it will share some of the properties of \nblockchain\n, which is the idea behind Bitcoin and other projects. Like blockchain, the ledger will be append-only, so once a record of data use is added, it can\u2019t later be erased. And like blockchain, the ledger will make it possible for third parties to verify that nobody has tampered with any of the entries.\nBut it\u2019ll also differ from blockchain in a few important ways. Blockchain is decentralised, and so the verification of any ledger is decided by consensus amongst a wide set of participants. To prevent abuse, most blockchains require participants to repeatedly carry out complex calculations, with huge associated costs (according to some estimates, the total energy usage of blockchain participants could be as much as the \npower consumption of Cyprus\n). This isn\u2019t necessary when it comes to the health service, because we already have trusted institutions like hospitals or national bodies who can be relied on to verify the integrity of ledgers, avoiding some of the wastefulness of blockchain.\nWe can also make this more efficient by replacing the chain part of blockchain, and using a tree-like structure instead (if you\u2019d like to understand more about Merkle trees, a good place to start would be \nthis blog\n from the UK\u2019s Government Digital Service). The overall effect is much the same. Every time we add an entry to the ledger, we\u2019ll generate a value known as a \u201ccryptographic hash\u201d. This hash process is special because it summarises not only the latest entry, but all of the previous values in the ledger too. This makes it effectively impossible for someone to go back and quietly alter one of the entries, since that will not only change the hash value of that entry but also that of the whole tree.\nIn simple terms, you can think of it as a bit like the last move of a game of Jenga. You might try to gently take or move one of the pieces - but due to the overall structure, that\u2019s going to end up making a big noise!\nSo, now we have an improved version of the humble audit log: a fully trustworthy, efficient ledger that we know captures all interactions with data, and which can be validated by a reputable third party in the healthcare community. What do we do with that?\nThe short answer is: massively improve the way in which these records can be audited. We\u2019ll build a dedicated online interface that authorised staff at our partner hospitals can use to examine the audit trail of DeepMind Health\u2019s data use in real-time. It will allow continuous verification that our systems are working as they should, and enable our partners to easily query the ledger to check for particular types of data use. We\u2019d also like to enable our partners to run automated queries, effectively setting alarms that would be triggered if anything unusual took place. And, in time, we could even give our partners the option of allowing others to check our data processing, such as individual patients or patient groups.\nBuilding this is going to be a major undertaking, but given the importance of the issue we think it\u2019s worth it. Right now, three big technical challenges stand out.\nNo blind spots.\n For this to be provably trustworthy, it can\u2019t be possible for data use to take place without being logged in the ledger - otherwise, the concept falls apart. As well as designing the logs to record the time, nature and purpose of any interaction with data, we\u2019d also like to be able to prove that there\u2019s no other software secretly interacting with data in the background. As well as logging every single data interaction in our ledger, we will also need to use \nformal methods\n as well as code and data centre audits by experts, to prove that every data access by every piece of software in the data centre is captured by these logs. We\u2019re also interested in efforts to guarantee the trustworthiness of the hardware on which these systems run - an active topic of computer science research!\nDifferent uses for different groups.\n The core implementation will be an interface to allow our partner hospitals to provably check in real-time that we\u2019re only using patient data for approved purposes. If these partners wanted to extend that ability to others, like patients or patient groups, there would be complex design questions to resolve.\nA long list of log entries may not be useful to many patients, and some may prefer to read a consolidated view or rely on a trusted intermediary instead. Equally, a patient group may not have the authority to see identified data, which would mean allowing our partners to provide some form of system-wide information - for example, whether machine learning algorithms have been run on particular datasets - without unintentionally revealing patient data.\nFor technical details on how we could provide verified access to subsets or summaries of the data, see the open source \nTrillian\n project, which we will be using, and this \npaper explaining how it works\n.\nDecentralised data and logs, without gaps.\n There\u2019s no single patient identified information database in the UK, and so the process of care involves data travelling back and forth between healthcare providers, IT systems, and even patient-controlled services like wearable devices. There\u2019s a lot of work going into making these systems interoperable (our mobile product, \nStreams, is built to interoperable standards\n) so they can work safely together. It would be helpful for these standards to include auditability as well, to avoid gaps where data becomes unauditable as it passes from one system to another.\nThis doesn\u2019t mean that a data processor like DeepMind should see data or audit logs from other systems. Logs should remain decentralised, just like the data itself. Audit interoperability would simply provide additional reassurance that this data can\u2019t be tampered with as it travels between systems.\nThis is a significant technical challenge, but we think it should be possible. Specifically, there\u2019s an emerging open standard for interoperability in healthcare called FHIR, which could be extended to include auditability in useful ways.\nWe\u2019re hoping to be able to implement the first pieces of this later this year, and are planning to blog about our progress and the challenges we encounter as we go. We recognise this is really hard, and the toughest challenges are by no means the technical ones. We hope that by sharing our process and documenting our pitfalls openly, we\u2019ll be able to partner with and get feedback from as many people as possible, and increase the chances of this kind of infrastructure being used more widely one day, within healthcare and maybe even beyond.\n\u200d\n"}
{"title": "Deep Reinforcement Learning", "contents": "Humans excel at solving a wide variety of challenging problems, from low-level motor control through to high-level cognitive tasks. Our goal at DeepMind is to create artificial agents that can achieve a similar level of performance and generality. Like a human, our agents learn for themselves to achieve successful strategies that lead to the greatest long-term rewards. This paradigm of learning by trial-and-error, solely from rewards or punishments, is known as \nreinforcement learning\n (RL). Also like a human, our agents construct and learn their own knowledge directly from raw inputs, such as vision, without any hand-engineered features or domain heuristics. This is achieved by \ndeep learning\n of neural networks. At DeepMind we have pioneered the combination of these approaches - deep reinforcement learning - to create the first artificial agents to achieve human-level performance across many challenging domains.\nOur agents must continually make value judgements so as to select good actions over bad. This knowledge is represented by a Q-network that estimates the total reward that an agent can expect to receive after taking a particular action. Two years ago we introduced the first widely successful \nalgorithm\n for deep reinforcement learning. The key idea was to use deep neural networks to represent the Q-network, and to train this Q-network to predict total reward. Previous attempts to combine RL with neural networks had largely failed due to unstable learning. To address these instabilities, our Deep Q-Networks (DQN) algorithm stores all of the agent's experiences and then randomly samples and replays these experiences to provide diverse and decorrelated training data. We applied DQN to learn to play games on the Atari 2600 console. At each time-step the agent observes the raw pixels on the screen, a reward signal corresponding to the game score, and selects a joystick direction. In our \nNature paper\n we trained separate DQN agents for 50 different Atari games, without any prior knowledge of the game rules.\nAmazingly, DQN achieved human-level performance in almost half of the 50 games to which it was applied; far beyond any previous method. The \nDQN source code\n and \nAtari 2600 emulator\n are freely available to anyone who wishes to experiment for themselves.\nWe have subsequently improved the DQN algorithm in many ways: further stabilising the \nlearning\ndynamics\n; prioritising the \nreplayed experiences\n; \nnormalising\n, \naggregating\n and \nre-scaling\n the outputs. Combining several of these improvements together led to a 300% improvement in mean score across Atari games; human-level performance has now been achieved in almost all of the Atari games. We can even train a \nsingle neural network\n to learn about \nmultiple Atari games\n. We have also built a massively distributed deep RL system, known as \nGorila\n, that utilises the Google Cloud platform to speed up training time by an order of magnitude; this system has been applied to recommender systems within Google.\nHowever, deep Q-networks are only one way to solve the deep RL problem. We recently introduced an even more practical and effective method based on asynchronous RL. This approach exploits the multithreading capabilities of standard CPUs. The idea is to execute many instances of our agent in parallel, but using a shared model. This provides a viable alternative to experience replay, since parallelisation also diversifies and decorrelates the data. Our asynchronous actor-critic algorithm, \nA3C\n, combines a deep Q-network with a deep policy network for selecting actions. It achieves state-of-the-art results, using a fraction of the training time of DQN and a fraction of the resource consumption of Gorila. By building novel approaches to \nintrinsic motivation\n and \ntemporally abstract planning\n, we have also achieved breakthrough results in the most notoriously challenging Atari games, such as Montezuma\u2019s Revenge.\nWhile Atari games demonstrate a wide degree of diversity, they are limited to 2D sprite-based video games. We have recently introduced Labyrinth: a challenging suite of 3D navigation and puzzle-solving environments. Again, the agent only observes pixel-based inputs from its immediate field-of-view, and must figure out the map to discover and exploit rewards.\nAmazingly, the A3C algorithm achieves human-level performance, out-of-the-box, on many Labyrinth tasks. An \nalternative approach\n based on episodic memory has also proven successful. Labyrinth will also be released open source in the coming months.\nWe have also developed a number of deep RL methods for continuous control problems such as robotic manipulation and locomotion. Our Deterministic Policy Gradients algorithm (\nDPG\n) provides a continuous analogue to DQN, exploiting the differentiability of the Q-network to solve a \nwide\n \nvariety\n of continuous control tasks. \nAsynchronous RL\n also performs well in these domains and, when augmented with a hierarchical control strategy, can solve challenging problems such as ant soccer and a 54-dimensional humanoid slalom, without any prior knowledge of the dynamics.\nThe game of Go is the most challenging of classic games. Despite decades of effort, prior methods had only achieved amateur level performance. We developed a deep RL algorithm that learns both a value network (which predicts the winner) and a policy network (which selects actions) through games of self-play. Our program AlphaGo combined these deep neural networks with a state-of-the-art tree search. In October 2015, AlphaGo became \nthe first program to defeat a professional human player\n. In March 2016, AlphaGo \ndefeated Lee Sedol\n (the strongest player of the last decade with an incredible 18 world titles) by 4 games to 1, in a match that was watched by an estimated 200 million viewers.\nSeparately, we have also developed \ngame theoretic\n approaches to \ndeep RL\n, culminating in a \nsuper-human\n poker player for heads-up limit Texas Hold\u2019em.\nFrom Atari to Labyrinth, from locomotion through manipulation, to poker and even the game of Go, our deep reinforcement learning agents have demonstrated remarkable progress on a wide variety of challenging tasks. Our goal is to continue to improve the capabilities of our agents, and to use them to make a positive impact on society, in important applications such as \nhealthcare\n.\n"}
{"title": "DeepMind AI Reduces Google Data Centre Cooling Bill by 40%", "contents": "From smartphone assistants to image recognition and translation, machine learning already helps us in our everyday lives. But it can also help us to tackle some of the world\u2019s most challenging physical problems - such as energy consumption. \u00a0Large-scale commercial and industrial systems like data centres consume a lot of energy, and while much has been done to \nstem the growth of energy use\n, there remains a lot more to do given the world\u2019s increasing need for computing power.\nReducing energy usage has been a major focus for us over the past \u00a010 years: we have built our own \nsuper-efficient servers\n at Google, invented \nmore efficient ways to cool our data centres\n and invested heavily in \ngreen energy sources\n, with the goal of being powered 100 percent by renewable energy. Compared to five years ago, we now get around 3.5 times the computing power out of the same amount of energy, and we continue to make many improvements each year.\nMajor breakthroughs, however, are few and far between - which is why we are excited to share that by applying DeepMind\u2019s machine learning to our own Google data centres, we\u2019ve managed to reduce the amount of energy we use for cooling by up to 40 percent. In any large scale energy-consuming environment, this would be a huge improvement. Given how sophisticated Google\u2019s data centres are already, it\u2019s a phenomenal step forward.\nThe implications are significant for Google\u2019s data centres, given its potential to greatly improve energy efficiency and reduce emissions overall. This will also help other companies who run on Google\u2019s cloud to \nimprove their own energy efficiency\n. While Google is only one of many data centre operators in the world, many are not powered by renewable energy as we are. Every improvement in data centre efficiency reduces total emissions into our environment and with technology like DeepMind\u2019s, we can use machine learning to consume less energy and help address one of the biggest challenges of all - climate change.\nOne of the primary sources of energy use in the data centre environment is cooling. Just as your laptop generates a lot of heat, our data centres - which contain servers powering Google Search, Gmail, YouTube, etc. - also generate a lot of heat that must be removed to keep the servers running. This cooling is typically accomplished via large industrial equipment such as pumps, chillers and cooling towers. However, dynamic environments like data centres make it difficult to operate optimally for several reasons:\nTo address this problem, we began applying \nmachine learning\n two years ago to operate our data centres more efficiently. And over the past few months, DeepMind researchers began working with Google\u2019s data centre team to significantly improve the system\u2019s utility. Using a system of neural networks trained on different operating scenarios and parameters within our data centres, we created a more efficient and adaptive framework to understand data centre dynamics and optimize efficiency.\nWe accomplished this by taking the historical data that had already been collected by thousands of sensors within the data centre - data such as temperatures, power, pump speeds, setpoints, etc. - and using it to train an ensemble of deep neural networks. Since our objective was to improve data centre energy efficiency, we trained the neural networks on the average future PUE (Power Usage Effectiveness), which is defined as the ratio of the total building energy usage to the IT energy usage. We then trained two additional ensembles of deep neural networks to predict the future temperature and pressure of the data centre over the next hour. The purpose of these predictions is to simulate the recommended actions from the PUE model, to ensure that we do not go beyond any operating constraints.\nWe tested our model by deploying on a live data centre. The graph below shows a typical day of testing, including when we turned the machine learning recommendations on, and when we turned them off.\nOur machine learning system was able to consistently achieve a 40 percent reduction in the amount of energy used for cooling, which equates to a 15 percent reduction in overall PUE overhead after accounting for electrical losses and other non-cooling inefficiencies. It also produced the lowest PUE the site had ever seen.\nBecause the algorithm is a general-purpose framework to understand complex dynamics, we plan to apply this to other challenges in the data centre environment and beyond in the coming months. Possible applications of this technology include improving power plant conversion efficiency (getting more energy from the same unit of input), reducing semiconductor manufacturing energy and water usage, or helping manufacturing facilities increase throughput.\nWe are planning to roll out this system more broadly and will share how we did it in an upcoming publication, so that other data centre and industrial system operators - and ultimately the environment - can benefit from this major step forward.\n"}
{"title": "Announcing DeepMind Health research partnership with Moorfields Eye Hospital", "contents": "We founded DeepMind to make the world a better place by developing technologies that help address some of society's toughest challenges.\nSo we\u2019re excited to announce our first medical research project with an NHS Trust.\nWe\u2019ll be working with \nMoorfields Eye Hospital NHS Foundation Trust\n, one of the world\u2019s leading eye hospitals with a 200 year track record in clinical care, research and education.\nThis collaboration came about when Pearse Keane, a consultant ophthalmologist at Moorfields, contacted DeepMind to explore how we could work together on two specific conditions that cause sight loss: diabetic retinopathy and age-related macular degeneration (AMD). Together, these affect more than 625,000 people in the UK and over 100 million people worldwide.\nDiabetes is on the rise. It\u2019s estimated that \n1 in 11\n of the world\u2019s adult population are affected. It\u2019s also the leading cause of blindness in the working age population - if you\u2019re diabetic you are 25 times more likely to suffer some kind of sight loss. Early detection and treatment can prevent 98% of severe visual loss resulting from diabetes - but that doesn\u2019t always happen.\nAge-related Macular Degeneration (AMD) is the commonest cause of blindness in the UK. Every single day - in the UK alone - \nnearly 200 people lose sight\n from the severe, blinding form of this condition and globally the number of people with AMD is set to rise to nearly 200m by 2020. By allowing earlier detection and treatment of AMD, machine learning has the potential to help save the sight of many of these people.\nAt the moment, eye care professionals use digital scans of the fundus (the back of the eye) and scans called optical coherence tomography (OCT) to diagnose and determine the correct treatment for these serious eye conditions. These scans are highly complex and take a long time for eye health professionals to analyse, which can have an impact on how quickly they can meet patients to discuss diagnosis and treatment. And to date, traditional computer analysis tools have been unable to explore them fully.\nOur research project aims to investigate how machine learning could help analyse these scans efficiently and effectively, leading to earlier detection and intervention for patients and reducing the number of cases of patient deterioration.\nThe set of one million anonymised eye scans and some related anonymous information about eye condition and disease management, which Moorfields will share with us for the research, has been collected over time through routine care. This means it\u2019s not possible to identify any individual patients from the scans. And they\u2019re also historic scans, meaning that while the results of our research may be used to improve future care, they won\u2019t affect the care any patient receives today.\nWe\u2019re proud to be contributing to the many thousands of medical research efforts underway at any given time. As is standard practice in such projects, we never own the data - the NHS does. And we\u2019re bound by clear rules covering what we can do with it, which are distinct to (though equally strict as) the rules that govern our direct patient care work with the Royal Free Hospital.\nMore information about the project can be found on our \nHealth Research page\n. We have submitted our research protocol for open peer review and we\u2019ll also submit any results from this research to peer-reviewed journals, as is normal, so others in the medical community can analyse them.\nIt\u2019s early days for this work, but we\u2019re optimistic about the long-term potential for machine learning technology to help eye health professionals diagnose and treat other diseases that, like macular degeneration, affect the lives of millions of people across the world. It\u2019s a hugely exciting opportunity to make a difference to the NHS and its patients, and we\u2019ll keep you updated as we continue on this journey.\nEarly detection and treatment can prevent 98% of severe visual loss resulting from diabetes. Access Economics (2009) Future Sight Loss UK 1: Economic Impact of Partial Sight and Blindness in the UK adult population. \nRNIB 2009\n\u200d\nPeople with diabetes are 25 times more likely to go blind. International Diabetes Federation Europe 2011.\n"}
{"title": "Enabling Continual Learning in Neural Networks", "contents": "Computer programs that learn to perform tasks also typically forget them very quickly. We show that the learning rule can be modified so that a program can remember old tasks when learning a new one. This is an important step towards more intelligent programs that are able to learn progressively and adaptively.\nDeep neural networks are currently the most successful machine learning technique for solving a variety of tasks including language translation, image classification and image generation. However, they have typically been designed to learn multiple tasks only if the data is presented all at once. As a network trains on a particular task its parameters are adapted to solve the task. When a new task is introduced, \u00a0new adaptations overwrite the knowledge that the neural network had previously acquired. This phenomenon is known in cognitive science as \u2018catastrophic forgetting\u2019, and is considered one of the fundamental limitations of neural networks.\nBy contrast, our brains work in a very different way. We are able to learn incrementally, acquiring skills one at a time and applying our previous knowledge when learning new tasks. As a starting point for our recent \nPNAS paper\n, in which we propose an approach to overcome catastrophic forgetting in neural networks, we took inspiration from neuroscience-based theories about the consolidation of previously acquired skills and memories in mammalian and human brains.\nNeuroscientists have distinguished two kinds of consolidation that occur in the brain: systems consolidation and synaptic consolidation. Systems consolidation is the process by which memories that have been acquired by the quick-learning parts of our brain are imprinted into the slow-learning parts. This imprinting is known to be mediated by conscious and unconscious recall - for instance, this can happen during dreaming. In the second mechanism, synaptic consolidation, connections between neurons are less likely to be overwritten if they have been important in previously learnt tasks. Our algorithm specifically takes inspiration from this mechanism to address the problem of catastrophic forgetting.\nA neural network consists of several connections in much the same way as a brain. After learning a task, we compute how important each connection is to that task. When we learn a new task, each connection is protected from modification by an amount proportional to its importance to the old tasks. Thus it is possible to learn the new task without overwriting what has been learnt in the previous task and without incurring a significant computational cost. In mathematical terms, we can think of the protection we attach to each connection in a new task as being linked to the old protection value by a spring, whose stiffness is proportional to the connection\u2019s importance. For this reason, we called our algorithm Elastic Weight Consolidation (EWC).\nTo test our algorithm, we exposed an agent to Atari games sequentially. Learning an individual game from the score alone is a challenging task, but learning multiple games sequentially is even more challenging as each game requires an individual strategy. \u00a0As shown in the figure below, without EWC, the agent quickly forgets each game after it stops playing it (blue). This means that on average, the agent barely learns a single game. However, if we use EWC (brown and red), the agent does not forget as easily and can learn to play several games, one after the other.\nToday, computer programs cannot learn from data adaptively and in real time. However, we have shown that catastrophic forgetting is not an insurmountable challenge for neural networks. We hope that this research represents a step towards programs that can learn in a more flexible and efficient way.\nOur research also progresses our understanding of how consolidation happens in the human brain. The neuroscientific theories that our work is based on, in fact, have mainly been proven in very simple examples. By showing that those same theories can be applied in a more realistic and complex machine learning context, we hope to give further weight to the idea that synaptic consolidation is key to retaining memories and know-how.\nRead the paper:\u00a0\nOvercoming catastrophic forgetting in neural networks\n"}
{"title": "We are very excited to announce the launch of DeepMind Health", "contents": "We founded DeepMind to solve intelligence and use it to make the world a better place by developing technologies that help address some of society's toughest challenges. It was clear to us that we should focus on healthcare because it\u2019s an area where we believe we can make a real difference to people\u2019s lives across the world.\nWe're starting in the UK, where the National Health Service is hugely important to our team. The NHS helped bring many of us into the world, and has looked after our loved ones when they've most needed help. We want to see the NHS thrive, and to ensure that its talented clinicians get the tools and support they need to continue providing world-class care.\nFrontline nurses, doctors and other healthcare professionals who spend their days treating patients know better than anyone what's needed to provide outstanding care. We at DeepMind Health aim to support clinicians by providing the technical expertise needed to build and scale technologies that help them provide the best possible care to their patients.\nWhile projects like Hark and AKI detection are in their early stages, the problems they solve are fundamental to the NHS. The hope is that these tools can help shift more resources away from reaction and towards better prevention. Ultimately the aim is to give nurses and doctors more time to focus on what\u2019s most important.\nThese past few months have given us a glimpse of what\u2019s possible. As we continue to explore what nurses and doctors need, and work with them to design and scale new and better tools, we will remain guided by the following principles:\nThe world\u2019s toughest problems become more tractable when diverse teams of leading practitioners work together in partnership. Building world-class technologies that support clinicians is one of the most important things we can do, and DeepMind Health is our promise to do just that.\n"}
